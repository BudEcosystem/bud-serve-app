Project Path: bud-serve-app

Source Tree:

```
bud-serve-app
├── requirements-lint.txt
├── LICENSE
├── requirements.txt
├── .env.sample
├── deploy
│   ├── docker-compose-redis.yaml
│   ├── start_dev.sh
│   ├── Dockerfile
│   ├── docker-compose-dev.yaml
│   ├── docker-compose-postgres.yaml
│   └── stop_dev.sh
├── requirements-test.txt
├── pyproject.toml
├── tests
│   ├── test_redis.py
│   ├── conftest.py
│   ├── test_update_credential.py
│   ├── test_pubsub.py
│   ├── test_cluster_metrics.py
│   └── test_cluster_metrics_fetcher.py
├── docs
│   └── workflow.md
├── README.md
├── setup.py
├── requirements-dev.txt
├── static
│   └── icons
│       ├── general
│       │   ├── model_mono.png
│       │   ├── key.png
│       │   ├── deployment_mono.png
│       │   ├── cluster_mono.png
│       │   └── memoji.png
│       ├── providers
│       │   ├── deepseek.png
│       │   ├── together_ai.png
│       │   ├── sambanova.png
│       │   ├── friendliai.png
│       │   ├── databricks.png
│       │   ├── openai.png
│       │   ├── groq.png
│       │   ├── ai21.png
│       │   ├── mistral_ai_api.png
│       │   ├── huggingface.png
│       │   ├── github.png
│       │   ├── ibm_watsonx.ai.png
│       │   ├── nlp_cloud.png
│       │   ├── baseten.png
│       │   ├── fireworks_ai.png
│       │   ├── vertexai.png
│       │   ├── clarifai.png
│       │   ├── petals.png
│       │   ├── cerebras.png
│       │   ├── cloudflare_workers_ai.png
│       │   ├── litellm_proxy.png
│       │   ├── cohere.png
│       │   ├── replicate.png
│       │   ├── predibase.png
│       │   ├── deepinfra.png
│       │   ├── openrouter.png
│       │   ├── anthropic.png
│       │   ├── volcano_engine.png
│       │   ├── aws_bedrock.png
│       │   ├── anyscale.png
│       │   ├── voyage_ai.png
│       │   ├── aws_sagemaker.png
│       │   ├── azure_ai_studio.png
│       │   ├── palm_api_google.png
│       │   ├── gemini_google_ai_studio.png
│       │   ├── aleph_alpha.png
│       │   └── perplexity_ai.png
│       └── template
│           ├── code_translation.png
│           ├── question_answering.png
│           ├── summarization.png
│           ├── code_gen.png
│           ├── rag.png
│           ├── other.png
│           ├── sentiment_analysis.png
│           ├── chat.png
│           ├── entity_extraction.png
│           └── document_analysis.png
├── scripts
│   ├── del_configs.sh
│   ├── update_configs.sh
│   └── install_hooks.sh
├── budapp
│   ├── migrations
│   │   ├── script.py.mako
│   │   ├── env.py
│   │   ├── versions
│   │   │   ├── 40a8d774f38f_modified_field_in_model.py
│   │   │   ├── 1a20ca43ac69_removed_register_from_cluster.py
│   │   │   ├── d92d92c1c90c_removed_palm_from_proprietary_.py
│   │   │   ├── 64e788fd946c_updated_endpoint_table.py
│   │   │   ├── e3ac5a852b2c_added_model_configuration_to_endpoint_.py
│   │   │   ├── af5cc669f51e_updated_model_table.py
│   │   │   ├── 61d0ff8cf1cd_added_model_scan_result_table.py
│   │   │   ├── ffb55b8f01fc_modified_model_table.py
│   │   │   ├── 5dc1d24e7b7e_model_table_update.py
│   │   │   ├── 8611a4c07457_added_model_table.py
│   │   │   ├── 5352d4677f70_added_status_to_security_scan_result.py
│   │   │   ├── 7420487c5f37_add_model_licneses_table.py
│   │   │   ├── f040fc25b476_updated_model_licence_paper_tables.py
│   │   │   ├── f759834112e2_removed_model_type.py
│   │   │   ├── 1d94eee5d3f2_added_user_permission_model.py
│   │   │   ├── 6bd72384f8ba_update_endpoint_table.py
│   │   │   ├── 68f026ad7360_updated_endpoint_status.py
│   │   │   ├── 04ef4a274216_added_status_to_project_table.py
│   │   │   ├── 73ed9f8ea677_added_workflow_step.py
│   │   │   ├── d337aa91475f_added_icon_model.py
│   │   │   ├── dac83b8cf76f_added_cluster_project_endpoint_table.py
│   │   │   ├── 4d5cd5449049_cluster_status_update.py
│   │   │   ├── cbbdd257bedd_added_status_to_model_and_cloud_model_.py
│   │   │   ├── 6f3f373ce6b2_added_deploy_config_in_endpoint.py
│   │   │   ├── bcd1218837d9_modified_endpoint_table.py
│   │   │   ├── 197143f29114_updated_workflow_table.py
│   │   │   ├── 42e713425e99_updated_model_table.py
│   │   │   ├── f91763325460_add_paper_published_table.py
│   │   │   ├── e2d7e5f38d3c_token_model_added.py
│   │   │   ├── 036fd90d4d6b_added_template_model.py
│   │   │   ├── da9cd25f9dcc_removed_is_active_from_user_table.py
│   │   │   ├── 919ebf0fe343_added_proprietary_credential_table.py
│   │   │   ├── b2a743972235_updated_model_table.py
│   │   │   ├── 97795f1e0340_added_cloud_model.py
│   │   │   ├── 55602d4240bb_modified_model_template.py
│   │   │   ├── e30d6bcba1dc_added_new_workflow_type.py
│   │   │   ├── 892c31d1e086_removed_is_active_from_cluster_table.py
│   │   │   ├── e7ae8abbf8f3_added_provider_table.py
│   │   │   ├── 375eb22cb3af_updated_endpoint_table.py
│   │   │   ├── 48acf0c963d2_added_active_replicas_to_endpoint_table.py
│   │   │   ├── 2ce2068d3257_updated_cluster_table.py
│   │   │   ├── d5d05bae8c2c_added_icon_to_cluster_table.py
│   │   │   ├── b15331009e3f_remove_replica_from_endpoint_table.py
│   │   │   ├── 87708bcf2070_added_status_in_security_scan_table.py
│   │   │   ├── 0c7fdc4b7072_removed_is_active_from_endpint_table.py
│   │   │   ├── 1b33efcd5611_tag_added_in_worflow.py
│   │   │   ├── b2c9ac9ba65b_modified_workflow_table.py
│   │   │   └── acb041e05c53_updated_cloud_model.py
│   │   └── README
│   ├── cluster_ops
│   │   ├── services.py
│   │   ├── models.py
│   │   ├── __init__.py
│   │   ├── schemas.py
│   │   ├── utils.py
│   │   ├── cluster_routes.py
│   │   └── crud.py
│   ├── core
│   │   ├── services.py
│   │   ├── models.py
│   │   ├── meta_routes.py
│   │   ├── common_routes.py
│   │   ├── __init__.py
│   │   ├── schemas.py
│   │   ├── notify_routes.py
│   │   └── crud.py
│   ├── auth
│   │   ├── services.py
│   │   ├── token.py
│   │   ├── models.py
│   │   ├── auth_routes.py
│   │   ├── schemas.py
│   │   ├── __inti__.py
│   │   └── crud.py
│   ├── commons
│   │   ├── logging.py
│   │   ├── config.py
│   │   ├── async_utils.py
│   │   ├── validators.py
│   │   ├── database.py
│   │   ├── security.py
│   │   ├── constants.py
│   │   ├── __init__.py
│   │   ├── types.py
│   │   ├── schemas.py
│   │   ├── exceptions.py
│   │   ├── db_utils.py
│   │   ├── helpers.py
│   │   ├── api_utils.py
│   │   ├── dependencies.py
│   │   └── resiliency.py
│   ├── alembic.ini
│   ├── model_ops
│   │   ├── services.py
│   │   ├── models.py
│   │   ├── __init__.py
│   │   ├── schemas.py
│   │   ├── model_routes.py
│   │   └── crud.py
│   ├── user_ops
│   │   ├── services.py
│   │   ├── models.py
│   │   ├── user_routes.py
│   │   ├── __init__.py
│   │   ├── schemas.py
│   │   └── crud.py
│   ├── credential_ops
│   │   ├── models.py
│   │   ├── __init__.py
│   │   ├── schemas.py
│   │   ├── credential_routes.py
│   │   └── crud.py
│   ├── __init__.py
│   ├── shared
│   │   ├── http_client.py
│   │   ├── notification_service.py
│   │   ├── __init__.py
│   │   ├── dapr_service.py
│   │   ├── singleton.py
│   │   └── redis_service.py
│   ├── metric_ops
│   │   ├── services.py
│   │   ├── __init__.py
│   │   ├── schemas.py
│   │   └── metric_routes.py
│   ├── permissions
│   │   ├── models.py
│   │   ├── __init__.py
│   │   └── crud.py
│   ├── endpoint_ops
│   │   ├── services.py
│   │   ├── models.py
│   │   ├── __init__.py
│   │   ├── endpoint_routes.py
│   │   ├── schemas.py
│   │   └── crud.py
│   ├── __about__.py
│   ├── project_ops
│   │   ├── services.py
│   │   ├── models.py
│   │   ├── __init__.py
│   │   ├── schemas.py
│   │   ├── project_routes.py
│   │   └── crud.py
│   ├── py.typed
│   ├── main.py
│   ├── workflow_ops
│   │   ├── services.py
│   │   ├── models.py
│   │   ├── workflow_routes.py
│   │   ├── schemas.py
│   │   └── crud.py
│   └── initializers
│       ├── provider_seeder.py
│       ├── user_seeder.py
│       ├── __init__.py
│       ├── icon_seeder.py
│       ├── template_seeder.py
│       ├── seeder.py
│       ├── base_seeder.py
│       ├── cloud_model_seeder.py
│       └── data
│           ├── providers_seeder.json
│           ├── template_seeder.json
│           └── cloud_model_seeder.json
└── output.txt

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/LICENSE`:

```
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/.env.sample`:

```sample
APP_NAME=budapp
NAMESPACE=development
LOG_LEVEL=DEBUG
CONFIGSTORE_NAME=configstore
SECRETSTORE_NAME=secretstore-local
APP_PORT=9081
DAPR_HTTP_PORT=3510
DAPR_GRPC_PORT=50001
DAPR_API_TOKEN=

SECRETS_REDIS_URI=budapp-redis:6379
SECRETS_REDIS_PASSWORD=

# Postgres
POSTGRES_USER=
POSTGRES_PASSWORD=
POSTGRES_DB=budserve
POSTGRES_PORT=5432
POSTGRES_HOST=budapp-postgres

# PGAdmin
PGADMIN_DEFAULT_EMAIL=
PGADMIN_DEFAULT_PASSWORD=
PGADMIN_PORT=8888

# Admin user
SUPER_USER_EMAIL=admin@bud.com
SUPER_USER_PASSWORD=

# Token
ACCESS_TOKEN_EXPIRE_MINUTES=30
REFRESH_TOKEN_EXPIRE_MINUTES=10080
# To create a new secret key string, run: openssl rand -hex 32
JWT_SECRET_KEY=

# Security
PASSWORD_SALT=bud_password_salt

# CORS
CORS_ORIGINS=http://localhost,https://localhost

# Dapr
DAPR_BASE_URL=http://host.docker.internal:3005
BUD_CLUSTER_APP_ID=budcluster
BUD_MODEL_APP_ID=budmodel
BUD_METRICS_APP_ID=budmetrics
BUD_NOTIFY_APP_ID=notify
BUD_SIMULATOR_APP_ID=budsim

# Directory
STATIC_DIR=

# Budserve host
BUD_SERVE_HOST=

# Metrics
PROMETHEUS_URL=https://metrics.fmops.in

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/deploy/start_dev.sh`:

```sh
#!/usr/bin/env bash

#
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------
#

DAPR_COMPONENTS="../.dapr/components/"
DAPR_APP_CONFIG="../.dapr/appconfig-dev.yaml"

DOCKER_COMPOSE_FILE="./deploy/docker-compose-dev.yaml"
BUILD_FLAG=""
DETACH_FLAG=""

function display_help() {
    echo "Usage: $0 [options]"
    echo
    echo "Options:"
    echo "  --dapr-components        Set the dapr components folder path, this should be relative to the deploy directory (default: $DAPR_COMPONENTS)"
    echo "  --dapr-app-config        Set the dapr app config path, this should be relative to the deploy directory (default: $DAPR_APP_CONFIG)"
    echo "  -f FILE                  Specify the Docker Compose file to use, this should be relative to your current directory (default: $DOCKER_COMPOSE_FILE)"
    echo "  --build                  Include this flag to force a rebuild of the Docker containers"
    echo "  -d                       Include this flag to detach and run the containers in background"
    echo "  --help                   Display this help message and exit"
    echo
    echo "Example:"
    echo "  $0 -f docker-compose-local.yaml --build"
    echo "  This will use 'docker-compose-local.yaml' and force a rebuild of the containers."
    echo
    exit 0
}

# Parse arguments
while [[ "$#" -gt 0 ]]; do
    case $1 in
        --dapr-components) DAPR_COMPONENTS="$2"; shift ;;
        --dapr-app-config) DAPR_APP_CONFIG="$2"; shift ;;
        -f) DOCKER_COMPOSE_FILE="$2"; shift ;;
        --build) BUILD_FLAG="--build" ;;
        -d) DETACH_FLAG="-d" ;;
        --help) display_help ;;
        *) echo "Unknown parameter passed: $1"; show_help; exit 1 ;;
    esac
    shift
done

set -a
source ./.env
set +a

export REDIS_PORT=$(echo "${SECRETS_REDIS_URI:-redis:6379}" | cut -d':' -f2)

: ${APP_NAME:?Application name is required, use APP_NAME in env to specify the name.}

# Print the environment variables
echo "****************************************************"
echo "*                                                  *"
echo "*         Starting Microservice Environment        *"
echo "*                                                  *"
echo "****************************************************"
echo ""
echo "🛠 App Name            : $APP_NAME"
echo "🌐 App Port             : $APP_PORT"
echo "🔑 Redis Uri           : $SECRETS_REDIS_URI"
echo "🌍 Dapr HTTP Port      : $DAPR_HTTP_PORT"
echo "🌍 Dapr gRPC Port      : $DAPR_GRPC_PORT"
echo "🛠 Namespace            : $NAMESPACE"
echo "📊 Log Level           : $LOG_LEVEL"
echo "🗂 Config Store Name    : $CONFIGSTORE_NAME"
echo "🔐 Secret Store Name   : $SECRETSTORE_NAME"
echo "🛠 Dapr Components     : $DAPR_COMPONENTS"
echo "🛠 Dapr App Config     : $DAPR_APP_CONFIG"
echo "🛠 Docker Compose File : $DOCKER_COMPOSE_FILE"
echo "🚀 Build flag          : $BUILD_FLAG"
echo ""
echo "****************************************************"

# Bring up Docker Compose
echo "Bringing up Docker Compose with file: $DOCKER_COMPOSE_FILE"
docker compose -f "$DOCKER_COMPOSE_FILE" up $BUILD_FLAG $DETACH_FLAG
```

`/Users/rahulvramesh/bud-v2/bud-serve-app/deploy/Dockerfile`:

```
FROM python:3.11.0

RUN mkdir /app/
COPY requirements.txt /app/

WORKDIR /app
RUN pip install --upgrade pip
RUN pip install --no-cache-dir -r ./requirements.txt

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/deploy/stop_dev.sh`:

```sh
#!/bin/bash

#
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------
#

# Default value for Docker Compose file
DOCKER_COMPOSE_FILE="./deploy/docker-compose-dev.yaml"

# Function to display help message
function display_help() {
    echo "Usage: $0 [options]"
    echo
    echo "Options:"
    echo "  -f FILE   Specify the Docker Compose file to use (default: deploy/docker-compose-dev.yaml)"
    echo "  --help                Display this help message and exit"
    echo
    echo "Example:"
    echo "  $0 -f docker-compose-local.yaml"
    echo "  This will stop the services using 'docker-compose-local.yaml'."
    echo
    exit 0
}

# Parse optional arguments
while [[ "$#" -gt 0 ]]; do
    case $1 in
        -f) DOCKER_COMPOSE_FILE="$2"; shift ;;
        --help) display_help ;;
        *) echo "Unknown parameter: $1"; exit 1 ;;
    esac
    shift
done

# Set dummy env to skip warnings & errors
export APP_NAME=dummy
export REDIS_PASSWORD=dummy
export APP_PORT=9081
export DAPR_HTTP_PORT=3510
export DAPR_GRPC_PORT=50001
export DAPR_API_TOKEN=
export DEPLOYMENT_ENV=
export LOG_LEVEL=
export CONFIGSTORE_NAME=
export SECRETSTORE_NAME=

# Stop Docker Compose services
echo "Stopping services defined in: $DOCKER_COMPOSE_FILE"
docker compose -f "$DOCKER_COMPOSE_FILE" stop

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/pyproject.toml`:

```toml
[build-system]
requires = ["setuptools >= 61.0"]
build-backend = "setuptools.build_meta"

[tool.ruff]
include = ["pyproject.toml", "*.py"]
extend-include = ["*.ipynb"]
# Exclude a variety of commonly ignored directories.
exclude = [
    ".bzr", ".direnv", ".eggs", ".git", ".git-rewrite", ".hg", ".ipynb_checkpoints", ".mypy_cache",
    ".nox", ".pants.d", ".pyenv", ".pytest_cache", ".pytype", ".ruff_cache", ".svn", ".tox", ".venv",
    ".vscode", ".idea", "__pypackages__", "_build", "buck-out", "build", "dist", "node_modules",
    "site-packages", "venv", "tests"
]
line-length = 119
indent-width = 4
target-version = "py38"

[tool.ruff.lint]
# Never enforce `E501` (line length violations).
# Relax the convention by _not_ requiring documentation for every function parameter.
ignore = ["C901", "E501", "E741", "F402", "F823", "D417", "D101", "D100"]
# Pycodestyle (E), Pyflakes (F), flake8-bugbear (B), flake8-simplify (SIM), isort (I), pydocstyle (D)
select = ["C", "E", "F", "B", "SIM", "I", "W", "D", "D401"]
# Avoid trying to fix flake8-bugbear (`B`) violations.
unfixable = ["B"]

# Ignore `E402` (import violations) in all `__init__.py` files, and in select subdirectories.
[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["E402"]
"**/{tests,docs,tools}/*" = ["E402"]

[tool.ruff.lint.isort]
lines-after-imports = 2
known-first-party = ["budapp"]

[tool.ruff.format]
# Like Black, use double quotes for strings.
quote-style = "double"
# Like Black, indent with spaces, rather than tabs.
indent-style = "space"
# Like Black, respect magic trailing commas.
skip-magic-trailing-comma = false
# Like Black, automatically detect the appropriate line ending.
line-ending = "auto"
# Enable auto-formatting of code examples in docstrings. Markdown,
# reStructuredText code/literal blocks and doctests are all supported.
docstring-code-format = true
# Set the line length limit used when formatting code snippets in
# docstrings.
docstring-code-line-length = "dynamic"

[tool.ruff.lint.pydocstyle]
convention = "google"

[tool.mypy]
plugins = [
    "pydantic.mypy"
]
python_version = "3.8"

follow_imports = "silent"
warn_redundant_casts = true
warn_unused_ignores = true
warn_return_any = true
warn_unused_configs = true
disallow_any_generics = true
check_untyped_defs = true
no_implicit_reexport = true
ignore_missing_imports = true
disallow_untyped_defs = true

[tool.pytest.ini_options]
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "session"
```

`/Users/rahulvramesh/bud-v2/bud-serve-app/tests/test_redis.py`:

```py
from budapp.shared.redis_service import RedisService
import asyncio
from threading import Thread

# test event loop
print("Event loop test started")
asyncio.run(RedisService().set("test", "test"))
asyncio.run(RedisService().set("test1", "test"))
asyncio.run(RedisService().get("test"))
asyncio.run(RedisService().keys("test*"))
asyncio.run(RedisService().delete("test", "test1"))
print("RedisService event loop test completed")

# test async function
async def async_main():
    """Async function to test RedisService."""
    print("Async function started")
    redis_service = RedisService()
    await redis_service.set("test", "test")
    await redis_service.set("test1", "test")
    keys = await redis_service.keys("test*")
    await redis_service.delete(*keys)
    print("Async function completed")
asyncio.run(async_main())
print("RedisService async function test completed")

def thread_main():
    """Thread function to test RedisService."""
    print("Thread function started")
    asyncio.run(RedisService().set("test", "test"))
    asyncio.run(RedisService().set("test1", "test"))
    print("Thread function completed")

# Thread safe test
process_1 = Thread(target=thread_main)
print("Thread 1 started")
process_2 = Thread(target=thread_main)
print("Thread 2 started")
process_1.start()
process_1.join()
process_2.start()
process_2.join()
print("RedisService thread safe test completed")

# python -m tests.test_redis

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/tests/conftest.py`:

```py
import os
import sys
from typing import Any

import pytest

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)


def pytest_addoption(parser: pytest.Parser) -> None:
    parser.addoption("--dapr-http-port", action="store", default=3510, type=int, help="Dapr HTTP port")
    parser.addoption("--dapr-api-token", action="store", default=None, type=str, help="Dapr API token")


@pytest.fixture(scope="session")
def dapr_http_port(request: pytest.FixtureRequest) -> Any:
    arg_value = request.config.getoption("--dapr-http-port")
    if arg_value is None:
        pytest.fail("--dapr-http-port is required to run the tests")
    return arg_value


@pytest.fixture(scope="session")
def dapr_api_token(request: pytest.FixtureRequest) -> Any:
    return request.config.getoption("--dapr-api-token")

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/tests/test_update_credential.py`:

```py
import os
import pytest
from datetime import datetime
from unittest.mock import AsyncMock, patch
from typing import Dict, Any
import aiohttp

# Set up all required environment variables before importing any application code
os.environ.update({
    # AppConfig variables
    "POSTGRES_USER": "test_user",
    "POSTGRES_PASSWORD": "test_password",
    "POSTGRES_DB": "test_db",
    "SUPER_USER_EMAIL": "test@example.com",
    "SUPER_USER_PASSWORD": "test_password",
    "DAPR_BASE_URL": "http://localhost:3500",
    "BUD_CLUSTER_APP_ID": "test_cluster_app",
    "BUD_MODEL_APP_ID": "test_model_app",
    "BUD_SIMULATOR_APP_ID": "test_simulator_app",
    "BUD_METRICS_APP_ID": "test_metrics_app",
    "BUD_NOTIFY_APP_ID": "test_notify_app",
    "APP_PORT": "8000",

    # SecretsConfig variables
    "JWT_SECRET_KEY": "test_jwt_secret_key",
    "REDIS_PASSWORD": "test_redis_password",
    "REDIS_URI": "redis://localhost:6379"
})

# Import application code after environment setup
from budapp.credential_ops.schemas import CredentialVerifyPayload, CredentialVerifyRequest
from budapp.shared.dapr_service import DaprService
from budapp.shared.redis_service import RedisService
from budapp.commons.db_utils import Session


async def add_verify_endpoint(payload: Dict[str, Any]):
    """Helper function to call the verify endpoint"""
    endpoint = f"http://localhost:{os.getenv('APP_PORT')}/credentials/verify"
    async with aiohttp.ClientSession() as session, session.post(endpoint, json=payload) as response:
        return await response.json()


@pytest.mark.asyncio
async def test_verify_credential():
    # Create mock objects
    mock_dapr_service = AsyncMock(spec=DaprService)
    mock_redis_service = AsyncMock(spec=RedisService)
    mock_session = AsyncMock(spec=Session)

    # Patch the services with our mocks
    with patch("budapp.credential_ops.services.DaprService", return_value=mock_dapr_service), \
         patch("budapp.credential_ops.services.RedisService", return_value=mock_redis_service), \
         patch("budapp.credential_ops.services.Session", return_value=mock_session):

        # Create test payload for verification
        payload = CredentialVerifyPayload(
            hashed_key="cb742dc90b3c735da84104d09715fde454e12bff5f6c7336c1e655628fe9d957",
            verification_token="test_verification_token"
        )
        data = CredentialVerifyRequest(payload=payload).model_dump(mode="json")

        # Call the verify endpoint
        response = await add_verify_endpoint(data)

        # Verify the interactions with the mocks
        mock_dapr_service.publish_to_topic.assert_called_once()
        mock_redis_service.get.assert_called_once()  # Changed from set to get for verification
        mock_session.query.assert_called_once()  # Changed from commit to query for verification

        # Additional assertions for verification endpoint
        assert response is not None
        assert "verification_status" in response
        assert response["verification_status"] in ["success", "failed"]


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_verify_credential())

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/tests/test_pubsub.py`:

```py
from typing import Optional
import pytest
from uuid import uuid4

from budapp.shared.dapr_service import DaprService


@pytest.mark.asyncio
async def test_pubsub(dapr_http_port: int, dapr_api_token: str) -> None:
    """Test the pubsub publish method."""
    with DaprService(dapr_http_port=dapr_http_port, dapr_api_token=dapr_api_token) as dapr_service:
        await dapr_service.publish_to_topic(
            pubsub_name="pubsub-redis",
            target_topic_name="notificationMessages",
            data={"subscriber_id": str(uuid4()), "title": "Test", "message": "Hello, World!"},
            event_type="notification",
        )

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/tests/test_cluster_metrics.py`:

```py
import os
from pathlib import Path
from dotenv import load_dotenv

from budapp.cluster_ops.schemas import MetricTypeEnum

# Load test environment variables before any other imports
env_path = Path(__file__).parent.parent / ".env.test"
load_dotenv(env_path, override=True)

import pytest
from fastapi.testclient import TestClient
from uuid import UUID
from fastapi import status
from unittest.mock import AsyncMock, patch
from budapp.commons.config import app_settings
from budapp.main import app
from budapp.user_ops.schemas import User
from budapp.commons.dependencies import get_current_active_user
from budapp.cluster_ops.services import ClusterService
from budapp.cluster_ops.models import Cluster as ClusterModel

pytest_plugins = ("pytest_asyncio",)

TEST_CLUSTER_ID = UUID("dff4578f-039e-4be4-aa36-db7932e404fa")


@pytest.fixture
def mock_user():
    user = AsyncMock(spec=User)
    user.is_active = True
    return user


@pytest.fixture(autouse=True)
def setup_test_env():
    """Fixture to set up test environment variables."""
    # Store original env vars
    original_env = dict(os.environ)

    # Set up test environment
    load_dotenv(env_path, override=True)

    yield

    # Restore original env vars
    os.environ.clear()
    os.environ.update(original_env)


@pytest.fixture
def test_client():
    """Fixture that creates a test client for testing."""
    return TestClient(app)


@pytest.fixture
def mock_cluster():
    cluster = AsyncMock(spec=ClusterModel)
    cluster.id = TEST_CLUSTER_ID
    cluster.name = "Test Cluster"
    cluster.status = "available"  # Using valid enum value
    cluster.icon = "test-icon"  # Actual string
    cluster.ingress_url = "https://test-cluster.example.com"  # Actual string
    cluster.cluster_id = str(TEST_CLUSTER_ID)  # String representation of UUID
    return cluster


@pytest.fixture
def mock_metrics():
    return {
        "nodes": [
            {
                "cpu_usage": 50,
                "memory_usage": 60,
                "disk_usage": 70,
                "gpu_usage": 80,
                "hpu_usage": 90,
                "network_stats": {"in": 100, "out": 200},
            }
        ],
        "cluster_summary": {
            "total_cpu": 100,
            "total_memory": 200,
            "total_disk": 300,
            "total_gpu": 400,
            "total_hpu": 500,
        },
        "historical_data": {
            "cpu_usage": [{"timestamp": 1234567890, "value": 45.5}, {"timestamp": 1234567900, "value": 50.2}]
        },
        "time_range": "today",
    }


@pytest.fixture
def mock_db_session():
    session = AsyncMock()
    session.commit = AsyncMock()
    session.close = AsyncMock()
    return session


@pytest.fixture
def override_dependencies(mock_user, mock_db_session):
    """Override FastAPI dependencies."""

    def override_get_current_active_user():
        return mock_user

    def override_get_session():
        return mock_db_session

    app.dependency_overrides[get_current_active_user] = override_get_current_active_user
    app.dependency_overrides["get_session"] = override_get_session
    yield
    app.dependency_overrides.clear()


@pytest.mark.parametrize("time_range", ["today", "7days", "month"])
@pytest.mark.asyncio
async def test_get_cluster_metrics(
    test_client: TestClient,
    mock_user,
    mock_cluster,
    mock_metrics,
    mock_db_session,
    override_dependencies,
    time_range: str,
):
    """Test the GET /clusters/{cluster_id}/metrics endpoint with different time ranges."""
    # Mock the database session
    with patch("budapp.commons.db_utils.get_session", return_value=mock_db_session), \
         patch("budapp.commons.db_utils.DataManagerUtils.retrieve_by_fields", return_value=mock_cluster), \
         patch("budapp.cluster_ops.utils.ClusterMetricsFetcher.get_cluster_metrics", return_value=mock_metrics):
        
        response = test_client.get(
            f"/clusters/{TEST_CLUSTER_ID}/metrics", 
            params={"time_range": time_range, "metric_type": "hpu"}
        )

        # Assert response
        assert response.status_code == 200
        data = response.json()
        assert data["code"] == 200
        assert data["message"] == "Successfully retrieved cluster metrics"
        assert data["time_range"] == time_range
        assert data["metric_type"] == "hpu"

        # Optional: Write response to file for debugging
        with open(f"cluster_metrics_response_{time_range}.json", "w") as file:
            file.write(response.text)


# def test_get_cluster_metrics_invalid_time_range(
#     test_client: TestClient,
#     mock_user,
#     mock_cluster,
#     override_dependencies,
# ):
#     """Test the GET /clusters/{cluster_id}/metrics endpoint with an invalid time range."""
#     response = test_client.get(
#         f"/clusters/{TEST_CLUSTER_ID}/metrics",
#         params={"time_range": "invalid_range"}
#     )

#     assert response.status_code == status.HTTP_422_UNPROCESSABLE_ENTITY
#     assert response.json()["object"] == "error"
#     assert "time_range" in response.json()["detail"][0]["loc"]


# def test_get_cluster_metrics_invalid_cluster_id(
#     test_client: TestClient,
#     mock_user,
#     override_dependencies
# ):
#     """Test the GET /clusters/{cluster_id}/metrics endpoint with an invalid cluster ID."""
#     response = test_client.get(f"/clusters/invalid-uuid/metrics")

#     assert response.status_code == status.HTTP_422_UNPROCESSABLE_ENTITY
#     assert response.json()["object"] == "error"
#     assert "message" in response.json()


# def test_get_cluster_metrics_unauthorized(test_client: TestClient):
#     """Test the GET /clusters/{cluster_id}/metrics endpoint with an invalid token."""
#     response = test_client.get(
#         f"/clusters/{TEST_CLUSTER_ID}/metrics",
#         headers={"Authorization": "Bearer invalid_token"},
#     )

#     assert response.status_code == status.HTTP_401_UNAUTHORIZED
#     assert response.json()["object"] == "error"
#     assert "message" in response.json()


# def test_get_cluster_metrics_not_found(
#     test_client: TestClient,
#     mock_user,
#     override_dependencies,
# ):
#     """Test the GET /clusters/{cluster_id}/metrics endpoint with a non-existent cluster."""
#     with patch(
#         "budapp.commons.db_utils.DataManagerUtils.retrieve_by_fields",
#         return_value=None
#     ):
#         response = test_client.get(f"/clusters/{TEST_CLUSTER_ID}/metrics")

#         assert response.status_code == status.HTTP_404_NOT_FOUND
#         assert response.json()["object"] == "error"
#         assert response.json()["message"] == "Cluster not found"


# def test_get_cluster_metrics_prometheus_error(
#     test_client: TestClient,
#     mock_user,
#     mock_cluster,
#     override_dependencies,
# ):
#     """Test handling of Prometheus service unavailability."""
#     with patch(
#         "budapp.commons.db_utils.DataManagerUtils.retrieve_by_fields",
#         return_value=mock_cluster
#     ), patch.object(
#         ClusterService,
#         "get_cluster_metrics",
#         return_value=None
#     ):
#         response = test_client.get(f"/clusters/{TEST_CLUSTER_ID}/metrics")

#         assert response.status_code == status.HTTP_503_SERVICE_UNAVAILABLE
#         assert response.json()["object"] == "error"
#         assert "Failed to fetch metrics from Prometheus" in response.json()["message"]

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/tests/test_cluster_metrics_fetcher.py`:

```py
import pytest
import json
import os
from pathlib import Path
from unittest.mock import AsyncMock, patch, MagicMock
from datetime import datetime, timezone
from uuid import UUID
from budapp.cluster_ops.utils import ClusterMetricsFetcher
from budapp.cluster_ops.schemas import MetricTypeEnum

# Use actual cluster ID and Prometheus endpoint
TEST_CLUSTER_ID = UUID("dff4578f-039e-4be4-aa36-db7932e404fa")
PROMETHEUS_URL = "https://metrics.fmops.in"

# Create directory for test outputs if it doesn't exist
TEST_OUTPUT_DIR = Path(__file__).parent / "test_outputs"
TEST_OUTPUT_DIR.mkdir(exist_ok=True)

def write_to_json(data: dict, filename: str):
    """Helper function to write data to a JSON file."""
    filepath = TEST_OUTPUT_DIR / filename
    with open(filepath, 'w') as f:
        json.dump(data, f, indent=2, default=str)
    print(f"Written to {filepath}")

@pytest.fixture
def metrics_fetcher():
    """Fixture providing a real ClusterMetricsFetcher instance."""
    return ClusterMetricsFetcher(PROMETHEUS_URL)

@pytest.mark.asyncio
async def test_get_cluster_metrics_live():
    """Test fetching metrics from live Prometheus endpoint."""
    fetcher = ClusterMetricsFetcher(PROMETHEUS_URL)
    metrics = await fetcher.get_cluster_metrics(
        cluster_id=TEST_CLUSTER_ID,
        time_range="today",
        metric_type="all"
    )

    assert metrics is not None
    write_to_json(metrics, "all_metrics.json")

@pytest.mark.asyncio
async def test_get_specific_metrics_live():
    """Test fetching specific metric types from live Prometheus."""
    fetcher = ClusterMetricsFetcher(PROMETHEUS_URL)
    metric_types = ["cpu", "memory", "disk", "gpu", "hpu", "network"]
    
    for metric_type in metric_types:
        metrics = await fetcher.get_cluster_metrics(
            cluster_id=TEST_CLUSTER_ID,
            time_range="today",
            metric_type=metric_type
        )
        
        assert metrics is not None
        write_to_json(metrics, f"{metric_type}_metrics.json")

@pytest.mark.asyncio
async def test_time_ranges_live():
    """Test different time ranges with live Prometheus."""
    fetcher = ClusterMetricsFetcher(PROMETHEUS_URL)
    time_ranges = ["today", "7days", "month"]
    
    for time_range in time_ranges:
        metrics = await fetcher.get_cluster_metrics(
            cluster_id=TEST_CLUSTER_ID,
            time_range=time_range,
            metric_type="cpu"  # Using CPU metrics as they're typically always available
        )
        
        assert metrics is not None
        write_to_json(metrics, f"cpu_metrics_{time_range}.json")

@pytest.mark.asyncio
async def test_historical_data_live():
    """Test historical data from live Prometheus."""
    fetcher = ClusterMetricsFetcher(PROMETHEUS_URL)
    metrics = await fetcher.get_cluster_metrics(
        cluster_id=TEST_CLUSTER_ID,
        time_range="7days",
        metric_type="all"
    )
    
    assert metrics is not None
    write_to_json(metrics, "historical_data_7days.json")

    # Create a summary of available metrics
    historical_summary = {
        metric_name: len(data_points) 
        for metric_name, data_points in metrics["historical_data"].items()
    }
    write_to_json(historical_summary, "historical_data_summary.json")

@pytest.mark.asyncio
async def test_invalid_cluster_id_live():
    """Test behavior with invalid cluster ID on live Prometheus."""
    fetcher = ClusterMetricsFetcher(PROMETHEUS_URL)
    invalid_cluster_id = UUID("00000000-0000-0000-0000-000000000000")
    
    metrics = await fetcher.get_cluster_metrics(
        cluster_id=invalid_cluster_id,
        time_range="today",
        metric_type="all"
    )
    
    if metrics is not None:
        write_to_json(metrics, "invalid_cluster_metrics.json")

@pytest.mark.asyncio
async def test_comprehensive_metrics():
    """Test to gather comprehensive metrics with all combinations."""
    fetcher = ClusterMetricsFetcher(PROMETHEUS_URL)
    time_ranges = ["today", "7days", "month"]
    metric_types = ["all", "cpu", "memory", "disk", "gpu", "hpu", "network"]
    
    comprehensive_results = {}
    
    for time_range in time_ranges:
        comprehensive_results[time_range] = {}
        for metric_type in metric_types:
            metrics = await fetcher.get_cluster_metrics(
                cluster_id=TEST_CLUSTER_ID,
                time_range=time_range,
                metric_type=metric_type
            )
            
            if metrics is not None:
                # Store basic info and summary
                comprehensive_results[time_range][metric_type] = {
                    "timestamp": metrics["timestamp"],
                    "node_count": len(metrics["nodes"]),
                    "has_historical_data": bool(metrics["historical_data"]),
                    "cluster_summary": metrics["cluster_summary"]
                }
                
                # Write full response to separate file
                write_to_json(
                    metrics, 
                    f"comprehensive_{time_range}_{metric_type}.json"
                )
    
    # Write summary of all results
    write_to_json(comprehensive_results, "comprehensive_summary.json")

@pytest.mark.asyncio
async def test_error_handling_live():
    """Test error handling with invalid Prometheus URL."""
    fetcher = ClusterMetricsFetcher("https://invalid.prometheus.url")
    
    metrics = await fetcher.get_cluster_metrics(
        cluster_id=TEST_CLUSTER_ID,
        time_range="today",
        metric_type="all"
    )
    
    assert metrics is None
    write_to_json({"error": "Failed to fetch metrics from invalid URL"}, "error_case.json")

if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
```

`/Users/rahulvramesh/bud-v2/bud-serve-app/setup.py`:

```py
"""setup.py: setuptools control."""

import codecs
import os.path
import sys
from typing import List

from setuptools import find_packages, setup


ROOT_DIR = os.path.abspath(os.path.dirname(__file__))


def read_file(rel_path: str) -> str:
    """Read a file and return the contents."""
    _path = os.path.join(ROOT_DIR, rel_path)
    if os.path.isfile(_path):
        with codecs.open(_path, "r") as fp:
            return fp.read()
    else:
        return ""


def get_project_name_and_version(rel_path: str) -> List[str]:
    """Get the project name and version from a file specified by __version__ = name@version."""
    for line in read_file(rel_path).splitlines():
        if line.startswith("__version__"):
            delim = '"' if '"' in line else "'"
            return line.split(delim)[1].split("@")
    else:
        raise RuntimeError("Unable to find version string.")


def get_requirements() -> List[str]:
    """Get Python package dependencies from requirements.txt."""

    def _read_requirements(filename: str) -> List[str]:
        requirements = read_file(filename).strip().split("\n")
        resolved_requirements = []
        for line in requirements:
            if line.startswith("-r "):
                resolved_requirements += _read_requirements(line.split()[1])
            else:
                resolved_requirements.append(line)
        return resolved_requirements

    return _read_requirements("requirements.txt")


name, version = get_project_name_and_version("budapp/__about__.py")
version_range_max = max(sys.version_info[1], 10) + 1

setup(
    name=name,
    version=version,
    description=(""),
    long_description=read_file("README.md"),
    long_description_content_type="text/markdown",
    url="https://github.com/BudEcosystem/bud-serve-app",
    project_urls={
        "Homepage": "https://github.com/BudEcosystem/bud-serve-app",
        "Documentation": "https://github.com/BudEcosystem/bud-serve-app/blob/main/README.md",
        "Issues": "https://github.com/BudEcosystem/bud-serve-app/issues",
        "Changelog": "https://github.com/BudEcosystem/bud-serve-app/blob/main/CHANGELOG.md",
    },
    keywords="budapp microservice",
    license="Apache 2.0 License",
    author="Bud Ecosystem Inc.",
    package_dir={"": "./"},
    packages=find_packages(
        where=".",
        include=["budapp*"],
        exclude=(
            "docs",
            "examples",
            "tests",
            "docker",
            "assets*",
            "dist*",
            "scripts*",
            "README.md",
            ".gitignore",
            "requirements*.txt",
        ),
    ),
    package_data={"budapp": ["py.typed"]},
    include_package_data=True,
    python_requires=">=3.8.0",
    install_requires=get_requirements(),
    extras_require={},
    classifiers=[
        "Development Status :: 5 - Production/Stable",
        "Intended Audience :: Developers",
        "Topic :: Software Development :: Quality Assurance",
        "Topic :: Software Development :: Testing",
        "Topic :: Software Development :: Documentation",
        "License :: OSI Approved :: Apache Software License",
        "Operating System :: OS Independent",
        "Environment :: Console",
        "Programming Language :: Python :: 3",
    ]
    + [f"Programming Language :: Python :: 3.{i}" for i in range(8, version_range_max)],
)

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/scripts/del_configs.sh`:

```sh
#!/bin/bash

#
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------
#

# Initialize variables
container=""
password=""
keys=()

function display_help() {
    echo "Usage: del_configs.sh --container <container_name> [--password <password>] [--key <key>]..."
    echo
    echo "Options:"
    echo "  --container   Name of the container where Redis is running (required)"
    echo "  --password    Redis password (optional)"
    echo "  --<key>         Configuration key to delete from the store (multiple keys can be deleted)"
    echo "  --help        Display this help message and exit"
    echo
    echo "Examples:"
    echo "  ./scripts/del_configs.sh --container my_redis_container --setting1 --setting2"
    echo "  ./scripts/del_configs.sh --container my_redis_container --password mypassword --setting1"
    exit 0
}

# Parse arguments
while [[ "$#" -gt 0 ]]; do
    case $1 in
        --container) container="$2"; shift ;;
        --password) password="$2"; shift ;;
        --help) display_help ;;
        --*) key="${1:2}"; keys+=("$key") ;;
        *) echo "Unknown parameter passed: $1"; exit 1 ;;
    esac
    shift
done

# Check if container name is provided
if [ -z "$container" ]; then
    echo "Error: --container is required"
    exit 1
fi

# Check if at least one key is provided
if [ ${#keys[@]} -eq 0 ]; then
    echo "Error: At least one key is required"
    exit 1
fi

# Construct the redis-cli DEL command
del_command="DEL ${keys[@]}"

# Check if password is provided and modify the command accordingly
if [ -n "$password" ]; then
    auth_command="-a $password $del_command"
else
    auth_command="$del_command"
fi

# Execute the command in the specified container
docker exec "$container" redis-cli $auth_command

# Print a message indicating success
echo "Executed command: docker exec $container redis-cli $auth_command"

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/scripts/update_configs.sh`:

```sh
#!/bin/bash

#
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------
#

# Initialize variables
container=""
password=""
keys_values=()

function display_help() {
    echo "Usage: ./update_configs.sh --container <container_name> [--password <password>] --<key> <value> [--<key2> <value2> ...]"
    echo
    echo "This script is used to add key-value pairs to a Redis config store running inside a Docker container."
    echo
    echo "Options:"
    echo "  --container   The name of the Docker container running Redis (Required)"
    echo "  --password    The password for Redis, if required (Optional)"
    echo "  --<key>       The key for the config value (At least one key-value pair is required)"
    echo "  <value>       The value associated with the key"
    echo "  --help        Display this help message and exit"
    echo
    echo "Example:"
    echo "  ./scripts/update_configs.sh --container my_redis_container --password mypassword --setting1 value1 --setting2 value2"
    exit 0
}

# Parse arguments
while [[ "$#" -gt 0 ]]; do
    case $1 in
        --container) container="$2"; shift ;;
        --password) password="$2"; shift ;;
        --help) display_help ;;
        --*) key="${1:2}"; value="$2"; keys_values+=("$key" "$value"); shift ;;
        *) echo "Unknown parameter passed: $1"; exit 1 ;;
    esac
    shift
done

# Check if container name is provided
if [ -z "$container" ]; then
    echo "Error: --container is required"
    exit 1
fi

# Check if at least one key-value pair is provided
if [ ${#keys_values[@]} -eq 0 ]; then
    echo "Error: At least one key-value pair is required"
    exit 1
fi

# Construct the redis-cli MSET command
mset_command="MSET ${keys_values[@]}"

# Check if password is provided and modify the command accordingly
if [ -n "$password" ]; then
    cli_command="-a $password $mset_command"
else
    cli_command="$mset_command"
fi

# Execute the command in the specified container
docker exec "$container" redis-cli $cli_command

# Print a message indicating success
echo "Executed command: docker exec $container redis-cli $cli_command"

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/scripts/install_hooks.sh`:

```sh
# Function to check if NVM is installed
check_nvm_installed() {
    if command -v nvm >/dev/null 2>&1; then
        echo "NVM is already installed."
        return 0
    else
        return 1
    fi
}

# Function to check if a specific Node.js version is installed
check_node_version_installed() {
    local version=$1
    if nvm ls "$version" >/dev/null 2>&1; then
        echo "Node.js $version is already installed."
        return 0
    else
        return 1
    fi
}

# Function to install NVM if it's not installed
install_nvm() {
    echo "Installing NVM..."
    # Install node version manager(https://github.com/nvm-sh/nvm)
    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.0/install.sh | bash

    # Source NVM to use it immediately in the same session
    export NVM_DIR="$([ -z "${XDG_CONFIG_HOME-}" ] && printf %s "${HOME}/.nvm" || printf %s "${XDG_CONFIG_HOME}/nvm")"
    [ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh" # This loads nvm
}

# Function to install Node.js if the specified version is not installed
install_node_version() {
    local version=$1
    echo "Installing Node.js $version..."
    nvm install "$version"
}

# Install nvm if not found
if check_nvm_installed; then
    echo "Skipping NVM installation."
else
    install_nvm
fi

# Verify nvm installation
command -v nvm

# Check and install the required Node.js version
NODE_VERSION="v20.16.0"
if ! check_node_version_installed "$NODE_VERSION"; then
    install_node_version "$NODE_VERSION"
else
    echo "Skipping Node.js $NODE_VERSION installation."
fi

# Install pre commit hooks assuming the python env is activated
pip install -r requirements-dev.txt
pre-commit install
pre-commit install --hook-type commit-msg

# Re-load the shell
#exec "$SHELL" -l
```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/script.py.mako`:

```mako
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    ${downgrades if downgrades else "pass"}

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/env.py`:

```py
from logging.config import fileConfig

import alembic_postgresql_enum
from alembic import context
from sqlalchemy import engine_from_config, pool

from budapp.commons import Base
from budapp.commons.config import app_settings


# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config
config.set_main_option("sqlalchemy.url", app_settings.postgres_url)

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(connection=connection, target_metadata=target_metadata)

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/40a8d774f38f_modified_field_in_model.py`:

```py
"""Modified Field In Model

Revision ID: 40a8d774f38f
Revises: 1d94eee5d3f2
Create Date: 2024-10-26 13:29:22.280647

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '40a8d774f38f'
down_revision: Union[str, None] = '1d94eee5d3f2'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('model', sa.Column('created_by', sa.Uuid(), nullable=False))
    op.create_foreign_key(op.f('fk_model_created_by_user'), 'model', 'user', ['created_by'], ['id'])
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint(op.f('fk_model_created_by_user'), 'model', type_='foreignkey')
    op.drop_column('model', 'created_by')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/1a20ca43ac69_removed_register_from_cluster.py`:

```py
"""Removed Register from cluster

Revision ID: 1a20ca43ac69
Revises: 375eb22cb3af
Create Date: 2025-01-10 06:14:24.322892

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from alembic_postgresql_enum import TableReference

# revision identifiers, used by Alembic.
revision: str = '1a20ca43ac69'
down_revision: Union[str, None] = '375eb22cb3af'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema='public',
        enum_name='cluster_status_enum',
        new_values=['available', 'not_available', 'error', 'deleting', 'deleted'],
        affected_columns=[TableReference(table_schema='public', table_name='cluster', column_name='status')],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema='public',
        enum_name='cluster_status_enum',
        new_values=['available', 'not_available', 'registering', 'error', 'deleting', 'deleted'],
        affected_columns=[TableReference(table_schema='public', table_name='cluster', column_name='status')],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/d92d92c1c90c_removed_palm_from_proprietary_.py`:

```py
"""removed palm from proprietary credential table

Revision ID: d92d92c1c90c
Revises: 04ef4a274216
Create Date: 2024-12-18 10:17:52.151093

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from alembic_postgresql_enum import TableReference

# revision identifiers, used by Alembic.
revision: str = "d92d92c1c90c"
down_revision: Union[str, None] = "04ef4a274216"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema="public",
        enum_name="proprietary_credential_type_enum",
        new_values=[
            "nlp_cloud",
            "deepinfra",
            "anthropic",
            "vertex_ai-vision-models",
            "vertex_ai-ai21_models",
            "cerebras",
            "watsonx",
            "predibase",
            "volcengine",
            "clarifai",
            "baseten",
            "sambanova",
            "github",
            "petals",
            "replicate",
            "vertex_ai-chat-models",
            "azure_ai",
            "perplexity",
            "vertex_ai-code-text-models",
            "vertex_ai-text-models",
            "cohere_chat",
            "vertex_ai-embedding-models",
            "text-completion-openai",
            "groq",
            "openai",
            "aleph_alpha",
            "sagemaker",
            "databricks",
            "fireworks_ai",
            "vertex_ai-anthropic_models",
            "vertex_ai-mistral_models",
            "voyage",
            "vertex_ai-language-models",
            "anyscale",
            "deepseek",
            "vertex_ai-image-models",
            "mistral",
            "ollama",
            "cohere",
            "gemini",
            "friendliai",
            "vertex_ai-code-chat-models",
            "azure",
            "codestral",
            "vertex_ai-llama_models",
            "together_ai",
            "cloudflare",
            "ai21",
            "openrouter",
            "bedrock",
            "text-completion-codestral",
            "huggingface",
        ],
        affected_columns=[
            TableReference(table_schema="public", table_name="proprietary_credential", column_name="type")
        ],
        enum_values_to_rename=[],
    )
    op.sync_enum_values(
        enum_schema="public",
        enum_name="credential_type_enum",
        new_values=[
            "nlp_cloud",
            "deepinfra",
            "anthropic",
            "vertex_ai-vision-models",
            "vertex_ai-ai21_models",
            "cerebras",
            "watsonx",
            "predibase",
            "volcengine",
            "clarifai",
            "baseten",
            "sambanova",
            "github",
            "petals",
            "replicate",
            "vertex_ai-chat-models",
            "azure_ai",
            "perplexity",
            "vertex_ai-code-text-models",
            "vertex_ai-text-models",
            "cohere_chat",
            "vertex_ai-embedding-models",
            "text-completion-openai",
            "groq",
            "openai",
            "aleph_alpha",
            "sagemaker",
            "databricks",
            "fireworks_ai",
            "vertex_ai-anthropic_models",
            "vertex_ai-mistral_models",
            "voyage",
            "vertex_ai-language-models",
            "anyscale",
            "deepseek",
            "vertex_ai-image-models",
            "mistral",
            "ollama",
            "cohere",
            "gemini",
            "friendliai",
            "vertex_ai-code-chat-models",
            "azure",
            "codestral",
            "vertex_ai-llama_models",
            "together_ai",
            "cloudflare",
            "ai21",
            "openrouter",
            "bedrock",
            "text-completion-codestral",
            "huggingface",
        ],
        affected_columns=[TableReference(table_schema="public", table_name="provider", column_name="type")],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema="public",
        enum_name="credential_type_enum",
        new_values=[
            "nlp_cloud",
            "deepinfra",
            "anthropic",
            "vertex_ai-vision-models",
            "vertex_ai-ai21_models",
            "cerebras",
            "watsonx",
            "predibase",
            "volcengine",
            "clarifai",
            "baseten",
            "sambanova",
            "github",
            "petals",
            "replicate",
            "vertex_ai-chat-models",
            "azure_ai",
            "perplexity",
            "vertex_ai-code-text-models",
            "vertex_ai-text-models",
            "palm",
            "cohere_chat",
            "vertex_ai-embedding-models",
            "text-completion-openai",
            "groq",
            "openai",
            "aleph_alpha",
            "sagemaker",
            "databricks",
            "fireworks_ai",
            "vertex_ai-anthropic_models",
            "vertex_ai-mistral_models",
            "voyage",
            "vertex_ai-language-models",
            "anyscale",
            "deepseek",
            "vertex_ai-image-models",
            "mistral",
            "ollama",
            "cohere",
            "gemini",
            "friendliai",
            "vertex_ai-code-chat-models",
            "azure",
            "codestral",
            "vertex_ai-llama_models",
            "together_ai",
            "cloudflare",
            "ai21",
            "openrouter",
            "bedrock",
            "text-completion-codestral",
            "huggingface",
        ],
        affected_columns=[TableReference(table_schema="public", table_name="provider", column_name="type")],
        enum_values_to_rename=[],
    )
    op.sync_enum_values(
        enum_schema="public",
        enum_name="proprietary_credential_type_enum",
        new_values=[
            "nlp_cloud",
            "deepinfra",
            "anthropic",
            "vertex_ai-vision-models",
            "vertex_ai-ai21_models",
            "cerebras",
            "watsonx",
            "predibase",
            "volcengine",
            "clarifai",
            "baseten",
            "sambanova",
            "github",
            "petals",
            "replicate",
            "vertex_ai-chat-models",
            "azure_ai",
            "perplexity",
            "vertex_ai-code-text-models",
            "vertex_ai-text-models",
            "palm",
            "cohere_chat",
            "vertex_ai-embedding-models",
            "text-completion-openai",
            "groq",
            "openai",
            "aleph_alpha",
            "sagemaker",
            "databricks",
            "fireworks_ai",
            "vertex_ai-anthropic_models",
            "vertex_ai-mistral_models",
            "voyage",
            "vertex_ai-language-models",
            "anyscale",
            "deepseek",
            "vertex_ai-image-models",
            "mistral",
            "ollama",
            "cohere",
            "gemini",
            "friendliai",
            "vertex_ai-code-chat-models",
            "azure",
            "codestral",
            "vertex_ai-llama_models",
            "together_ai",
            "cloudflare",
            "ai21",
            "openrouter",
            "bedrock",
            "text-completion-codestral",
            "huggingface",
        ],
        affected_columns=[
            TableReference(table_schema="public", table_name="proprietary_credential", column_name="type")
        ],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/64e788fd946c_updated_endpoint_table.py`:

```py
"""Updated Endpoint Table

Revision ID: 64e788fd946c
Revises: 919ebf0fe343
Create Date: 2024-11-29 03:04:06.089062

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '64e788fd946c'
down_revision: Union[str, None] = '919ebf0fe343'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('endpoint', sa.Column('credential_id', sa.Uuid(), nullable=True))
    op.create_foreign_key(op.f('fk_endpoint_credential_id_proprietary_credential'), 'endpoint', 'proprietary_credential', ['credential_id'], ['id'])
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint(op.f('fk_endpoint_credential_id_proprietary_credential'), 'endpoint', type_='foreignkey')
    op.drop_column('endpoint', 'credential_id')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/e3ac5a852b2c_added_model_configuration_to_endpoint_.py`:

```py
"""added model configuration to endpoint table

Revision ID: e3ac5a852b2c
Revises: 0c7fdc4b7072
Create Date: 2024-12-24 05:44:36.018289

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "e3ac5a852b2c"
down_revision: Union[str, None] = "0c7fdc4b7072"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column("endpoint", sa.Column("model_configuration", postgresql.JSONB(astext_type=sa.Text()), nullable=True))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column("endpoint", "model_configuration")
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/af5cc669f51e_updated_model_table.py`:

```py
"""Updated Model Table

Revision ID: af5cc669f51e
Revises: 1b33efcd5611
Create Date: 2025-01-02 18:41:55.839398

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from alembic_postgresql_enum import TableReference
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'af5cc669f51e'
down_revision: Union[str, None] = '1b33efcd5611'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('model', sa.Column('model_weights_size', sa.BigInteger(), nullable=True))
    op.add_column('model', sa.Column('kv_cache_size', sa.BigInteger(), nullable=True))
    op.add_column('model', sa.Column('architecture_text_config', postgresql.JSONB(astext_type=sa.Text()), nullable=True))
    op.add_column('model', sa.Column('architecture_vision_config', postgresql.JSONB(astext_type=sa.Text()), nullable=True))
    op.drop_column('model', 'context_length')
    op.drop_column('model', 'num_layers')
    op.drop_column('model', 'architecture')
    op.drop_column('model', 'hidden_size')
    op.drop_column('model', 'torch_dtype')
    op.sync_enum_values(
        enum_schema='public',
        enum_name='modality_enum',
        new_values=['llm', 'mllm', 'image', 'embedding', 'text_to_speech', 'speech_to_text'],
        affected_columns=[TableReference(table_schema='public', table_name='cloud_model', column_name='modality'), TableReference(table_schema='public', table_name='model', column_name='modality')],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema='public',
        enum_name='modality_enum',
        new_values=['llm', 'image', 'embedding', 'text_to_speech', 'speech_to_text'],
        affected_columns=[TableReference(table_schema='public', table_name='cloud_model', column_name='modality'), TableReference(table_schema='public', table_name='model', column_name='modality')],
        enum_values_to_rename=[],
    )
    op.add_column('model', sa.Column('torch_dtype', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('model', sa.Column('hidden_size', sa.BIGINT(), autoincrement=False, nullable=True))
    op.add_column('model', sa.Column('architecture', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('model', sa.Column('num_layers', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('model', sa.Column('context_length', sa.BIGINT(), autoincrement=False, nullable=True))
    op.drop_column('model', 'architecture_vision_config')
    op.drop_column('model', 'architecture_text_config')
    op.drop_column('model', 'kv_cache_size')
    op.drop_column('model', 'model_weights_size')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/61d0ff8cf1cd_added_model_scan_result_table.py`:

```py
"""Added Model Scan Result Table

Revision ID: 61d0ff8cf1cd
Revises: f040fc25b476
Create Date: 2024-12-10 05:39:42.317345

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '61d0ff8cf1cd'
down_revision: Union[str, None] = 'f040fc25b476'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('model_security_scan_result',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('model_id', sa.Uuid(), nullable=False),
    sa.Column('total_issues', sa.Integer(), nullable=False),
    sa.Column('total_scanned_files', sa.Integer(), nullable=False),
    sa.Column('total_skipped_files', sa.Integer(), nullable=False),
    sa.Column('scanned_files', postgresql.ARRAY(sa.String()), nullable=False),
    sa.Column('low_severity_count', sa.Integer(), nullable=False),
    sa.Column('medium_severity_count', sa.Integer(), nullable=False),
    sa.Column('high_severity_count', sa.Integer(), nullable=False),
    sa.Column('critical_severity_count', sa.Integer(), nullable=False),
    sa.Column('model_issues', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['model_id'], ['model.id'], name=op.f('fk_model_security_scan_result_model_id_model')),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_model_security_scan_result'))
    )
    op.alter_column('model', 'scan_verified',
               existing_type=sa.BOOLEAN(),
               nullable=True,
               existing_server_default=sa.text('false'))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('model', 'scan_verified',
               existing_type=sa.BOOLEAN(),
               nullable=False,
               existing_server_default=sa.text('false'))
    op.drop_table('model_security_scan_result')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/ffb55b8f01fc_modified_model_table.py`:

```py
"""Modified model table

Revision ID: ffb55b8f01fc
Revises: e2d7e5f38d3c
Create Date: 2024-10-27 12:50:50.443071

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'ffb55b8f01fc'
down_revision: Union[str, None] = 'e2d7e5f38d3c'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('model', 'author',
               existing_type=sa.VARCHAR(),
               nullable=True)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('model', 'author',
               existing_type=sa.VARCHAR(),
               nullable=False)
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/5dc1d24e7b7e_model_table_update.py`:

```py
"""Model Table Update

Revision ID: 5dc1d24e7b7e
Revises: e30d6bcba1dc
Create Date: 2025-01-21 04:53:03.489660

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql


# revision identifiers, used by Alembic.
revision: str = '5dc1d24e7b7e'
down_revision: Union[str, None] = 'e30d6bcba1dc'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('model', 'base_model', type_=postgresql.ARRAY(sa.String()), existing_type=sa.String(), nullable=True, postgresql_using="CASE WHEN base_model IS NULL THEN NULL ELSE array[base_model] END::character varying[]",)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('model', 'base_model', type_=sa.String(), existing_type=postgresql.ARRAY(sa.String()), nullable=True, postgresql_using="CASE WHEN base_model IS NULL THEN NULL ELSE base_model[1] END::character varying")
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/8611a4c07457_added_model_table.py`:

```py
"""Added Model Table

Revision ID: 8611a4c07457
Revises: 
Create Date: 2024-10-24 18:14:56.715599

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '8611a4c07457'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('model',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('description', sa.String(), nullable=True),
    sa.Column('tags', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('tasks', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('author', sa.String(), nullable=False),
    sa.Column('model_size', sa.BigInteger(), nullable=True),
    sa.Column('icon', sa.String(), nullable=False),
    sa.Column('github_url', sa.String(), nullable=True),
    sa.Column('huggingface_url', sa.String(), nullable=True),
    sa.Column('website_url', sa.String(), nullable=True),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('modality', sa.Enum('llm', 'image', 'embedding', 'text_to_speech', 'speech_to_text', name='modality_enum'), nullable=False),
    sa.Column('type', sa.Enum('pretrained', 'chat', 'codegen', name='model_type_enum'), nullable=False),
    sa.Column('source', sa.String(), nullable=False),
    sa.Column('provider_type', sa.Enum('cloud_model', 'hugging_face', 'url', 'disk', name='model_provider_type_enum'), nullable=False),
    sa.Column('uri', sa.String(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_model'))
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('model')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/5352d4677f70_added_status_to_security_scan_result.py`:

```py
"""Added Status To Security Scan Result

Revision ID: 5352d4677f70
Revises: 87708bcf2070
Create Date: 2024-12-11 15:02:32.697023

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from alembic_postgresql_enum import TableReference

# revision identifiers, used by Alembic.
revision: str = '5352d4677f70'
down_revision: Union[str, None] = '87708bcf2070'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values('public', 'model_security_scan_status_enum', ['low', 'medium', 'high', 'critical', 'safe'],
                        [TableReference(table_schema='public', table_name='model_security_scan_result', column_name='status')],
                        enum_values_to_rename=[])
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values('public', 'model_security_scan_status_enum', ['low', 'medium', 'high', 'critical'],
                        [TableReference(table_schema='public', table_name='model_security_scan_result', column_name='status')],
                        enum_values_to_rename=[])
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/7420487c5f37_add_model_licneses_table.py`:

```py
"""add model licneses table

Revision ID: 7420487c5f37
Revises: f91763325460
Create Date: 2024-11-04 18:01:06.341718

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '7420487c5f37'
down_revision: Union[str, None] = 'f91763325460'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    op.create_table(
        "model_licenses",
        sa.Column("id", sa.Uuid(), primary_key=True),
        sa.Column("name", sa.String(), nullable=True),
        sa.Column("path", sa.String(), nullable=True),
        sa.Column("model_id", sa.Uuid(), sa.ForeignKey("model.id"), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("modified_at", sa.DateTime(), nullable=False),
    )


def downgrade() -> None:
    op.drop_table('model_licenses')

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/f040fc25b476_updated_model_licence_paper_tables.py`:

```py
"""Updated Model, Licence, Paper Tables

Revision ID: f040fc25b476
Revises: 64e788fd946c
Create Date: 2024-12-06 08:35:11.658385

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'f040fc25b476'
down_revision: Union[str, None] = '64e788fd946c'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    sa.Enum('adapter', 'merge', 'quantized', 'finetune', name='base_model_relation_enum').create(op.get_bind())
    op.add_column('model', sa.Column('bud_verified', sa.Boolean(), server_default=sa.text('false'), nullable=False))
    op.add_column('model', sa.Column('scan_verified', sa.Boolean(), server_default=sa.text('false'), nullable=False))
    op.add_column('model', sa.Column('eval_verified', sa.Boolean(), server_default=sa.text('false'), nullable=False))
    op.add_column('model', sa.Column('strengths', postgresql.ARRAY(sa.String()), nullable=True))
    op.add_column('model', sa.Column('limitations', postgresql.ARRAY(sa.String()), nullable=True))
    op.add_column('model', sa.Column('languages', postgresql.ARRAY(sa.String()), nullable=True))
    op.add_column('model', sa.Column('use_cases', postgresql.ARRAY(sa.String()), nullable=True))
    op.add_column('model', sa.Column('minimum_requirements', postgresql.JSONB(astext_type=sa.Text()), nullable=True))
    op.add_column('model', sa.Column('examples', postgresql.JSONB(astext_type=sa.Text()), nullable=True))
    op.add_column('model', sa.Column('base_model', sa.String(), nullable=True))
    op.add_column('model', sa.Column('base_model_relation', postgresql.ENUM('adapter', 'merge', 'quantized', 'finetune', name='base_model_relation_enum', create_type=False), nullable=True))
    op.add_column('model', sa.Column('model_type', sa.String(), nullable=True))
    op.add_column('model', sa.Column('family', sa.String(), nullable=True))
    op.add_column('model', sa.Column('num_layers', sa.Integer(), nullable=True))
    op.add_column('model', sa.Column('hidden_size', sa.BigInteger(), nullable=True))
    op.add_column('model', sa.Column('context_length', sa.BigInteger(), nullable=True))
    op.add_column('model', sa.Column('torch_dtype', sa.String(), nullable=True))
    op.add_column('model', sa.Column('architecture', postgresql.JSONB(astext_type=sa.Text()), nullable=True))
    op.add_column('model_licenses', sa.Column('url', sa.String(), nullable=True))
    op.add_column('model_licenses', sa.Column('faqs', postgresql.JSONB(astext_type=sa.Text()), nullable=True))
    op.add_column('paper_published', sa.Column('authors', postgresql.ARRAY(sa.String()), nullable=True))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('paper_published', 'authors')
    op.drop_column('model_licenses', 'faqs')
    op.drop_column('model_licenses', 'url')
    op.drop_column('model', 'architecture')
    op.drop_column('model', 'torch_dtype')
    op.drop_column('model', 'context_length')
    op.drop_column('model', 'hidden_size')
    op.drop_column('model', 'num_layers')
    op.drop_column('model', 'family')
    op.drop_column('model', 'model_type')
    op.drop_column('model', 'base_model_relation')
    op.drop_column('model', 'base_model')
    op.drop_column('model', 'examples')
    op.drop_column('model', 'minimum_requirements')
    op.drop_column('model', 'use_cases')
    op.drop_column('model', 'languages')
    op.drop_column('model', 'limitations')
    op.drop_column('model', 'strengths')
    op.drop_column('model', 'eval_verified')
    op.drop_column('model', 'scan_verified')
    op.drop_column('model', 'bud_verified')
    sa.Enum('adapter', 'merge', 'quantized', 'finetune', name='base_model_relation_enum').drop(op.get_bind())
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/f759834112e2_removed_model_type.py`:

```py
"""Removed model type

Revision ID: f759834112e2
Revises: d337aa91475f
Create Date: 2024-11-01 05:27:40.482128

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'f759834112e2'
down_revision: Union[str, None] = 'd337aa91475f'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('cloud_model', 'type')
    op.drop_column('model', 'type')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('model', sa.Column('type', postgresql.ENUM('pretrained', 'chat', 'codegen', name='model_type_enum'), autoincrement=False, nullable=True))
    op.add_column('cloud_model', sa.Column('type', postgresql.ENUM('pretrained', 'chat', 'codegen', name='model_type_enum'), autoincrement=False, nullable=True))
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/1d94eee5d3f2_added_user_permission_model.py`:

```py
"""Added User, Permission Model

Revision ID: 1d94eee5d3f2
Revises: 8611a4c07457
Create Date: 2024-10-26 11:48:48.654992

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '1d94eee5d3f2'
down_revision: Union[str, None] = '8611a4c07457'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('user',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('auth_id', sa.Uuid(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('email', sa.String(), nullable=False),
    sa.Column('role', sa.Enum('admin', 'super_admin', 'developer', 'devops', 'tester', name='user_role_enum'), nullable=False),
    sa.Column('status', sa.Enum('active', 'inactive', 'invited', name='user_status_enum'), nullable=False),
    sa.Column('password', sa.String(), nullable=False),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('is_superuser', sa.Boolean(), nullable=False),
    sa.Column('is_reset_password', sa.Boolean(), nullable=False),
    sa.Column('color', sa.String(), nullable=False),
    sa.Column('first_login', sa.Boolean(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.Column('is_subscriber', sa.Boolean(), nullable=False),
    sa.Column('reset_password_attempt', sa.Integer(), nullable=False),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_user')),
    sa.UniqueConstraint('auth_id', name=op.f('uq_user_auth_id')),
    sa.UniqueConstraint('email', name=op.f('uq_user_email'))
    )
    op.create_table('permission',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('user_id', sa.Uuid(), nullable=False),
    sa.Column('auth_id', sa.Uuid(), nullable=False),
    sa.Column('scopes', sa.String(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['user_id'], ['user.id'], name=op.f('fk_permission_user_id_user'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_permission'))
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('permission')
    op.drop_table('user')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/6bd72384f8ba_update_endpoint_table.py`:

```py
"""Update endpoint table

Revision ID: 6bd72384f8ba
Revises: 1a20ca43ac69
Create Date: 2025-01-12 12:53:55.092197

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '6bd72384f8ba'
down_revision: Union[str, None] = '1a20ca43ac69'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('endpoint', sa.Column('total_replicas', sa.Integer(), nullable=False))
    op.add_column('endpoint', sa.Column('number_of_nodes', sa.Integer(), nullable=False))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('endpoint', 'number_of_nodes')
    op.drop_column('endpoint', 'total_replicas')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/68f026ad7360_updated_endpoint_status.py`:

```py
"""Updated Endpoint Status

Revision ID: 68f026ad7360
Revises: 4d5cd5449049
Create Date: 2024-12-16 17:19:08.344175

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from alembic_postgresql_enum import TableReference

# revision identifiers, used by Alembic.
revision: str = '68f026ad7360'
down_revision: Union[str, None] = '4d5cd5449049'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema='public',
        enum_name='endpoint_status_enum',
        new_values=['running', 'failure', 'deploying', 'unhealthy', 'deleting', 'deleted'],
        affected_columns=[TableReference(table_schema='public', table_name='endpoint', column_name='status')],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema='public',
        enum_name='endpoint_status_enum',
        new_values=['running', 'failure', 'deploying', 'unhealthy', 'deleting'],
        affected_columns=[TableReference(table_schema='public', table_name='endpoint', column_name='status')],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/04ef4a274216_added_status_to_project_table.py`:

```py
"""added status to project table

Revision ID: 04ef4a274216
Revises: cbbdd257bedd
Create Date: 2024-12-18 10:16:17.791188

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "04ef4a274216"
down_revision: Union[str, None] = "cbbdd257bedd"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    sa.Enum("active", "deleted", name="project_status_enum").create(op.get_bind())
    op.add_column(
        "project",
        sa.Column(
            "status",
            postgresql.ENUM("active", "deleted", name="project_status_enum", create_type=False),
            nullable=False,
        ),
    )
    op.drop_column("project", "is_active")
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column("project", sa.Column("is_active", sa.BOOLEAN(), autoincrement=False, nullable=False))
    op.drop_column("project", "status")
    sa.Enum("active", "deleted", name="project_status_enum").drop(op.get_bind())
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/73ed9f8ea677_added_workflow_step.py`:

```py
"""Added Workflow, Step

Revision ID: 73ed9f8ea677
Revises: e7ae8abbf8f3
Create Date: 2024-10-28 01:08:08.603863

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '73ed9f8ea677'
down_revision: Union[str, None] = 'e7ae8abbf8f3'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('workflow',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('created_by', sa.Uuid(), nullable=False),
    sa.Column('status', sa.Enum('in_progress', 'completed', 'failed', name='workflow_status_enum'), nullable=False),
    sa.Column('current_step', sa.Integer(), nullable=False),
    sa.Column('total_steps', sa.Integer(), nullable=False),
    sa.Column('reason', sa.String(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['created_by'], ['user.id'], name=op.f('fk_workflow_created_by_user')),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_workflow'))
    )
    op.create_index(op.f('ix_workflow_created_by'), 'workflow', ['created_by'], unique=False)
    op.create_table('workflow_step',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('workflow_id', sa.Uuid(), nullable=False),
    sa.Column('step_number', sa.Integer(), nullable=False),
    sa.Column('data', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['workflow_id'], ['workflow.id'], name=op.f('fk_workflow_step_workflow_id_workflow')),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_workflow_step'))
    )
    op.create_index(op.f('ix_workflow_step_workflow_id'), 'workflow_step', ['workflow_id'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_workflow_step_workflow_id'), table_name='workflow_step')
    op.drop_table('workflow_step')
    op.drop_index(op.f('ix_workflow_created_by'), table_name='workflow')
    op.drop_table('workflow')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/d337aa91475f_added_icon_model.py`:

```py
"""Added Icon Model

Revision ID: d337aa91475f
Revises: 97795f1e0340
Create Date: 2024-11-01 03:11:51.288688

"""

from typing import Sequence, Union

import sqlalchemy as sa
from alembic import op


# revision identifiers, used by Alembic.
revision: str = "d337aa91475f"
down_revision: Union[str, None] = "97795f1e0340"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "icon",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("name", sa.String(), nullable=False),
        sa.Column("file_path", sa.String(), nullable=False),
        sa.Column("category", sa.String(), nullable=False),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_icon")),
        sa.UniqueConstraint("file_path", name=op.f("uq_icon_file_path")),
    )
    op.create_index(op.f("ix_icon_category"), "icon", ["category"], unique=False)
    op.create_index(op.f("ix_icon_name"), "icon", ["name"], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f("ix_icon_name"), table_name="icon")
    op.drop_index(op.f("ix_icon_category"), table_name="icon")
    op.drop_table("icon")
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/dac83b8cf76f_added_cluster_project_endpoint_table.py`:

```py
"""Added cluster, project, endpoint table

Revision ID: dac83b8cf76f
Revises: 7420487c5f37
Create Date: 2024-11-12 06:57:41.104642

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'dac83b8cf76f'
down_revision: Union[str, None] = '7420487c5f37'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('cluster',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('type', sa.Enum('cpu', 'gpu', 'hpu', name='cluster_type_enum'), nullable=False),
    sa.Column('platform', sa.Enum('kubernetes', 'openshift', name='cluster_platform_enum'), nullable=False),
    sa.Column('configuration', sa.String(), nullable=False),
    sa.Column('host', sa.String(), nullable=False),
    sa.Column('status', sa.Enum('available', 'not_available', 'registering', 'error', name='cluster_status_enum'), nullable=False),
    sa.Column('total_workers', sa.Integer(), nullable=False),
    sa.Column('available_workers', sa.Integer(), nullable=False),
    sa.Column('used_workers', sa.Integer(), nullable=False),
    sa.Column('kube_nodes', sa.Integer(), nullable=False),
    sa.Column('kubernetes_metadata', sa.String(), nullable=True),
    sa.Column('status_sync_at', sa.DateTime(), nullable=True),
    sa.Column('threads_per_core', sa.Integer(), nullable=True),
    sa.Column('core_count', sa.Integer(), nullable=True),
    sa.Column('enable_master_node', sa.Boolean(), nullable=False),
    sa.Column('reason', sa.String(), nullable=True),
    sa.Column('created_by', sa.Uuid(), nullable=False),
    sa.Column('cluster_id', sa.Uuid(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['created_by'], ['user.id'], name=op.f('fk_cluster_created_by_user')),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_cluster'))
    )
    op.create_table('project',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('description', sa.String(), nullable=False),
    sa.Column('tags', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('icon', sa.String(), nullable=True),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('benchmark', sa.Boolean(), nullable=False),
    sa.Column('created_by', sa.Uuid(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['created_by'], ['user.id'], name=op.f('fk_project_created_by_user')),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_project'))
    )
    op.create_table('endpoint',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('project_id', sa.Uuid(), nullable=False),
    sa.Column('model_id', sa.Uuid(), nullable=False),
    sa.Column('cache_enabled', sa.Boolean(), nullable=False),
    sa.Column('cache_config', sa.String(), nullable=True),
    sa.Column('cluster_id', sa.Uuid(), nullable=False),
    sa.Column('url', sa.String(), nullable=False),
    sa.Column('namespace', sa.String(), nullable=False),
    sa.Column('replicas', sa.Integer(), nullable=False),
    sa.Column('created_by', sa.Uuid(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.Column('status', sa.Enum('running', 'failure', 'deploying', 'unhealthy', 'deleting', name='endpoint_status_enum'), nullable=False),
    sa.Column('status_sync_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['cluster_id'], ['cluster.id'], name=op.f('fk_endpoint_cluster_id_cluster'), ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['created_by'], ['user.id'], name=op.f('fk_endpoint_created_by_user')),
    sa.ForeignKeyConstraint(['model_id'], ['model.id'], name=op.f('fk_endpoint_model_id_model'), ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['project_id'], ['project.id'], name=op.f('fk_endpoint_project_id_project'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_endpoint'))
    )
    op.create_table('project_user_association',
    sa.Column('project_id', sa.Uuid(), nullable=False),
    sa.Column('user_id', sa.Uuid(), nullable=False),
    sa.ForeignKeyConstraint(['project_id'], ['project.id'], name=op.f('fk_project_user_association_project_id_project')),
    sa.ForeignKeyConstraint(['user_id'], ['user.id'], name=op.f('fk_project_user_association_user_id_user')),
    sa.PrimaryKeyConstraint('project_id', 'user_id', name=op.f('pk_project_user_association'))
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('project_user_association')
    op.drop_table('endpoint')
    op.drop_table('project')
    op.drop_table('cluster')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/4d5cd5449049_cluster_status_update.py`:

```py
"""Cluster Status Update

Revision ID: 4d5cd5449049
Revises: b15331009e3f
Create Date: 2024-12-16 03:20:23.743431

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from alembic_postgresql_enum import TableReference

# revision identifiers, used by Alembic.
revision: str = '4d5cd5449049'
down_revision: Union[str, None] = 'b15331009e3f'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema='public',
        enum_name='cluster_status_enum',
        new_values=['available', 'not_available', 'registering', 'error', 'deleting', 'deleted'],
        affected_columns=[TableReference(table_schema='public', table_name='cluster', column_name='status')],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema='public',
        enum_name='cluster_status_enum',
        new_values=['available', 'not_available', 'registering', 'error'],
        affected_columns=[TableReference(table_schema='public', table_name='cluster', column_name='status')],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/cbbdd257bedd_added_status_to_model_and_cloud_model_.py`:

```py
"""added status to model and cloud model table

Revision ID: cbbdd257bedd
Revises: 68f026ad7360
Create Date: 2024-12-17 09:16:24.851050

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "cbbdd257bedd"
down_revision: Union[str, None] = "68f026ad7360"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    sa.Enum("active", "deleted", name="cloud_model_status_enum").create(op.get_bind())
    sa.Enum("active", "deleted", name="model_status_enum").create(op.get_bind())
    op.add_column(
        "cloud_model",
        sa.Column(
            "status",
            postgresql.ENUM("active", "deleted", name="cloud_model_status_enum", create_type=False),
            nullable=False,
        ),
    )
    op.drop_column("cloud_model", "is_active")
    op.add_column(
        "model",
        sa.Column(
            "status", postgresql.ENUM("active", "deleted", name="model_status_enum", create_type=False), nullable=False
        ),
    )
    op.drop_column("model", "is_active")
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column("model", sa.Column("is_active", sa.BOOLEAN(), autoincrement=False, nullable=False))
    op.drop_column("model", "status")
    op.add_column("cloud_model", sa.Column("is_active", sa.BOOLEAN(), autoincrement=False, nullable=False))
    op.drop_column("cloud_model", "status")
    sa.Enum("active", "deleted", name="model_status_enum").drop(op.get_bind())
    sa.Enum("active", "deleted", name="cloud_model_status_enum").drop(op.get_bind())
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/6f3f373ce6b2_added_deploy_config_in_endpoint.py`:

```py
"""Added Deploy Config In Endpoint

Revision ID: 6f3f373ce6b2
Revises: 6bd72384f8ba
Create Date: 2025-01-19 03:47:19.766794

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '6f3f373ce6b2'
down_revision: Union[str, None] = '6bd72384f8ba'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('endpoint', sa.Column('deployment_config', postgresql.JSONB(astext_type=sa.Text()), nullable=True))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('endpoint', 'deployment_config')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/bcd1218837d9_modified_endpoint_table.py`:

```py
"""Modified endpoint table

Revision ID: bcd1218837d9
Revises: dac83b8cf76f
Create Date: 2024-11-13 14:16:14.672059

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'bcd1218837d9'
down_revision: Union[str, None] = 'dac83b8cf76f'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('endpoint', sa.Column('bud_cluster_id', sa.Uuid(), nullable=False))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('endpoint', 'bud_cluster_id')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/197143f29114_updated_workflow_table.py`:

```py
"""updated workflow table

Revision ID: 197143f29114
Revises: 5dc1d24e7b7e
Create Date: 2025-01-29 12:14:05.465867

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from alembic_postgresql_enum import TableReference
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "197143f29114"
down_revision: Union[str, None] = "5dc1d24e7b7e"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    sa.Enum("public", "internal", name="visibility_enum").create(op.get_bind())
    op.create_table(
        "credential",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("user_id", sa.Uuid(), nullable=False),
        sa.Column("key", sa.String(), nullable=False),
        sa.Column("project_id", sa.Uuid(), nullable=True),
        sa.Column("expiry", sa.DateTime(), nullable=True),
        sa.Column("max_budget", sa.Float(), nullable=True),
        sa.Column("model_budgets", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("last_used_at", sa.DateTime(), nullable=True),
        sa.Column("name", sa.String(), nullable=False),
        sa.Column("hashed_key", sa.String(), nullable=True),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("modified_at", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["project_id"], ["project.id"], name=op.f("fk_credential_project_id_project"), ondelete="CASCADE"
        ),
        sa.ForeignKeyConstraint(["user_id"], ["user.id"], name=op.f("fk_credential_user_id_user"), ondelete="CASCADE"),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_credential")),
        sa.UniqueConstraint("key", name=op.f("uq_credential_key")),
    )
    op.add_column(
        "workflow",
        sa.Column(
            "visibility",
            postgresql.ENUM("public", "internal", name="visibility_enum", create_type=False),
            nullable=False,
        ),
    )
    op.sync_enum_values(
        enum_schema="public",
        enum_name="workflow_type_enum",
        new_values=[
            "model_deployment",
            "model_security_scan",
            "cluster_onboarding",
            "cluster_deletion",
            "endpoint_deletion",
            "endpoint_worker_deletion",
            "cloud_model_onboarding",
            "local_model_onboarding",
            "add_worker_to_endpoint",
            "license_faq_fetch",
        ],
        affected_columns=[TableReference(table_schema="public", table_name="workflow", column_name="workflow_type")],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema="public",
        enum_name="workflow_type_enum",
        new_values=[
            "model_deployment",
            "model_security_scan",
            "cluster_onboarding",
            "cluster_deletion",
            "endpoint_deletion",
            "cloud_model_onboarding",
            "local_model_onboarding",
            "add_worker_to_endpoint",
        ],
        affected_columns=[TableReference(table_schema="public", table_name="workflow", column_name="workflow_type")],
        enum_values_to_rename=[],
    )
    op.drop_column("workflow", "visibility")
    op.drop_table("credential")
    sa.Enum("public", "internal", name="visibility_enum").drop(op.get_bind())
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/42e713425e99_updated_model_table.py`:

```py
"""Updated model table

Revision ID: 42e713425e99
Revises: acb041e05c53
Create Date: 2024-11-27 04:43:42.175004

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '42e713425e99'
down_revision: Union[str, None] = 'acb041e05c53'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('model', sa.Column('local_path', sa.String(), nullable=True))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('model', 'local_path')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/f91763325460_add_paper_published_table.py`:

```py
"""add paper published table

Revision ID: f91763325460
Revises: f759834112e2
Create Date: 2024-11-04 17:55:27.987795

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'f91763325460'
down_revision: Union[str, None] = 'f759834112e2'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    op.create_table(
        "paper_published",
        sa.Column("id", sa.Uuid(), primary_key=True),
        sa.Column("title", sa.String(), nullable=True),
        sa.Column("url", sa.String(), nullable=True),
        sa.Column("model_id", sa.Uuid(), sa.ForeignKey("model.id"), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("modified_at", sa.DateTime(), nullable=False),
    )


def downgrade() -> None:
    op.drop_table('paper_published')

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/e2d7e5f38d3c_token_model_added.py`:

```py
"""Token Model Added

Revision ID: e2d7e5f38d3c
Revises: 40a8d774f38f
Create Date: 2024-10-26 15:43:08.189614

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'e2d7e5f38d3c'
down_revision: Union[str, None] = '40a8d774f38f'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('token',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('auth_id', sa.Uuid(), nullable=False),
    sa.Column('secret_key', sa.String(), nullable=False),
    sa.Column('token_hash', sa.String(), nullable=False),
    sa.Column('type', sa.Enum('access', 'refresh', name='token_type_enum'), nullable=False),
    sa.Column('blacklisted', sa.Boolean(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_token'))
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('token')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/036fd90d4d6b_added_template_model.py`:

```py
"""added template model

Revision ID: 036fd90d4d6b
Revises: d5d05bae8c2c
Create Date: 2024-11-14 10:20:47.850438

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '036fd90d4d6b'
down_revision: Union[str, None] = 'd5d05bae8c2c'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('model_template',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('description', sa.String(), nullable=False),
    sa.Column('icon', sa.String(), nullable=False),
    sa.Column('template_type', sa.Enum('summarization', 'chat', 'question_answering', 'rag', 'code_gen', 'code_translation', 'entity_extraction', 'sentiment_analysis', 'document_analysis', name='template_type_enum'), nullable=False),
    sa.Column('avg_sequence_length', sa.Integer(), nullable=True),
    sa.Column('avg_context_length', sa.Integer(), nullable=True),
    sa.Column('per_session_tokens_per_sec', sa.ARRAY(sa.Integer()), nullable=True),
    sa.Column('ttft', sa.ARRAY(sa.Integer()), nullable=True),
    sa.Column('e2e_latency', sa.ARRAY(sa.Integer()), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_model_template')),
    sa.UniqueConstraint('template_type', name=op.f('uq_model_template_template_type'))
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('model_template')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/da9cd25f9dcc_removed_is_active_from_user_table.py`:

```py
"""removed is active from user table

Revision ID: da9cd25f9dcc
Revises: e3ac5a852b2c
Create Date: 2024-12-23 06:33:11.002691

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from alembic_postgresql_enum import TableReference

# revision identifiers, used by Alembic.
revision: str = "da9cd25f9dcc"
down_revision: Union[str, None] = "e3ac5a852b2c"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column("user", "is_active")
    op.sync_enum_values(
        enum_schema="public",
        enum_name="user_status_enum",
        new_values=["active", "deleted", "invited"],
        affected_columns=[TableReference(table_schema="public", table_name="user", column_name="status")],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema="public",
        enum_name="user_status_enum",
        new_values=["active", "inactive", "invited"],
        affected_columns=[TableReference(table_schema="public", table_name="user", column_name="status")],
        enum_values_to_rename=[],
    )
    op.add_column("user", sa.Column("is_active", sa.BOOLEAN(), autoincrement=False, nullable=False))
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/919ebf0fe343_added_proprietary_credential_table.py`:

```py
"""Added Proprietary Credential Table

Revision ID: 919ebf0fe343
Revises: 42e713425e99
Create Date: 2024-11-29 01:52:40.038500

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '919ebf0fe343'
down_revision: Union[str, None] = '42e713425e99'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    sa.Enum('nlp_cloud', 'deepinfra', 'anthropic', 'vertex_ai-vision-models', 'vertex_ai-ai21_models', 'cerebras', 'watsonx', 'predibase', 'volcengine', 'clarifai', 'baseten', 'sambanova', 'github', 'petals', 'replicate', 'vertex_ai-chat-models', 'azure_ai', 'perplexity', 'vertex_ai-code-text-models', 'vertex_ai-text-models', 'palm', 'cohere_chat', 'vertex_ai-embedding-models', 'text-completion-openai', 'groq', 'openai', 'aleph_alpha', 'sagemaker', 'databricks', 'fireworks_ai', 'vertex_ai-anthropic_models', 'vertex_ai-mistral_models', 'voyage', 'vertex_ai-language-models', 'anyscale', 'deepseek', 'vertex_ai-image-models', 'mistral', 'ollama', 'cohere', 'gemini', 'friendliai', 'vertex_ai-code-chat-models', 'azure', 'codestral', 'vertex_ai-llama_models', 'together_ai', 'cloudflare', 'ai21', 'openrouter', 'bedrock', 'text-completion-codestral', 'huggingface', name='proprietary_credential_type_enum').create(op.get_bind())
    op.create_table('proprietary_credential',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('user_id', sa.Uuid(), nullable=False),
    sa.Column('type', postgresql.ENUM('nlp_cloud', 'deepinfra', 'anthropic', 'vertex_ai-vision-models', 'vertex_ai-ai21_models', 'cerebras', 'watsonx', 'predibase', 'volcengine', 'clarifai', 'baseten', 'sambanova', 'github', 'petals', 'replicate', 'vertex_ai-chat-models', 'azure_ai', 'perplexity', 'vertex_ai-code-text-models', 'vertex_ai-text-models', 'palm', 'cohere_chat', 'vertex_ai-embedding-models', 'text-completion-openai', 'groq', 'openai', 'aleph_alpha', 'sagemaker', 'databricks', 'fireworks_ai', 'vertex_ai-anthropic_models', 'vertex_ai-mistral_models', 'voyage', 'vertex_ai-language-models', 'anyscale', 'deepseek', 'vertex_ai-image-models', 'mistral', 'ollama', 'cohere', 'gemini', 'friendliai', 'vertex_ai-code-chat-models', 'azure', 'codestral', 'vertex_ai-llama_models', 'together_ai', 'cloudflare', 'ai21', 'openrouter', 'bedrock', 'text-completion-codestral', 'huggingface', name='proprietary_credential_type_enum', create_type=False), nullable=False),
    sa.Column('other_provider_creds', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['user_id'], ['user.id'], name=op.f('fk_proprietary_credential_user_id_user'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_proprietary_credential'))
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('proprietary_credential')
    sa.Enum('nlp_cloud', 'deepinfra', 'anthropic', 'vertex_ai-vision-models', 'vertex_ai-ai21_models', 'cerebras', 'watsonx', 'predibase', 'volcengine', 'clarifai', 'baseten', 'sambanova', 'github', 'petals', 'replicate', 'vertex_ai-chat-models', 'azure_ai', 'perplexity', 'vertex_ai-code-text-models', 'vertex_ai-text-models', 'palm', 'cohere_chat', 'vertex_ai-embedding-models', 'text-completion-openai', 'groq', 'openai', 'aleph_alpha', 'sagemaker', 'databricks', 'fireworks_ai', 'vertex_ai-anthropic_models', 'vertex_ai-mistral_models', 'voyage', 'vertex_ai-language-models', 'anyscale', 'deepseek', 'vertex_ai-image-models', 'mistral', 'ollama', 'cohere', 'gemini', 'friendliai', 'vertex_ai-code-chat-models', 'azure', 'codestral', 'vertex_ai-llama_models', 'together_ai', 'cloudflare', 'ai21', 'openrouter', 'bedrock', 'text-completion-codestral', 'huggingface', name='proprietary_credential_type_enum').drop(op.get_bind())
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/b2a743972235_updated_model_table.py`:

```py
"""Updated model table

Revision ID: b2a743972235
Revises: 55602d4240bb
Create Date: 2024-11-20 08:10:03.255087

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'b2a743972235'
down_revision: Union[str, None] = '55602d4240bb'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('cloud_model', 'icon')
    op.add_column('model', sa.Column('provider_id', sa.Uuid(), nullable=True))
    op.alter_column('model', 'icon',
               existing_type=sa.VARCHAR(),
               nullable=True)
    op.create_foreign_key(op.f('fk_model_provider_id_provider'), 'model', 'provider', ['provider_id'], ['id'])
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint(op.f('fk_model_provider_id_provider'), 'model', type_='foreignkey')
    op.alter_column('model', 'icon',
               existing_type=sa.VARCHAR(),
               nullable=False)
    op.drop_column('model', 'provider_id')
    op.add_column('cloud_model', sa.Column('icon', sa.VARCHAR(), autoincrement=False, nullable=False))
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/97795f1e0340_added_cloud_model.py`:

```py
"""Added Cloud Model

Revision ID: 97795f1e0340
Revises: 73ed9f8ea677
Create Date: 2024-10-28 05:55:59.610125

"""
from typing import Sequence, Union

import sqlalchemy as sa
from alembic import op
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '97795f1e0340'
down_revision: Union[str, None] = '73ed9f8ea677'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('cloud_model',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('description', sa.String(), nullable=True),
    sa.Column('tags', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('tasks', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('author', sa.String(), nullable=True),
    sa.Column('model_size', sa.BigInteger(), nullable=True),
    sa.Column('icon', sa.String(), nullable=False),
    sa.Column('github_url', sa.String(), nullable=True),
    sa.Column('huggingface_url', sa.String(), nullable=True),
    sa.Column('website_url', sa.String(), nullable=True),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('modality', postgresql.ENUM('llm', 'image', 'embedding', 'text_to_speech', 'speech_to_text', name='modality_enum', create_type=False), nullable=False),
    sa.Column('type', postgresql.ENUM('pretrained', 'chat', 'codegen', name='model_type_enum', create_type=False), nullable=True),
    sa.Column('source', sa.String(), nullable=False),
    sa.Column('provider_type', postgresql.ENUM('cloud_model', 'hugging_face', 'url', 'disk', name='model_provider_type_enum', create_type=False), nullable=False),
    sa.Column('uri', sa.String(), nullable=False),
    sa.Column('provider_id', sa.Uuid(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('modified_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['provider_id'], ['provider.id'], name=op.f('fk_cloud_model_provider_id_provider')),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_cloud_model'))
    )
    op.alter_column('model', 'type',
               existing_type=postgresql.ENUM('pretrained', 'chat', 'codegen', name='model_type_enum', create_type=False),
               nullable=True)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('model', 'type',
               existing_type=postgresql.ENUM('pretrained', 'chat', 'codegen', name='model_type_enum', create_type=False),
               nullable=False)
    op.drop_table('cloud_model')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/55602d4240bb_modified_model_template.py`:

```py
"""Modified Model Template

Revision ID: 55602d4240bb
Revises: 2ce2068d3257
Create Date: 2024-11-19 10:14:11.127218

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from alembic_postgresql_enum import TableReference

# revision identifiers, used by Alembic.
revision: str = '55602d4240bb'
down_revision: Union[str, None] = '2ce2068d3257'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    sa.Enum('pretrained', 'chat', 'codegen', name='model_type_enum').drop(op.get_bind())
    sa.Enum('cpu', 'gpu', 'hpu', name='cluster_type_enum').drop(op.get_bind())
    sa.Enum('kubernetes', 'openshift', name='cluster_platform_enum').drop(op.get_bind())
    op.sync_enum_values('public', 'template_type_enum', ['summarization', 'chat', 'question_answering', 'rag', 'code_gen', 'code_translation', 'entity_extraction', 'sentiment_analysis', 'document_analysis', 'other'],
                        [TableReference(table_schema='public', table_name='model_template', column_name='template_type')],
                        enum_values_to_rename=[])
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values('public', 'template_type_enum', ['summarization', 'chat', 'question_answering', 'rag', 'code_gen', 'code_translation', 'entity_extraction', 'sentiment_analysis', 'document_analysis'],
                        [TableReference(table_schema='public', table_name='model_template', column_name='template_type')],
                        enum_values_to_rename=[])
    sa.Enum('kubernetes', 'openshift', name='cluster_platform_enum').create(op.get_bind())
    sa.Enum('cpu', 'gpu', 'hpu', name='cluster_type_enum').create(op.get_bind())
    sa.Enum('pretrained', 'chat', 'codegen', name='model_type_enum').create(op.get_bind())
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/e30d6bcba1dc_added_new_workflow_type.py`:

```py
"""Added New Workflow Type

Revision ID: e30d6bcba1dc
Revises: 6f3f373ce6b2
Create Date: 2025-01-20 05:12:05.368295

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from alembic_postgresql_enum import TableReference

# revision identifiers, used by Alembic.
revision: str = 'e30d6bcba1dc'
down_revision: Union[str, None] = '6f3f373ce6b2'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema='public',
        enum_name='workflow_type_enum',
        new_values=['model_deployment', 'model_security_scan', 'cluster_onboarding', 'cluster_deletion', 'endpoint_deletion', 'cloud_model_onboarding', 'local_model_onboarding', 'add_worker_to_endpoint'],
        affected_columns=[TableReference(table_schema='public', table_name='workflow', column_name='workflow_type')],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema='public',
        enum_name='workflow_type_enum',
        new_values=['model_deployment', 'model_security_scan', 'cluster_onboarding', 'cluster_deletion', 'endpoint_deletion', 'cloud_model_onboarding', 'local_model_onboarding'],
        affected_columns=[TableReference(table_schema='public', table_name='workflow', column_name='workflow_type')],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/892c31d1e086_removed_is_active_from_cluster_table.py`:

```py
"""removed is active from cluster table

Revision ID: 892c31d1e086
Revises: b2c9ac9ba65b
Create Date: 2024-12-19 09:58:34.806743

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = "892c31d1e086"
down_revision: Union[str, None] = "b2c9ac9ba65b"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column("cluster", "is_active")
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column("cluster", sa.Column("is_active", sa.BOOLEAN(), autoincrement=False, nullable=False))
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/e7ae8abbf8f3_added_provider_table.py`:

```py
"""Added Provider Table

Revision ID: e7ae8abbf8f3
Revises: ffb55b8f01fc
Create Date: 2024-10-27 18:17:20.635035

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'e7ae8abbf8f3'
down_revision: Union[str, None] = 'ffb55b8f01fc'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('provider',
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('type', sa.Enum('nlp_cloud', 'deepinfra', 'anthropic', 'vertex_ai-vision-models', 'vertex_ai-ai21_models', 'cerebras', 'watsonx', 'predibase', 'volcengine', 'clarifai', 'baseten', 'sambanova', 'github', 'petals', 'replicate', 'vertex_ai-chat-models', 'azure_ai', 'perplexity', 'vertex_ai-code-text-models', 'vertex_ai-text-models', 'palm', 'cohere_chat', 'vertex_ai-embedding-models', 'text-completion-openai', 'groq', 'openai', 'aleph_alpha', 'sagemaker', 'databricks', 'fireworks_ai', 'vertex_ai-anthropic_models', 'vertex_ai-mistral_models', 'voyage', 'vertex_ai-language-models', 'anyscale', 'deepseek', 'vertex_ai-image-models', 'mistral', 'ollama', 'cohere', 'gemini', 'friendliai', 'vertex_ai-code-chat-models', 'azure', 'codestral', 'vertex_ai-llama_models', 'together_ai', 'cloudflare', 'ai21', 'openrouter', 'bedrock', 'text-completion-codestral', 'huggingface', name='credential_type_enum'), nullable=False),
    sa.Column('description', sa.String(), nullable=True),
    sa.Column('icon', sa.String(), nullable=False),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_provider'))
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('provider')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/375eb22cb3af_updated_endpoint_table.py`:

```py
"""Updated Endpoint Table

Revision ID: 375eb22cb3af
Revises: af5cc669f51e
Create Date: 2025-01-04 10:23:34.365587

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from alembic_postgresql_enum import TableReference

# revision identifiers, used by Alembic.
revision: str = '375eb22cb3af'
down_revision: Union[str, None] = 'af5cc669f51e'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema='public',
        enum_name='endpoint_status_enum',
        new_values=['running', 'failure', 'deploying', 'unhealthy', 'deleting', 'deleted', 'pending'],
        affected_columns=[TableReference(table_schema='public', table_name='endpoint', column_name='status')],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema='public',
        enum_name='endpoint_status_enum',
        new_values=['running', 'failure', 'deploying', 'unhealthy', 'deleting', 'deleted'],
        affected_columns=[TableReference(table_schema='public', table_name='endpoint', column_name='status')],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/48acf0c963d2_added_active_replicas_to_endpoint_table.py`:

```py
"""added active replicas to endpoint table

Revision ID: 48acf0c963d2
Revises: 197143f29114
Create Date: 2025-01-28 13:31:21.812798

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from alembic_postgresql_enum import TableReference

# revision identifiers, used by Alembic.
revision: str = "48acf0c963d2"
down_revision: Union[str, None] = "197143f29114"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column("endpoint", sa.Column("active_replicas", sa.Integer(), nullable=False))
    op.sync_enum_values(
        enum_schema="public",
        enum_name="workflow_type_enum",
        new_values=[
            "model_deployment",
            "model_security_scan",
            "cluster_onboarding",
            "cluster_deletion",
            "endpoint_deletion",
            "endpoint_worker_deletion",
            "cloud_model_onboarding",
            "local_model_onboarding",
            "add_worker_to_endpoint",
        ],
        affected_columns=[TableReference(table_schema="public", table_name="workflow", column_name="workflow_type")],
        enum_values_to_rename=[],
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.sync_enum_values(
        enum_schema="public",
        enum_name="workflow_type_enum",
        new_values=[
            "model_deployment",
            "model_security_scan",
            "cluster_onboarding",
            "cluster_deletion",
            "endpoint_deletion",
            "cloud_model_onboarding",
            "local_model_onboarding",
        ],
        affected_columns=[TableReference(table_schema="public", table_name="workflow", column_name="workflow_type")],
        enum_values_to_rename=[],
    )
    op.drop_column("endpoint", "active_replicas")
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/2ce2068d3257_updated_cluster_table.py`:

```py
"""Updated cluster table

Revision ID: 2ce2068d3257
Revises: 036fd90d4d6b
Create Date: 2024-11-18 03:58:47.083974

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '2ce2068d3257'
down_revision: Union[str, None] = '036fd90d4d6b'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('cluster', sa.Column('ingress_url', sa.String(), nullable=False))
    op.add_column('cluster', sa.Column('cpu_count', sa.Integer(), nullable=False))
    op.add_column('cluster', sa.Column('gpu_count', sa.Integer(), nullable=False))
    op.add_column('cluster', sa.Column('hpu_count', sa.Integer(), nullable=False))
    op.add_column('cluster', sa.Column('cpu_total_workers', sa.Integer(), nullable=False))
    op.add_column('cluster', sa.Column('cpu_available_workers', sa.Integer(), nullable=False))
    op.add_column('cluster', sa.Column('gpu_total_workers', sa.Integer(), nullable=False))
    op.add_column('cluster', sa.Column('gpu_available_workers', sa.Integer(), nullable=False))
    op.add_column('cluster', sa.Column('hpu_total_workers', sa.Integer(), nullable=False))
    op.add_column('cluster', sa.Column('hpu_available_workers', sa.Integer(), nullable=False))
    op.drop_column('cluster', 'type')
    op.drop_column('cluster', 'configuration')
    op.drop_column('cluster', 'total_workers')
    op.drop_column('cluster', 'core_count')
    op.drop_column('cluster', 'kube_nodes')
    op.drop_column('cluster', 'host')
    op.drop_column('cluster', 'used_workers')
    op.drop_column('cluster', 'available_workers')
    op.drop_column('cluster', 'kubernetes_metadata')
    op.drop_column('cluster', 'enable_master_node')
    op.drop_column('cluster', 'platform')
    op.drop_column('cluster', 'threads_per_core')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('cluster', sa.Column('threads_per_core', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('cluster', sa.Column('platform', postgresql.ENUM('kubernetes', 'openshift', name='cluster_platform_enum'), autoincrement=False, nullable=False))
    op.add_column('cluster', sa.Column('enable_master_node', sa.BOOLEAN(), autoincrement=False, nullable=False))
    op.add_column('cluster', sa.Column('kubernetes_metadata', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.add_column('cluster', sa.Column('available_workers', sa.INTEGER(), autoincrement=False, nullable=False))
    op.add_column('cluster', sa.Column('used_workers', sa.INTEGER(), autoincrement=False, nullable=False))
    op.add_column('cluster', sa.Column('host', sa.VARCHAR(), autoincrement=False, nullable=False))
    op.add_column('cluster', sa.Column('kube_nodes', sa.INTEGER(), autoincrement=False, nullable=False))
    op.add_column('cluster', sa.Column('core_count', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('cluster', sa.Column('total_workers', sa.INTEGER(), autoincrement=False, nullable=False))
    op.add_column('cluster', sa.Column('configuration', sa.VARCHAR(), autoincrement=False, nullable=False))
    op.add_column('cluster', sa.Column('type', postgresql.ENUM('cpu', 'gpu', 'hpu', name='cluster_type_enum'), autoincrement=False, nullable=False))
    op.drop_column('cluster', 'hpu_available_workers')
    op.drop_column('cluster', 'hpu_total_workers')
    op.drop_column('cluster', 'gpu_available_workers')
    op.drop_column('cluster', 'gpu_total_workers')
    op.drop_column('cluster', 'cpu_available_workers')
    op.drop_column('cluster', 'cpu_total_workers')
    op.drop_column('cluster', 'hpu_count')
    op.drop_column('cluster', 'gpu_count')
    op.drop_column('cluster', 'cpu_count')
    op.drop_column('cluster', 'ingress_url')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/d5d05bae8c2c_added_icon_to_cluster_table.py`:

```py
"""added icon to cluster table

Revision ID: d5d05bae8c2c
Revises: bcd1218837d9
Create Date: 2024-11-13 16:52:43.088313

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'd5d05bae8c2c'
down_revision: Union[str, None] = 'bcd1218837d9'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('cluster', sa.Column('icon', sa.String(), nullable=False))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('cluster', 'icon')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/b15331009e3f_remove_replica_from_endpoint_table.py`:

```py
"""Remove replica from endpoint table

Revision ID: b15331009e3f
Revises: 5352d4677f70
Create Date: 2024-12-13 15:30:28.338183

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'b15331009e3f'
down_revision: Union[str, None] = '5352d4677f70'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('endpoint', 'replicas')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('endpoint', sa.Column('replicas', sa.INTEGER(), autoincrement=False, nullable=False))
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/87708bcf2070_added_status_in_security_scan_table.py`:

```py
"""Added Status In Security Scan Table

Revision ID: 87708bcf2070
Revises: 61d0ff8cf1cd
Create Date: 2024-12-10 08:31:19.223251

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '87708bcf2070'
down_revision: Union[str, None] = '61d0ff8cf1cd'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    sa.Enum('low', 'medium', 'high', 'critical', name='model_security_scan_status_enum').create(op.get_bind())
    op.add_column('model_security_scan_result', sa.Column('status', postgresql.ENUM('low', 'medium', 'high', 'critical', name='model_security_scan_status_enum', create_type=False), nullable=False))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('model_security_scan_result', 'status')
    sa.Enum('low', 'medium', 'high', 'critical', name='model_security_scan_status_enum').drop(op.get_bind())
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/0c7fdc4b7072_removed_is_active_from_endpint_table.py`:

```py
"""removed is active from endpint table

Revision ID: 0c7fdc4b7072
Revises: 892c31d1e086
Create Date: 2024-12-23 11:55:08.540869

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = "0c7fdc4b7072"
down_revision: Union[str, None] = "892c31d1e086"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column("endpoint", "is_active")
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column("endpoint", sa.Column("is_active", sa.BOOLEAN(), autoincrement=False, nullable=False))
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/1b33efcd5611_tag_added_in_worflow.py`:

```py
"""Tag added in worflow

Revision ID: 1b33efcd5611
Revises: da9cd25f9dcc
Create Date: 2025-01-01 05:48:11.955993

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '1b33efcd5611'
down_revision: Union[str, None] = 'da9cd25f9dcc'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('workflow', sa.Column('tag', sa.String(), nullable=True))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('workflow', 'tag')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/b2c9ac9ba65b_modified_workflow_table.py`:

```py
"""Modified Workflow Table

Revision ID: b2c9ac9ba65b
Revises: d92d92c1c90c
Create Date: 2024-12-19 13:30:12.338808

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'b2c9ac9ba65b'
down_revision: Union[str, None] = 'd92d92c1c90c'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    sa.Enum('model_deployment', 'model_security_scan', 'cluster_onboarding', 'cluster_deletion', 'endpoint_deletion', 'cloud_model_onboarding', 'local_model_onboarding', name='workflow_type_enum').create(op.get_bind())
    op.add_column('workflow', sa.Column('workflow_type', postgresql.ENUM('model_deployment', 'model_security_scan', 'cluster_onboarding', 'cluster_deletion', 'endpoint_deletion', 'cloud_model_onboarding', 'local_model_onboarding', name='workflow_type_enum', create_type=False), nullable=False))
    op.add_column('workflow', sa.Column('title', sa.String(), nullable=True))
    op.add_column('workflow', sa.Column('icon', sa.String(), nullable=True))
    op.add_column('workflow', sa.Column('progress', postgresql.JSONB(astext_type=sa.Text()), nullable=True))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('workflow', 'progress')
    op.drop_column('workflow', 'icon')
    op.drop_column('workflow', 'title')
    op.drop_column('workflow', 'workflow_type')
    sa.Enum('model_deployment', 'model_security_scan', 'cluster_onboarding', 'cluster_deletion', 'endpoint_deletion', 'cloud_model_onboarding', 'local_model_onboarding', name='workflow_type_enum').drop(op.get_bind())
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/versions/acb041e05c53_updated_cloud_model.py`:

```py
"""Updated cloud model

Revision ID: acb041e05c53
Revises: b2a743972235
Create Date: 2024-11-21 05:24:36.533236

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'acb041e05c53'
down_revision: Union[str, None] = 'b2a743972235'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('cloud_model', sa.Column('is_present_in_model', sa.Boolean(), nullable=False))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('cloud_model', 'is_present_in_model')
    # ### end Alembic commands ###

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/migrations/README`:

```
Generic single-database configuration.
```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/cluster_ops/services.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The cluster ops services. Contains business logic for cluster ops."""

import json
import tempfile
from datetime import datetime, timezone
from typing import Any, Dict, List, Tuple
from uuid import UUID

import aiohttp
import yaml
from fastapi import UploadFile, status

from budapp.commons import logging
from budapp.commons.async_utils import check_file_extension
from budapp.commons.config import app_settings
from budapp.commons.db_utils import SessionMixin
from budapp.commons.exceptions import ClientException
from budapp.core.schemas import NotificationPayload
from budapp.endpoint_ops.crud import EndpointDataManager
from budapp.endpoint_ops.models import Endpoint as EndpointModel
from budapp.workflow_ops.crud import WorkflowDataManager, WorkflowStepDataManager
from budapp.workflow_ops.models import Workflow as WorkflowModel
from budapp.workflow_ops.models import WorkflowStep as WorkflowStepModel
from budapp.workflow_ops.services import WorkflowService, WorkflowStepService
from budapp.cluster_ops.utils import ClusterMetricsFetcher
from budapp.cluster_ops.schemas import ClusterMetricsResponse

from ..commons.constants import (
    APP_ICONS,
    BUD_INTERNAL_WORKFLOW,
    BudServeWorkflowStepEventName,
    ClusterStatusEnum,
    EndpointStatusEnum,
    ModelStatusEnum,
    NotificationTypeEnum,
    WorkflowStatusEnum,
    WorkflowTypeEnum,
)
from ..commons.helpers import get_hardware_types
from ..core.schemas import NotificationResult
from ..endpoint_ops.schemas import WorkerInfoFilter
from ..model_ops.crud import ModelDataManager
from ..model_ops.models import Model
from ..model_ops.services import ModelServiceUtil
from ..shared.notification_service import BudNotifyService, NotificationBuilder
from ..workflow_ops.schemas import WorkflowUtilCreate
from .crud import ClusterDataManager
from .models import Cluster as ClusterModel
from .schemas import (
    ClusterCreate,
    ClusterEndpointResponse,
    ClusterPaginatedResponse,
    ClusterResourcesInfo,
    ClusterResponse,
    CreateClusterWorkflowRequest,
    CreateClusterWorkflowSteps,
    MetricTypeEnum,
    ClusterDetailResponse,
)


logger = logging.get_logger(__name__)


class ClusterService(SessionMixin):
    """Cluster service."""

    async def get_all_active_clusters(
        self,
        offset: int = 0,
        limit: int = 10,
        filters: Dict = {},
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[ClusterPaginatedResponse], int]:
        """Get all active clusters."""
        results, count = await ClusterDataManager(self.session).get_all_clusters(
            offset, limit, filters, order_by, search
        )
        updated_clusters = []
        for result in results:
            cluster, endpoints_count = result
            updated_cluster = ClusterPaginatedResponse(
                id=cluster.id,
                cluster_id=cluster.cluster_id,
                name=cluster.name,
                icon=cluster.icon,
                ingress_url=cluster.ingress_url,
                created_at=cluster.created_at,
                modified_at=cluster.modified_at,
                endpoint_count=endpoints_count,
                status=cluster.status,
                gpu_count=cluster.gpu_count,
                cpu_count=cluster.cpu_count,
                hpu_count=cluster.hpu_count,
                cpu_total_workers=cluster.cpu_total_workers,
                cpu_available_workers=cluster.cpu_available_workers,
                gpu_total_workers=cluster.gpu_total_workers,
                gpu_available_workers=cluster.gpu_available_workers,
                hpu_total_workers=cluster.hpu_total_workers,
                hpu_available_workers=cluster.hpu_available_workers,
            )
            updated_clusters.append(updated_cluster)

        return updated_clusters, count

    async def create_cluster_workflow(
        self,
        current_user_id: UUID,
        request: CreateClusterWorkflowRequest,
        configuration_file: UploadFile | None = None,
    ) -> None:
        """Create a cluster workflow.

        Args:
            current_user_id: The current user id.
            request: The request to create the cluster workflow with.
            configuration_file: The configuration file to create the cluster with.

        Raises:
            ClientException: If the cluster already exists.
        """
        # Get request data
        workflow_id = request.workflow_id
        workflow_total_steps = request.workflow_total_steps
        step_number = request.step_number
        cluster_name = request.name
        cluster_icon = request.icon
        ingress_url = request.ingress_url
        trigger_workflow = request.trigger_workflow

        current_step_number = step_number

        # Retrieve or create workflow
        workflow_create = WorkflowUtilCreate(
            workflow_type=WorkflowTypeEnum.CLUSTER_ONBOARDING,
            title="Cluster Onboarding",
            total_steps=workflow_total_steps,
            icon=APP_ICONS["general"]["cluster_mono"],
            tag="Cluster Onboarding",
        )
        db_workflow = await WorkflowService(self.session).retrieve_or_create_workflow(
            workflow_id, workflow_create, current_user_id
        )

        # Validate the configuration file
        if configuration_file:
            if not await check_file_extension(configuration_file.filename, ["yaml", "yml"]):
                logger.error("Invalid file extension for configuration file")
                raise ClientException("Invalid file extension for configuration file")

            try:
                configuration_yaml = yaml.safe_load(configuration_file.file)
            except yaml.YAMLError as e:
                logger.exception(f"Invalid cluster configuration yaml file found: {e}")
                raise ClientException("Invalid cluster configuration yaml file found") from e

        if cluster_name:
            # Check duplicate cluster name
            db_cluster = await ClusterDataManager(self.session).retrieve_by_fields(
                ClusterModel,
                fields={"name": cluster_name},
                exclude_fields={"status": ClusterStatusEnum.DELETED},
                missing_ok=True,
                case_sensitive=False,
            )
            if db_cluster:
                raise ClientException("Cluster name already exists")

            # Update title on workflow
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"title": cluster_name},
            )

        if cluster_icon:
            # Update icon on workflow
            # NOTE: Multiple queries because of considering future orchestration upgrade
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"icon": cluster_icon},
            )

        # Prepare workflow step data
        workflow_step_data = CreateClusterWorkflowSteps(
            name=cluster_name,
            icon=cluster_icon,
            ingress_url=ingress_url,
            configuration_yaml=configuration_yaml if configuration_file else None,
        ).model_dump(exclude_none=True, exclude_unset=True, mode="json")

        # Get workflow steps
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": db_workflow.id}
        )

        # For avoiding another db call for record retrieval, storing db object while iterating over db_workflow_steps
        db_current_workflow_step = None

        if db_workflow_steps:
            for db_step in db_workflow_steps:
                # Get current workflow step
                if db_step.step_number == current_step_number:
                    db_current_workflow_step = db_step

        if db_current_workflow_step:
            logger.debug(f"Workflow {db_workflow.id} step {current_step_number} already exists")

            # Update workflow step data in db
            db_workflow_step = await WorkflowStepDataManager(self.session).update_by_fields(
                db_current_workflow_step,
                {"data": workflow_step_data},
            )
            logger.debug(f"Workflow {db_workflow.id} step {current_step_number} updated")
        else:
            logger.debug(f"Creating workflow step {current_step_number} for workflow {db_workflow.id}")

            # Insert step details in db
            db_workflow_step = await WorkflowStepDataManager(self.session).insert_one(
                WorkflowStepModel(
                    workflow_id=db_workflow.id,
                    step_number=current_step_number,
                    data=workflow_step_data,
                )
            )

        # Update workflow current step as the highest step_number
        db_max_workflow_step_number = max(step.step_number for step in db_workflow_steps) if db_workflow_steps else 0
        workflow_current_step = max(current_step_number, db_max_workflow_step_number)
        logger.info(f"The current step of workflow {db_workflow.id} is {workflow_current_step}")

        # Update workflow step data in db
        db_workflow = await WorkflowDataManager(self.session).update_by_fields(
            db_workflow,
            {"current_step": workflow_current_step},
        )

        # Execute workflow
        # Create next step if workflow is triggered
        if trigger_workflow:
            logger.debug("Workflow triggered")

            # TODO: Currently querying workflow steps again by ordering steps in ascending order
            # To ensure the latest step update is fetched, Consider excluding it later
            db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
                {"workflow_id": db_workflow.id}
            )

            # Define the keys required for cluster creation
            keys_of_interest = [
                "name",
                "icon",
                "ingress_url",
                "configuration_yaml",
            ]

            # from workflow steps extract necessary information
            required_data = {}
            for db_workflow_step in db_workflow_steps:
                for key in keys_of_interest:
                    if key in db_workflow_step.data:
                        required_data[key] = db_workflow_step.data[key]

            # Check if all required keys are present
            required_keys = ["name", "icon", "ingress_url", "configuration_yaml"]
            missing_keys = [key for key in required_keys if key not in required_data]
            if missing_keys:
                raise ClientException(f"Missing required data: {', '.join(missing_keys)}")

            # Check duplicate cluster name
            db_cluster = await ClusterDataManager(self.session).retrieve_by_fields(
                ClusterModel,
                {"name": required_data["name"]},
                exclude_fields={"status": ClusterStatusEnum.DELETED},
                missing_ok=True,
                case_sensitive=False,
            )
            if db_cluster:
                raise ClientException("Cluster name already exists")

            # Trigger create cluster workflow by step
            await self._execute_create_cluster_workflow(
                required_data, current_user_id, db_workflow, current_step_number
            )
            logger.debug("Successfully executed create cluster workflow")

        return db_workflow

    async def _execute_create_cluster_workflow(
        self, data: Dict[str, Any], current_user_id: UUID, db_workflow: WorkflowModel, current_step_number: int
    ) -> None:
        """Execute create cluster workflow."""
        # Create cluster in bud_cluster app
        bud_cluster_response = await self._perform_create_cluster_request(data, db_workflow.id, current_user_id)

        # Add payload dict to response
        for step in bud_cluster_response["steps"]:
            step["payload"] = {}

        create_cluster_events = {BudServeWorkflowStepEventName.CREATE_CLUSTER_EVENTS.value: bud_cluster_response}

        # Increment step number of workflow and workflow step
        current_step_number = current_step_number + 1
        workflow_current_step = current_step_number

        # Update or create next workflow step
        db_workflow_step = await WorkflowStepService(self.session).create_or_update_next_workflow_step(
            db_workflow.id, current_step_number, create_cluster_events
        )
        logger.debug(f"Created workflow step {db_workflow_step.id} for storing create cluster events")

        # Update progress in workflow
        bud_cluster_response["progress_type"] = BudServeWorkflowStepEventName.CREATE_CLUSTER_EVENTS.value
        db_workflow = await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"progress": bud_cluster_response, "current_step": workflow_current_step}
        )
        logger.debug(f"Updated progress, current step in workflow {db_workflow.id}")

    async def _perform_create_cluster_request(
        self, data: Dict[str, str], workflow_id: UUID, current_user_id: UUID
    ) -> dict:
        """Make async POST request to create cluster to budcluster app.

        Args:
            data (Dict[str, str]): Data to be sent in the request.

        Returns:
            dict: Response from the server.

        Raises:
            aiohttp.ClientError: If the request fails.
        """
        create_cluster_endpoint = (
            f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_cluster_app_id}/method/cluster"
        )
        cluster_create_request = {
            "enable_master_node": True,
            "name": data["name"],
            "ingress_url": data["ingress_url"],
            "notification_metadata": {
                "name": BUD_INTERNAL_WORKFLOW,
                "subscriber_ids": str(current_user_id),
                "workflow_id": str(workflow_id),
            },
            "source_topic": f"{app_settings.source_topic}",
        }

        # Create temporary yaml file
        with tempfile.NamedTemporaryFile(delete=False, suffix=".yaml", mode="w") as temp_file:
            try:
                # Write configuration yaml to temporary yaml file
                yaml.safe_dump(data["configuration_yaml"], temp_file)

                logger.debug(f"cluster_create_request: {cluster_create_request}")
                # Perform the request as a form data
                async with aiohttp.ClientSession() as session:
                    form = aiohttp.FormData()
                    form.add_field("cluster_create_request", json.dumps(cluster_create_request))

                    # Open the file for reading after writing is complete
                    with open(temp_file.name, "rb") as config_file:
                        form.add_field("configuration", config_file, filename=temp_file.name)
                        try:
                            async with session.post(create_cluster_endpoint, data=form) as response:
                                response_data = await response.json()
                                logger.debug(f"Response from budcluster service: {response_data}")

                                if response.status != 200:
                                    error_message = response_data.get("message", "Failed to create cluster")
                                    logger.error(f"Failed to create cluster with external service: {error_message}")
                                    raise ClientException(error_message)

                                logger.debug("Successfully created cluster with budcluster service")
                                return response_data

                        except ClientException as e:
                            raise e

                        except Exception as e:
                            logger.error(f"Failed to make request to budcluster service: {e}")
                            raise ClientException("Unable to create cluster with external service") from e

            except yaml.YAMLError as e:
                logger.error(f"Failed to process YAML configuration: {e}")
                raise ClientException("Invalid YAML configuration") from e

            except IOError as e:
                logger.error(f"Failed to write temporary file: {e}")
                raise ClientException("Failed to process configuration file") from e

            except ClientException as e:
                raise e

            except Exception as e:
                logger.exception(f"Unexpected error during cluster creation request {e}")
                raise ClientException("Unexpected error during cluster creation") from e

            finally:
                # Delete the temporary file
                temp_file.close()

    async def create_cluster_from_notification_event(self, payload: NotificationPayload) -> None:
        """Create a cluster in database.

        Args:
            payload: The payload to create the cluster with.

        Raises:
            ClientException: If the cluster already exists.
        """
        logger.debug("Received event for creating cluster")

        # Get workflow and steps
        workflow_id = payload.workflow_id
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        # Define the keys required for endpoint creation
        keys_of_interest = [
            "name",
            "icon",
            "ingress_url",
        ]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]

        logger.debug("Collected required data from workflow steps")

        # Check duplicate cluster name
        db_cluster = await ClusterDataManager(self.session).retrieve_by_fields(
            ClusterModel,
            {"name": required_data["name"]},
            exclude_fields={"status": ClusterStatusEnum.DELETED},
            missing_ok=True,
            case_sensitive=False,
        )

        if db_cluster:
            logger.error(f"Cluster {required_data['name']} already exists")
            raise ClientException(f"Cluster {required_data['name']} already exists")

        # Get cluster resources from event
        cluster_resources = await self._calculate_cluster_resources(payload.content.result)
        logger.debug("Cluster resources calculated.")

        # Get bud cluster id from event
        bud_cluster_id = payload.content.result["id"]

        cluster_data = ClusterCreate(
            name=required_data["name"],
            icon=required_data["icon"],
            ingress_url=required_data["ingress_url"],
            created_by=db_workflow.created_by,
            cluster_id=UUID(bud_cluster_id),
            **cluster_resources.model_dump(exclude_unset=True, exclude_none=True),
            status=ClusterStatusEnum.AVAILABLE,
            status_sync_at=datetime.now(tz=timezone.utc),
        )

        # Mark workflow as completed
        logger.debug(f"Updating workflow status: {workflow_id}")

        # Update status for last step
        execution_status = {"status": "success", "message": "Cluster successfully created"}
        try:
            db_cluster = await ClusterDataManager(self.session).insert_one(
                ClusterModel(**cluster_data.model_dump(exclude_unset=True, exclude_none=True))
            )
            logger.debug(f"Cluster created successfully: {db_cluster.id}")
        except Exception as e:
            logger.exception(f"Failed to create cluster: {e}")
            execution_status.update({"status": "error", "message": "Failed to create cluster"})
            workflow_data = {"status": WorkflowStatusEnum.FAILED, "reason": str(e)}
        else:
            workflow_data = {"status": WorkflowStatusEnum.COMPLETED}
        finally:
            execution_status_data = {"workflow_execution_status": execution_status}

            # Update current step number
            current_step_number = db_workflow.current_step + 1
            workflow_current_step = current_step_number

            # Update or create next workflow step
            db_workflow_step = await WorkflowStepService(self.session).create_or_update_next_workflow_step(
                db_workflow.id, current_step_number, execution_status_data
            )
            logger.debug(f"Upsert workflow step {db_workflow_step.id} for storing create cluster status")

            # Update workflow step data
            workflow_data.update({"current_step": workflow_current_step})
            await WorkflowDataManager(self.session).update_by_fields(db_workflow, workflow_data)

            # Send notification to workflow creator
            notification_request = (
                NotificationBuilder()
                .set_content(
                    title=db_cluster.name,
                    message="Cluster is onboarded",
                    icon=db_cluster.icon,
                    result=NotificationResult(target_id=db_cluster.id, target_type="cluster").model_dump(
                        exclude_none=True, exclude_unset=True
                    ),
                )
                .set_payload(
                    workflow_id=str(db_workflow.id), type=NotificationTypeEnum.CLUSTER_ONBOARDING_SUCCESS.value
                )
                .set_notification_request(subscriber_ids=[str(db_workflow.created_by)])
                .build()
            )
            await BudNotifyService().send_notification(notification_request)

            # Create request to trigger cluster status update periodic task
            await self._perform_cluster_status_update_request(db_cluster.cluster_id)

    async def delete_cluster_from_notification_event(self, payload: NotificationPayload) -> None:
        """Delete a cluster in database.

        Args:
            payload: The payload to delete the cluster with.

        Raises:
            ClientException: If the cluster already exists.
        """
        logger.debug("Received event for deleting cluster")

        # Get workflow and steps
        workflow_id = payload.workflow_id
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        # Define the keys required for endpoint creation
        keys_of_interest = [
            "cluster_id",
        ]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]

        logger.debug("Collected required data from workflow steps")

        # Retrieve cluster from db
        db_cluster = await ClusterDataManager(self.session).retrieve_by_fields(
            ClusterModel,
            {"id": required_data["cluster_id"]},
            exclude_fields={"status": ClusterStatusEnum.DELETED},
            missing_ok=True,
        )
        logger.debug(f"Cluster retrieved successfully: {db_cluster.id}")

        # Mark cluster as deleted
        db_cluster = await ClusterDataManager(self.session).update_by_fields(
            db_cluster, {"status": ClusterStatusEnum.DELETED}
        )
        logger.debug(f"Cluster {db_cluster.id} marked as deleted")

        # Mark workflow as completed
        await WorkflowDataManager(self.session).update_by_fields(db_workflow, {"status": WorkflowStatusEnum.COMPLETED})
        logger.debug(f"Workflow {db_workflow.id} marked as completed")

        # Send notification to workflow creator
        notification_request = (
            NotificationBuilder()
            .set_content(
                title=db_cluster.name,
                message="Cluster Deleted",
                icon=db_cluster.icon,
            )
            .set_payload(workflow_id=str(db_workflow.id), type=NotificationTypeEnum.CLUSTER_DELETION_SUCCESS.value)
            .set_notification_request(subscriber_ids=[str(db_workflow.created_by)])
            .build()
        )
        await BudNotifyService().send_notification(notification_request)

    async def update_cluster_status_from_notification_event(self, payload: NotificationPayload) -> None:
        """Delete a cluster in database.

        Args:
            payload: The payload to delete the cluster with.

        Raises:
            ClientException: If the cluster already exists.
        """
        logger.debug("Received event for updating cluster status")

        # Get cluster from db
        db_cluster = await ClusterDataManager(self.session).retrieve_by_fields(
            ClusterModel,
            {"cluster_id": payload.content.result["cluster_id"]},
            exclude_fields={"status": ClusterStatusEnum.DELETED},
        )
        logger.debug(f"Cluster retrieved successfully: {db_cluster.id}")

        # Check if cluster is already in deleting state
        if db_cluster.status == ClusterStatusEnum.DELETING:
            logger.error("Cluster %s is already in deleting state", db_cluster.id)
            raise ClientException("Cluster is already in deleting state")

        # Update data
        update_data = {"status": payload.content.result["status"]}

        if "node_info" in payload.content.result:
            if (
                "nodes" in payload.content.result["node_info"]
                and len(payload.content.result["node_info"]["nodes"]) > 0
            ):
                cluster_resources = await self._calculate_cluster_resources(payload.content.result["node_info"])
                update_data.update(cluster_resources.model_dump(exclude_unset=True, exclude_none=True))
                logger.debug(f"Cluster resources updated: {update_data}")

        # Update cluster status
        db_cluster = await ClusterDataManager(self.session).update_by_fields(db_cluster, update_data)
        logger.debug(f"Cluster {db_cluster.id} status updated to {payload.content.result['status']}")

    async def _calculate_cluster_resources(self, data: Dict[str, Any]) -> ClusterResourcesInfo:
        """Calculate the cluster resources.

        Args:
            data: The data to calculate the cluster resources with.

        Returns:
            ClusterResourcesInfo: The cluster resources.
        """
        cpu_count = 0
        gpu_count = 0
        hpu_count = 0
        cpu_total_workers = 0
        gpu_total_workers = 0
        hpu_total_workers = 0

        # Iterate through each node
        for node in data.get("nodes", []):
            # Iterate through devices in each node
            for device in node.get("devices", []):
                # Get the available count and device type
                worker_count = device.get("available_count", 0)
                device_type = device.get("type", "").lower()

                # Increment the appropriate counter
                if device_type == "cpu":
                    cpu_count += 1
                    cpu_total_workers += worker_count
                elif device_type == "gpu":
                    gpu_count += 1
                    gpu_total_workers += worker_count
                elif device_type == "hpu":
                    hpu_count += 1
                    hpu_total_workers += worker_count

        return ClusterResourcesInfo(
            cpu_count=cpu_count,
            gpu_count=gpu_count,
            hpu_count=hpu_count,
            cpu_total_workers=cpu_total_workers,
            cpu_available_workers=cpu_total_workers,
            gpu_total_workers=gpu_total_workers,
            gpu_available_workers=gpu_total_workers,
            hpu_total_workers=hpu_total_workers,
            hpu_available_workers=hpu_total_workers,
        )

    async def edit_cluster(self, cluster_id: UUID, data: Dict[str, Any]) -> ClusterResponse:
        """Edit cloud model by validating and updating specific fields, and saving an uploaded file if provided."""
        # Retrieve existing model
        db_cluster = await ClusterDataManager(self.session).retrieve_by_fields(
            model=ClusterModel, fields={"id": cluster_id}
        )

        if "name" in data:
            duplicate_cluster = await ClusterDataManager(self.session).retrieve_by_fields(
                model=ClusterModel,
                fields={"name": data["name"]},
                exclude_fields={"id": cluster_id, "status": ClusterStatusEnum.DELETED},
                missing_ok=True,
                case_sensitive=False,
            )
            if duplicate_cluster:
                raise ClientException("Cluster name already exists")

        db_cluster = await ClusterDataManager(self.session).update_by_fields(db_cluster, data)

        return db_cluster

    async def get_cluster_details(self, cluster_id: UUID) -> ClusterDetailResponse:
        """Retrieve cluster details."""
        # Retrieve cluster details
        db_cluster = await ClusterDataManager(self.session).retrieve_by_fields(
            ClusterModel, fields={"id": cluster_id}, exclude_fields={"status": ClusterStatusEnum.DELETED}
        )

        cluster_details = ClusterResponse.model_validate(db_cluster)

        total_endpoints_count, running_endpoints_count, active_replicas, total_replicas = await EndpointDataManager(
            self.session
        ).get_cluster_count_details(cluster_id)
        # Determine hardware types
        hardware_type = get_hardware_types(db_cluster.cpu_count, db_cluster.gpu_count, db_cluster.hpu_count)

        # Combine details and stats
        cluster_details_response = ClusterDetailResponse.model_validate(
            {
                **cluster_details.model_dump(),
                "total_endpoints_count": total_endpoints_count,
                "running_endpoints_count": running_endpoints_count,
                "active_workers_count": active_replicas,
                "total_workers_count": total_replicas,
                "hardware_type": hardware_type,
            }
        )

        return cluster_details_response

    async def delete_cluster(self, cluster_id: UUID, current_user_id: UUID) -> WorkflowModel:
        """Delete a cluster by its ID.

        Args:
            cluster_id: The ID of the cluster to delete.
        """
        db_cluster = await ClusterDataManager(self.session).retrieve_by_fields(
            ClusterModel, fields={"id": cluster_id}, exclude_fields={"status": ClusterStatusEnum.DELETED}
        )

        if db_cluster.status == ClusterStatusEnum.DELETING:
            raise ClientException("Cluster is already deleting")

        # Check for active endpoints
        db_endpoints = await EndpointDataManager(self.session).get_all_by_fields(
            EndpointModel,
            fields={"cluster_id": cluster_id},
            exclude_fields={"status": EndpointStatusEnum.DELETED},
        )

        # Raise error if cluster has active endpoints
        if db_endpoints:
            raise ClientException("Cannot delete cluster with active deployments")

        current_step_number = 1

        # Retrieve or create workflow
        workflow_create = WorkflowUtilCreate(
            workflow_type=WorkflowTypeEnum.CLUSTER_DELETION,
            title=db_cluster.name,
            total_steps=current_step_number,
            icon=db_cluster.icon,
            tag="Cluster Repository",
        )
        db_workflow = await WorkflowService(self.session).retrieve_or_create_workflow(
            workflow_id=None, workflow_data=workflow_create, current_user_id=current_user_id
        )
        logger.debug(f"Delete cluster workflow {db_workflow.id} created")

        # Perform delete cluster request to bud_cluster app
        try:
            bud_cluster_response = await self._perform_bud_cluster_delete_request(
                db_cluster.cluster_id, current_user_id, db_workflow.id
            )
        except ClientException as e:
            await WorkflowDataManager(self.session).update_by_fields(
                db_workflow, {"status": WorkflowStatusEnum.FAILED}
            )
            raise e

        # Add payload dict to response
        for step in bud_cluster_response["steps"]:
            step["payload"] = {}

        delete_cluster_workflow_id = bud_cluster_response.get("workflow_id")
        delete_cluster_events = {
            BudServeWorkflowStepEventName.DELETE_CLUSTER_EVENTS.value: bud_cluster_response,
            "delete_cluster_workflow_id": delete_cluster_workflow_id,
            "cluster_id": str(db_cluster.id),
        }

        # Insert step details in db
        db_workflow_step = await WorkflowStepDataManager(self.session).insert_one(
            WorkflowStepModel(
                workflow_id=db_workflow.id,
                step_number=current_step_number,
                data=delete_cluster_events,
            )
        )
        logger.debug(f"Created workflow step {current_step_number} for workflow {db_workflow.id}")

        # Update progress in workflow
        bud_cluster_response["progress_type"] = BudServeWorkflowStepEventName.DELETE_CLUSTER_EVENTS.value
        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"progress": bud_cluster_response, "current_step": current_step_number}
        )

        # Update cluster status to deleting
        await ClusterDataManager(self.session).update_by_fields(db_cluster, {"status": ClusterStatusEnum.DELETING})
        logger.debug(f"Cluster {db_cluster.id} status updated to {ClusterStatusEnum.DELETING.value}")

        return db_workflow

    async def _perform_bud_cluster_delete_request(
        self, bud_cluster_id: UUID, current_user_id: UUID, workflow_id: UUID
    ) -> Dict:
        """Perform delete cluster request to bud_cluster app.

        Args:
            bud_cluster_id: The ID of the cluster to delete.
        """
        delete_cluster_endpoint = (
            f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_cluster_app_id}/method/cluster/delete"
        )

        payload = {
            "cluster_id": str(bud_cluster_id),
            "notification_metadata": {
                "name": BUD_INTERNAL_WORKFLOW,
                "subscriber_ids": str(current_user_id),
                "workflow_id": str(workflow_id),
            },
            "source_topic": f"{app_settings.source_topic}",
        }

        logger.debug(f"Performing delete cluster request to budcluster {payload}")
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(delete_cluster_endpoint, json=payload) as response:
                    response_data = await response.json()
                    if response.status != 200:
                        logger.error(f"Failed to delete cluster: {response.status} {response_data}")
                        raise ClientException(
                            "Failed to delete cluster", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
                        )

                    logger.debug("Successfully deleted cluster from budcluster")
                    return response_data
        except Exception as e:
            logger.exception(f"Failed to send delete cluster request: {e}")
            raise ClientException("Failed to delete cluster", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR) from e

    async def _notify_recommended_cluster_from_notification_event(self, payload: NotificationPayload) -> None:
        logger.debug("Received event of recommending cluster")

        # Get workflow and steps
        workflow_id = payload.workflow_id
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        # Define the keys required for endpoint creation
        keys_of_interest = [
            "model_id",
        ]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]
        logger.debug("Collected required data from workflow steps")

        # NOTE: Frontend will fetch this step details from clusters/recommended/{workflow_id}
        # In order to navigate from widget this extra step need to be created.
        # Update current step number
        current_step_number = db_workflow.current_step + 1
        workflow_current_step = current_step_number

        db_workflow_step = await WorkflowStepService(self.session).create_or_update_next_workflow_step(
            db_workflow.id, current_step_number, {}
        )
        logger.debug(f"Upsert workflow step {db_workflow_step.id} as empty step")

        # Update current step number in workflow
        await WorkflowDataManager(self.session).update_by_fields(db_workflow, {"current_step": workflow_current_step})
        logger.debug(f"Updated current step number in workflow: {workflow_id}")

        # Fetch model
        db_model = await ModelDataManager(self.session).retrieve_by_fields(
            Model, {"id": required_data["model_id"], "status": ModelStatusEnum.ACTIVE}, missing_ok=True
        )
        model_icon = await ModelServiceUtil(self.session).get_model_icon(db_model)

        # Get bud cluster ids from recommendations
        recommendations = payload.content.result.get("recommendations", [])
        bud_cluster_ids = [UUID(recommendation["cluster_id"]) for recommendation in recommendations]
        logger.debug(f"Found {len(bud_cluster_ids)} clusters from budsim")
        logger.debug(f"bud cluster_ids from budsim: {bud_cluster_ids}")

        # Get active clusters by cluster ids
        _, db_active_clusters_count = await ClusterDataManager(self.session).get_available_clusters_by_cluster_ids(
            bud_cluster_ids
        )
        logger.debug(f"Found {db_active_clusters_count} active clusters from db")

        # Update cluster count in workflow current progress
        await self._update_workflow_progress_cluster_count(db_workflow, db_active_clusters_count)
        logger.debug(f"Updated cluster count in workflow progress: {db_workflow.id}")

        if db_active_clusters_count == 0:
            message = "Clusters Not Found"
        else:
            message = f"Found Top {db_active_clusters_count} Clusters"

        # Send notification to workflow creator
        notification_request = (
            NotificationBuilder()
            .set_content(
                title=db_model.name,
                message=message,
                icon=model_icon,
                result=NotificationResult(target_id=db_workflow.id, target_type="workflow").model_dump(
                    exclude_none=True, exclude_unset=True
                ),
            )
            .set_payload(workflow_id=str(db_workflow.id), type=NotificationTypeEnum.RECOMMENDED_CLUSTER_SUCCESS.value)
            .set_notification_request(subscriber_ids=[str(db_workflow.created_by)])
            .build()
        )
        await BudNotifyService().send_notification(notification_request)

    async def _update_workflow_progress_cluster_count(self, db_workflow: WorkflowModel, cluster_count: int) -> None:
        """Update workflow progress cluster count."""
        if not isinstance(db_workflow.progress, dict):
            logger.warning(f"Workflow {db_workflow.id} progress is not in expected format")
            return

        progress_type = db_workflow.progress.get("progress_type")
        if progress_type != BudServeWorkflowStepEventName.BUD_SIMULATOR_EVENTS.value:
            logger.warning(
                f"Progress type {progress_type} does not match event name {BudServeWorkflowStepEventName.BUD_SIMULATOR_EVENTS.value}"
            )
            return

        workflow_progress = db_workflow.progress
        workflow_progress["recommended_cluster_count"] = cluster_count

        # Update progress in workflow
        self.session.refresh(db_workflow)
        db_workflow = await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"progress": workflow_progress}
        )
        logger.debug(f"Updated workflow progress cluster count: {db_workflow.id}")

    async def cancel_cluster_onboarding_workflow(self, workflow_id: UUID) -> None:
        """Cancel cluster onboarding workflow."""
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": db_workflow.id}
        )

        # Define the keys required for endpoint creation
        keys_of_interest = [
            BudServeWorkflowStepEventName.CREATE_CLUSTER_EVENTS.value,
        ]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]
        logger.debug("Collected required data from workflow steps")

        if required_data.get(BudServeWorkflowStepEventName.CREATE_CLUSTER_EVENTS.value) is None:
            raise ClientException("Cluster onboarding process has not been initiated")

        create_cluster_response = required_data.get(BudServeWorkflowStepEventName.CREATE_CLUSTER_EVENTS.value)
        dapr_workflow_id = create_cluster_response.get("workflow_id")

        try:
            await self._perform_cancel_cluster_onboarding_request(dapr_workflow_id)
        except ClientException as e:
            raise e

    async def _perform_cancel_cluster_onboarding_request(self, workflow_id: str) -> Dict:
        """Perform delete cluster request to bud_cluster app.

        Args:
            workflow_id: The ID of the workflow to cancel.
        """
        cancel_cluster_endpoint = f"{app_settings.dapr_base_url}v1.0/invoke/{app_settings.bud_cluster_app_id}/method/cluster/cancel/{workflow_id}"

        logger.debug(f"Performing cancel cluster onboarding request to budcluster {cancel_cluster_endpoint}")
        try:
            async with aiohttp.ClientSession() as session, session.post(cancel_cluster_endpoint) as response:
                response_data = await response.json()
                if response.status != 200:
                    logger.error(f"Failed to cancel cluster onboarding: {response.status} {response_data}")
                    raise ClientException(
                        "Failed to cancel cluster onboarding", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
                    )

                logger.debug("Successfully cancelled cluster onboarding")
                return response_data
        except Exception as e:
            logger.exception(f"Failed to send cancel cluster onboarding request: {e}")
            raise ClientException(
                "Failed to cancel cluster onboarding", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
            ) from e

    async def _perform_cluster_status_update_request(self, cluster_id: UUID) -> Dict:
        """Perform update cluster node status request to bud_cluster app.

        Args:
            cluster_id: The ID of the cluster to update.
            current_user_id: The ID of the current user.
        """
        update_cluster_endpoint = f"{app_settings.dapr_base_url}v1.0/invoke/{app_settings.bud_cluster_app_id}/method/cluster/update-node-status"

        try:
            payload = {"cluster_id": str(cluster_id)}
            logger.debug(
                f"Performing update cluster node status request. payload: {payload}, endpoint: {update_cluster_endpoint}"
            )
            async with aiohttp.ClientSession() as session, session.post(
                update_cluster_endpoint, json=payload
            ) as response:
                response_data = await response.json()
                if response.status != 200:
                    logger.error(f"Failed to update cluster node status: {response.status} {response_data}")
                    raise ClientException(
                        "Failed to update cluster node status", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
                    )

                logger.debug("Successfully updated cluster node status")
                return response_data
        except Exception as e:
            logger.exception(f"Failed to send update cluster node status request: {e}")
            raise ClientException(
                "Failed to update cluster node status", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
            ) from e

    async def get_all_endpoints_in_cluster(
        self,
        cluster_id: UUID,
        offset: int,
        limit: int,
        filters: Dict[str, Any],
        order_by: List[str],
        search: bool,
    ) -> Tuple[List[ClusterEndpointResponse], int]:
        """Get all endpoints in a cluster."""
        # verify cluster id
        db_cluster = await ClusterDataManager(self.session).retrieve_by_fields(
            ClusterModel,
            fields={"id": cluster_id},
            exclude_fields={"status": ClusterStatusEnum.DELETED},
        )

        db_results, count = await EndpointDataManager(self.session).get_all_endpoints_in_cluster(
            cluster_id, offset, limit, filters, order_by, search
        )

        result = []
        for db_result in db_results:
            db_endpoint = db_result[0]
            project_name = db_result[1]
            model_name = db_result[2]
            total_workers = db_result[3]
            active_workers = db_result[4]

            result.append(
                ClusterEndpointResponse(
                    name=db_endpoint.name,
                    status=db_endpoint.status,
                    created_at=db_endpoint.created_at,
                    project_name=project_name,
                    model_name=model_name,
                    active_workers=active_workers,
                    total_workers=total_workers,
                    roi=12,  # Dummy value for ROI
                )
            )

        return result, count

    async def get_cluster_metrics(
        self, cluster_id: UUID, time_range: str = "today", metric_type: MetricTypeEnum = MetricTypeEnum.ALL
    ) -> Dict[str, Any]:
        """Get cluster metrics.

        Args:
            cluster_id: The cluster ID to get metrics for
            time_range: The time range to get metrics for ('today', '7days', 'month')
            metric_type: The type of metrics to return (ALL, MEMORY, CPU, DISK, GPU, HPU, NETWORK)

        Returns:
            Dict containing the filtered metrics based on metric_type
        """
        # Get cluster details to verify it exists
        db_cluster = await self.get_cluster_details(cluster_id)

        # Get metrics from Prometheus with filtering at query level
        metrics_fetcher = ClusterMetricsFetcher(app_settings.prometheus_url)
        metrics = await metrics_fetcher.get_cluster_metrics(
            cluster_id=db_cluster.cluster_id, time_range=time_range, metric_type=metric_type.value.lower()
        )

        if not metrics:
            raise ClientException("Failed to fetch metrics from Prometheus")

        # Add metric type to response
        metrics["metric_type"] = metric_type
        return metrics

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/cluster_ops/models.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The cluster ops package, containing essential business logic, services, and routing configurations for the cluster ops."""

import json
from datetime import datetime
from uuid import UUID, uuid4

from sqlalchemy import Boolean, DateTime, Enum, ForeignKey, Integer, String, Uuid
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy.orm import Mapped, mapped_column, relationship

from budapp.commons.constants import ClusterStatusEnum
from budapp.commons.database import Base


class Cluster(Base):
    """Cluster model."""

    __tablename__ = "cluster"
    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    name: Mapped[str] = mapped_column(String, nullable=False)
    ingress_url: Mapped[str] = mapped_column(String, nullable=False)
    status: Mapped[str] = mapped_column(
        Enum(
            ClusterStatusEnum,
            name="cluster_status_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=False,
    )
    icon: Mapped[str] = mapped_column(String, nullable=False)
    cpu_count: Mapped[int] = mapped_column(Integer, default=0)
    gpu_count: Mapped[int] = mapped_column(Integer, default=0)
    hpu_count: Mapped[int] = mapped_column(Integer, default=0)
    cpu_total_workers: Mapped[int] = mapped_column(Integer, default=0)
    cpu_available_workers: Mapped[int] = mapped_column(Integer, default=0)
    gpu_total_workers: Mapped[int] = mapped_column(Integer, default=0)
    gpu_available_workers: Mapped[int] = mapped_column(Integer, default=0)
    hpu_total_workers: Mapped[int] = mapped_column(Integer, default=0)
    hpu_available_workers: Mapped[int] = mapped_column(Integer, default=0)
    reason: Mapped[str] = mapped_column(String, nullable=True)
    created_by: Mapped[UUID] = mapped_column(ForeignKey("user.id"), nullable=False)
    cluster_id: Mapped[UUID] = mapped_column(Uuid, nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    status_sync_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)

    endpoints: Mapped[list["Endpoint"]] = relationship(
        "Endpoint",
        back_populates="cluster",
    )
    # benchmarks: Mapped[list["Benchmark"]] = relationship(
    #     "Benchmark",
    #     back_populates="cluster",
    # )
    created_user: Mapped["User"] = relationship(back_populates="created_clusters", foreign_keys=[created_by])

    @hybrid_property
    def kubernetes_info_dict(self):
        if not self.kubernetes_metadata:
            return {}
        return json.loads(self.kubernetes_metadata)

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/cluster_ops/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, containing essential business logic, services, and routing configurations for the cluster ops."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/cluster_ops/schemas.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Contains core Pydantic schemas used for data validation and serialization within the cluster ops services."""

from datetime import datetime
from typing import List, Dict, Union
from uuid import UUID
from enum import Enum

from pydantic import UUID4, AnyHttpUrl, BaseModel, ConfigDict, Field, computed_field, field_validator

from budapp.commons.constants import ClusterStatusEnum, EndpointStatusEnum
from budapp.commons.schemas import PaginatedSuccessResponse, SuccessResponse

from ..commons.helpers import validate_icon


class ClusterBase(BaseModel):
    """Cluster base schema."""

    name: str
    ingress_url: str
    icon: str


class ClusterCreate(ClusterBase):
    """Cluster create schema."""

    status: ClusterStatusEnum
    cpu_count: int
    gpu_count: int
    hpu_count: int
    cpu_total_workers: int
    cpu_available_workers: int
    gpu_total_workers: int
    gpu_available_workers: int
    hpu_total_workers: int
    hpu_available_workers: int
    created_by: UUID4
    cluster_id: UUID4
    status_sync_at: datetime


class ClusterResourcesInfo(BaseModel):
    """Cluster resources schema."""

    cpu_count: int
    gpu_count: int
    hpu_count: int
    cpu_total_workers: int
    cpu_available_workers: int
    gpu_total_workers: int
    gpu_available_workers: int
    hpu_total_workers: int
    hpu_available_workers: int


class ClusterResponse(BaseModel):
    """Cluster response schema."""

    model_config = ConfigDict(from_attributes=True)

    id: UUID
    name: str
    icon: str
    ingress_url: str
    created_at: datetime
    modified_at: datetime
    status: ClusterStatusEnum
    cluster_id: UUID
    cpu_count: int
    gpu_count: int
    hpu_count: int
    cpu_total_workers: int
    cpu_available_workers: int
    gpu_total_workers: int
    gpu_available_workers: int
    hpu_total_workers: int
    hpu_available_workers: int

    @computed_field
    @property
    def total_nodes(self) -> int:
        """Total nodes."""
        return self.cpu_total_workers + self.gpu_total_workers + self.hpu_total_workers

    @computed_field
    @property
    def available_nodes(self) -> int:
        """Available nodes."""
        return self.cpu_available_workers + self.gpu_available_workers + self.hpu_available_workers


class ClusterPaginatedResponse(ClusterResponse):
    endpoint_count: int


class ClusterFilter(BaseModel):
    """Filter cluster schema."""

    name: str | None = None


class ClusterListResponse(PaginatedSuccessResponse):
    """Cluster response schema."""

    model_config = ConfigDict(extra="ignore")

    clusters: List[ClusterPaginatedResponse]


class CreateClusterWorkflowRequest(BaseModel):
    """Create cluster workflow request schema."""

    name: str | None = None
    icon: str | None = None
    ingress_url: str | None = None
    workflow_id: UUID4 | None = None
    workflow_total_steps: int | None = None
    step_number: int | None = None
    trigger_workflow: bool | None = None

    @field_validator("icon", mode="before")
    @classmethod
    def icon_validate(cls, value: str | None) -> str | None:
        """Validate the icon."""
        if value is not None and not validate_icon(value):
            raise ValueError("invalid icon")
        return value


class CreateClusterWorkflowSteps(BaseModel):
    """Create cluster workflow step data schema."""

    name: str | None = None
    icon: str | None = None
    ingress_url: AnyHttpUrl | None = None
    configuration_yaml: dict | None = None


class EditClusterRequest(BaseModel):
    name: str | None = Field(
        None,
        min_length=1,
        max_length=100,
        description="Name of the cluster, must be non-empty and at most 100 characters.",
    )
    icon: str | None = Field(None, description="URL or path of the cluster icon.")
    ingress_url: AnyHttpUrl | None = Field(None, description="ingress_url.")

    @field_validator("name", mode="before")
    @classmethod
    def validate_name(cls, value: str | None) -> str | None:
        """Ensure the name is not empty or only whitespace."""
        if value is not None and not value.strip():
            raise ValueError("Cluster name cannot be empty or only whitespace.")
        return value

    @field_validator("ingress_url", mode="after")
    @classmethod
    def convert_url_to_string(cls, value: AnyHttpUrl | None) -> str | None:
        """Convert AnyHttpUrl to string."""
        return str(value) if value is not None else None

    @field_validator("icon", mode="before")
    @classmethod
    def icon_validate(cls, value: str | None) -> str | None:
        """Validate the icon."""
        if value is not None and not validate_icon(value):
            raise ValueError("invalid icon")
        return value


class ClusterDetailResponse(ClusterResponse):
    """Cluster detail response schema"""

    total_workers_count: int
    active_workers_count: int
    total_endpoints_count: int
    running_endpoints_count: int
    hardware_type: list


class SingleClusterResponse(SuccessResponse):
    """Single cluster entity"""

    cluster: Union[ClusterResponse, ClusterDetailResponse]


class CancelClusterOnboardingRequest(BaseModel):
    """Cancel cluster onboarding request schema."""

    workflow_id: UUID4


class ClusterEndpointResponse(BaseModel):
    """Cluster endpoint response schema."""

    name: str
    status: EndpointStatusEnum
    created_at: datetime
    project_name: str
    model_name: str
    active_workers: int
    total_workers: int
    roi: int


class ClusterEndpointPaginatedResponse(PaginatedSuccessResponse):
    """Cluster endpoint paginated response schema."""

    endpoints: list[ClusterEndpointResponse] = []


class ClusterEndpointFilter(BaseModel):
    """Filter schema for endpoints."""

    name: str | None = None
    status: EndpointStatusEnum | None = None


# Cluster Metrics Schema
class ClusterNodeNetwork(BaseModel):
    """Network metrics for a cluster node."""

    total_receive_mbps: float
    total_transmit_mbps: float
    total_bandwidth_mbps: float
    total_errors: float


class ClusterNodeMetrics(BaseModel):
    """Metrics for a single node in the cluster."""

    memory: Dict[str, float]  # total_gib, used_gib, available_gib, usage_percent
    cpu: Dict[str, float]  # cpu_usage_percent
    disk: Dict[str, Dict]  # paths with disk metrics
    gpu: Dict[str, float]  # memory and utilization metrics
    hpu: Dict[str, float]  # memory and utilization metrics
    network: Dict[str, Union[Dict[str, Dict], Dict[str, float]]]  # interfaces and summary


class ClusterSummaryMetrics(BaseModel):
    """Summary metrics for the entire cluster."""

    total_nodes: int
    memory: Dict[str, float]  # total_gib, used_gib, available_gib, usage_percent
    disk: Dict[str, float]  # total_gib, used_gib, available_gib, usage_percent
    gpu: Dict[str, float]  # memory and utilization metrics
    hpu: Dict[str, float]  # memory and utilization metrics
    cpu: Dict[str, float]  # average_usage_percent
    network: Dict[str, float]  # network metrics


class MetricTypeEnum(Enum):
    """Enum for metric types."""

    ALL = "all"
    MEMORY = "memory"
    CPU = "cpu"
    DISK = "disk"
    GPU = "gpu"
    HPU = "hpu"
    NETWORK = "network"


class ClusterMetricsResponse(SuccessResponse):
    """Cluster metrics response schema."""

    nodes: Dict[str, ClusterNodeMetrics]
    cluster_summary: ClusterSummaryMetrics
    historical_data: Dict[str, List[Dict[str, Union[int, float]]]]  # key -> list of {timestamp, value} pairs
    time_range: str  # 'today', '7days', or 'month'
    metric_type: MetricTypeEnum  # The type of metrics being returned
    timestamp: datetime

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/cluster_ops/utils.py`:

```py
import requests
import asyncio
import aiohttp
from typing import Dict, Any, Optional, List
from datetime import datetime, timezone, timedelta
from budapp.commons import logging
from uuid import UUID
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
from cachetools import TTLCache, cached
from contextlib import asynccontextmanager

logger = logging.get_logger(__name__)


class ClusterMetricsFetcher:
    """Fetches cluster metrics from Prometheus."""

    def __init__(self, prometheus_url: str):
        """Initialize with Prometheus server URL."""
        self.prometheus_url = prometheus_url
        self.api_url = f"{prometheus_url}/api/v1"
        self._time_range_cache = TTLCache(maxsize=100, ttl=300)  # 5 minutes TTL

    @asynccontextmanager
    async def _get_executor(self):
        """Get a thread pool executor context manager."""
        executor = ThreadPoolExecutor(max_workers=10)
        try:
            yield executor
        finally:
            executor.shutdown(wait=True)

    @cached(cache=TTLCache(maxsize=100, ttl=300))  # Cache results for 5 minutes
    def _get_time_range_params(self, time_range: str) -> dict:
        """Get start and end timestamps based on time range.

        Args:
            time_range: One of 'today', '7days', 'month'

        Returns:
            Dict with start and end timestamps and step interval
        """
        now = datetime.now(timezone.utc)

        if time_range == "today":
            start_time = now.replace(hour=0, minute=0, second=0, microsecond=0)
            step = "5m"  # 5-minute intervals for today
        elif time_range == "7days":
            start_time = now - timedelta(days=7)
            step = "1h"  # 1-hour intervals for 7 days
        elif time_range == "month":
            start_time = now - timedelta(days=30)
            step = "6h"  # 6-hour intervals for monthly view
        else:
            # Default to today if invalid range
            start_time = now.replace(hour=0, minute=0, second=0, microsecond=0)
            step = "5m"

        return {"start": start_time.timestamp(), "end": now.timestamp(), "step": step}

    async def _async_query_range(self, session: aiohttp.ClientSession, query: str, time_params: dict) -> dict:
        """Execute a single Prometheus range query asynchronously."""
        try:
            async with session.get(
                f"{self.api_url}/query_range",
                params={
                    "query": query,
                    "start": time_params["start"],
                    "end": time_params["end"],
                    "step": time_params["step"],
                },
                timeout=10,
            ) as response:
                response.raise_for_status()
                result = await response.json()
                return result["data"]["result"]
        except Exception as e:
            logger.error(f"Failed to execute Prometheus range query: {e}")
            return []

    async def _fetch_all_metrics(self, queries: Dict[str, str], time_range: str) -> Dict[str, List]:
        """Fetch all metrics concurrently using asyncio."""
        time_params = self._get_time_range_params(time_range)
        async with aiohttp.ClientSession() as session:
            tasks = []
            for key, query in queries.items():
                task = asyncio.create_task(self._async_query_range(session, query, time_params))
                tasks.append((key, task))

            results = {}
            for key, task in tasks:
                try:
                    results[key] = await task
                except Exception as e:
                    logger.error(f"Failed to fetch metrics for {key}: {e}")
                    results[key] = []

            return results

    def _get_metric_queries(self, cluster_name: str, rate_interval: str, metric_type: str = "all") -> Dict[str, str]:
        """Get Prometheus queries based on metric type.

        Args:
            cluster_name: Name/ID of the cluster
            rate_interval: Rate interval for rate queries
            metric_type: Type of metrics to fetch ('all', 'memory', 'cpu', 'disk', 'gpu', 'hpu', 'network')

        Returns:
            Dict of query names and their Prometheus queries
        """
        queries = {}

        if metric_type in ["all", "memory"]:
            queries.update(
                {
                    "memory_total": f'node_memory_MemTotal_bytes{{cluster="{cluster_name}"}} / 1024 / 1024 / 1024',
                    "memory_available": f'node_memory_MemAvailable_bytes{{cluster="{cluster_name}"}} / 1024 / 1024 / 1024',
                }
            )

        if metric_type in ["all", "cpu"]:
            queries.update(
                {
                    "cpu_usage": f"""100 * sum(rate(node_cpu_seconds_total{{cluster="{cluster_name}",mode!="idle"}}[{rate_interval}])) by (instance) /
                            count by (instance) (node_cpu_seconds_total{{cluster="{cluster_name}"}})""",
                }
            )

        if metric_type in ["all", "disk"]:
            queries.update(
                {
                    "disk_total": f'sum by (instance, mountpoint) (node_filesystem_size_bytes{{cluster="{cluster_name}"}}) / 1024 / 1024 / 1024',
                    "disk_used": f'sum by (instance, mountpoint) ((node_filesystem_size_bytes{{cluster="{cluster_name}"}} - node_filesystem_free_bytes{{cluster="{cluster_name}"}})) / 1024 / 1024 / 1024',
                }
            )

        if metric_type in ["all", "network"]:
            queries.update(
                {
                    "network_receive": f'sum by (instance, device) (rate(node_network_receive_bytes_total{{cluster="{cluster_name}"}}[{rate_interval}]) * 8 / 1024 / 1024)',
                    "network_transmit": f'sum by (instance, device) (rate(node_network_transmit_bytes_total{{cluster="{cluster_name}"}}[{rate_interval}]) * 8 / 1024 / 1024)',
                    "network_errors": f'sum by (instance) (node_network_transmit_errs_total{{cluster="{cluster_name}"}} + node_network_receive_errs_total{{cluster="{cluster_name}"}}) or vector(0)',
                }
            )

        if metric_type in ["all", "gpu"]:
            queries.update(
                {
                    "gpu_memory_used": f'sum by (instance, gpu) (nvidia_gpu_memory_used_bytes{{cluster="{cluster_name}"}}) / 1024 / 1024 / 1024',
                    "gpu_memory_total": f'sum by (instance, gpu) (nvidia_gpu_memory_total_bytes{{cluster="{cluster_name}"}}) / 1024 / 1024 / 1024',
                    "gpu_utilization": f'avg by (instance, gpu) (nvidia_gpu_duty_cycle{{cluster="{cluster_name}"}}) or vector(0)',
                }
            )

        if metric_type in ["all", "hpu"]:
            queries.update(
                {
                    "hpu_memory_used": f'sum by (instance, hpu) (habana_memory_used_bytes{{cluster="{cluster_name}"}}) / 1024 / 1024 / 1024',
                    "hpu_memory_total": f'sum by (instance, hpu) (habana_memory_total_bytes{{cluster="{cluster_name}"}}) / 1024 / 1024 / 1024',
                    "hpu_utilization": f'avg by (instance, hpu) (habana_duty_cycle{{cluster="{cluster_name}"}}) or vector(0)',
                }
            )

        # Always include total nodes count regardless of metric type
        queries["total_nodes"] = f'count(node_uname_info{{cluster="{cluster_name}"}})'

        return queries

    async def get_cluster_metrics(
        self, cluster_id: UUID, time_range: str = "today", metric_type: str = "all"
    ) -> Optional[Dict[str, Any]]:
        """Fetch comprehensive cluster metrics.

        Args:
            cluster_id: The cluster ID to fetch metrics for
            time_range: One of 'today', '7days', 'month'
            metric_type: Type of metrics to fetch ('all', 'memory', 'cpu', 'disk', 'gpu', 'hpu', 'network')

        Returns:
            Dict with node and summary metrics, filtered by metric_type
        """
        if not cluster_id:
            logger.error("Cluster ID is required to fetch cluster metrics")
            return None

        cluster_name = str(cluster_id)
        rate_interval = "5m" if time_range == "today" else "1h" if time_range == "7days" else "6h"

        # Get filtered queries based on metric type
        queries = self._get_metric_queries(cluster_name, rate_interval, metric_type)

        try:
            # Fetch metrics asynchronously
            results = await self._fetch_all_metrics(queries, time_range)

            if not any(result for result in results.values()):
                logger.error("No data returned from Prometheus queries")
                return None

            async with self._get_executor() as executor:
                # Process current values in parallel
                current_results = {
                    key: executor.submit(self._process_current_values, result)
                    for key, result in results.items()
                    if result
                }

                # Wait for all processing to complete
                current_results = {key: future.result() for key, future in current_results.items()}

                # Process node metrics
                nodes = self._process_node_metrics(current_results, time_range)
                current_results["nodes"] = nodes

                # Process cluster summary
                cluster_summary = self._process_cluster_summary(current_results, time_range)

                # Process historical data in parallel
                historical_data = self._process_historical_data(results, executor)

                metrics = {
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "time_range": time_range,
                    "historical_data": historical_data,
                    "nodes": nodes,
                    "cluster_summary": cluster_summary,
                }

            logger.info(f"Successfully fetched cluster metrics for time range: {time_range}")
            return metrics

        except Exception as e:
            logger.error(f"Failed to fetch cluster metrics: {e}")
            return None

    def _process_current_values(self, result: List) -> List:
        """Process current values from time series data."""
        return [{"metric": series["metric"], "value": series["values"][-1]} for series in result]

    def _process_historical_data(self, results: Dict, executor: ThreadPoolExecutor) -> Dict:
        """Process historical data in parallel."""
        historical_futures = {
            key: executor.submit(self._process_series_historical_data, result)
            for key, result in results.items()
            if result
        }

        return {key: future.result() for key, future in historical_futures.items()}

    def _process_series_historical_data(self, result: List) -> List:
        """Process historical data for a single series."""
        return [{"timestamp": value[0], "value": float(value[1])} for series in result for value in series["values"]]

    def _get_unique_instances(self, results: Dict) -> List[str]:
        """Extract unique instance names from results."""
        instances = set()
        for result in results.values():
            for metric in result:
                if "instance" in metric["metric"]:
                    instances.add(metric["metric"]["instance"])
        return sorted(list(instances))

    def _get_instance_value(self, metrics: List, instance: str) -> float:
        """Get metric value for a specific instance."""
        for metric in metrics:
            if metric["metric"].get("instance") == instance:
                return metric["value"][1]
        return 0.0

    def _calculate_metric_change(self, current_value: float, historical_data: List, time_range: str) -> float:
        """Calculate the change in a metric based on the time range.
        
        Args:
            current_value: The current value of the metric
            historical_data: List of historical values for the metric
            time_range: One of 'today', '7days', 'month'
            
        Returns:
            Float representing the change in the metric (in the same units as the input)
        """
        if not historical_data or not len(historical_data):
            return 0.0
        
        try:
            # Get the first series of values
            first_series = historical_data[0]
            if not first_series.get("values"):
                return 0.0

            values = first_series["values"]
            # Get the first value (oldest) in the time series
            previous_value = float(values[0][1])
            return round(current_value - previous_value, 2)
            
        except (IndexError, ValueError, KeyError) as e:
            logger.error(f"Error calculating metric change: {e}")
            return 0.0

    def _process_node_metrics(self, results: Dict, time_range: str) -> Dict:
        """Process node metrics from results."""
        nodes = {}
        instances = self._get_unique_instances(results)

        for instance in instances:
            memory_total = float(self._get_instance_value(results.get("memory_total", []), instance))
            memory_available = float(self._get_instance_value(results.get("memory_available", []), instance))
            memory_used = memory_total - memory_available
            memory_usage_percent = (memory_used / memory_total * 100) if memory_total > 0 else 0

            current_cpu_usage = float(self._get_instance_value(results.get("cpu_usage", []), instance))

            nodes[instance] = {
                "memory": {
                    "total_gib": memory_total,
                    "available_gib": memory_available,
                    "used_gib": memory_used,
                    "usage_percent": memory_usage_percent,
                    "change_percent": self._calculate_metric_change(
                        memory_usage_percent,
                        [m for m in results.get("memory_available", []) if m["metric"].get("instance") == instance],
                        time_range
                    )
                },
                "cpu": {
                    "cpu_usage_percent": current_cpu_usage,
                    "change_percent": self._calculate_metric_change(
                        current_cpu_usage,
                        [m for m in results.get("cpu_usage", []) if m["metric"].get("instance") == instance],
                        time_range
                    )
                },
                "disk": self._process_disk_metrics(results, instance, time_range),
                "network": self._process_network_metrics(results, instance, time_range),
                "gpu": self._process_gpu_metrics(results, instance, time_range),
                "hpu": self._process_hpu_metrics(results, instance, time_range),
            }

        return nodes

    def _process_disk_metrics(self, results: Dict, instance: str, time_range: str) -> Dict:
        """Process disk metrics for a specific instance."""
        disk_metrics = {}
        mountpoints = set()

        disk_total = results.get("disk_total", [])
        disk_used = results.get("disk_used", [])

        for metric in disk_total:
            if metric["metric"]["instance"] == instance:
                mountpoints.add(metric["metric"]["mountpoint"])

        for mountpoint in mountpoints:
            total = float(self._get_instance_mountpoint_value(disk_total, instance, mountpoint))
            used = float(self._get_instance_mountpoint_value(disk_used, instance, mountpoint))
            usage_percent = (used / total * 100) if total > 0 else 0

            disk_metrics[mountpoint] = {
                "total_gib": total,
                "used_gib": used,
                "available_gib": total - used,
                "usage_percent": usage_percent,
                "change_percent": self._calculate_metric_change(
                    usage_percent,
                    [m for m in disk_used if m["metric"].get("instance") == instance 
                     and m["metric"].get("mountpoint") == mountpoint],
                    time_range
                )
            }

        return disk_metrics

    def _process_network_metrics(self, results: Dict, instance: str, time_range: str) -> Dict:
        """Process network metrics for a specific instance."""
        interfaces = {}
        total_receive = 0
        total_transmit = 0

        # Safely get network metrics
        network_receive = results.get("network_receive", [])
        network_transmit = results.get("network_transmit", [])
        network_errors = results.get("network_errors", [])

        # Process per-interface metrics
        for metric in network_receive:
            if metric["metric"]["instance"] == instance:
                device = metric["metric"]["device"]
                receive = float(metric["value"][1])
                transmit = float(self._get_device_value(network_transmit, instance, device))

                interfaces[device] = {
                    "receive_mbps": receive,
                    "transmit_mbps": transmit,
                    "bandwidth_mbps": receive + transmit,
                }
                total_receive += receive
                total_transmit += transmit

        total_errors = float(self._get_instance_value(network_errors, instance))

        return {
            "interfaces": interfaces,
            "summary": {
                "total_receive_mbps": total_receive,
                "total_transmit_mbps": total_transmit,
                "total_bandwidth_mbps": total_receive + total_transmit,
                "total_errors": total_errors,
            },
            "change_percent": self._calculate_metric_change(
                total_errors,
                [m for m in network_errors if m["metric"].get("instance") == instance],
                time_range
            )
        }

    def _process_gpu_metrics(self, results: Dict, instance: str, time_range: str) -> Dict[str, float]:
        """Process GPU metrics for a specific instance."""
        gpu_memory_used = results.get("gpu_memory_used", [])
        gpu_memory_total = results.get("gpu_memory_total", [])
        gpu_utilization = results.get("gpu_utilization", [])

        try:
            return {
                "memory_used_gib": float(self._get_instance_value(gpu_memory_used, instance)),
                "memory_total_gib": float(self._get_instance_value(gpu_memory_total, instance)),
                "utilization_percent": float(self._get_instance_value(gpu_utilization, instance)),
                "change_percent": self._calculate_metric_change(
                    float(self._get_instance_value(gpu_utilization, instance)),
                    [m for m in gpu_utilization if m["metric"].get("instance") == instance],
                    time_range
                )
            }
        except (KeyError, TypeError, ValueError):
            return {"memory_used_gib": 0.0, "memory_total_gib": 0.0, "utilization_percent": 0.0}

    def _process_hpu_metrics(self, results: Dict, instance: str, time_range: str) -> Dict[str, float]:
        """Process HPU metrics for a specific instance."""
        hpu_memory_used = results.get("hpu_memory_used", [])
        hpu_memory_total = results.get("hpu_memory_total", [])
        hpu_utilization = results.get("hpu_utilization", [])

        try:
            return {
                "memory_used_gib": float(self._get_instance_value(hpu_memory_used, instance)),
                "memory_total_gib": float(self._get_instance_value(hpu_memory_total, instance)),
                "utilization_percent": float(self._get_instance_value(hpu_utilization, instance)),
                "change_percent": self._calculate_metric_change(
                    float(self._get_instance_value(hpu_utilization, instance)),
                    [m for m in hpu_utilization if m["metric"].get("instance") == instance],
                    time_range
                )
            }
        except (KeyError, TypeError, ValueError):
            return {"memory_used_gib": 0.0, "memory_total_gib": 0.0, "utilization_percent": 0.0}

    def _aggregate_disk_metrics(self, nodes: Dict) -> Dict[str, float]:
        """Aggregate disk metrics across all nodes."""
        total_size = 0.0
        total_used = 0.0

        for node in nodes.values():
            for mount_metrics in node["disk"].values():
                total_size += mount_metrics["total_gib"]
                total_used += mount_metrics["used_gib"]

        return {
            "total_gib": total_size,
            "used_gib": total_used,
            "available_gib": total_size - total_used,
            "usage_percent": (total_used / total_size * 100) if total_size > 0 else 0,
        }

    def _aggregate_network_metrics(self, nodes: Dict) -> Dict[str, float]:
        """Aggregate network metrics across all nodes."""
        total_receive = sum(node["network"]["summary"]["total_receive_mbps"] for node in nodes.values())
        total_transmit = sum(node["network"]["summary"]["total_transmit_mbps"] for node in nodes.values())
        total_errors = sum(node["network"]["summary"]["total_errors"] for node in nodes.values())

        return {
            "total_receive_mbps": total_receive,
            "total_transmit_mbps": total_transmit,
            "total_bandwidth_mbps": total_receive + total_transmit,
            "total_errors": total_errors,
        }

    def _aggregate_gpu_metrics(self, nodes: Dict) -> Dict[str, float]:
        """Aggregate GPU metrics across all nodes."""
        total_memory = sum(node["gpu"]["memory_total_gib"] for node in nodes.values())
        used_memory = sum(node["gpu"]["memory_used_gib"] for node in nodes.values())
        avg_utilization = (
            sum(node["gpu"]["utilization_percent"] for node in nodes.values()) / len(nodes) if nodes else 0.0
        )

        return {
            "memory_total_gib": total_memory,
            "memory_used_gib": used_memory,
            "memory_available_gib": total_memory - used_memory,
            "utilization_percent": avg_utilization,
            "memory_usage_percent": (used_memory / total_memory * 100) if total_memory > 0 else 0.0,
        }

    def _aggregate_hpu_metrics(self, nodes: Dict) -> Dict[str, float]:
        """Aggregate HPU metrics across all nodes."""
        total_memory = sum(node["hpu"]["memory_total_gib"] for node in nodes.values())
        used_memory = sum(node["hpu"]["memory_used_gib"] for node in nodes.values())
        avg_utilization = (
            sum(node["hpu"]["utilization_percent"] for node in nodes.values()) / len(nodes) if nodes else 0.0
        )

        return {
            "memory_total_gib": total_memory,
            "memory_used_gib": used_memory,
            "memory_available_gib": total_memory - used_memory,
            "utilization_percent": avg_utilization,
            "memory_usage_percent": (used_memory / total_memory * 100) if total_memory > 0 else 0.0,
        }

    def _get_instance_mountpoint_value(self, metrics: List, instance: str, mountpoint: str) -> float:
        """Get metric value for a specific instance and mountpoint."""
        try:
            for metric in metrics:
                if metric["metric"].get("instance") == instance and metric["metric"].get("mountpoint") == mountpoint:
                    return float(metric["value"][1])
        except (KeyError, TypeError, ValueError, IndexError):
            pass
        return 0.0

    def _get_device_value(self, metrics: List, instance: str, device: str) -> float:
        """Get metric value for a specific instance and network device."""
        try:
            for metric in metrics:
                if metric["metric"].get("instance") == instance and metric["metric"].get("device") == device:
                    return float(metric["value"][1])
        except (KeyError, TypeError, ValueError, IndexError):
            pass
        return 0.0

    def _process_cluster_summary(self, results: Dict, time_range: str) -> Dict:
        """Process cluster summary metrics from results."""
        nodes = results.get("nodes", {})
        if not nodes:
            return {
                "total_nodes": 0,
                "memory": {"total_gib": 0, "used_gib": 0, "available_gib": 0, "usage_percent": 0, "change_percent": 0},
                "cpu": {"average_usage_percent": 0, "change_percent": 0},
                "disk": {"total_gib": 0, "used_gib": 0, "available_gib": 0, "usage_percent": 0, "change_percent": 0},
                "network": {
                    "total_receive_mbps": 0,
                    "total_transmit_mbps": 0,
                    "total_bandwidth_mbps": 0,
                    "total_errors": 0,
                    "change_percent": 0
                },
                "gpu": {
                    "memory_total_gib": 0,
                    "memory_used_gib": 0,
                    "memory_available_gib": 0,
                    "utilization_percent": 0,
                    "memory_usage_percent": 0,
                    "change_percent": 0
                },
                "hpu": {
                    "memory_total_gib": 0,
                    "memory_used_gib": 0,
                    "memory_available_gib": 0,
                    "utilization_percent": 0,
                    "memory_usage_percent": 0,
                    "change_percent": 0
                },
            }

        # Calculate current values
        total_memory = sum(node["memory"]["total_gib"] for node in nodes.values())
        used_memory = sum(node["memory"]["used_gib"] for node in nodes.values())
        memory_usage_percent = (used_memory / total_memory * 100) if total_memory > 0 else 0
        
        current_cpu_usage = sum(node["cpu"]["cpu_usage_percent"] for node in nodes.values()) / len(nodes) if nodes else 0

        summary = {
            "total_nodes": int(self._get_instance_value(results.get("total_nodes", []), "total_nodes")),
            "memory": {
                "total_gib": total_memory,
                "used_gib": used_memory,
                "available_gib": total_memory - used_memory,
                "usage_percent": memory_usage_percent,
                "change_percent": self._calculate_metric_change(
                    memory_usage_percent,
                    results.get("memory_available", []),
                    time_range
                )
            },
            "cpu": {
                "average_usage_percent": current_cpu_usage,
                "change_percent": self._calculate_metric_change(
                    current_cpu_usage,
                    results.get("cpu_usage", []),
                    time_range
                )
            },
            "disk": self._aggregate_disk_metrics(nodes),
            "network": self._aggregate_network_metrics(nodes),
            "gpu": self._aggregate_gpu_metrics(nodes),
            "hpu": self._aggregate_hpu_metrics(nodes),
        }

        return summary

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/cluster_ops/cluster_routes.py`:

```py
# budapp/cluster_ops/cluster_routes.py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The cluster ops package, containing essential business logic, services, and routing configurations for the cluster ops."""

from typing import List, Optional, Union
from uuid import UUID

from fastapi import APIRouter, Depends, File, Form, Query, UploadFile, status
from fastapi.exceptions import RequestValidationError
from pydantic import AnyHttpUrl, ValidationError
from sqlalchemy.orm import Session
from typing_extensions import Annotated

from budapp.commons import logging
from budapp.commons.dependencies import (
    get_current_active_user,
    get_session,
    parse_ordering_fields,
)
from budapp.commons.exceptions import ClientException
from budapp.commons.schemas import ErrorResponse, SuccessResponse
from budapp.user_ops.schemas import User
from budapp.workflow_ops.schemas import RetrieveWorkflowDataResponse
from budapp.workflow_ops.services import WorkflowService

from .schemas import (
    CancelClusterOnboardingRequest,
    ClusterEndpointFilter,
    ClusterEndpointPaginatedResponse,
    ClusterFilter,
    ClusterListResponse,
    CreateClusterWorkflowRequest,
    EditClusterRequest,
    SingleClusterResponse,
    ClusterMetricsResponse,
    MetricTypeEnum,
)
from .services import ClusterService


logger = logging.get_logger(__name__)

cluster_router = APIRouter(prefix="/clusters", tags=["cluster"])


@cluster_router.post(
    "/clusters",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": RetrieveWorkflowDataResponse,
            "description": "Create cluster workflow executed successfully",
        },
    },
    description="Create cluster workflow",
)
async def create_cluster_workflow(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    step_number: Annotated[int, Form(gt=0)],
    name: Annotated[str | None, Form(min_length=1, max_length=100)] = None,
    icon: Annotated[str | None, Form(min_length=1, max_length=100)] = None,
    ingress_url: Annotated[AnyHttpUrl | None, Form()] = None,
    configuration_file: Annotated[
        UploadFile | None, File(description="The configuration file for the cluster")
    ] = None,
    workflow_id: Annotated[UUID | None, Form()] = None,
    workflow_total_steps: Annotated[int | None, Form()] = None,
    trigger_workflow: Annotated[bool, Form()] = False,
) -> Union[RetrieveWorkflowDataResponse, ErrorResponse]:
    """Create cluster workflow."""
    # Perform router level validation
    if workflow_id is None and workflow_total_steps is None:
        return ErrorResponse(
            code=status.HTTP_400_BAD_REQUEST,
            message="workflow_total_steps is required when workflow_id is not provided",
        ).to_http_response()

    if workflow_id is not None and workflow_total_steps is not None:
        return ErrorResponse(
            code=status.HTTP_400_BAD_REQUEST,
            message="workflow_total_steps and workflow_id cannot be provided together",
        ).to_http_response()

    # Check if at least one of the other fields is provided
    other_fields = [name, ingress_url, configuration_file]
    required_fields = ["name", "ingress_url", "configuration_file"]
    if not any(other_fields):
        return ErrorResponse(
            code=status.HTTP_400_BAD_REQUEST,
            message=f"At least one of {', '.join(required_fields)} is required when workflow_id is provided",
        )

    try:
        db_workflow = await ClusterService(session).create_cluster_workflow(
            current_user_id=current_user.id,
            request=CreateClusterWorkflowRequest(
                name=name,
                icon=icon,
                ingress_url=str(ingress_url) if ingress_url else None,
                workflow_id=workflow_id,
                workflow_total_steps=workflow_total_steps,
                step_number=step_number,
                trigger_workflow=trigger_workflow,
            ),
            configuration_file=configuration_file,
        )

        return await WorkflowService(session).retrieve_workflow_data(db_workflow.id)
    except ClientException as e:
        logger.exception(f"Failed to execute create cluster workflow: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except ValidationError as e:
        logger.exception(f"ValidationErrors: {str(e)}")
        raise RequestValidationError(e.errors())
    except Exception as e:
        logger.error(f"Error occurred while executing create cluster workflow: {str(e)}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to execute create cluster workflow"
        ).to_http_response()


@cluster_router.get(
    "/clusters",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": ClusterListResponse,
            "description": "Successfully listed all clusters",
        },
    },
    description="List all clusters",
)
async def list_clusters(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    filters: ClusterFilter = Depends(),
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=0),
    order_by: Optional[List[str]] = Depends(parse_ordering_fields),
    search: bool = False,
) -> Union[ClusterListResponse, ErrorResponse]:
    """List all clusters."""
    offset = (page - 1) * limit

    filters_dict = filters.model_dump(exclude_none=True)

    try:
        db_clusters, count = await ClusterService(session).get_all_active_clusters(
            offset, limit, filters_dict, order_by, search
        )
    except Exception as e:
        logger.error(f"Error occurred while listing clusters: {str(e)}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to list clusters"
        ).to_http_response()

    return ClusterListResponse(
        clusters=db_clusters,
        total_record=count,
        page=page,
        limit=limit,
        object="cluster.list",
        code=status.HTTP_200_OK,
    ).to_http_response()


@cluster_router.patch(
    "/{cluster_id}",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": SingleClusterResponse,
            "description": "Successfully edited cluster",
        },
    },
    description="Edit cluster",
)
async def edit_cluster(
    cluster_id: UUID,
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    edit_cluster: EditClusterRequest,
) -> Union[SingleClusterResponse, ErrorResponse]:
    """Edit cluster."""
    try:
        db_cluster = await ClusterService(session).edit_cluster(
            cluster_id=cluster_id, data=edit_cluster.model_dump(exclude_unset=True, exclude_none=True)
        )
        return SingleClusterResponse(
            cluster=db_cluster,
            message="Cluster details updated successfully",
            code=status.HTTP_200_OK,
            object="cluster.edit",
        )
    except ClientException as e:
        logger.exception(f"Failed to edit cluster: {e}")
        return ErrorResponse(code=status.HTTP_400_BAD_REQUEST, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to edit cluster: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to edit cluster"
        ).to_http_response()


@cluster_router.get(
    "/{cluster_id}",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Invalid request parameters",
        },
        status.HTTP_200_OK: {
            "model": SingleClusterResponse,
            "description": "Successfully retrieved cluster details",
        },
    },
    description="Retrieve details of a cluster by ID",
)
async def get_cluster_details(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    cluster_id: UUID,
) -> Union[SingleClusterResponse, ErrorResponse]:
    """Retrieve details of a cluster by its ID."""
    try:
        cluster_details = await ClusterService(session).get_cluster_details(cluster_id)
    except ClientException as e:
        logger.exception(f"Failed to get cluster details: {e}")
        return ErrorResponse(code=status.HTTP_400_BAD_REQUEST, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to get cluster details: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            message="Failed to retrieve cluster details",
        ).to_http_response()

    return SingleClusterResponse(
        cluster=cluster_details,
        message="Cluster details fetched successfully",
        code=status.HTTP_200_OK,
        object="cluster.get",
    )


@cluster_router.post(
    "/{cluster_id}/delete-workflow",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Invalid request parameters",
        },
        status.HTTP_200_OK: {
            "model": SuccessResponse,
            "description": "Successfully executed delete cluster workflow",
        },
    },
    description="Delete a cluster by ID",
)
async def delete_cluster(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    cluster_id: UUID,
) -> Union[SuccessResponse, ErrorResponse]:
    """Delete a cluster by its ID."""
    try:
        db_workflow = await ClusterService(session).delete_cluster(cluster_id, current_user.id)
        logger.debug(f"Cluster deleting initiated with workflow id: {db_workflow.id}")
        return SuccessResponse(
            message="Cluster deleting initiated successfully",
            code=status.HTTP_200_OK,
            object="cluster.delete",
        )
    except ClientException as e:
        logger.exception(f"Failed to delete cluster: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to delete cluster: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            message="Failed to delete cluster",
        ).to_http_response()


@cluster_router.post(
    "/cancel-onboarding",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": SuccessResponse,
            "description": "Successfully cancel cluster onboarding",
        },
    },
    description="Cancel cluster onboarding",
)
async def cancel_cluster_onboarding(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    cancel_request: CancelClusterOnboardingRequest,
) -> Union[SuccessResponse, ErrorResponse]:
    """Cancel cluster onboarding."""
    try:
        await ClusterService(session).cancel_cluster_onboarding_workflow(cancel_request.workflow_id)
        return SuccessResponse(
            message="Cluster onboarding cancelled successfully",
            code=status.HTTP_200_OK,
            object="cluster.cancel",
        )
    except ClientException as e:
        logger.exception(f"Failed to cancel cluster onboarding: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to cancel cluster onboarding: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to cancel cluster onboarding"
        ).to_http_response()


@cluster_router.get(
    "/{cluster_id}/endpoints",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": ClusterEndpointPaginatedResponse,
            "description": "Successfully listed all endpoints in the cluster",
        },
    },
    description="List all endpoints in a cluster.\n\nOrder by values are: name, status, created_at, project_name, model_name.",
)
async def list_all_endpoints(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    cluster_id: UUID,
    filters: Annotated[ClusterEndpointFilter, Depends()],
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=0),
    order_by: Optional[List[str]] = Depends(parse_ordering_fields),
    search: bool = False,
) -> Union[ClusterEndpointPaginatedResponse, ErrorResponse]:
    """List all endpoints in a cluster."""
    # Calculate offset
    offset = (page - 1) * limit

    # Construct filters
    filters_dict = filters.model_dump(exclude_none=True, exclude_unset=True)

    try:
        result, count = await ClusterService(session).get_all_endpoints_in_cluster(
            cluster_id, offset, limit, filters_dict, order_by, search
        )
    except ClientException as e:
        logger.exception(f"Failed to get all endpoints: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to get all endpoints: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to get all endpoints"
        ).to_http_response()

    return ClusterEndpointPaginatedResponse(
        endpoints=result,
        total_record=count,
        page=page,
        limit=limit,
        object="cluster.endpoints.list",
        code=status.HTTP_200_OK,
        message="Successfully listed all endpoints in the cluster",
    ).to_http_response()


# Cluster Metrics Endpoint
@cluster_router.get(
    "/{cluster_id}/metrics",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": ClusterMetricsResponse,
            "description": "Successfully retrieved cluster metrics",
        },
    },
    description="Get detailed metrics for a specific cluster including CPU, memory, disk, GPU, HPU, and network statistics. Use metric_type to filter specific metrics.",
)
async def get_cluster_metrics(
    cluster_id: UUID,
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    time_range: str = Query("today", enum=["today", "7days", "month"]),
    metric_type: MetricTypeEnum = Query(MetricTypeEnum.ALL, description="Type of metrics to return"),
) -> Union[ClusterMetricsResponse, ErrorResponse]:
    """Get cluster metrics."""
    try:
        metrics = await ClusterService(session).get_cluster_metrics(cluster_id, time_range, metric_type)
        return ClusterMetricsResponse(
            code=status.HTTP_200_OK, message="Successfully retrieved cluster metrics", **metrics
        )
    except ClientException as e:
        return ErrorResponse(
            code=status.HTTP_400_BAD_REQUEST,
            message=str(e),
        ).to_http_response()
    except Exception as e:
        logger.exception(f"Error retrieving cluster metrics: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            message="Error retrieving cluster metrics",
        ).to_http_response()

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/cluster_ops/crud.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The crud package, containing essential business logic, services, and routing configurations for the cluster ops."""

from typing import Any, Dict, List, Tuple
from uuid import UUID

from sqlalchemy import and_, asc, case, desc, distinct, func, select

from budapp.cluster_ops.models import Cluster
from budapp.commons import logging
from budapp.commons.db_utils import DataManagerUtils

from ..commons.constants import ClusterStatusEnum, EndpointStatusEnum
from ..endpoint_ops.models import Endpoint


logger = logging.get_logger(__name__)


class ClusterDataManager(DataManagerUtils):
    """Data manager for the Cluster model."""

    async def get_all_clusters(
        self,
        offset: int,
        limit: int,
        filters: Dict = {},  # endpoint count need to consider in future
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[Cluster], int]:
        """List all clusters from the database."""
        await self.validate_fields(Cluster, filters)

        # Subquery to count endpoints per cluster
        endpoint_count_subquery = (
            select(
                Endpoint.cluster_id,
                func.count(Endpoint.id.distinct()).label("endpoint_count"),
            )
            .where(Endpoint.status != EndpointStatusEnum.DELETED)
            .group_by(Endpoint.cluster_id)
            .alias("endpoint_count_subquery")
        )

        # Generate statements based on search or filters
        if search:
            search_conditions = await self.generate_search_stmt(Cluster, filters)
            stmt = (
                select(
                    Cluster,
                    func.coalesce(endpoint_count_subquery.c.endpoint_count, 0).label("endpoint_count"),
                )
                .filter(and_(*search_conditions))
                .select_from(Cluster)
                .join(
                    endpoint_count_subquery,
                    Cluster.id == endpoint_count_subquery.c.cluster_id,
                    isouter=True,
                )
            )
            count_stmt = select(func.count()).select_from(Cluster).filter(and_(*search_conditions))
        else:
            stmt = (
                select(
                    Cluster,
                    func.coalesce(endpoint_count_subquery.c.endpoint_count, 0).label("endpoint_count"),
                )
                .filter_by(**filters)
                .select_from(Cluster)
                .join(
                    endpoint_count_subquery,
                    Cluster.id == endpoint_count_subquery.c.cluster_id,
                    isouter=True,
                )
            )
            count_stmt = select(func.count()).select_from(Cluster).filter_by(**filters)

        # Exclude deleted clusters
        stmt = stmt.filter(Cluster.status != ClusterStatusEnum.DELETED)
        count_stmt = count_stmt.filter(Cluster.status != ClusterStatusEnum.DELETED)

        # Calculate count before applying limit and offset
        count = self.execute_scalar(count_stmt)

        # Apply limit and offset
        stmt = stmt.limit(limit).offset(offset)

        # Apply sorting
        if order_by:
            sort_conditions = await self.generate_sorting_stmt(Cluster, order_by)
            stmt = stmt.order_by(*sort_conditions)

        result = self.execute_all(stmt)

        return result, count

    async def get_available_clusters_by_cluster_ids(self, cluster_ids: List[UUID]) -> Tuple[List[Cluster], int]:
        """Get active clusters by cluster ids."""
        stmt = select(Cluster).filter(
            Cluster.cluster_id.in_(cluster_ids), Cluster.status == ClusterStatusEnum.AVAILABLE
        )
        count_stmt = (
            select(func.count())
            .select_from(Cluster)
            .filter(Cluster.cluster_id.in_(cluster_ids), Cluster.status == ClusterStatusEnum.AVAILABLE)
        )

        count = self.execute_scalar(count_stmt)
        result = self.scalars_all(stmt)

        return result, count

    async def get_inactive_clusters(self) -> Tuple[List[Cluster], int]:
        """Retrieve a list of inactive clusters and count."""
        stmt = select(Cluster).filter(
            Cluster.status.in_([ClusterStatusEnum.NOT_AVAILABLE, ClusterStatusEnum.ERROR, ClusterStatusEnum.DELETING])
        )
        count_stmt = (
            select(func.count())
            .select_from(Cluster)
            .filter(
                Cluster.status.in_(
                    [ClusterStatusEnum.NOT_AVAILABLE, ClusterStatusEnum.ERROR, ClusterStatusEnum.DELETING]
                )
            )
        )

        count = self.execute_scalar(count_stmt)
        result = self.scalars_all(stmt)

        return result, count

    async def get_all_clusters_in_project(
        self, project_id: UUID, offset: int, limit: int, filters: Dict[str, Any], order_by: List[str], search: bool
    ) -> Tuple[List[Cluster], int, int, int]:
        """Get all clusters in a project."""
        await self.validate_fields(Cluster, filters)

        # Generate statements based on search or filters
        base_conditions = [
            Endpoint.project_id == project_id,
            Cluster.status != ClusterStatusEnum.DELETED,
            Endpoint.status != EndpointStatusEnum.DELETED,
        ]
        if search:
            search_conditions = await self.generate_search_stmt(Cluster, filters)

            stmt = (
                select(
                    Cluster,
                    func.count(Endpoint.id).label("endpoint_count"),
                    func.coalesce(func.sum(Endpoint.number_of_nodes), 0).label("total_nodes"),
                    func.coalesce(func.sum(Endpoint.total_replicas), 0).label("total_replicas"),
                )
                .join(Endpoint, Endpoint.cluster_id == Cluster.id)
                .filter(*base_conditions)
                .filter(and_(*search_conditions))
                .group_by(Cluster.id)
            )
            count_stmt = (
                select(func.count(distinct(Cluster.id)))
                .select_from(Cluster)
                .join(Endpoint, Endpoint.cluster_id == Cluster.id)
                .filter(*base_conditions)
                .filter(and_(*search_conditions))
            )
        else:
            filter_conditions = [getattr(Cluster, field) == value for field, value in filters.items()]
            stmt = (
                select(
                    Cluster,
                    func.count(Endpoint.id).label("endpoint_count"),
                    func.coalesce(func.sum(Endpoint.number_of_nodes), 0).label("total_nodes"),
                    func.coalesce(func.sum(Endpoint.total_replicas), 0).label("total_replicas"),
                )
                .join(Endpoint, Endpoint.cluster_id == Cluster.id)
                .filter(*base_conditions)
                .filter(*filter_conditions)
                .group_by(Cluster.id)
            )
            count_stmt = (
                select(func.count(distinct(Cluster.id)))
                .select_from(Cluster)
                .join(Endpoint, Endpoint.cluster_id == Cluster.id)
                .filter(*base_conditions)
                .filter(*filter_conditions)
            )

        # Calculate count before applying limit and offset
        count = self.execute_scalar(count_stmt)

        # Apply limit and offset
        stmt = stmt.limit(limit).offset(offset)

        # Apply sorting
        if order_by:
            sort_conditions = await self.generate_sorting_stmt(Cluster, order_by)

            # Handle endpoint_count sorting
            for field, direction in order_by:
                if field == "endpoint_count":
                    sort_func = asc if direction == "asc" else desc
                    stmt = stmt.order_by(sort_func("endpoint_count"))
                elif field == "node_count":
                    sort_func = asc if direction == "asc" else desc
                    stmt = stmt.order_by(sort_func("total_nodes"))
                elif field == "worker_count":
                    sort_func = asc if direction == "asc" else desc
                    stmt = stmt.order_by(sort_func("total_replicas"))
                elif field == "hardware_type":
                    # Sorting by hardware type
                    hardware_type_expr = (
                        case((Cluster.cpu_count > 0, 1), else_=0)
                        + case((Cluster.gpu_count > 0, 1), else_=0)
                        + case((Cluster.hpu_count > 0, 1), else_=0)
                    ).label("hardware_type")
                    sort_func = asc if direction == "asc" else desc
                    stmt = stmt.order_by(sort_func(hardware_type_expr))

            stmt = stmt.order_by(*sort_conditions)

        result = self.session.execute(stmt)

        return result, count

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/core/services.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Implements core services and business logic that power the microservices, including key functionality and integrations."""

from typing import Any, Dict, List, Optional, Tuple

from sqlalchemy.orm import Session

from budapp.cluster_ops.services import ClusterService
from budapp.commons import logging
from budapp.commons.constants import (
    BudServeWorkflowStepEventName,
    NotificationCategory,
    PayloadType,
)
from budapp.commons.db_utils import SessionMixin
from budapp.workflow_ops.crud import WorkflowDataManager, WorkflowStepDataManager
from budapp.workflow_ops.models import Workflow as WorkflowModel
from budapp.workflow_ops.models import WorkflowStep as WorkflowStepModel

from ..endpoint_ops.services import EndpointService
from ..model_ops.services import LocalModelWorkflowService, ModelService
from .crud import IconDataManager
from .models import Icon as IconModel
from .schemas import NotificationPayload, NotificationResponse


logger = logging.get_logger(__name__)


# Notification related business logic


class NotificationService(SessionMixin):
    """Service for managing notifications."""

    async def update_recommended_cluster_events(self, payload: NotificationPayload) -> None:
        """Update the recommended cluster events for a workflow step.

        Args:
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Update workflow step data event
        await self._update_workflow_step_events(BudServeWorkflowStepEventName.BUD_SIMULATOR_EVENTS.value, payload)

        # Update progress in workflow
        await self._update_workflow_progress(BudServeWorkflowStepEventName.BUD_SIMULATOR_EVENTS.value, payload)

        # Send number of recommended cluster as notification
        if payload.event == "results":
            await ClusterService(self.session)._notify_recommended_cluster_from_notification_event(payload)

    async def update_model_deployment_events(self, payload: NotificationPayload) -> None:
        """Update the model deployment events for a workflow step.

        Args:
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Update workflow step data event
        await self._update_workflow_step_events(BudServeWorkflowStepEventName.BUDSERVE_CLUSTER_EVENTS.value, payload)

        # Update progress in workflow
        await self._update_workflow_progress(BudServeWorkflowStepEventName.BUDSERVE_CLUSTER_EVENTS.value, payload)

        # Create endpoint when deployment is completed
        if payload.event == "results":
            await EndpointService(self.session).create_endpoint_from_notification_event(payload)

    async def update_cluster_creation_events(self, payload: NotificationPayload) -> None:
        """Update the cluster creation events for a workflow step.

        Args:
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Update workflow step data event
        await self._update_workflow_step_events(BudServeWorkflowStepEventName.CREATE_CLUSTER_EVENTS.value, payload)

        # Update progress in workflow
        await self._update_workflow_progress(BudServeWorkflowStepEventName.CREATE_CLUSTER_EVENTS.value, payload)

        # Create cluster in database if node info fetched successfully
        if payload.event == "results":
            await ClusterService(self.session).create_cluster_from_notification_event(payload)

    async def update_model_extraction_events(self, payload: NotificationPayload) -> None:
        """Update the model extraction events for a workflow step.

        Args:
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Update workflow step data event
        await self._update_workflow_step_events(BudServeWorkflowStepEventName.MODEL_EXTRACTION_EVENTS.value, payload)

        # Update progress in workflow
        await self._update_workflow_progress(BudServeWorkflowStepEventName.MODEL_EXTRACTION_EVENTS.value, payload)

        # Create cluster in database if node info fetched successfully
        if payload.event == "results":
            await LocalModelWorkflowService(self.session).create_model_from_notification_event(payload)

    async def update_delete_cluster_events(self, payload: NotificationPayload) -> None:
        """Update the delete cluster events for a workflow step.

        Args:
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Update workflow step data event
        await self._update_workflow_step_events(BudServeWorkflowStepEventName.DELETE_CLUSTER_EVENTS.value, payload)

        # Update progress in workflow
        await self._update_workflow_progress(BudServeWorkflowStepEventName.DELETE_CLUSTER_EVENTS.value, payload)

        # Create cluster in database if node info fetched successfully
        if payload.event == "results":
            await ClusterService(self.session).delete_cluster_from_notification_event(payload)

    async def update_delete_endpoint_events(self, payload: NotificationPayload) -> None:
        """Update the delete endpoint events for a workflow step.

        Args:
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Update workflow step data event
        await self._update_workflow_step_events(BudServeWorkflowStepEventName.DELETE_ENDPOINT_EVENTS.value, payload)

        # Update progress in workflow
        await self._update_workflow_progress(BudServeWorkflowStepEventName.DELETE_ENDPOINT_EVENTS.value, payload)

        # Create cluster in database if node info fetched successfully
        if payload.event == "results":
            await EndpointService(self.session).delete_endpoint_from_notification_event(payload)

    async def update_delete_worker_events(self, payload: NotificationPayload) -> None:
        """Update the delete worker events for a workflow step.

        Args:
            payload: The payload to update the step with.

        Returns:
            None
        """
        await self._update_workflow_step_events(BudServeWorkflowStepEventName.DELETE_WORKER_EVENTS.value, payload)
        # Create cluster in database if node info fetched successfully
        if payload.event == "results":
            await EndpointService(self.session).delete_worker_from_notification_event(payload)

    async def update_model_security_scan_events(self, payload: NotificationPayload) -> None:
        """Update the model security scan events for a workflow step.

        Args:
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Update workflow step data event
        await self._update_workflow_step_events(
            BudServeWorkflowStepEventName.MODEL_SECURITY_SCAN_EVENTS.value, payload
        )

        # Update progress in workflow
        await self._update_workflow_progress(BudServeWorkflowStepEventName.MODEL_SECURITY_SCAN_EVENTS.value, payload)

        # Create cluster in database if node info fetched successfully
        if payload.event == "results":
            await LocalModelWorkflowService(self.session).create_scan_result_from_notification_event(payload)

    async def update_license_faqs_update_events(self, payload: NotificationPayload) -> None:
        """Update the model license faqs events for a workflow step.

        Args:
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Update workflow step data event
        await self._update_workflow_step_events(BudServeWorkflowStepEventName.LICENSE_FAQ_EVENTS.value, payload)

        # Update progress in workflow
        await self._update_workflow_progress(BudServeWorkflowStepEventName.LICENSE_FAQ_EVENTS.value, payload)

        # update faqs in database if node info fetched successfully
        if payload.event == "results":
            await ModelService(self.session).update_license_faqs_from_notification_event(payload)

    async def update_cluster_status_update_events(self, payload: NotificationPayload) -> None:
        """Update the cluster status update events for a workflow step.

        Args:
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Update cluster status in database
        if payload.event == "results":
            await ClusterService(self.session).update_cluster_status_from_notification_event(payload)

    async def update_endpoint_status_update_events(self, payload: NotificationPayload) -> None:
        """Update the endpoint status update events for a workflow step.

        Args:
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Update endpoint status in database
        if payload.event == "results":
            await EndpointService(self.session).update_endpoint_status_from_notification_event(payload)

    async def update_add_worker_to_deployment_events(self, payload: NotificationPayload) -> None:
        """Update the add worker to deployment events for a workflow step.

        Args:
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Update workflow step data event
        await self._update_workflow_step_events(BudServeWorkflowStepEventName.BUDSERVE_CLUSTER_EVENTS.value, payload)

        # Update progress in workflow
        await self._update_workflow_progress(BudServeWorkflowStepEventName.BUDSERVE_CLUSTER_EVENTS.value, payload)

        # Add worker to deployment
        if payload.event == "results":
            await EndpointService(self.session).add_worker_from_notification_event(payload)

    async def _update_workflow_step_events(self, event_name: str, payload: NotificationPayload) -> None:
        """Update the workflow step events for a workflow step.

        Args:
            event_name: The name of event to update.
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Fetch workflow steps with simulator events
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps_by_data(
            data_key=event_name, workflow_id=payload.workflow_id
        )

        if not db_workflow_steps:
            logger.warning(f"No workflow steps found for workflow {payload.workflow_id}")
            return

        # Get and validate latest step
        latest_step = db_workflow_steps[-1]

        # Update the payload for the event
        updated_data = await self._update_step_data(event_name, latest_step, payload)

        if not updated_data:
            logger.warning(f"No matching event found for {payload.event}")
            return

        # Update the workflow step data
        # refresh sqlalchemy session otherwise the updated data will not be reflected in the session
        self.session.refresh(latest_step)
        db_workflow_step = await WorkflowStepDataManager(self.session).update_by_fields(
            latest_step, {"data": updated_data}
        )
        logger.info(f"Updated workflow step with {event_name} events: {db_workflow_step.id}")

    async def _update_step_data(self, event_name: str, step: WorkflowStepModel, payload: NotificationPayload) -> dict:
        """Update the payload for the event in the step data.

        Args:
            event_name: The name of event to update.
            step: The workflow step to update.
            payload: The payload to update the step with.

        Returns:
            The updated step data or None if the update failed.
        """
        data = step.data
        simulator_events = data.get(event_name, {})
        steps = simulator_events.get("steps", [])

        if not isinstance(steps, list):
            logger.warning("Steps data is not in expected format")
            return None

        updated = False
        for step_data in steps:
            if isinstance(step_data, dict) and step_data.get("id") == payload.event:
                step_data["payload"] = payload.model_dump(exclude_unset=True, mode="json")
                updated = True
                break

        return data if updated else None

    async def _update_workflow_progress(self, event_name: str, payload: NotificationPayload) -> None:
        """Update the workflow progress for a workflow step.

        Args:
            event_name: The name of event to update.
            payload: The payload to update the step with.

        Returns:
            None
        """
        # Fetch workflow with progress
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(
            WorkflowModel, {"id": payload.workflow_id}
        )

        if not isinstance(db_workflow.progress, dict):
            logger.warning(f"Workflow {payload.workflow_id} progress is not in expected format")
            return

        progress_type = db_workflow.progress.get("progress_type")
        if progress_type != event_name:
            logger.warning(f"Progress type {progress_type} does not match event name {event_name}")
            return

        updated_progress = await self._update_progress_data(db_workflow.progress, payload)
        if not updated_progress:
            logger.warning(f"No matching event found for {payload.event}")
            return

        # Update progress in workflow
        self.session.refresh(db_workflow)
        db_workflow = await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"progress": updated_progress}
        )
        logger.info(f"Updated workflow progress: {db_workflow.id}")

    async def _update_progress_data(
        self, progress: Dict[str, Any], payload: NotificationPayload
    ) -> Optional[Dict[str, Any]]:
        """Update the progress data for a workflow.

        Args:
            payload: The payload to update the step.
            progress: The progress data to update.

        Returns:
            The updated progress data or None if the update failed.
        """
        steps = progress.get("steps", [])

        if not isinstance(steps, list):
            logger.warning("Steps data is not in expected format")
            return

        updated = False
        for step_data in steps:
            if isinstance(step_data, dict) and step_data.get("id") == payload.event:
                step_data["payload"] = payload.model_dump(exclude_unset=True, mode="json")
                updated = True
                break

        return progress if updated else None


class SubscriberHandler:
    """Service for handling subscriber events."""

    def __init__(self, session: Session):
        """Initialize the SubscriberHandler with a database session."""
        self.session = session

    async def handle_subscriber_event(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the subscriber event."""
        logger.debug(f"Received subscriber event: {payload}")

        if payload.category != NotificationCategory.INTERNAL:
            logger.warning("Skipping non-internal notification")
            return NotificationResponse(
                object="notification",
                message="Pubsub notification received",
            ).to_http_response()

        handlers = {
            PayloadType.DEPLOYMENT_RECOMMENDATION: self._handle_deployment_recommendation,
            PayloadType.DEPLOY_MODEL: self._handle_deploy_model,
            PayloadType.REGISTER_CLUSTER: self._handle_register_cluster,
            PayloadType.PERFORM_MODEL_EXTRACTION: self._handle_perform_model_extraction,
            PayloadType.PERFORM_MODEL_SECURITY_SCAN: self._handle_perform_model_security_scan,
            PayloadType.DELETE_CLUSTER: self._handle_delete_cluster,
            PayloadType.DELETE_DEPLOYMENT: self._handle_delete_endpoint,
            PayloadType.CLUSTER_STATUS_UPDATE: self._handle_cluster_status_update,
            PayloadType.DEPLOYMENT_STATUS_UPDATE: self._handle_endpoint_status_update,
            PayloadType.DELETE_WORKER: self._handle_delete_worker,
            PayloadType.ADD_WORKER: self._handle_add_worker_to_deployment,
            PayloadType.FETCH_LICENSE_FAQS: self._handle_license_faqs_update,
        }

        handler = handlers.get(payload.type)
        if not handler:
            logger.warning(f"No handler found for payload type: {payload.type}")
            return NotificationResponse(
                object="notification", message="Pubsub notification received"
            ).to_http_response()

        return await handler(payload)

    async def _handle_deployment_recommendation(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the deployment recommendation event."""
        await NotificationService(self.session).update_recommended_cluster_events(payload)
        return NotificationResponse(
            object="notification",
            message="Updated recommended cluster event in workflow step",
        ).to_http_response()

    async def _handle_deploy_model(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the deploy model event."""
        await NotificationService(self.session).update_model_deployment_events(payload)
        return NotificationResponse(
            object="notification",
            message="Updated model deployment event in workflow step",
        ).to_http_response()

    async def _handle_register_cluster(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the register cluster event."""
        await NotificationService(self.session).update_cluster_creation_events(payload)
        return NotificationResponse(
            object="notification",
            message="Updated cluster creation event in workflow step",
        ).to_http_response()

    async def _handle_perform_model_extraction(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the perform model extraction event."""
        await NotificationService(self.session).update_model_extraction_events(payload)
        return NotificationResponse(
            object="notification",
            message="Updated model extraction event in workflow step",
        ).to_http_response()

    async def _handle_perform_model_security_scan(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the perform model security scan event."""
        await NotificationService(self.session).update_model_security_scan_events(payload)
        return NotificationResponse(
            object="notification",
            message="Updated model security scan event in workflow step",
        ).to_http_response()

    async def _handle_delete_cluster(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the delete cluster event."""
        await NotificationService(self.session).update_delete_cluster_events(payload)
        return NotificationResponse(
            object="notification",
            message="Updated delete cluster event in workflow step",
        ).to_http_response()

    async def _handle_delete_endpoint(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the delete endpoint event."""
        await NotificationService(self.session).update_delete_endpoint_events(payload)
        return NotificationResponse(
            object="notification",
            message="Updated delete endpoint event in workflow step",
        ).to_http_response()

    async def _handle_cluster_status_update(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the cluster status update event."""
        await NotificationService(self.session).update_cluster_status_update_events(payload)
        return NotificationResponse(
            object="notification",
            message="Update cluster status in db",
        ).to_http_response()

    async def _handle_endpoint_status_update(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the endpoint status update event."""
        await NotificationService(self.session).update_endpoint_status_update_events(payload)
        return NotificationResponse(
            object="notification",
            message="Update endpoint status in db",
        ).to_http_response()

    async def _handle_delete_worker(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the delete worker event."""
        await NotificationService(self.session).update_delete_worker_events(payload)
        return NotificationResponse(
            object="notification",
            message="Updated delete worker event in workflow step",
        ).to_http_response()

    async def _handle_add_worker_to_deployment(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the add worker to deployment event."""
        await NotificationService(self.session).update_add_worker_to_deployment_events(payload)
        return NotificationResponse(
            object="notification",
            message="Updated add worker to deployment event in workflow step",
        ).to_http_response()

    async def _handle_license_faqs_update(self, payload: NotificationPayload) -> NotificationResponse:
        """Handle the license faq update event."""
        await NotificationService(self.session).update_license_faqs_update_events(payload)
        return NotificationResponse(
            object="notification",
            message="Update license faqs in db",
        ).to_http_response()


class IconService(SessionMixin):
    """Service for managing icons."""

    async def get_all_icons(
        self,
        offset: int = 0,
        limit: int = 10,
        filters: Dict = {},
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[IconModel], int]:
        """Get all icon icons."""
        return await IconDataManager(self.session).get_all_icons(offset, limit, filters, order_by, search)

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/core/models.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, containing essential business logic, services, and routing configurations for the microservices."""

from datetime import datetime
from typing import Optional
from uuid import UUID, uuid4

from sqlalchemy import ARRAY, DateTime, Integer, String, Uuid
from sqlalchemy.dialects.postgresql import ENUM as PG_ENUM
from sqlalchemy.orm import Mapped, mapped_column

from budapp.commons.constants import ModelTemplateTypeEnum
from budapp.commons.database import Base


class Icon(Base):
    """Icon model."""

    __tablename__ = "icon"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    name: Mapped[str] = mapped_column(String, index=True, nullable=False)
    file_path: Mapped[str] = mapped_column(String, unique=True, nullable=False)
    category: Mapped[str] = mapped_column(String, index=True, nullable=False)


class ModelTemplate(Base):
    """Model template model."""

    __tablename__ = "model_template"
    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    name: Mapped[str] = mapped_column(String, nullable=False)
    description: Mapped[str] = mapped_column(String, nullable=False)
    icon: Mapped[str] = mapped_column(String, nullable=False)
    template_type: Mapped[str] = mapped_column(
        PG_ENUM(
            ModelTemplateTypeEnum,
            name="template_type_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=False,
        unique=True,
    )
    avg_sequence_length: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)
    avg_context_length: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)
    per_session_tokens_per_sec: Mapped[list[int]] = mapped_column(ARRAY(Integer), nullable=True)
    ttft: Mapped[list[int]] = mapped_column(ARRAY(Integer), nullable=True)
    e2e_latency: Mapped[list[int]] = mapped_column(ARRAY(Integer), nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/core/meta_routes.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Defines metadata routes for the microservices, providing endpoints for retrieving service-level information."""

from datetime import datetime

from fastapi import APIRouter, Response, status

from budapp.commons import logging
from budapp.commons.config import app_settings, secrets_settings
from budapp.commons.schemas import ErrorResponse, SuccessResponse
from budapp.shared.dapr_service import DaprService


logger = logging.get_logger(__name__)

meta_router = APIRouter()


@meta_router.get(
    "/",
    response_model=SuccessResponse,
    status_code=status.HTTP_200_OK,
    description="Get microservice details.",
    tags=["Metadata"],
)
async def ping() -> Response:
    r"""Handle the endpoint to return details about the microservice.

    Calculate and return information including service name, version, description, environment, debugging status,
    deployment time, and uptime. The response is modeled using `SuccessResponse`.

    Returns:
        Response: A `SuccessResponse` containing the service information and HTTP status code 200.

    Example:
        >>> response = await ping()
        >>> response.status_code
        200
        >>> response.json()
        {
            "object": "info",
            "message": "Microservice: MyService v1.0\nDescription: A sample service\nEnvironment: DEVELOPMENT\nDebugging: Enabled\nDeployed at: 2024-01-01 12:00:00\nUptime: 1h:30m:45s"
        }
    """
    uptime_in_seconds = int((datetime.now(tz=app_settings.tzone) - app_settings.deployed_at).total_seconds())
    hours, remainder = divmod(uptime_in_seconds, 3600)
    minutes, seconds = divmod(remainder, 60)

    info = (
        f"Microservice: {app_settings.name} v{app_settings.version}\n"
        f"Description: {app_settings.description}\n"
        f"Environment: {app_settings.env}\n"
        f"Debugging: {'Enabled' if app_settings.debug else 'Disabled'}\n"
        f"Deployed at: {app_settings.deployed_at}\n"
        f"Uptime: {hours}h:{minutes}m:{seconds}s"
    )

    return SuccessResponse(message=info, code=status.HTTP_200_OK).to_http_response()


@meta_router.get(
    "/health",
    response_model=SuccessResponse,
    status_code=status.HTTP_200_OK,
    description="Get microservice health.",
    tags=["Metadata"],
)
async def health() -> Response:
    """Handle the endpoint to return the health status of the microservice.

    Provides a simple acknowledgment response to indicate that the microservice is running and healthy.
    The response is modeled using `SuccessResponse`.

    Returns:
        Response: A `SuccessResponse` containing an acknowledgment message and HTTP status code 200.

    Example:
        >>> response = await health()
        >>> response.status_code
        200
        >>> response.json()
        {
            "object": "info",
            "message": "ack"
        }
    """
    return SuccessResponse(message="ack", code=status.HTTP_200_OK).to_http_response()


@meta_router.get(
    "/sync/configurations",
    response_model=SuccessResponse,
    status_code=status.HTTP_200_OK,
    responses={
        status.HTTP_503_SERVICE_UNAVAILABLE: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to a misconfigured configuration store",
        }
    },
    description="Sync microservice configuration from a supported configstore.",
    tags=["Sync"],
)
async def sync_configurations() -> Response:
    """Synchronize the microservice configuration from a supported configstore.

    Check if a configstore is configured and syncs the microservice configuration fields from it.
    The configurations are fetched from the configstore, updated in the application settings,
    and a success message with the count of configurations synced is returned.

    Returns:
        Response: A `SuccessResponse` with the count of configurations synced and HTTP status code 200,
        or an `ErrorResponse` if the configstore is not configured, with HTTP status code 503.

    Raises:
        HTTPException: If the configstore is not configured, an HTTP 503 Service Unavailable error is returned.

    Example:
        >>> response = await sync_configurations()
        >>> response.status_code
        200
        >>> response.json()
        {
            "object": "info",
            "message": "5/10 configuration(s) synced."
        }
    """
    if app_settings.configstore_name:
        fields_to_sync = app_settings.get_fields_to_sync()

        with DaprService() as dapr_service:
            values, _ = await dapr_service.sync_configurations(fields_to_sync)

        app_settings.update_fields(values)

        return SuccessResponse(
            message=f"{len(values)}/{len(fields_to_sync)} configuration(s) synced.",
            code=status.HTTP_200_OK,
        ).to_http_response()
    else:
        return ErrorResponse(
            message="Config store is not configured.",
            code=status.HTTP_503_SERVICE_UNAVAILABLE,
        ).to_http_response()


@meta_router.get(
    "/sync/secrets",
    response_model=SuccessResponse,
    status_code=status.HTTP_200_OK,
    responses={
        status.HTTP_503_SERVICE_UNAVAILABLE: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to a misconfigured secret store",
        }
    },
    description="Sync microservice secrets from a supported secret store.",
    tags=["Sync"],
)
async def sync_secrets() -> Response:
    """Synchronize microservice secrets from a supported secret store.

    Check if a secret store is configured and syncs the microservice secret fields from it.
    The secrets are fetched from the secret store, updated in the application settings,
    and a success message with the count of secrets synced is returned.

    Returns:
        Response: A `SuccessResponse` with the count of secrets synced and HTTP status code 200,
        or an `ErrorResponse` if the secret store is not configured, with HTTP status code 503.

    Raises:
        HTTPException: If the secret store is not configured, an HTTP 503 Service Unavailable error is returned.

    Example:
        >>> response = await sync_secrets()
        >>> response.status_code
        200
        >>> response.json()
        {
            "object": "info",
            "message": "7/10 secret(s) synced."
        }
    """
    if app_settings.secretstore_name:
        fields_to_sync = secrets_settings.get_fields_to_sync()

        with DaprService() as dapr_service:
            values = await dapr_service.sync_secrets(fields_to_sync)

        secrets_settings.update_fields(values)

        return SuccessResponse(
            message=f"{len(values)}/{len(fields_to_sync)} secret(s) synced.",
            code=status.HTTP_200_OK,
        ).to_http_response()
    else:
        return ErrorResponse(
            message="Secret store is not configured.",
            code=status.HTTP_503_SERVICE_UNAVAILABLE,
        ).to_http_response()


@meta_router.get(
    "/register",
    response_model=SuccessResponse,
    status_code=status.HTTP_200_OK,
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service registration failures due to internal issues.",
        }
    },
    description="Register the microservice to the ecosystem.",
    tags=["Sync"],
)
async def register_service() -> Response:
    """Register the microservice to the ecosystem.

    This endpoint attempts to register the current microservice with the ecosystem
    using the DaprService. If successful, it returns a SuccessResponse. In case of
    any failures during the registration process, it returns an ErrorResponse.

    Returns:
        Response: A SuccessResponse with HTTP status code 200 if registration is successful,
                  or an ErrorResponse with HTTP status code 500 if registration fails.

    Raises:
        HTTPException: Implicitly raised with status code 500 if an exception occurs during registration.

    Example:
        >>> response = await register_service()
        >>> response.status_code
        200
        >>> response.json()
        {
            "object": "info",
            "message": "Service registration successful."
        }
    """
    try:
        with DaprService() as dapr_service:
            await dapr_service.register_service()

        return SuccessResponse(
            message="Service registration successful.",
            code=status.HTTP_200_OK,
        ).to_http_response()
    except Exception as e:
        logger.exception("Service registration failed with %s", str(e))

        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Service registration failed."
        ).to_http_response()

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/core/common_routes.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Defines common routes for the microservices, providing endpoints for retrieving common information."""

from typing import Annotated, List, Optional, Union

from fastapi import APIRouter, Depends, Query, status
from sqlalchemy.orm import Session

from budapp.commons import logging
from budapp.commons.dependencies import (
    get_current_active_user,
    get_session,
    parse_ordering_fields,
)
from budapp.commons.schemas import ErrorResponse
from budapp.user_ops.schemas import User

from .schemas import IconFilter, IconListResponse
from .services import IconService


logger = logging.get_logger(__name__)

common_router = APIRouter()


@common_router.get(
    "/icons",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": IconListResponse,
            "description": "Successfully list all icons",
        },
    },
    description="List all icons",
    tags=["icon"],
)
async def list_all_icons(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    filters: Annotated[IconFilter, Depends()],
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=0),
    order_by: Optional[List[str]] = Depends(parse_ordering_fields),
    search: bool = False,
) -> Union[IconListResponse, ErrorResponse]:
    """List all icons."""
    # Calculate offset
    offset = (page - 1) * limit

    # Convert UserFilter to dictionary
    filters_dict = filters.model_dump(exclude_none=True)

    try:
        db_icons, count = await IconService(session).get_all_icons(offset, limit, filters_dict, order_by, search)
    except Exception as e:
        logger.exception(f"Failed to get all icons: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to get all icons"
        ).to_http_response()

    return IconListResponse(
        icons=db_icons,
        total_record=count,
        page=page,
        limit=limit,
        object="icons.list",
        code=status.HTTP_200_OK,
    ).to_http_response()

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/core/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, containing essential business logic, services, and routing configurations for the microservices."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/core/schemas.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Contains core Pydantic schemas used for data validation and serialization within the core services."""

from typing import Any, Dict, List, Literal, Optional, Self, Union

from pydantic import UUID4, BaseModel, ConfigDict, field_validator, model_validator

from budapp.commons import logging
from budapp.commons.constants import (
    ModelTemplateTypeEnum,
    NotificationCategory,
    NotificationType,
)
from budapp.commons.schemas import (
    CloudEventBase,
    PaginatedSuccessResponse,
    SuccessResponse,
)


logger = logging.get_logger(__name__)


# Schemas related to icons
class IconBase(BaseModel):
    """Base icon schema."""

    name: str
    file_path: str
    category: str


class IconCreate(IconBase):
    """Create icon schema."""

    pass


class IconUpdate(BaseModel):
    """Update icon schema."""

    category: str
    name: str


class IconResponse(IconBase):
    """Icon response schema."""

    model_config = ConfigDict(from_attributes=True)

    id: UUID4


class IconFilter(BaseModel):
    """Icon filter schema."""

    name: str | None = None
    category: str | None = None


class IconListResponse(PaginatedSuccessResponse):
    """Icon list response schema."""

    icons: List[IconResponse]


# Schemas related to notifications


class NotificationContent(BaseModel):
    """Represents the content of a notification."""

    title: str | None = None
    message: str | None = None
    status: str | None = None
    result: Optional[Dict[str, Any]] = None
    primary_action: str | None = None
    secondary_action: str | None = None
    icon: str | None = None
    tag: str | None = None


class NotificationPayload(BaseModel):
    """Schema for notification payload."""

    category: NotificationCategory
    type: str | None = None
    event: str | None = None
    workflow_id: str | None = None
    source: str
    content: NotificationContent


class NotificationRequest(CloudEventBase):
    """Represents a notification request."""

    notification_type: NotificationType = NotificationType.EVENT
    name: str  # Workflow identifier
    subscriber_ids: Optional[Union[str, List[str]]] = None
    actor: Optional[str] = None
    topic_keys: Optional[Union[str, List[str]]] = None
    payload: NotificationPayload

    @model_validator(mode="before")
    def log_notification_hits(cls, data):
        """Log the notification hits for debugging purposes."""
        # TODO: remove this function after Debugging
        logger.info("================================================")
        logger.info("Received hit in notifications/:")
        logger.info(f"{data}")
        logger.info("================================================")
        return data

    @model_validator(mode="after")
    def validate_notification_rules(self) -> Self:
        """Check if required fields are present in the request.

        Raises:
            ValueError: If `subscriber_ids` is not present for event notifications.
            ValueError: If `topic_keys` is not present for topic notifications.

        Returns:
            Self: The instance of the class.
        """
        # NOTE: Commented out this condition for deployment, cluster status updates.
        # if self.notification_type == NotificationType.EVENT and not self.subscriber_ids:
        #     raise ValueError("subscriber_ids is required for event notifications")
        if self.notification_type == NotificationType.TOPIC and not self.topic_keys:
            raise ValueError("topic_keys is required for topic notifications")
        if self.notification_type == NotificationType.BROADCAST and (self.subscriber_ids or self.topic_keys):
            raise ValueError("subscriber_ids and topic_keys are not allowed for broadcast notifications")

        return self


class NotificationResponse(SuccessResponse):
    """Represents a notification response."""

    pass


class ModelTemplateCreate(BaseModel):
    """Model template create schema."""

    name: str
    description: str
    icon: str
    template_type: ModelTemplateTypeEnum
    avg_sequence_length: Optional[int] = None
    avg_context_length: Optional[int] = None
    per_session_tokens_per_sec: Optional[list[int]] = None
    ttft: Optional[list[int]] = None
    e2e_latency: Optional[list[int]] = None

    @field_validator("per_session_tokens_per_sec", "ttft", "e2e_latency", mode="before")
    @classmethod
    def validate_int_range(cls, value):
        if value is not None and (
            not isinstance(value, list) or len(value) != 2 or not all(isinstance(x, int) for x in value)
        ):
            raise ValueError("Must be a list of two integers")
        return value


class ModelTemplateUpdate(ModelTemplateCreate):
    """Model template update schema."""

    pass


class NotificationResult(BaseModel):
    """Notification result schema."""

    target_type: Literal["model", "cluster", "endpoint", "project", "workflow", "user"] | None = None
    target_id: UUID4 | None = None

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/core/notify_routes.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Defines metadata routes for the microservices, providing endpoints for retrieving service-level information."""

from typing import Annotated

from fastapi import APIRouter, Depends, Response, status
from sqlalchemy.orm import Session

from budapp.commons import logging
from budapp.commons.api_utils import pubsub_api_endpoint
from budapp.commons.dependencies import get_session
from budapp.commons.exceptions import ClientException
from budapp.commons.schemas import ErrorResponse

from .schemas import NotificationRequest, NotificationResponse
from .services import SubscriberHandler


logger = logging.get_logger(__name__)

notify_router = APIRouter()


@notify_router.post(
    "/notifications",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": NotificationResponse,
            "description": "Successfully triggered notification",
        },
    },
    status_code=status.HTTP_200_OK,
    description="Triggers a notification. Can be used for both API and PubSub. Refer to NotificationRequest schema for details.",
    tags=["Notifications"],
)
@pubsub_api_endpoint(NotificationRequest)
async def receive_notification(
    notification: NotificationRequest,
    session: Annotated[Session, Depends(get_session)],
) -> Response:
    """Receives a notification.

    This method interacts with other microservices to receive notifications.

    Args:
        notification (NotificationRequest): The request object containing notification details
            such as payload.

    Returns:
        Response: A response object containing the status of the notification receiving
        process and related information.
    """
    logger.debug("Received request to subscribe to bud-serve-app notifications")
    try:
        logger.info("Subscribed to bud-serve-app notifications successfully")
        payload = notification.payload
        return await SubscriberHandler(session).handle_subscriber_event(payload)
    except ClientException as e:
        logger.exception(f"Failed to execute notification: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as err:
        logger.exception(f"Unexpected error occurred while receiving notification. {err}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            type="InternalServerError",
            message="Unexpected error occurred while receiving notification.",
        )

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/core/crud.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The crud package, containing essential business logic, services, and routing configurations for the microservices."""

from typing import Dict, List, Tuple

from sqlalchemy import func, or_, select

from budapp.commons import logging
from budapp.commons.db_utils import DataManagerUtils

from .models import Icon as IconModel
from .models import ModelTemplate


logger = logging.get_logger(__name__)


class IconDataManager(DataManagerUtils):
    """Data manager for the Icon model."""

    async def get_all_icons(
        self,
        offset: int,
        limit: int,
        filters: Dict = {},
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[IconModel], int]:
        """List all icons in the database."""
        # Validate filter fields
        await self.validate_fields(IconModel, filters)

        # Generate statements according to search or filters
        if search:
            search_conditions = await self.generate_search_stmt(IconModel, filters)
            stmt = select(
                IconModel,
            ).filter(or_(*search_conditions))
            count_stmt = select(func.count()).select_from(IconModel).filter(or_(*search_conditions))
        else:
            stmt = select(
                IconModel,
            ).filter_by(**filters)
            count_stmt = select(func.count()).select_from(IconModel).filter_by(**filters)

        # Calculate count before applying limit and offset
        count = self.execute_scalar(count_stmt)

        # Apply limit and offset
        stmt = stmt.limit(limit).offset(offset)

        # Apply sorting
        if order_by:
            sort_conditions = await self.generate_sorting_stmt(IconModel, order_by)
            stmt = stmt.order_by(*sort_conditions)

        result = self.scalars_all(stmt)

        return result, count


class ModelTemplateDataManager(DataManagerUtils):
    """Model template data manager class responsible for operations over database."""

    async def get_all_model_templates(
        self,
        offset: int,
        limit: int,
        filters: Dict = {},
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[ModelTemplate], int]:
        """List all model templates in the database."""
        # Validate filter fields
        await self.validate_fields(ModelTemplate, filters)

        # Generate statements according to search or filters
        if search:
            search_conditions = await self.generate_search_stmt(ModelTemplate, filters)
            stmt = select(
                ModelTemplate,
            ).filter(or_(*search_conditions))
            count_stmt = select(func.count()).select_from(ModelTemplate).filter(or_(*search_conditions))
        else:
            stmt = select(
                ModelTemplate,
            ).filter_by(**filters)
            count_stmt = select(func.count()).select_from(ModelTemplate).filter_by(**filters)

        # Calculate count before applying limit and offset
        count = self.execute_scalar(count_stmt)

        # Apply limit and offset
        stmt = stmt.limit(limit).offset(offset)

        # Apply sorting
        if order_by:
            sort_conditions = await self.generate_sorting_stmt(ModelTemplate, order_by)
            stmt = stmt.order_by(*sort_conditions)

        result = self.scalars_all(stmt)

        return result, count

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/auth/services.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Implements auth services and business logic that power the microservices, including key functionality and integrations."""

from budapp.commons import logging
from budapp.commons.config import secrets_settings
from budapp.commons.constants import UserStatusEnum
from budapp.commons.db_utils import SessionMixin
from budapp.commons.exceptions import ClientException
from budapp.commons.security import HashManager
from budapp.user_ops.crud import UserDataManager
from budapp.user_ops.models import User as UserModel

from .schemas import UserLogin, UserLoginData
from .token import TokenService


logger = logging.get_logger(__name__)


class AuthService(SessionMixin):
    async def login_user(self, user: UserLogin) -> UserLoginData:
        """Login a user with email and password."""
        # Get user
        db_user = await UserDataManager(self.session).retrieve_by_fields(
            UserModel, {"email": user.email}, missing_ok=True
        )

        # Check if user exists
        if not db_user:
            logger.debug(f"User not found in database: {user.email}")
            raise ClientException("This email is not registered")

        # Check if password is correct
        salted_password = user.password + secrets_settings.password_salt
        if not await HashManager().verify_hash(salted_password, db_user.password):
            logger.debug(f"Password incorrect for {user.email}")
            raise ClientException("Incorrect email or password")

        if db_user.status == UserStatusEnum.DELETED:
            logger.debug(f"User account is not active: {user.email}")
            raise ClientException("User account is not active")

        logger.debug(f"User Retrieved: {user.email}")

        # Create auth token
        token = await TokenService(self.session).create_auth_token(str(db_user.auth_id))

        return UserLoginData(
            token=token,
            first_login=db_user.first_login,
            is_reset_password=db_user.is_reset_password,
        )

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/auth/token.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Implements token services and business logic that power the microservices, including key functionality and integrations."""

from datetime import datetime, timedelta, timezone
from typing import Any, Dict
from uuid import UUID, uuid4

from jose import jwt

from budapp.auth.schemas import AccessTokenData, AuthToken
from budapp.commons import logging
from budapp.commons.config import app_settings, secrets_settings
from budapp.commons.constants import JWT_ALGORITHM, TokenTypeEnum
from budapp.commons.db_utils import SessionMixin
from budapp.commons.security import HashManager

from .crud import TokenDataManager
from .models import Token as TokenModel
from .schemas import RefreshTokenData, TokenCreate


logger = logging.get_logger(__name__)


class TokenService(SessionMixin):
    async def create_auth_token(self, auth_id: str) -> AuthToken:
        """Create an authentication token for a given auth ID.

        This method generates both an access token and a refresh token
        for the provided auth ID and returns them wrapped in an AuthToken object.

        Args:
            auth_id (str): The unique identifier for the authentication entity.

        Returns:
            AuthToken: An object containing the access token, refresh token,
                and token type.

        Raises:
            ValueError: If the auth_id is empty or invalid.
            TokenCreationError: If there's an error during token creation.
        """
        # Create and return the Token instance
        auth_token = AuthToken(
            access_token=await self._create_access_token(auth_id),
            refresh_token=await self._create_refresh_token(auth_id),
            token_type="Bearer",
        )

        return auth_token

    async def _create_access_token(self, auth_id: str) -> str:
        """Create an access token for the given auth ID.

        This method generates an access token using the provided auth ID. The token
        includes the auth ID as the subject and has an expiration time set according
        to the application settings.

        Args:
            auth_id (str): The unique identifier for the authentication entity.

        Returns:
            str: A JWT access token as a string.

        Raises:
            ValueError: If the auth_id is empty or invalid.
            JWTError: If there's an error during JWT token creation.
        """
        access_token_data = AccessTokenData(sub=auth_id)

        # Generate access token
        access_token = await self._create_jwt_token(
            access_token_data.model_dump(),
            expires_delta=timedelta(minutes=app_settings.access_token_expire_minutes),
        )
        logger.debug("Access token generated")

        return access_token

    async def _create_refresh_token(self, auth_id: str) -> str:
        """Create a refresh token for the given auth ID and stores its information in the database.

        This method generates a refresh token using the provided auth ID and a dynamically
        generated secret key. The token is then stored in the database along with its hash
        and other relevant information.

        Args:
            auth_id (str): The unique identifier for the authentication entity.

        Returns:
            str: A JWT refresh token as a string.

        Raises:
            ValueError: If the auth_id is empty or invalid.
            JWTError: If there's an error during JWT token creation.
            DatabaseError: If there's an error storing the token information in the database.
        """
        # Generate a dynamic secret key for the refresh token
        dynamic_secret_key = uuid4().hex
        refresh_token_data = RefreshTokenData(sub=auth_id, secret_key=dynamic_secret_key)

        refresh_token = await self._create_jwt_token(
            refresh_token_data.model_dump(),
            expires_delta=timedelta(minutes=app_settings.refresh_token_expire_minutes),
        )
        logger.debug("Refresh token generated")

        # Store refresh token info in the database
        token_data = TokenCreate(
            auth_id=UUID(auth_id),
            secret_key=await HashManager().get_hash(refresh_token_data.secret_key),
            token_hash=await HashManager().create_sha_256_hash(refresh_token),
            type=TokenTypeEnum.REFRESH.value,
        )

        token_model = TokenModel(**token_data.model_dump())

        _ = await TokenDataManager(self.session).insert_one(token_model)
        logger.debug("Refresh token info stored in the database")

        return refresh_token

    async def _create_jwt_token(self, data: Dict[str, Any], expires_delta: timedelta | None = None) -> str:
        """Create a JWT token with the given data and expiration time.

        This method generates a JWT token using the provided data and expiration time.
        If no expiration time is provided, it defaults to 15 minutes from the current time.

        Args:
            data (Dict[str, Any]): A dictionary containing the data to be encoded in the token.
            expires_delta (timedelta | None, optional): The time delta for token expiration.
                If None, defaults to 15 minutes. Defaults to None.

        Returns:
            str: A JWT token as a string.

        Raises:
            JWTError: If there's an error during JWT token creation.
        """
        # Make a copy of the data to encode
        to_encode = data.copy()

        # Default token expiration time if expires_delta is not provided
        if expires_delta:
            expire = datetime.now(timezone.utc) + expires_delta
        else:
            expire = datetime.now(timezone.utc) + timedelta(minutes=15)

        to_encode.update({"exp": expire})

        # Encode and return the token
        encoded_jwt = jwt.encode(to_encode, secrets_settings.jwt_secret_key, algorithm=JWT_ALGORITHM)

        return encoded_jwt

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/auth/models.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Implements auth models. Use sqlalchemy to define models."""

from datetime import datetime
from uuid import UUID, uuid4

from sqlalchemy import Boolean, DateTime, Enum, String, Uuid
from sqlalchemy.orm import Mapped, mapped_column

from budapp.commons.constants import TokenTypeEnum
from budapp.commons.database import Base


class Token(Base):
    """Token model."""

    __tablename__ = "token"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    auth_id: Mapped[UUID] = mapped_column(Uuid, nullable=False)
    secret_key: Mapped[str] = mapped_column(String)
    token_hash: Mapped[str] = mapped_column(String, nullable=False)
    type: Mapped[str] = mapped_column(
        Enum(
            TokenTypeEnum,
            name="token_type_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=False,
    )
    blacklisted: Mapped[bool] = mapped_column(Boolean, default=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/auth/auth_routes.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Defines authentication routes for the microservices, providing endpoints for user authentication."""

from typing import Union

from fastapi import APIRouter, Depends, status
from sqlalchemy.orm import Session
from typing_extensions import Annotated

from budapp.commons import logging
from budapp.commons.dependencies import get_session
from budapp.commons.exceptions import ClientException
from budapp.commons.schemas import ErrorResponse

from .schemas import UserLogin, UserLoginResponse
from .services import AuthService


logger = logging.get_logger(__name__)

auth_router = APIRouter(prefix="/auth", tags=["auth"])


@auth_router.post(
    "/login",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": UserLoginResponse,
            "description": "Successfully logged in user",
        },
    },
    description="Login a user with email and password",
)
async def login_user(
    user: UserLogin, session: Annotated[Session, Depends(get_session)]
) -> Union[UserLoginResponse, ErrorResponse]:
    """Login a user with email and password."""
    try:
        auth_token = await AuthService(session).login_user(user)
        return UserLoginResponse(
            code=status.HTTP_200_OK,
            message="User logged in successfully",
            token=auth_token.token,
            first_login=auth_token.first_login,
            is_reset_password=auth_token.is_reset_password,
            object="auth_token",
        ).to_http_response()
    except ClientException as e:
        logger.error(f"ClientException: {e}")
        return ErrorResponse(code=status.HTTP_400_BAD_REQUEST, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Exception: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Something went wrong"
        ).to_http_response()

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/auth/schemas.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Contains Pydantic schemas used for data validation and serialization within the auth services."""

from pydantic import UUID4, BaseModel, ConfigDict, EmailStr, Field

from budapp.commons.constants import TokenTypeEnum
from budapp.commons.schemas import SuccessResponse


class AuthToken(BaseModel):
    """Token schema."""

    access_token: str
    refresh_token: str
    token_type: str  # Bearer


class AccessTokenData(BaseModel):
    """Access token data schema."""

    sub: str
    type: str = TokenTypeEnum.ACCESS.value


class RefreshTokenData(BaseModel):
    """Refresh token data schema."""

    sub: str
    type: str = TokenTypeEnum.REFRESH.value
    secret_key: str


class TokenCreate(BaseModel):
    """Token create schema."""

    auth_id: UUID4
    secret_key: str | None = None
    token_hash: str
    type: TokenTypeEnum


class UserLogin(BaseModel):
    """Login user schema."""

    email: EmailStr = Field(min_length=1, max_length=100)
    password: str = Field(min_length=8, max_length=100)


class UserLoginData(BaseModel):
    """User login data schema."""

    token: AuthToken
    first_login: bool
    is_reset_password: bool


class UserLoginResponse(SuccessResponse):
    """User login response schema."""

    model_config = ConfigDict(extra="ignore")
    token: AuthToken
    first_login: bool
    is_reset_password: bool

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/auth/__inti__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The auth package, containing essential business logic, services, and routing configurations for the microservices."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/auth/crud.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Implements auth crud. Used to perform CRUD operations on auth models."""

from budapp.commons.db_utils import DataManagerUtils


class TokenDataManager(DataManagerUtils):
    """Manager for token data operations."""

    pass

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/logging.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Provides logging utilities and preconfigured loggers for consistent and structured logging across the microservices."""

import logging.config
from enum import Enum
from pathlib import Path
from typing import Any, Union

import structlog
from structlog import BoundLogger


def configure_logging(log_dir: Union[str, Path], log_level: Any) -> None:
    """Configure logging settings for the application.

    Set up logging with the specified log directory and log level. This function configures the logging handlers
    and formatters to ensure that logs are written to the specified directory with the desired log level.

    Args:
        log_dir (str | Path): Directory where log files will be stored. This can be a string path or a Path object.
        log_level (Any): The log level to set for the logger. It should be one of the standard logging levels
                         such as logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR, or logging.CRITICAL.

    Returns:
        None: This function does not return any value.
    """
    log_dir = Path(log_dir) if isinstance(log_dir, str) else log_dir
    log_dir.mkdir(exist_ok=True, parents=True)

    if isinstance(log_level, Enum):
        log_level = log_level.value
    elif isinstance(log_level, str):
        log_level = log_level.upper()

    logging.config.dictConfig(
        {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "json_formatter": {
                    "()": structlog.stdlib.ProcessorFormatter,
                    "processor": structlog.processors.JSONRenderer(),
                },
                "plain_formatter": {
                    "()": structlog.stdlib.ProcessorFormatter,
                    "processor": structlog.dev.ConsoleRenderer(),
                },
            },
            "handlers": {
                "console_plain": {
                    "class": "logging.StreamHandler",
                    "formatter": "plain_formatter",
                },
                "console_json": {
                    "class": "logging.StreamHandler",
                    "formatter": "json_formatter",
                },
                "plain_file": {
                    "class": "logging.handlers.WatchedFileHandler",
                    "filename": f"{log_dir.as_posix()}/app.log",
                    "formatter": "plain_formatter",
                },
                "json_file": {
                    "class": "logging.handlers.WatchedFileHandler",
                    "filename": f"{log_dir.as_posix()}/app.log",
                    "formatter": "json_formatter",
                },
            },
            "loggers": {
                "structlog": {
                    "handlers": ["console_json"],
                    "level": log_level,
                },
                "root": {
                    "handlers": ["console_json"],
                    "level": log_level,
                },
            },
        }
    )

    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.stdlib.filter_by_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )


def get_logger(name: str) -> BoundLogger:
    """Retrieve a logger instance with the specified name.

    Obtain a `BoundLogger` instance from `structlog` with the given name. This logger can be used to log messages
    with the provided logger name.

    Args:
        name (str): The name to associate with the logger instance.

    Returns:
        BoundLogger: A `BoundLogger` instance configured with the specified name.
    """
    return structlog.get_logger(name)

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/config.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Manages application and secret configurations, utilizing environment variables and Dapr's configuration store for syncing."""

import os
from datetime import datetime, timedelta, timezone
from distutils.util import strtobool
from pathlib import Path
from typing import Annotated, Any, Dict, List, Optional

from dapr.conf import settings as dapr_settings
from pydantic import (
    AnyHttpUrl,
    AnyUrl,
    BeforeValidator,
    ConfigDict,
    DirectoryPath,
    Field,
    computed_field,
    model_validator,
)
from pydantic_settings import BaseSettings

from budapp.__about__ import __version__

from . import logging
from .constants import Environment, LogLevel


def parse_cors(v: Any) -> List[str] | str:
    """Parse CORS_ORIGINS into a list of strings."""
    if isinstance(v, str) and not v.startswith("["):
        return [i.strip() for i in v.split(",")]
    elif isinstance(v, list | str):
        return v
    raise ValueError(v)


def enable_periodic_sync_from_store(is_global: bool = False) -> Dict[str, Any]:
    """Enable periodic synchronization from the configuration store.

    Args:
        is_global (bool): Indicates if the configuration is global across all services.

    Returns:
        Dict[str, Any]: A dictionary with sync settings.
    """
    return {"sync": True, "is_global": is_global}


class BaseConfig(BaseSettings):
    """Base Config to be used as a parent class for other Config classes. Extra fields are not allowed."""

    model_config = ConfigDict(extra="forbid")

    max_sync_interval: int = Field(
        timedelta(hours=12).seconds, alias="MAX_STORE_SYNC_INTERVAL", ge=timedelta(hours=1).seconds
    )

    def get_fields_to_sync(self) -> List[str]:
        """Retrieve a list of field names that are configured for synchronization with a store.

        This method inspects the fields defined in the class and checks their `json_schema_extra` attribute
        to determine if they should be synced. It collects field names based on the `sync` attribute and
        applies any `key_prefix` or `alias` settings if provided.

        Returns:
            list: A list of field names to be synced from the store. The field names are formatted
                  according to the `key_prefix` and `alias` settings if applicable.

        Example:
            ```python
            fields = instance.get_fields_to_sync()
            # Output could be something like ['description', 'config.debug', 'app.name']
            ```
        """
        fields_to_sync = []
        app_name = __version__.split("@")[0]
        for name, info in self.__fields__.items():
            extra = info.json_schema_extra or {}
            if extra.get("sync") is True:
                fields_to_sync.append(
                    f"{app_name}." if extra.get("is_global", False) is True else "" + (info.alias or name)
                )

        return fields_to_sync

    def update_fields(self, mapping: Dict[str, Any]) -> None:
        """Update fields in the instance based on the provided mapping.

        Inspect each field defined in the class and update its value using the corresponding key in the provided
        `mapping` dictionary. The key used for lookup is determined by the field's `json_schema_extra` settings,
        applying any `key_prefix` or `alias` if specified.

        Args:
            mapping (dict): A dictionary where keys are the names of the fields to update and values are the new
                            values to assign to these fields.

        Example:
            ```python
            instance.update_fields({"description": "", "config.debug": True, "app.name": "MyApp"})
            ```
        """
        app_name = __version__.split("@")[0]
        for name, info in self.__fields__.items():
            extra = info.json_schema_extra or {}
            key = f"{app_name}." if extra.get("is_global", False) is True else "" + (info.alias or name)
            if key in mapping:
                self.__setattr__(name, mapping[key])


class AppConfig(BaseConfig):
    """Manages configuration settings for the microservice.

    This class is used to define and access the configuration settings for the microservice. It supports syncing
    fields from a dapr config store and allows configuration via environment variables.

    Attributes:
        env (str): The environment in which the application is running (e.g., 'development', 'production').
        debug (Optional[bool]): Enable or disable debugging mode.
        Other mandatory fields as required by the application.

    Sync Details:
        Fields annotated with `json_schema_extra` will be synced from the config store. The sync behavior is controlled
        by `sync=True` and additional configurations can be made using `key_prefix` and `alias`.

    Usage:
        Configure the settings via environment variables or sync with a config store. Access settings as attributes of
        an instance of `AppConfig`.

    Example:
        ```python
        from budapp.commons.config import app_settings

        if app_settings.env == "dev":
            # Development-specific logic
            ...
        ```
    """

    # App Info
    name: str = __version__.split("@")[0]
    version: str = __version__.split("@")[-1]
    description: str = ""
    api_root: str = ""

    # Deployment configs
    env: Environment = Field(Environment.DEVELOPMENT, alias="NAMESPACE")
    debug: Optional[bool] = Field(
        None,
        alias="DEBUG",
        json_schema_extra=enable_periodic_sync_from_store(),
    )
    log_level: Optional[LogLevel] = Field(None, alias="LOG_LEVEL")
    log_dir: Path = Field(Path("logs"), alias="LOG_DIR")

    tzone: timezone = timezone.utc
    deployed_at: datetime = datetime.now(tzone)

    # Dapr configs
    dapr_http_port: Optional[int] = Field(dapr_settings.DAPR_HTTP_PORT)
    dapr_grpc_port: Optional[int] = Field(dapr_settings.DAPR_GRPC_PORT)
    dapr_health_timeout: Optional[int] = Field(
        dapr_settings.DAPR_HEALTH_TIMEOUT, json_schema_extra=enable_periodic_sync_from_store(is_global=True)
    )
    dapr_api_method_invocation_protocol: Optional[str] = Field(
        "grpc", json_schema_extra=enable_periodic_sync_from_store(is_global=True)
    )

    # Config store
    configstore_name: Optional[str] = None
    config_subscription_id: Optional[str] = None

    # Secret store
    secretstore_name: Optional[str] = None

    # State store
    statestore_name: Optional[str] = None

    # Pubsub
    pubsub_name: Optional[str] = None
    pubsub_topic: Optional[str] = None
    dead_letter_topic: Optional[str] = None

    # Base Directory
    base_dir: DirectoryPath = Field(default_factory=lambda: Path(__file__).parent.parent.parent.resolve())

    # Profiling
    profiler_enabled: bool = Field(False, alias="ENABLE_PROFILER")

    # DB connection
    postgres_host: str = Field("localhost", alias="POSTGRES_HOST")
    postgres_port: int = Field(5432, alias="POSTGRES_PORT")
    postgres_user: str = Field(alias="POSTGRES_USER")
    postgres_password: str = Field(alias="POSTGRES_PASSWORD")
    postgres_db: str = Field(alias="POSTGRES_DB")

    # Superuser
    superuser_email: str = Field(alias="SUPER_USER_EMAIL")
    superuser_password: str = Field(alias="SUPER_USER_PASSWORD")

    # Token
    access_token_expire_minutes: int = Field(30, alias="ACCESS_TOKEN_EXPIRE_MINUTES")
    refresh_token_expire_minutes: int = Field(60 * 24 * 7, alias="REFRESH_TOKEN_EXPIRE_MINUTES")

    # Static
    static_dir_path: DirectoryPath | None = Field(None, alias="STATIC_DIR")

    # CORS
    cors_origins: Annotated[list[AnyUrl] | str, BeforeValidator(parse_cors)] = []

    # Bud microservice
    dapr_base_url: AnyHttpUrl = Field(alias="DAPR_BASE_URL")
    bud_cluster_app_id: str = Field(alias="BUD_CLUSTER_APP_ID")
    bud_model_app_id: str = Field(alias="BUD_MODEL_APP_ID")
    bud_simulator_app_id: str = Field(alias="BUD_SIMULATOR_APP_ID")
    bud_metrics_app_id: str = Field(alias="BUD_METRICS_APP_ID")
    bud_notify_app_id: str = Field(alias="BUD_NOTIFY_APP_ID")
    source_topic: str = Field(alias="SOURCE_TOPIC", default="budAppMessages")

    # Budserve host
    budserve_host: str = Field(alias="BUD_SERVE_HOST", default="https://api-dev.bud.studio")

    # Prometheus URL
    prometheus_url: str = Field(alias="PROMETHEUS_URL", default="https://metrics.fmops.in")

    @computed_field
    def static_dir(self) -> str:
        """Get the static directory."""
        if self.static_dir_path is None:
            return os.path.join(str(self.base_dir), "static")

        return self.static_dir_path

    @computed_field
    def icon_dir(self) -> DirectoryPath:
        """Directory for icon."""
        return os.path.join(self.static_dir, "icons")

    @computed_field
    def postgres_url(self) -> str:
        """Construct and returns a PostgreSQL connection URL.

        This property combines the individual PostgreSQL connection parameters
        into a single connection URL string.

        Returns:
            A formatted PostgreSQL connection string.
        """
        return f"postgresql://{self.postgres_user}:{self.postgres_password}@{self.postgres_host}:{self.postgres_port}/{self.postgres_db}"

    @model_validator(mode="before")
    @classmethod
    def resolve_env(cls, data: Dict[str, Any]) -> Dict[str, Any]:
        """Convert environment and namespace values in the input data to `Environment` instances.

        This method processes the provided dictionary to convert the values associated with `env` and `NAMESPACE` keys
        into `Environment` instances, if they are given as strings.

        Args:
            data (dict): A dictionary containing configuration data. It may include the keys `env` and `NAMESPACE`
                         which need to be converted to `Environment` instances.

        Returns:
            dict: The updated dictionary with `env` and `NAMESPACE` values converted to `Environment` instances.
        """
        if isinstance(data.get("env"), str):
            data["env"] = Environment.from_string(data["env"])
        elif isinstance(data.get("NAMESPACE"), str):
            data["NAMESPACE"] = Environment.from_string(data["NAMESPACE"])
        return data

    @model_validator(mode="after")
    def set_env_details(self) -> "AppConfig":
        """Set environment-specific details in the configuration.

        Update the configuration attributes `log_level` and `debug` based on the values from the `env` attribute if they
        are not already set. This ensures that the configuration uses environment-specific defaults where applicable.

        Returns:
            AppConfig: The updated instance of `AppConfig` with environment details applied.
        """
        self.log_level = self.env.log_level if self.log_level is None else self.log_level
        self.debug = self.env.debug if self.debug is None else self.debug

        return self

    def __setattr__(self, name: str, value: Any) -> None:
        """Set an attribute with type conversion based on the attribute name.

        Convert the value of `log_level` to an uppercase `LogLevel` enum if it is a string, and convert the `debug`
        attribute to a boolean using `strtobool` if it is a string. For all other attributes, set the value directly
        using the superclass's `__setattr__`.

        Args:
            name (str): The name of the attribute to set.
            value (Any): The value to assign to the attribute. Type conversion is applied for specific attributes.

        Example:
            ```python
            config = AppConfig()
            config.log_level = "debug"  # This will be converted to LogLevel.DEBUG
            config.debug = "true"  # This will be converted to True
            ```
        """
        if name == "log_level" and isinstance(value, str):
            value = LogLevel(value.upper())
        elif name == "debug" and isinstance(value, str):
            value = strtobool(value)

        super().__setattr__(name, value)


class SecretsConfig(BaseConfig):
    """Manages secret configurations for the microservice.

    This class handles the configuration of secrets required by the microservice. It supports secret management via
    a Dapr secret store.

    Attributes:
        dapr_api_token (str): The API token required for Dapr interactions (mandatory field).
        Other placeholder fields for secrets which can be removed or customized as needed.

    Configuration:
        Secrets should be defined in a `.env` file located in the project root. Use the prefix `SECRETS_` for syncing them with the secret store.
        For example:

        ```
        DAPR_API_TOKEN = your_api_token_here  # Won't be synced to the secret store
        SECRETS_OPENAI_TOKEN = your_api_token_here  # Will be synced to the secret store
        ```

    Sync Details:
        Similar to `AppConfig`, fields can be synced from the secret store using the `json_schema` settings.

    Usage:
        Configure secrets in `.env` for development. Ensure that the `.env` file is not included in the repository.

    Example:
        ```python
        from budapp.commons.config import secrets_settings

        api_token = secrets_settings.dapr_api_token
        ```
    """

    dapr_api_token: Optional[str] = Field(None, alias="DAPR_API_TOKEN")
    password_salt: str = Field("bud_password_salt", alias="PASSWORD_SALT")
    jwt_secret_key: str = Field(alias="JWT_SECRET_KEY")
    redis_password: str = Field(
        alias="REDIS_PASSWORD", json_schema_extra=enable_periodic_sync_from_store(is_global=True)
    )
    redis_uri: str = Field(alias="REDIS_URI", json_schema_extra=enable_periodic_sync_from_store(is_global=True))

    @computed_field
    def redis_url(self) -> str:
        """Construct and returns a Redis connection URL."""
        return f"redis://:{self.redis_password}@{self.redis_uri}"


app_settings = AppConfig()
secrets_settings = SecretsConfig()

logging.configure_logging(app_settings.log_dir, app_settings.log_level)

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/async_utils.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Provides utility functions for managing asynchronous tasks."""

import asyncio
from typing import Any, Awaitable, Callable, List, Tuple, TypeVar, Union


T = TypeVar("T")


def dispatch_async(func: Callable[..., Any], *args: Tuple[Any], **kwargs: Any) -> Union[T, Awaitable[T]]:
    """Dispatch a function call asynchronously, ensuring compatibility with both synchronous and asynchronous functions.

    Wrap the given function in an asynchronous wrapper if it is a coroutine function. Execute the wrapped function
    asynchronously if the event loop is running; otherwise, run the function using `asyncio.run`. For non-coroutine
    functions, execute them normally.

    Args:
        func (Callable): The function to dispatch. Can be either a coroutine function or a regular function.
        *args: Positional arguments to pass to the function.
        **kwargs: Keyword arguments to pass to the function.

    Returns:
        Any: The result of the function call.

    Example:
        ```python
        async def async_func(x):
            return x + 1


        def sync_func(x):
            return x + 1


        result_async = dispatch_async(async_func, 10)  # Asynchronous call
        result_sync = dispatch_async(sync_func, 10)  # Synchronous call
        ```
    """

    async def async_wrapper() -> Any:
        return await func(*args, **kwargs)

    if asyncio.iscoroutinefunction(func):
        try:
            event_loop = asyncio.get_running_loop()
            result = async_wrapper() if event_loop.is_running() else asyncio.run(func(*args, **kwargs))
        except RuntimeError:
            event_loop = asyncio.get_event_loop()
            result = event_loop.run_until_complete(func(*args, **kwargs))
    else:
        result = func(*args, **kwargs)

    return result


async def check_file_extension(filename: str, allowed_extensions: List[str]) -> bool:
    """Check if the file has an allowed extension.

    Args:
        filename (str): Name of the file to validate.
        allowed_extensions (list[str]): List of allowed extensions (without dot, e.g. ['yaml', 'yml']).

    Returns:
        bool: True if file extension is allowed, False otherwise.
    """
    if not filename or "." not in filename:
        return False

    # Get the file extension from the filename
    file_extension = filename.split(".")[-1].lower()

    # Convert allowed extensions to lowercase for case-insensitive comparison
    allowed_extensions = [ext.lower() for ext in allowed_extensions]

    return file_extension in allowed_extensions

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/validators.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Provides validation functions and utilities for ensuring data integrity and correctness."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/database.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Provides utility functions for managing the database connection."""

import os

from alembic import command
from alembic.config import Config
from sqlalchemy import Engine, MetaData, create_engine
from sqlalchemy.orm import declarative_base, sessionmaker

from . import logging
from .config import app_settings


logger = logging.get_logger(__name__)


def get_engine() -> Engine:
    """Create and return a SQLAlchemy engine instance.

    This function initializes a new SQLAlchemy engine using the PostgreSQL
    connection URL specified in the application settings. The engine is
    configured with echo=True for SQL query logging.

    Returns:
        Engine: A SQLAlchemy Engine instance connected to the PostgreSQL database.

    Raises:
        SQLAlchemyError: If there's an error creating the engine or connecting to the database.
    """
    return create_engine(app_settings.postgres_url)


# Create sqlalchemy engine
engine = get_engine()

# Create session class
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Constraint naming convention to fix alembic autogenerate command issues
# https://docs.sqlalchemy.org/en/20/core/constraints.html#constraint-naming-conventions
convention = {
    "ix": "ix_%(column_0_label)s",
    "uq": "uq_%(table_name)s_%(column_0_name)s",
    "ck": "ck_%(table_name)s_%(constraint_name)s",
    "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
    "pk": "pk_%(table_name)s",
}

metadata_obj = MetaData(naming_convention=convention)

# Create base class for creating models
Base = declarative_base(metadata=metadata_obj)


def run_migrations() -> None:
    """Run Alembic migrations on application startup.

    This function executes Alembic database migrations to bring the database
    schema up to the latest version. It uses the configuration file specified
    in the application settings.

    The function performs the following steps:
    1. Constructs the path to the Alembic configuration file.
    2. Creates an Alembic Config object with the configuration file.
    3. Runs the upgrade command to apply all pending migrations.

    Raises:
        FileNotFoundError: If the Alembic configuration file is not found.
        alembic.util.exc.CommandError: If there's an error during migration.

    Note:
        This function should be called during application startup to ensure
        the database schema is up to date before the application begins
        serving requests.
    """
    logger.info("Starting database migrations")

    alembic_cfg_file = os.path.join(app_settings.base_dir, "budapp", "alembic.ini")

    if not os.path.exists(alembic_cfg_file):
        logger.error(f"Alembic configuration file not found: {alembic_cfg_file}")
        raise FileNotFoundError(f"Alembic configuration file not found: {alembic_cfg_file}")

    try:
        alembic_cfg = Config(alembic_cfg_file)
        command.upgrade(alembic_cfg, "head")
        logger.info("Database migrations completed successfully")
    except Exception as e:
        logger.error(f"Error occurred during database migration: {str(e)}")
        raise

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/security.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Provides utility functions for managing security tasks."""

import hashlib

from passlib.context import CryptContext


class HashManager:
    """A class for managing various hashing operations.

    This class provides methods for hashing and verifying passwords using bcrypt,
    as well as creating SHA-256 hashes.

    Attributes:
        pwd_context (CryptContext): A CryptContext instance for password hashing.
    """

    def __init__(self) -> None:
        """Initialize the HashManager with a CryptContext for bcrypt."""
        self.pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

    async def get_hash(self, plain_string: str) -> str:
        """Hash a plain string using bcrypt.

        Args:
            plain_string (str): The string to be hashed.

        Returns:
            str: The bcrypt hash of the input string.
        """
        return self.pwd_context.hash(plain_string)

    async def verify_hash(self, plain_string: str, hashed_string: str) -> bool:
        """Verify a plain string against a bcrypt hash.

        Args:
            plain_string (str): The plain string to verify.
            hashed_string (str): The bcrypt hash to verify against.

        Returns:
            bool: True if the plain string matches the hash, False otherwise.
        """
        return self.pwd_context.verify(plain_string, hashed_string)

    @staticmethod
    async def create_sha_256_hash(input_string: str) -> str:
        """Create a SHA-256 hash of the input string.

        Args:
            input_string (str): The string to be hashed.

        Returns:
            str: The hexadecimal representation of the SHA-256 hash.
        """
        # Convert the input string to bytes
        input_bytes = input_string.encode("utf-8")

        # Create a SHA-256 hash object
        sha_256_hash = hashlib.sha256(input_bytes)

        # Get the hexadecimal representation of the hash
        return sha_256_hash.hexdigest()

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/constants.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Defines constant values used throughout the project, including application-specific constants."""

import random
from enum import Enum, StrEnum, auto
from typing import List

from .helpers import create_dynamic_enum


class LogLevel(Enum):
    """Define logging levels with associated priority values.

    Inherit from `str` and `Enum` to create a logging level enumeration. Each level has a string representation and a
    corresponding priority value, which aligns with Python's built-in `logging` module levels.

    Attributes:
        DEBUG (LogLevel): Represents debug-level logging with a priority value of `logging.DEBUG`.
        INFO (LogLevel): Represents info-level logging with a priority value of `logging.INFO`.
        WARNING (LogLevel): Represents warning-level logging with a priority value of `logging.WARNING`.
        ERROR (LogLevel): Represents error-level logging with a priority value of `logging.ERROR`.
        CRITICAL (LogLevel): Represents critical-level logging with a priority value of `logging.CRITICAL`.
        NOTSET (LogLevel): Represents no logging level with a priority value of `logging.NOTSET`.
    """

    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"
    NOTSET = "NOTSET"


class Environment(str, Enum):
    """Enumerate application environments and provide utilities for environment-specific settings.

    Inherit from `str` and `Enum` to define application environments with associated string values. The class also
    includes utility methods to convert string representations to `Environment` values and determine logging and
    debugging settings based on the environment.

    Attributes:
        PRODUCTION (Environment): Represents the production environment.
        DEVELOPMENT (Environment): Represents the development environment.
        TESTING (Environment): Represents the testing environment.
    """

    PRODUCTION = "PRODUCTION"
    DEVELOPMENT = "DEVELOPMENT"
    TESTING = "TESTING"

    @staticmethod
    def from_string(value: str) -> "Environment":
        """Convert a string representation to an `Environment` instance.

        Use regular expressions to match and identify the environment from a string. Raise a `ValueError` if the string
        does not correspond to a valid environment.

        Args:
            value (str): The string representation of the environment.

        Returns:
            Environment: The corresponding `Environment` instance.

        Raises:
            ValueError: If the string does not match any valid environment.
        """
        import re

        matches = re.findall(r"(?i)\b(dev|prod|test)(elop|elopment|uction|ing|er)?\b", value)

        env = matches[0][0].lower() if len(matches) else ""
        if env == "dev":
            return Environment.DEVELOPMENT
        elif env == "prod":
            return Environment.PRODUCTION
        elif env == "test":
            return Environment.TESTING
        else:
            raise ValueError(
                f"Invalid environment: {value}. Only the following environments are allowed: "
                f"{', '.join(map(str, Environment.__members__))}"
            )

    @property
    def log_level(self) -> LogLevel:
        """Return the appropriate logging level for the current environment.

        Returns:
            LogLevel: The logging level for the current environment.
        """
        return {"PRODUCTION": LogLevel.INFO}.get(self.value, LogLevel.DEBUG)

    @property
    def debug(self) -> bool:
        """Return whether debugging is enabled for the current environment.

        Returns:
            bool: `True` if debugging is enabled, `False` otherwise.
        """
        return {"PRODUCTION": False}.get(self.value, True)


class ModalityEnum(Enum):
    """Enumeration of model modalities.

    This enum represents different types of AI model modalities or capabilities.

    Attributes:
        LLM (str): Represents Large Language Models for text generation and processing.
        IMAGE (str): Represents image-related models for tasks like generation or analysis.
        EMBEDDING (str): Represents models that create vector embeddings of input data.
        TEXT_TO_SPEECH (str): Represents models that convert text to spoken audio.
        SPEECH_TO_TEXT (str): Represents models that transcribe spoken audio to text.
    """

    LLM = "llm"
    MLLM = "mllm"
    IMAGE = "image"
    EMBEDDING = "embedding"
    TEXT_TO_SPEECH = "text_to_speech"
    SPEECH_TO_TEXT = "speech_to_text"


ModelSourceEnum = create_dynamic_enum(
    "ModelSourceEnum",
    [
        "local",
        "nlp_cloud",
        "deepinfra",
        "anthropic",
        "vertex_ai-vision-models",
        "vertex_ai-ai21_models",
        "cerebras",
        "watsonx",
        "predibase",
        "volcengine",
        "clarifai",
        "baseten",
        "sambanova",
        "github",
        "petals",
        "replicate",
        "vertex_ai-chat-models",
        "azure_ai",
        "perplexity",
        "vertex_ai-code-text-models",
        "vertex_ai-text-models",
        "cohere_chat",
        "vertex_ai-embedding-models",
        "text-completion-openai",
        "groq",
        "openai",
        "aleph_alpha",
        "sagemaker",
        "databricks",
        "fireworks_ai",
        "vertex_ai-anthropic_models",
        "vertex_ai-mistral_models",
        "voyage",
        "vertex_ai-language-models",
        "anyscale",
        "deepseek",
        "vertex_ai-image-models",
        "mistral",
        "ollama",
        "cohere",
        "gemini",
        "friendliai",
        "vertex_ai-code-chat-models",
        "azure",
        "codestral",
        "vertex_ai-llama_models",
        "together_ai",
        "cloudflare",
        "ai21",
        "openrouter",
        "bedrock",
        "text-completion-codestral",
        "huggingface",
    ],
)

CredentialTypeEnum = Enum(
    "CredentialTypeEnum",
    {
        name: member.value
        for name, member in ModelSourceEnum.__members__.items()
        if member not in [ModelSourceEnum.LOCAL]
    },
)


class ModelProviderTypeEnum(Enum):
    """Enumeration of model provider types.

    This enum represents different types of model providers or sources.

    Attributes:
        CLOUD_MODEL (str): Represents cloud-based model providers.
        HUGGING_FACE (str): Represents models from the Hugging Face platform.
        URL (str): Represents models accessible via a URL.
        DISK (str): Represents locally stored models on disk.
    """

    CLOUD_MODEL = "cloud_model"
    HUGGING_FACE = "hugging_face"
    URL = "url"
    DISK = "disk"


class UserRoleEnum(Enum):
    """Enumeration of user roles in the system.

    This enum defines the various roles that a user can have in the application.
    Each role represents a different level of access and permissions.

    Attributes:
        ADMIN (str): Administrator role with high-level permissions.
        SUPER_ADMIN (str): Super administrator role with the highest level of permissions.
        DEVELOPER (str): Role for software developers.
        DEVOPS (str): Role for DevOps engineers.
        TESTER (str): Role for quality assurance testers.
    """

    ADMIN = "admin"
    SUPER_ADMIN = "super_admin"
    DEVELOPER = "developer"
    DEVOPS = "devops"
    TESTER = "tester"


class UserStatusEnum(StrEnum):
    """Enumeration of user statuses in the system.

    This enum defines the possible statuses that a user account can have.
    It uses auto() to automatically assign string values equal to the member names.

    Attributes:
        ACTIVE: Represents an active user account.
        DELETED: Represents an deleted or disabled user account.
        INVITED: Represents a user who has been invited but hasn't yet activated their account.
    """

    ACTIVE = auto()
    DELETED = auto()
    INVITED = auto()


class UserColorEnum(Enum):
    """Enumeration of predefined user colors.

    This enum defines a set of color options that can be assigned to users.
    Each color is represented by its hexadecimal code.

    Attributes:
        COLOR_1 (str): color (#E57333).
        COLOR_2 (str): color (#FFC442).
        COLOR_3 (str): color (#61A560).
        COLOR_4 (str): color (#3F8EF7).
        COLOR_5 (str): color (#C64C9C).
        COLOR_6 (str): color (#95E0FB).
    """

    COLOR_1 = "#E57333"
    COLOR_2 = "#FFC442"
    COLOR_3 = "#61A560"
    COLOR_4 = "#3F8EF7"
    COLOR_5 = "#C64C9C"
    COLOR_6 = "#95E0FB"

    @classmethod
    def get_random_color(cls) -> str:
        """Get a random color."""
        colors = list(cls)
        return random.choice(colors).value


class PermissionEnum(Enum):
    """Enumeration of system permissions.

    This enum defines various permission levels for different aspects of the system,
    including models, projects, endpoints, clusters, and user management.

    Attributes:
        MODEL_VIEW (str): Permission to view models.
        MODEL_MANAGE (str): Permission to manage models.
        MODEL_BENCHMARK (str): Permission to benchmark models.
        PROJECT_VIEW (str): Permission to view projects.
        PROJECT_MANAGE (str): Permission to manage projects.
        ENDPOINT_VIEW (str): Permission to view endpoints.
        ENDPOINT_MANAGE (str): Permission to manage endpoints.
        CLUSTER_VIEW (str): Permission to view clusters.
        CLUSTER_MANAGE (str): Permission to manage clusters.
        USER_MANAGE (str): Permission to manage users.
    """

    MODEL_VIEW = "model:view"
    MODEL_MANAGE = "model:manage"
    MODEL_BENCHMARK = "model:benchmark"

    PROJECT_VIEW = "project:view"
    PROJECT_MANAGE = "project:manage"

    ENDPOINT_VIEW = "endpoint:view"
    ENDPOINT_MANAGE = "endpoint:manage"

    CLUSTER_VIEW = "cluster:view"
    CLUSTER_MANAGE = "cluster:manage"

    USER_MANAGE = "user:manage"

    @classmethod
    def get_global_permissions(cls) -> List[str]:
        """Return all permission values in a list."""
        return [
            cls.MODEL_VIEW.value,
            cls.MODEL_MANAGE.value,
            cls.MODEL_BENCHMARK.value,
            cls.PROJECT_VIEW.value,
            cls.PROJECT_MANAGE.value,
            cls.CLUSTER_VIEW.value,
            cls.CLUSTER_MANAGE.value,
            cls.USER_MANAGE.value,
        ]

    @classmethod
    def get_default_permissions(cls) -> List[str]:
        """Return default permission values in a list."""
        return [
            cls.MODEL_VIEW.value,
            cls.MODEL_MANAGE.value,
            cls.PROJECT_VIEW.value,
            cls.CLUSTER_VIEW.value,
        ]

    @classmethod
    def get_protected_permissions(cls) -> List[str]:
        """Return restrictive permission values in a list."""
        return [
            cls.MODEL_VIEW.value,
            cls.PROJECT_VIEW.value,
            cls.CLUSTER_VIEW.value,
        ]

    @classmethod
    def get_project_default_permissions(cls) -> List[str]:
        """Return default permission values in a list."""
        return [
            cls.ENDPOINT_VIEW.value,
        ]

    @classmethod
    def get_project_level_scopes(cls) -> List[str]:
        """Return project-level scope values in a list."""
        return [
            cls.ENDPOINT_VIEW.value,
            cls.ENDPOINT_MANAGE.value,
        ]

    @classmethod
    def get_project_protected_scopes(cls) -> List[str]:
        """Return project-level protected scope values in a list."""
        return [
            cls.ENDPOINT_VIEW.value,
        ]


class TokenTypeEnum(Enum):
    """Enumeration of token types used in the application.

    This enum defines the different types of authentication tokens
    that can be used within the application.

    Attributes:
        ACCESS (str): Represents an access token.
        REFRESH (str): Represents a refresh token.
    """

    ACCESS = "access"
    REFRESH = "refresh"


# Algorithm used for signing tokens
JWT_ALGORITHM = "HS256"


class WorkflowStatusEnum(StrEnum):
    """Enumeration of workflow statuses."""

    IN_PROGRESS = auto()
    COMPLETED = auto()
    FAILED = auto()
    # Cancelled status not required since workflow delete api delete record


class WorkflowTypeEnum(StrEnum):
    """Enumeration of workflow types."""

    MODEL_DEPLOYMENT = auto()
    MODEL_SECURITY_SCAN = auto()
    CLUSTER_ONBOARDING = auto()
    CLUSTER_DELETION = auto()
    ENDPOINT_DELETION = auto()
    ENDPOINT_WORKER_DELETION = auto()
    CLOUD_MODEL_ONBOARDING = auto()
    LOCAL_MODEL_ONBOARDING = auto()
    ADD_WORKER_TO_ENDPOINT = auto()
    LICENSE_FAQ_FETCH = auto()


class NotificationType(Enum):
    """Represents the type of a notification.

    Attributes:
        EVENT: Notification triggered by an event.
        TOPIC: Notification related to a specific topic.
        BROADCAST: Notification triggered by a broadcast.
    """

    EVENT = "event"
    TOPIC = "topic"
    BROADCAST = "broadcast"


class NotificationCategory(str, Enum):
    """Represents the type of an internal notification.

    Attributes:
        INAPP: Represents the in-app notification type.
        INTERNAL: Represents the internal notification type.
    """

    INAPP = "inapp"
    INTERNAL = "internal"


class PayloadType(str, Enum):
    """Represents the type of a payload.

    Attributes:
        DEPLOYMENT_RECOMMENDATION: Represents the deployment recommendation payload type.
        DEPLOY_MODEL: Represents the model deployment payload type.
    """

    DEPLOYMENT_RECOMMENDATION = "get_cluster_recommendations"
    DEPLOY_MODEL = "deploy_model"
    REGISTER_CLUSTER = "register_cluster"
    DELETE_CLUSTER = "delete_cluster"
    DELETE_DEPLOYMENT = "delete_deployment"
    PERFORM_MODEL_EXTRACTION = "perform_model_extraction"
    PERFORM_MODEL_SECURITY_SCAN = "perform_model_security_scan"
    CLUSTER_STATUS_UPDATE = "cluster-status-update"
    DEPLOYMENT_STATUS_UPDATE = "deployment-status-update"
    DELETE_WORKER = "delete_worker"
    ADD_WORKER = "add_worker"
    FETCH_LICENSE_FAQS = "fetch_license_faqs"


class BudServeWorkflowStepEventName(str, Enum):
    """Represents the name of a workflow step event.

    Attributes:
        BUD_SIMULATOR_EVENTS: Represents the Bud simulator workflow step event name.
        BUDSERVE_CLUSTER_EVENTS: Represents the Budserve cluster workflow step event name.
        CREATE_CLUSTER_EVENTS: Represents the create cluster workflow step event name.
        MODEL_EXTRACTION_EVENTS: Represents the model extraction workflow step event name.
        MODEL_SECURITY_SCAN_EVENTS: Represents the model security scan workflow step event name.
    """

    BUD_SIMULATOR_EVENTS = "bud_simulator_events"
    BUDSERVE_CLUSTER_EVENTS = "budserve_cluster_events"
    CREATE_CLUSTER_EVENTS = "create_cluster_events"
    MODEL_EXTRACTION_EVENTS = "model_extraction_events"
    MODEL_SECURITY_SCAN_EVENTS = "model_security_scan_events"
    DELETE_CLUSTER_EVENTS = "delete_cluster_events"
    DELETE_ENDPOINT_EVENTS = "delete_endpoint_events"
    DELETE_WORKER_EVENTS = "delete_worker_events"
    LICENSE_FAQ_EVENTS = "license_faq_events"


class ClusterStatusEnum(StrEnum):
    """Cluster status types.

    Attributes:
        AVAILABLE: Represents the available cluster status.
        NOT_AVAILABLE: Represents the not available cluster status.
        REGISTERING: Represents the registering cluster status.
        ERROR: Represents the error cluster status.
    """

    AVAILABLE = auto()
    NOT_AVAILABLE = auto()
    ERROR = auto()
    DELETING = auto()
    DELETED = auto()


class EndpointStatusEnum(StrEnum):
    """Status for endpoint.

    Attributes:
        RUNNING: Represents the running endpoint status.
        FAILURE: Represents the failure endpoint status.
        DEPLOYING: Represents the deploying endpoint status.
        UNHEALTHY: Represents the unhealthy endpoint status.
        DELETING: Represents the deleting endpoint status.
        DELETED: Represents the deleted endpoint status.
        PENDING: Represents the pending endpoint status.
    """

    RUNNING = auto()
    FAILURE = auto()
    DEPLOYING = auto()
    UNHEALTHY = auto()
    DELETING = auto()
    DELETED = auto()
    PENDING = auto()


class ModelTemplateTypeEnum(StrEnum):
    """Model template types."""

    SUMMARIZATION = auto()
    CHAT = auto()
    QUESTION_ANSWERING = auto()
    RAG = auto()
    CODE_GEN = auto()
    CODE_TRANSLATION = auto()
    ENTITY_EXTRACTION = auto()
    SENTIMENT_ANALYSIS = auto()
    DOCUMENT_ANALYSIS = auto()
    OTHER = auto()


class DropdownBackgroundColor(str, Enum):
    """Background hex color for dropdown."""

    COLOR_1 = "#EEEEEE"
    COLOR_2 = "#965CDE"
    COLOR_3 = "#EC7575"
    COLOR_4 = "#479D5F"
    COLOR_5 = "#D1B854"
    COLOR_6 = "#ECAE75"
    COLOR_7 = "#42CACF"
    COLOR_8 = "#DE5CD1"
    COLOR_9 = "#4077E6"
    COLOR_10 = "#8DE640"
    COLOR_11 = "#8E5EFF"
    COLOR_12 = "#FF895E"
    COLOR_13 = "#FF5E99"
    COLOR_14 = "#F4FF5E"
    COLOR_15 = "#FF5E5E"
    COLOR_16 = "#5EA3FF"
    COLOR_17 = "#5EFFBE"

    @classmethod
    def get_random_color(cls) -> str:
        """Get a random color."""
        colors = list(cls)
        return random.choice(colors).value


class BaseModelRelationEnum(StrEnum):
    """Base model relation types."""

    ADAPTER = "adapter"
    MERGE = "merge"
    QUANTIZED = "quantized"
    FINETUNE = "finetune"


class ModelSecurityScanStatusEnum(StrEnum):
    """Model security scan status types."""

    LOW = auto()
    MEDIUM = auto()
    HIGH = auto()
    CRITICAL = auto()
    SAFE = auto()


LICENSE_DIR = "licenses"


class ModelStatusEnum(StrEnum):
    """Enumeration of entity statuses in the system.

    Attributes:
        ACTIVE: Represents an active entity.
        DELETED: Represents an deleted entity.
    """

    ACTIVE = auto()
    DELETED = auto()


class CloudModelStatusEnum(StrEnum):
    """Enumeration of entity statuses in the system.

    Attributes:
        ACTIVE: Represents an active entity.
        DELETED: Represents an deleted entity.
    """

    ACTIVE = auto()
    DELETED = auto()


class ProjectStatusEnum(StrEnum):
    """Enumeration of entity statuses in the system.

    Attributes:
        ACTIVE: Represents an active entity.
        DELETED: Represents an deleted entity.
    """

    ACTIVE = auto()
    DELETED = auto()


# Bud Notify Workflow
BUD_NOTIFICATION_WORKFLOW = "bud-notification"
BUD_INTERNAL_WORKFLOW = "bud-internal"


class NotificationStatus(Enum):
    """Enumerate notification statuses."""

    RUNNING = "RUNNING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
    PENDING = "PENDING"


class VisibilityEnum(Enum):
    """Enumeration of visibility statuses in the system.

    Attributes:
        PUBLIC: Represents an publicly visible entity.
        INERNAL: Represents an internal entity.
    """

    PUBLIC = "public"
    INTERNAL = "internal"


APP_ICONS = {
    "general": {
        "model_mono": "icons/general/model_mono.png",
        "cluster_mono": "icons/general/cluster_mono.png",
        "deployment_mono": "icons/general/deployment_mono.png",
    }
}

EMOJIS = [
    "😀",
    "😃",
    "😄",
    "😁",
    "😆",
    "😅",
    "🤣",
    "😂",
    "🙂",
    "🙃",
    "🫠",
    "😉",
    "😊",
    "😇",
    "🥰",
    "😍",
    "🤩",
    "😘",
    "😗",
    "☺️",
    "😚",
    "😙",
    "🥲",
    "😋",
    "😛",
    "😜",
    "🤪",
    "😝",
    "🤑",
    "🤗",
    "🤭",
    "🫢",
    "🫣",
    "🤫",
    "🤔",
    "🫡",
    "🤐",
    "🤨",
    "😐",
    "😑",
    "😶",
    "🫥",
    "😶‍🌫️",
    "😏",
    "😒",
    "🙄",
    "😬",
    "😮‍💨",
    "🤥",
    "😌",
    "😔",
    "😪",
    "🤤",
    "😴",
    "😷",
    "🤒",
    "🤕",
    "🤢",
    "🤮",
    "🤧",
    "🥵",
    "🥶",
    "🥴",
    "😵",
    "😵‍💫",
    "🤯",
    "🤠",
    "🥳",
    "🥸",
    "😎",
    "🤓",
    "🧐",
    "😕",
    "🫤",
    "😟",
    "🙁",
    "☹️",
    "😮",
    "😯",
    "😲",
    "😳",
    "🥺",
    "🥹",
    "😦",
    "😧",
    "😨",
    "😰",
    "😥",
    "😢",
    "😭",
    "😱",
    "😖",
    "😣",
    "😞",
    "😓",
    "😩",
    "😫",
    "🥱",
    "😤",
    "😡",
    "😠",
    "🤬",
    "😈",
    "👿",
    "💀",
    "☠️",
    "💩",
    "🤡",
    "👹",
    "👺",
    "👻",
    "👽",
    "👾",
    "🤖",
    "😺",
    "😸",
    "😹",
    "😻",
    "😼",
    "😽",
    "🙀",
    "😿",
    "😾",
    "🙈",
    "🙉",
    "🙊",
    "💋",
    "💌",
    "💘",
    "💝",
    "💖",
    "💗",
    "💓",
    "💞",
    "💕",
    "💟",
    "❣️",
    "💔",
    "❤️‍🔥",
    "❤️‍🩹",
    "❤️",
    "🧡",
    "💛",
    "💚",
    "💙",
    "💜",
    "🤎",
    "🖤",
    "🤍",
    "💯",
    "💢",
    "💥",
    "💫",
    "💦",
    "💨",
    "🕳️",
    "💣",
    "💬",
    "👁️‍🗨️",
    "🗨️",
    "🗯️",
    "💭",
    "💤",
    "👋",
    "🤚",
    "🖐️",
    "✋",
    "🖖",
    "🫱",
    "🫲",
    "🫳",
    "🫴",
    "👌",
    "🤌",
    "🤏",
    "✌️",
    "🤞",
    "🫰",
    "🤟",
    "🤘",
    "🤙",
    "👈",
    "👉",
    "👆",
    "🖕",
    "👇",
    "☝️",
    "🫵",
    "👍",
    "👎",
    "✊",
    "👊",
    "🤛",
    "🤜",
    "👏",
    "🙌",
    "🫶",
    "👐",
    "🤲",
    "🤝",
    "🙏",
    "✍️",
    "💅",
    "🤳",
    "💪",
    "🦾",
    "🦿",
    "🦵",
    "🦶",
    "👂",
    "🦻",
    "👃",
    "🧠",
    "🫀",
    "🫁",
    "🦷",
    "🦴",
    "👀",
    "👁️",
    "👅",
    "👄",
    "🫦",
    "👶",
    "🧒",
    "👦",
    "👧",
    "🧑",
    "👱",
    "👨",
    "🧔",
    "🧔‍♂️",
    "🧔‍♀️",
    "👨‍🦰",
    "👨‍🦱",
    "👨‍🦳",
    "👨‍🦲",
    "👩",
    "👩‍🦰",
    "🧑‍🦰",
    "👩‍🦱",
    "🧑‍🦱",
    "👩‍🦳",
    "🧑‍🦳",
    "👩‍🦲",
    "🧑‍🦲",
    "👱‍♀️",
    "👱‍♂️",
    "🧓",
    "👴",
    "👵",
    "🙍",
    "🙍‍♂️",
    "🙍‍♀️",
    "🙎",
    "🙎‍♂️",
    "🙎‍♀️",
    "🙅",
    "🙅‍♂️",
    "🙅‍♀️",
    "🙆",
    "🙆‍♂️",
    "🙆‍♀️",
    "💁",
    "💁‍♂️",
    "💁‍♀️",
    "🙋",
    "🙋‍♂️",
    "🙋‍♀️",
    "🧏",
    "🧏‍♂️",
    "🧏‍♀️",
    "🙇",
    "🙇‍♂️",
    "🙇‍♀️",
    "🤦",
    "🤦‍♂️",
    "🤦‍♀️",
    "🤷",
    "🤷‍♂️",
    "🤷‍♀️",
    "🧑‍⚕️",
    "👨‍⚕️",
    "👩‍⚕️",
    "🧑‍🎓",
    "👨‍🎓",
    "👩‍🎓",
    "🧑‍🏫",
    "👨‍🏫",
    "👩‍🏫",
    "🧑‍⚖️",
    "👨‍⚖️",
    "👩‍⚖️",
    "🧑‍🌾",
    "👨‍🌾",
    "👩‍🌾",
    "🧑‍🍳",
    "👨‍🍳",
    "👩‍🍳",
    "🧑‍🔧",
    "👨‍🔧",
    "👩‍🔧",
    "🧑‍🏭",
    "👨‍🏭",
    "👩‍🏭",
    "🧑‍💼",
    "👨‍💼",
    "👩‍💼",
    "🧑‍🔬",
    "👨‍🔬",
    "👩‍🔬",
    "🧑‍💻",
    "👨‍💻",
    "👩‍💻",
    "🧑‍🎤",
    "👨‍🎤",
    "👩‍🎤",
    "🧑‍🎨",
    "👨‍🎨",
    "👩‍🎨",
    "🧑‍✈️",
    "👨‍✈️",
    "👩‍✈️",
    "🧑‍🚀",
    "👨‍🚀",
    "👩‍🚀",
    "🧑‍🚒",
    "👨‍🚒",
    "👩‍🚒",
    "👮",
    "👮‍♂️",
    "👮‍♀️",
    "🕵️",
    "🕵️‍♂️",
    "🕵️‍♀️",
    "💂",
    "💂‍♂️",
    "💂‍♀️",
    "🥷",
    "👷",
    "👷‍♂️",
    "👷‍♀️",
    "🫅",
    "🤴",
    "👸",
    "👳",
    "👳‍♂️",
    "👳‍♀️",
    "👲",
    "🧕",
    "🤵",
    "🤵‍♂️",
    "🤵‍♀️",
    "👰",
    "👰‍♂️",
    "👰‍♀️",
    "🤰",
    "🫃",
    "🫄",
    "🤱",
    "👩‍🍼",
    "👨‍🍼",
    "🧑‍🍼",
    "👼",
    "🎅",
    "🤶",
    "🧑‍🎄",
    "🦸",
    "🦸‍♂️",
    "🦸‍♀️",
    "🦹",
    "🦹‍♂️",
    "🦹‍♀️",
    "🧙",
    "🧙‍♂️",
    "🧙‍♀️",
    "🧚",
    "🧚‍♂️",
    "🧚‍♀️",
    "🧛",
    "🧛‍♂️",
    "🧛‍♀️",
    "🧜",
    "🧜‍♂️",
    "🧜‍♀️",
    "🧝",
    "🧝‍♂️",
    "🧝‍♀️",
    "🧞",
    "🧞‍♂️",
    "🧞‍♀️",
    "🧟",
    "🧟‍♂️",
    "🧟‍♀️",
    "🧌",
    "💆",
    "💆‍♂️",
    "💆‍♀️",
    "💇",
    "💇‍♂️",
    "💇‍♀️",
    "🚶",
    "🚶‍♂️",
    "🚶‍♀️",
    "🧍",
    "🧍‍♂️",
    "🧍‍♀️",
    "🧎",
    "🧎‍♂️",
    "🧎‍♀️",
    "🧑‍🦯",
    "👨‍🦯",
    "👩‍🦯",
    "🧑‍🦼",
    "👨‍🦼",
    "👩‍🦼",
    "🧑‍🦽",
    "👨‍🦽",
    "👩‍🦽",
    "🏃",
    "🏃‍♂️",
    "🏃‍♀️",
    "💃",
    "🕺",
    "🕴️",
    "👯",
    "👯‍♂️",
    "👯‍♀️",
    "🧖",
    "🧖‍♂️",
    "🧖‍♀️",
    "🧗",
    "🧗‍♂️",
    "🧗‍♀️",
    "🤺",
    "🏇",
    "⛷️",
    "🏂",
    "🏌️",
    "🏌️‍♂️",
    "🏌️‍♀️",
    "🏄",
    "🏄‍♂️",
    "🏄‍♀️",
    "🚣",
    "🚣‍♂️",
    "🚣‍♀️",
    "🏊",
    "🏊‍♂️",
    "🏊‍♀️",
    "⛹️",
    "⛹️‍♂️",
    "⛹️‍♀️",
    "🏋️",
    "🏋️‍♂️",
    "🏋️‍♀️",
    "🚴",
    "🚴‍♂️",
    "🚴‍♀️",
    "🚵",
    "🚵‍♂️",
    "🚵‍♀️",
    "🤸",
    "🤸‍♂️",
    "🤸‍♀️",
    "🤼",
    "🤼‍♂️",
    "🤼‍♀️",
    "🤽",
    "🤽‍♂️",
    "🤽‍♀️",
    "🤾",
    "🤾‍♂️",
    "🤾‍♀️",
    "🤹",
    "🤹‍♂️",
    "🤹‍♀️",
    "🧘",
    "🧘‍♂️",
    "🧘‍♀️",
    "🛀",
    "🛌",
    "🧑‍🤝‍🧑",
    "👭",
    "👫",
    "👬",
    "💏",
    "👩‍❤️‍💋‍👨",
    "👨‍❤️‍💋‍👨",
    "👩‍❤️‍💋‍👩",
    "💑",
    "👩‍❤️‍👨",
    "👨‍❤️‍👨",
    "👩‍❤️‍👩",
    "👪",
    "👨‍👩‍👦",
    "👨‍👩‍👧",
    "👨‍👩‍👧‍👦",
    "👨‍👩‍👦‍👦",
    "👨‍👩‍👧‍👧",
    "👨‍👨‍👦",
    "👨‍👨‍👧",
    "👨‍👨‍👧‍👦",
    "👨‍👨‍👦‍👦",
    "👨‍👨‍👧‍👧",
    "👩‍👩‍👦",
    "👩‍👩‍👧",
    "👩‍👩‍👧‍👦",
    "👩‍👩‍👦‍👦",
    "👩‍👩‍👧‍👧",
    "👨‍👦",
    "👨‍👦‍👦",
    "👨‍👧",
    "👨‍👧‍👦",
    "👨‍👧‍👧",
    "👩‍👦",
    "👩‍👦‍👦",
    "👩‍👧",
    "👩‍👧‍👦",
    "👩‍👧‍👧",
    "🗣️",
    "👤",
    "👥",
    "🫂",
    "👣",
    "🐵",
    "🐒",
    "🦍",
    "🦧",
    "🐶",
    "🐕",
    "🦮",
    "🐕‍🦺",
    "🐩",
    "🐺",
    "🦊",
    "🦝",
    "🐱",
    "🐈",
    "🐈‍⬛",
    "🦁",
    "🐯",
    "🐅",
    "🐆",
    "🐴",
    "🐎",
    "🦄",
    "🦓",
    "🦌",
    "🦬",
    "🐮",
    "🐂",
    "🐃",
    "🐄",
    "🐷",
    "🐖",
    "🐗",
    "🐽",
    "🐏",
    "🐑",
    "🐐",
    "🐪",
    "🐫",
    "🦙",
    "🦒",
    "🐘",
    "🦣",
    "🦏",
    "🦛",
    "🐭",
    "🐁",
    "🐀",
    "🐹",
    "🐰",
    "🐇",
    "🐿️",
    "🦫",
    "🦔",
    "🦇",
    "🐻",
    "🐻‍❄️",
    "🐨",
    "🐼",
    "🦥",
    "🦦",
    "🦨",
    "🦘",
    "🦡",
    "🐾",
    "🦃",
    "🐔",
    "🐓",
    "🐣",
    "🐤",
    "🐥",
    "🐦",
    "🐧",
    "🕊️",
    "🦅",
    "🦆",
    "🦢",
    "🦉",
    "🦤",
    "🪶",
    "🦩",
    "🦚",
    "🦜",
    "🐸",
    "🐊",
    "🐢",
    "🦎",
    "🐍",
    "🐲",
    "🐉",
    "🦕",
    "🦖",
    "🐳",
    "🐋",
    "🐬",
    "🦭",
    "🐟",
    "🐠",
    "🐡",
    "🦈",
    "🐙",
    "🐚",
    "🪸",
    "🐌",
    "🦋",
    "🐛",
    "🐜",
    "🐝",
    "🪲",
    "🐞",
    "🦗",
    "🪳",
    "🕷️",
    "🕸️",
    "🦂",
    "🦟",
    "🪰",
    "🪱",
    "🦠",
    "💐",
    "🌸",
    "💮",
    "🪷",
    "🏵️",
    "🌹",
    "🥀",
    "🌺",
    "🌻",
    "🌼",
    "🌷",
    "🌱",
    "🪴",
    "🌲",
    "🌳",
    "🌴",
    "🌵",
    "🌾",
    "🌿",
    "☘️",
    "🍀",
    "🍁",
    "🍂",
    "🍃",
    "🪹",
    "🪺",
    "🍇",
    "🍈",
    "🍉",
    "🍊",
    "🍋",
    "🍌",
    "🍍",
    "🥭",
    "🍎",
    "🍏",
    "🍐",
    "🍑",
    "🍒",
    "🍓",
    "🫐",
    "🥝",
    "🍅",
    "🫒",
    "🥥",
    "🥑",
    "🍆",
    "🥔",
    "🥕",
    "🌽",
    "🌶️",
    "🫑",
    "🥒",
    "🥬",
    "🥦",
    "🧄",
    "🧅",
    "🍄",
    "🥜",
    "🫘",
    "🌰",
    "🍞",
    "🥐",
    "🥖",
    "🫓",
    "🥨",
    "🥯",
    "🥞",
    "🧇",
    "🧀",
    "🍖",
    "🍗",
    "🥩",
    "🥓",
    "🍔",
    "🍟",
    "🍕",
    "🌭",
    "🥪",
    "🌮",
    "🌯",
    "🫔",
    "🥙",
    "🧆",
    "🥚",
    "🍳",
    "🥘",
    "🍲",
    "🫕",
    "🥣",
    "🥗",
    "🍿",
    "🧈",
    "🧂",
    "🥫",
    "🍱",
    "🍘",
    "🍙",
    "🍚",
    "🍛",
    "🍜",
    "🍝",
    "🍠",
    "🍢",
    "🍣",
    "🍤",
    "🍥",
    "🥮",
    "🍡",
    "🥟",
    "🥠",
    "🥡",
    "🦀",
    "🦞",
    "🦐",
    "🦑",
    "🦪",
    "🍦",
    "🍧",
    "🍨",
    "🍩",
    "🍪",
    "🎂",
    "🍰",
    "🧁",
    "🥧",
    "🍫",
    "🍬",
    "🍭",
    "🍮",
    "🍯",
    "🍼",
    "🥛",
    "☕",
    "🫖",
    "🍵",
    "🍶",
    "🍾",
    "🍷",
    "🍸",
    "🍹",
    "🍺",
    "🍻",
    "🥂",
    "🥃",
    "🫗",
    "🥤",
    "🧋",
    "🧃",
    "🧉",
    "🧊",
    "🥢",
    "🍽️",
    "🍴",
    "🥄",
    "🔪",
    "🫙",
    "🏺",
    "🌍",
    "🌎",
    "🌏",
    "🌐",
    "🗺️",
    "🗾",
    "🧭",
    "🏔️",
    "⛰️",
    "🌋",
    "🗻",
    "🏕️",
    "🏖️",
    "🏜️",
    "🏝️",
    "🏞️",
    "🏟️",
    "🏛️",
    "🏗️",
    "🧱",
    "🪨",
    "🪵",
    "🛖",
    "🏘️",
    "🏚️",
    "🏠",
    "🏡",
    "🏢",
    "🏣",
    "🏤",
    "🏥",
    "🏦",
    "🏨",
    "🏩",
    "🏪",
    "🏫",
    "🏬",
    "🏭",
    "🏯",
    "🏰",
    "💒",
    "🗼",
    "🗽",
    "⛪",
    "🕌",
    "🛕",
    "🕍",
    "⛩️",
    "🕋",
    "⛲",
    "⛺",
    "🌁",
    "🌃",
    "🏙️",
    "🌄",
    "🌅",
    "🌆",
    "🌇",
    "🌉",
    "♨️",
    "🎠",
    "🛝",
    "🎡",
    "🎢",
    "💈",
    "🎪",
    "🚂",
    "🚃",
    "🚄",
    "🚅",
    "🚆",
    "🚇",
    "🚈",
    "🚉",
    "🚊",
    "🚝",
    "🚞",
    "🚋",
    "🚌",
    "🚍",
    "🚎",
    "🚐",
    "🚑",
    "🚒",
    "🚓",
    "🚔",
    "🚕",
    "🚖",
    "🚗",
    "🚘",
    "🚙",
    "🛻",
    "🚚",
    "🚛",
    "🚜",
    "🏎️",
    "🏍️",
    "🛵",
    "🦽",
    "🦼",
    "🛺",
    "🚲",
    "🛴",
    "🛹",
    "🛼",
    "🚏",
    "🛣️",
    "🛤️",
    "🛢️",
    "⛽",
    "🛞",
    "🚨",
    "🚥",
    "🚦",
    "🛑",
    "🚧",
    "⚓",
    "🛟",
    "⛵",
    "🛶",
    "🚤",
    "🛳️",
    "⛴️",
    "🛥️",
    "🚢",
    "✈️",
    "🛩️",
    "🛫",
    "🛬",
    "🪂",
    "💺",
    "🚁",
    "🚟",
    "🚠",
    "🚡",
    "🛰️",
    "🚀",
    "🛸",
    "🛎️",
    "🧳",
    "⌛",
    "⏳",
    "⌚",
    "⏰",
    "⏱️",
    "⏲️",
    "🕰️",
    "🕛",
    "🕧",
    "🕐",
    "🕜",
    "🕑",
    "🕝",
    "🕒",
    "🕞",
    "🕓",
    "🕟",
    "🕔",
    "🕠",
    "🕕",
    "🕡",
    "🕖",
    "🕢",
    "🕗",
    "🕣",
    "🕘",
    "🕤",
    "🕙",
    "🕥",
    "🕚",
    "🕦",
    "🌑",
    "🌒",
    "🌓",
    "🌔",
    "🌕",
    "🌖",
    "🌗",
    "🌘",
    "🌙",
    "🌚",
    "🌛",
    "🌜",
    "🌡️",
    "☀️",
    "🌝",
    "🌞",
    "🪐",
    "⭐",
    "🌟",
    "🌠",
    "🌌",
    "☁️",
    "⛅",
    "⛈️",
    "🌤️",
    "🌥️",
    "🌦️",
    "🌧️",
    "🌨️",
    "🌩️",
    "🌪️",
    "🌫️",
    "🌬️",
    "🌀",
    "🌈",
    "🌂",
    "☂️",
    "☔",
    "⛱️",
    "⚡",
    "❄️",
    "☃️",
    "⛄",
    "☄️",
    "🔥",
    "💧",
    "🌊",
    "🎃",
    "🎄",
    "🎆",
    "🎇",
    "🧨",
    "✨",
    "🎈",
    "🎉",
    "🎊",
    "🎋",
    "🎍",
    "🎎",
    "🎏",
    "🎐",
    "🎑",
    "🧧",
    "🎀",
    "🎁",
    "🎗️",
    "🎟️",
    "🎫",
    "🎖️",
    "🏆",
    "🏅",
    "🥇",
    "🥈",
    "🥉",
    "⚽",
    "⚾",
    "🥎",
    "🏀",
    "🏐",
    "🏈",
    "🏉",
    "🎾",
    "🥏",
    "🎳",
    "🏏",
    "🏑",
    "🏒",
    "🥍",
    "🏓",
    "🏸",
    "🥊",
    "🥋",
    "🥅",
    "⛳",
    "⛸️",
    "🎣",
    "🤿",
    "🎽",
    "🎿",
    "🛷",
    "🥌",
    "🎯",
    "🪀",
    "🪁",
    "🎱",
    "🔮",
    "🪄",
    "🧿",
    "🪬",
    "🎮",
    "🕹️",
    "🎰",
    "🎲",
    "🧩",
    "🧸",
    "🪅",
    "🪩",
    "🪆",
    "♠️",
    "♥️",
    "♦️",
    "♣️",
    "♟️",
    "🃏",
    "🀄",
    "🎴",
    "🎭",
    "🖼️",
    "🎨",
    "🧵",
    "🪡",
    "🧶",
    "🪢",
    "👓",
    "🕶️",
    "🥽",
    "🥼",
    "🦺",
    "👔",
    "👕",
    "👖",
    "🧣",
    "🧤",
    "🧥",
    "🧦",
    "👗",
    "👘",
    "🥻",
    "🩱",
    "🩲",
    "🩳",
    "👙",
    "👚",
    "👛",
    "👜",
    "👝",
    "🛍️",
    "🎒",
    "🩴",
    "👞",
    "👟",
    "🥾",
    "🥿",
    "👠",
    "👡",
    "🩰",
    "👢",
    "👑",
    "👒",
    "🎩",
    "🎓",
    "🧢",
    "🪖",
    "⛑️",
    "📿",
    "💄",
    "💍",
    "💎",
    "🔇",
    "🔈",
    "🔉",
    "🔊",
    "📢",
    "📣",
    "📯",
    "🔔",
    "🔕",
    "🎼",
    "🎵",
    "🎶",
    "🎙️",
    "🎚️",
    "🎛️",
    "🎤",
    "🎧",
    "📻",
    "🎷",
    "🪗",
    "🎸",
    "🎹",
    "🎺",
    "🎻",
    "🪕",
    "🥁",
    "🪘",
    "📱",
    "📲",
    "☎️",
    "📞",
    "📟",
    "📠",
    "🔋",
    "🪫",
    "🔌",
    "💻",
    "🖥️",
    "🖨️",
    "⌨️",
    "🖱️",
    "🖲️",
    "💽",
    "💾",
    "💿",
    "📀",
    "🧮",
    "🎥",
    "🎞️",
    "📽️",
    "🎬",
    "📺",
    "📷",
    "📸",
    "📹",
    "📼",
    "🔍",
    "🔎",
    "🕯️",
    "💡",
    "🔦",
    "🏮",
    "🪔",
    "📔",
    "📕",
    "📖",
    "📗",
    "📘",
    "📙",
    "📚",
    "📓",
    "📒",
    "📃",
    "📜",
    "📄",
    "📰",
    "🗞️",
    "📑",
    "🔖",
    "🏷️",
    "💰",
    "🪙",
    "💴",
    "💵",
    "💶",
    "💷",
    "💸",
    "💳",
    "🧾",
    "💹",
    "✉️",
    "📧",
    "📨",
    "📩",
    "📤",
    "📥",
    "📦",
    "📫",
    "📪",
    "📬",
    "📭",
    "📮",
    "🗳️",
    "✏️",
    "✒️",
    "🖋️",
    "🖊️",
    "🖌️",
    "🖍️",
    "📝",
    "💼",
    "📁",
    "📂",
    "🗂️",
    "📅",
    "📆",
    "🗒️",
    "🗓️",
    "📇",
    "📈",
    "📉",
    "📊",
    "📋",
    "📌",
    "📍",
    "📎",
    "🖇️",
    "📏",
    "📐",
    "✂️",
    "🗃️",
    "🗄️",
    "🗑️",
    "🔒",
    "🔓",
    "🔏",
    "🔐",
    "🔑",
    "🗝️",
    "🔨",
    "🪓",
    "⛏️",
    "⚒️",
    "🛠️",
    "🗡️",
    "⚔️",
    "🔫",
    "🪃",
    "🏹",
    "🛡️",
    "🪚",
    "🔧",
    "🪛",
    "🔩",
    "⚙️",
    "🗜️",
    "⚖️",
    "🦯",
    "🔗",
    "⛓️",
    "🪝",
    "🧰",
    "🧲",
    "🪜",
    "⚗️",
    "🧪",
    "🧫",
    "🧬",
    "🔬",
    "🔭",
    "📡",
    "💉",
    "🩸",
    "💊",
    "🩹",
    "🩼",
    "🩺",
    "🩻",
    "🚪",
    "🛗",
    "🪞",
    "🪟",
    "🛏️",
    "🛋️",
    "🪑",
    "🚽",
    "🪠",
    "🚿",
    "🛁",
    "🪤",
    "🪒",
    "🧴",
    "🧷",
    "🧹",
    "🧺",
    "🧻",
    "🪣",
    "🧼",
    "🫧",
    "🪥",
    "🧽",
    "🧯",
    "🛒",
    "🚬",
    "⚰️",
    "🪦",
    "⚱️",
    "🗿",
    "🪧",
    "🪪",
    "🏧",
    "🚮",
    "🚰",
    "♿",
    "🚹",
    "🚺",
    "🚻",
    "🚼",
    "🚾",
    "🛂",
    "🛃",
    "🛄",
    "🛅",
    "⚠️",
    "🚸",
    "⛔",
    "🚫",
    "🚳",
    "🚭",
    "🚯",
    "🚱",
    "🚷",
    "📵",
    "🔞",
    "☢️",
    "☣️",
    "⬆️",
    "↗️",
    "➡️",
    "↘️",
    "⬇️",
    "↙️",
    "⬅️",
    "↖️",
    "↕️",
    "↔️",
    "↩️",
    "↪️",
    "⤴️",
    "⤵️",
    "🔃",
    "🔄",
    "🔙",
    "🔚",
    "🔛",
    "🔜",
    "🔝",
    "🛐",
    "⚛️",
    "🕉️",
    "✡️",
    "☸️",
    "☯️",
    "✝️",
    "☦️",
    "☪️",
    "☮️",
    "🕎",
    "🔯",
    "♈",
    "♉",
    "♊",
    "♋",
    "♌",
    "♍",
    "♎",
    "♏",
    "♐",
    "♑",
    "♒",
    "♓",
    "⛎",
    "🔀",
    "🔁",
    "🔂",
    "▶️",
    "⏩",
    "⏭️",
    "⏯️",
    "◀️",
    "⏪",
    "⏮️",
    "🔼",
    "⏫",
    "🔽",
    "⏬",
    "⏸️",
    "⏹️",
    "⏺️",
    "⏏️",
    "🎦",
    "🔅",
    "🔆",
    "📶",
    "📳",
    "📴",
    "♀️",
    "♂️",
    "⚧️",
    "✖️",
    "➕",
    "➖",
    "➗",
    "🟰",
    "♾️",
    "‼️",
    "⁉️",
    "❓",
    "❔",
    "❕",
    "❗",
    "〰️",
    "💱",
    "💲",
    "⚕️",
    "♻️",
    "⚜️",
    "🔱",
    "📛",
    "🔰",
    "⭕",
    "✅",
    "☑️",
    "✔️",
    "❌",
    "❎",
    "➰",
    "➿",
    "〽️",
    "✳️",
    "✴️",
    "❇️",
    "©️",
    "®️",
    "™️",
    "#️⃣",
    "*️⃣",
    "0️⃣",
    "1️⃣",
    "2️⃣",
    "3️⃣",
    "4️⃣",
    "5️⃣",
    "6️⃣",
    "7️⃣",
    "8️⃣",
    "9️⃣",
    "🔟",
    "🔠",
    "🔡",
    "🔢",
    "🔣",
    "🔤",
    "🅰️",
    "🆎",
    "🅱️",
    "🆑",
    "🆒",
    "🆓",
    "ℹ️",
    "🆔",
    "Ⓜ️",
    "🆕",
    "🆖",
    "🅾️",
    "🆗",
    "🅿️",
    "🆘",
    "🆙",
    "🆚",
    "🈁",
    "🈂️",
    "🈷️",
    "🈶",
    "🈯",
    "🉐",
    "🈹",
    "🈚",
    "🈲",
    "🉑",
    "🈸",
    "🈴",
    "🈳",
    "㊗️",
    "㊙️",
    "🈺",
    "🈵",
    "🔴",
    "🟠",
    "🟡",
    "🟢",
    "🔵",
    "🟣",
    "🟤",
    "⚫",
    "⚪",
    "🟥",
    "🟧",
    "🟨",
    "🟩",
    "🟦",
    "🟪",
    "🟫",
    "⬛",
    "⬜",
    "◼️",
    "◻️",
    "◾",
    "◽",
    "▪️",
    "▫️",
    "🔶",
    "🔷",
    "🔸",
    "🔹",
    "🔺",
    "🔻",
    "💠",
    "🔘",
    "🔳",
    "🔲",
    "🏁",
    "🚩",
    "🎌",
    "🏴",
    "🏳️",
    "🏳️‍🌈",
    "🏳️‍⚧️",
    "🏴‍☠️",
    "🇦🇨",
    "🇦🇩",
    "🇦🇪",
    "🇦🇫",
    "🇦🇬",
    "🇦🇮",
    "🇦🇱",
    "🇦🇲",
    "🇦🇴",
    "🇦🇶",
    "🇦🇷",
    "🇦🇸",
    "🇦🇹",
    "🇦🇺",
    "🇦🇼",
    "🇦🇽",
    "🇦🇿",
    "🇧🇦",
    "🇧🇧",
    "🇧🇩",
    "🇧🇪",
    "🇧🇫",
    "🇧🇬",
    "🇧🇭",
    "🇧🇮",
    "🇧🇯",
    "🇧🇱",
    "🇧🇲",
    "🇧🇳",
    "🇧🇴",
    "🇧🇶",
    "🇧🇷",
    "🇧🇸",
    "🇧🇹",
    "🇧🇻",
    "🇧🇼",
    "🇧🇾",
    "🇧🇿",
    "🇨🇦",
    "🇨🇨",
    "🇨🇩",
    "🇨🇫",
    "🇨🇬",
    "🇨🇭",
    "🇨🇮",
    "🇨🇰",
    "🇨🇱",
    "🇨🇲",
    "🇨🇳",
    "🇨🇴",
    "🇨🇵",
    "🇨🇷",
    "🇨🇺",
    "🇨🇻",
    "🇨🇼",
    "🇨🇽",
    "🇨🇾",
    "🇨🇿",
    "🇩🇪",
    "🇩🇬",
    "🇩🇯",
    "🇩🇰",
    "🇩🇲",
    "🇩🇴",
    "🇩🇿",
    "🇪🇦",
    "🇪🇨",
    "🇪🇪",
    "🇪🇬",
    "🇪🇭",
    "🇪🇷",
    "🇪🇸",
    "🇪🇹",
    "🇪🇺",
    "🇫🇮",
    "🇫🇯",
    "🇫🇰",
    "🇫🇲",
    "🇫🇴",
    "🇫🇷",
    "🇬🇦",
    "🇬🇧",
    "🇬🇩",
    "🇬🇪",
    "🇬🇫",
    "🇬🇬",
    "🇬🇭",
    "🇬🇮",
    "🇬🇱",
    "🇬🇲",
    "🇬🇳",
    "🇬🇵",
    "🇬🇶",
    "🇬🇷",
    "🇬🇸",
    "🇬🇹",
    "🇬🇺",
    "🇬🇼",
    "🇬🇾",
    "🇭🇰",
    "🇭🇲",
    "🇭🇳",
    "🇭🇷",
    "🇭🇹",
    "🇭🇺",
    "🇮🇨",
    "🇮🇩",
    "🇮🇪",
    "🇮🇱",
    "🇮🇲",
    "🇮🇳",
    "🇮🇴",
    "🇮🇶",
    "🇮🇷",
    "🇮🇸",
    "🇮🇹",
    "🇯🇪",
    "🇯🇲",
    "🇯🇴",
    "🇯🇵",
    "🇰🇪",
    "🇰🇬",
    "🇰🇭",
    "🇰🇮",
    "🇰🇲",
    "🇰🇳",
    "🇰🇵",
    "🇰🇷",
    "🇰🇼",
    "🇰🇾",
    "🇰🇿",
    "🇱🇦",
    "🇱🇧",
    "🇱🇨",
    "🇱🇮",
    "🇱🇰",
    "🇱🇷",
    "🇱🇸",
    "🇱🇹",
    "🇱🇺",
    "🇱🇻",
    "🇱🇾",
    "🇲🇦",
    "🇲🇨",
    "🇲🇩",
    "🇲🇪",
    "🇲🇫",
    "🇲🇬",
    "🇲🇭",
    "🇲🇰",
    "🇲🇱",
    "🇲🇲",
    "🇲🇳",
    "🇲🇴",
    "🇲🇵",
    "🇲🇶",
    "🇲🇷",
    "🇲🇸",
    "🇲🇹",
    "🇲🇺",
    "🇲🇻",
    "🇲🇼",
    "🇲🇽",
    "🇲🇾",
    "🇲🇿",
    "🇳🇦",
    "🇳🇨",
    "🇳🇪",
    "🇳🇫",
    "🇳🇬",
    "🇳🇮",
    "🇳🇱",
    "🇳🇴",
    "🇳🇵",
    "🇳🇷",
    "🇳🇺",
    "🇳🇿",
    "🇴🇲",
    "🇵🇦",
    "🇵🇪",
    "🇵🇫",
    "🇵🇬",
    "🇵🇭",
    "🇵🇰",
    "🇵🇱",
    "🇵🇲",
    "🇵🇳",
    "🇵🇷",
    "🇵🇸",
    "🇵🇹",
    "🇵🇼",
    "🇵🇾",
    "🇶🇦",
    "🇷🇪",
    "🇷🇴",
    "🇷🇸",
    "🇷🇺",
    "🇷🇼",
    "🇸🇦",
    "🇸🇧",
    "🇸🇨",
    "🇸🇩",
    "🇸🇪",
    "🇸🇬",
    "🇸🇭",
    "🇸🇮",
    "🇸🇯",
    "🇸🇰",
    "🇸🇱",
    "🇸🇲",
    "🇸🇳",
    "🇸🇴",
    "🇸🇷",
    "🇸🇸",
    "🇸🇹",
    "🇸🇻",
    "🇸🇽",
    "🇸🇾",
    "🇸🇿",
    "🇹🇦",
    "🇹🇨",
    "🇹🇩",
    "🇹🇫",
    "🇹🇬",
    "🇹🇭",
    "🇹🇯",
    "🇹🇰",
    "🇹🇱",
    "🇹🇲",
    "🇹🇳",
    "🇹🇴",
    "🇹🇷",
    "🇹🇹",
    "🇹🇻",
    "🇹🇼",
    "🇹🇿",
    "🇺🇦",
    "🇺🇬",
    "🇺🇲",
    "🇺🇳",
    "🇺🇸",
    "🇺🇾",
    "🇺🇿",
    "🇻🇦",
    "🇻🇨",
    "🇻🇪",
    "🇻🇬",
    "🇻🇮",
    "🇻🇳",
    "🇻🇺",
    "🇼🇫",
    "🇼🇸",
    "🇽🇰",
    "🇾🇪",
    "🇾🇹",
    "🇿🇦",
    "🇿🇲",
    "🇿🇼",
    "🏴󠁧󠁢󠁥󠁮󠁧󠁿",
    "🏴󠁧󠁢󠁳󠁣󠁴󠁿",
    "🏴󠁧󠁢󠁷󠁬󠁳󠁿",
]

# Define success messages for different workflow types
WORKFLOW_DELETE_MESSAGES = {
    WorkflowTypeEnum.MODEL_DEPLOYMENT: "Successfully cancelled model deployment.",
    WorkflowTypeEnum.MODEL_SECURITY_SCAN: "Successfully cancelled model security scan.",
    WorkflowTypeEnum.CLUSTER_ONBOARDING: "Successfully cancelled cluster onboarding.",
    WorkflowTypeEnum.CLUSTER_DELETION: "Successfully cancelled cluster deletion.",
    WorkflowTypeEnum.ENDPOINT_DELETION: "Successfully cancelled deployment deletion.",
    WorkflowTypeEnum.CLOUD_MODEL_ONBOARDING: "Successfully cancelled model onboarding.",
    WorkflowTypeEnum.LOCAL_MODEL_ONBOARDING: "Successfully cancelled model onboarding.",
    WorkflowTypeEnum.ADD_WORKER_TO_ENDPOINT: "Successfully cancelled worker to deployment.",
}


# Notification types
class NotificationTypeEnum(StrEnum):
    """Notification types.

    Attributes:
        DEPLOYMENT_SUCCESS: Represents the deployment success notification.
        MODEL_ONBOARDING_SUCCESS: Represents the model onboarding success notification.
        CLUSTER_ONBOARDING_SUCCESS: Represents the cluster onboarding success notification.
        MODEL_SCAN_SUCCESS: Represents the model scan success notification.
        RECOMMENDED_CLUSTER_SUCCESS: Represents the recommended cluster success notification.
        UPDATE_PASSWORD_SUCCESS: Represents the update password success notification.
        CLUSTER_DELETION_SUCCESS: Represents the cluster deletion success notification.
        DEPLOYMENT_DELETION_SUCCESS: Represents the deployment deletion success notification.
    """

    DEPLOYMENT_SUCCESS = auto()
    MODEL_ONBOARDING_SUCCESS = auto()
    CLUSTER_ONBOARDING_SUCCESS = auto()
    MODEL_SCAN_SUCCESS = auto()
    RECOMMENDED_CLUSTER_SUCCESS = auto()
    UPDATE_PASSWORD_SUCCESS = auto()
    CLUSTER_DELETION_SUCCESS = auto()
    DEPLOYMENT_DELETION_SUCCESS = auto()

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Initialization module for the `commons` subpackage. Contains common utilities, configurations, constants, and helper functions that are shared across the project."""

from budapp.auth.models import Token as Token
from budapp.cluster_ops.models import Cluster as Cluster
from budapp.core.models import Icon as Icon
from budapp.core.models import ModelTemplate as ModelTemplate
from budapp.credential_ops.models import ProprietaryCredential as ProprietaryCredential
from budapp.endpoint_ops.models import Endpoint as Endpoint
from budapp.model_ops.models import CloudModel as CloudModel
from budapp.model_ops.models import Model as Model
from budapp.model_ops.models import Provider as Provider
from budapp.permissions.models import Permission as Permission
from budapp.project_ops.models import Project as Project
from budapp.user_ops.models import User as User
from budapp.workflow_ops.models import Workflow as Workflow
from budapp.workflow_ops.models import WorkflowStep as WorkflowStep

from .database import Base as Base

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/types.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Defines custom types and type aliases to ensure consistent data handling and type checking."""

from structlog import BoundLogger


Logger = BoundLogger

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/schemas.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Contains Pydantic schemas used for data validation and serialization within the microservices."""

import datetime
import math
import re
from http import HTTPStatus
from typing import Any, ClassVar, Dict, Optional, Set, Tuple, Type, Union

from fastapi.responses import JSONResponse
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    StringConstraints,
    computed_field,
    create_model,
    field_validator,
    model_validator,
)
from typing_extensions import Annotated


lowercase_string = Annotated[str, StringConstraints(to_lower=True)]


class CloudEventBase(BaseModel):
    """Base class for handling HTTP requests with cloud event compatible validation.

    Configures the model to forbid extra fields not defined in the model schema.

    Attributes:
        id (str): The id of the cloud event, excluded from serialization.
        specversion (str): The spec version of the cloud event, excluded from serialization.
        datacontenttype (str): The data content type of the cloud event, excluded from serialization.
        topic (str): The topic of the cloud event, excluded from serialization.
        pubsubname (str): The pubsub name of the cloud event, excluded from serialization.
        source (str): The source of the cloud event, excluded from serialization.
        data (dict): The data of the cloud event, excluded from serialization.
        traceid (str): The trace id of the cloud event, excluded from serialization.
        tracestate (str): The trace state of the cloud event, excluded from serialization.
        traceparent (str): The trace parent of the cloud event, excluded from serialization.
        type (str): The type of the cloud event
        time (str): The time of the cloud event
    """

    model_config = ConfigDict(extra="forbid")

    excluded_fields_in_api: ClassVar[Tuple[str, ...]] = (
        "specversion",
        "topic",
        "pubsubname",
        "source",
        "out_topic",
        "data",
        "traceid",
        "tracestate",
        "traceparent",
    )

    specversion: Optional[str] = None

    topic: Optional[str] = None
    pubsubname: Optional[str] = None
    source: Optional[str] = None
    source_topic: Optional[str] = None

    data: Optional[Dict[str, Any]] = None

    traceid: Optional[str] = None
    tracestate: Optional[str] = None
    traceparent: Optional[str] = None

    type: Optional[str] = None
    time: str = Field(default_factory=lambda: datetime.datetime.now(datetime.UTC).isoformat() + "Z")  # type: ignore
    debug: bool = Field(default=False)

    def is_pubsub(self) -> bool:
        """Check if the event is a PubSub event."""
        return self.topic is not None and self.pubsubname is not None

    @classmethod
    def create_pubsub_model(cls) -> Type[BaseModel]:
        """Create a model for PubSub with data field containing the data model.

        This method generates a new Pydantic model specifically for PubSub events. It includes all the fields
        from the CloudEventBase class as outer fields, and creates an inner model (DataModel) containing
        the fields specific to the inheriting class. The resulting model has a 'data' field that uses
        the inner model as its type.

        Returns:
            Type[BaseModel]: A new Pydantic model class for PubSub events, with a name in the format
            '{ClassName}PubSub'. This model includes all CloudEventBase fields and a 'data' field
            containing the custom fields of the inheriting class.
        """
        # Fields for the outer model (CloudEventBase fields)
        outer_fields = {
            field_name: (field_info.annotation, field_info)
            for field_name, field_info in cls.model_fields.items()
            if field_name in CloudEventBase.model_fields
        }

        # Fields for the inner model (fields specific to the inheriting class)
        inner_fields = {
            field_name: (field_info.annotation, field_info)
            for field_name, field_info in cls.model_fields.items()
            if field_name not in CloudEventBase.model_fields
        }

        # Create the inner model
        DataModel = create_model(f"{cls.__name__}Schema", **inner_fields)  # type: ignore

        # Add the data field with the inner model
        outer_fields["data"] = (DataModel, Field(...))
        return create_model(f"{cls.__name__}PubSub", **outer_fields)  # type: ignore

    @classmethod
    def create_api_model(cls) -> Type[BaseModel]:
        """Create a model for API requests.

        This method generates a new Pydantic model for API requests by excluding
        certain fields from the current class. The resulting model is suitable
        for validating and serializing API request data.

        Returns:
            Type[BaseModel]: A new Pydantic model class for API requests, with
            a name in the format '{ClassName}API'.
        """
        outer_fields = {}
        fields = {}
        for field_name, field_info in cls.model_fields.items():
            if field_name in CloudEventBase.model_fields and field_name not in cls.excluded_fields_in_api:
                outer_fields[field_name] = (field_info.annotation, field_info)
            elif field_name not in cls.excluded_fields_in_api:
                fields[field_name] = (field_info.annotation, field_info)

        fields.update(outer_fields)

        return create_model(f"{cls.__name__}Schema", **fields)  # type: ignore

    @model_validator(mode="before")
    @classmethod
    def root_validator(cls, data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate and adjust the input data.

        If the `id` field and the `data` field is set, then the data field key-value pairs will be added to the instance.

        Args:
            data (dict): The input data to validate and adjust.

        Returns:
            dict: The validated and potentially adjusted data.
        """
        if all(data.get(key) is not None for key in ("topic", "pubsubname", "data")):
            data.update({k: v for k, v in data["data"].items() if k not in data})

        return data


class ResponseBase(BaseModel):
    """Base class for handling HTTP responses with customizable serialization.

    Configures the model to forbid extra fields not defined in the model schema.

    Attributes:
        object (str): The type of response object, converted to lowercase.
        code (int): The HTTP status code for the response, excluded from serialization.
    """

    model_config = ConfigDict(extra="forbid")

    # TODO: Add snake case validation
    object: lowercase_string
    code: int = Field(HTTPStatus.OK.value, exclude=True)

    @staticmethod
    def to_pascal_case(string: str, prefix: Optional[str] = None, suffix: Optional[str] = None) -> str:
        """Convert a string to Pascal case.

        Transform the input string to Pascal case, with optional prefix and suffix.

        Args:
            string (str): The string to convert.
            prefix (Optional[str]): Optional prefix to add before the string.
            suffix (Optional[str]): Optional suffix to add after the string.

        Returns:
            str: The Pascal case representation of the input string.
        """
        string = (prefix or "") + string + (suffix or "")
        return re.sub(r"([_\-])+", " ", string).title().replace(" ", "")

    def to_http_response(
        self,
        include: Union[Set[int], Set[str], Dict[int, Any], Dict[str, Any], None] = None,
        exclude: Union[Set[int], Set[str], Dict[int, Any], Dict[str, Any], None] = None,
        exclude_unset: bool = False,
        exclude_defaults: bool = False,
        exclude_none: bool = False,
    ) -> JSONResponse:
        """Convert the model instance to an HTTP response.

        Serializes the model instance into a JSON response, with options to include or exclude specific fields
        and customize the response based on various parameters.

        Args:
            include (set[int] | set[str] | dict[int, Any] | dict[str, Any] | None): Fields to include in the response.
            exclude (set[int] | set[str] | dict[int, Any] | dict[str, Any] | None): Fields to exclude from the response.
            exclude_unset (bool): Whether to exclude unset fields from the response.
            exclude_defaults (bool): Whether to exclude default values from the response.
            exclude_none (bool): Whether to exclude fields with None values from the response.

        Returns:
            JSONResponse: The serialized JSON response with the appropriate status code.
        """
        if getattr(self, "object", "") == "error":
            details = self.model_dump(mode="json")
            status_code = details["code"]
        else:
            details = self.model_dump(
                include=include,
                exclude=exclude,
                exclude_unset=exclude_unset,
                exclude_defaults=exclude_defaults,
                exclude_none=exclude_none,
                mode="json",
            )
            status_code = self.code

        return JSONResponse(content=details, status_code=status_code)


class SuccessResponse(ResponseBase):
    """Define a success response with optional message and parameters.

    Inherits from `ResponseBase` and specifies default values and validation for success responses.

    Attributes:
        object (str): The type of response object, defaulting to "info".
        message (Optional[str]): An optional message for the response.
    """

    object: str = "info"
    message: Optional[str]

    @model_validator(mode="before")
    @classmethod
    def root_validator(cls, data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate and set default message for the response data.

        Ensure that the `message` field is set to a default value if not provided, based on the HTTP status code.

        Args:
            data (dict): The input data to validate and adjust.

        Returns:
            dict: The validated and potentially adjusted data.
        """
        if data.get("code") is not None and data.get("message") is None:
            data["message"] = HTTPStatus(data["code"]).description

        return data


class PaginatedSuccessResponse(SuccessResponse):
    """Define a paginated success response with optional message and parameters.

    Inherits from `SuccessResponse` and specifies default values and validation for success responses.

    Attributes:
        page (int): The current page number.
        limit (int): The number of items per page.
        total_record (int): The total number of records.
    """

    page: int
    limit: int
    total_record: int = 0

    @computed_field
    @property
    def total_pages(self) -> int:
        """Calculate the total number of pages based on the total number of records and the limit.

        Args:
            self (PaginatedSuccessResponse): The paginated success response instance.

        Returns:
            int: The total number of pages.
        """
        if self.limit > 0:
            return math.ceil(self.total_record / self.limit) or 1
        else:
            return 1


class ErrorResponse(ResponseBase):
    """Define an error response with a code and type derived from the status code.

    Inherits from `ResponseBase` and specifies the default values and validation for error responses.

    Attributes:
        object (str): The type of response object, defaulting to "error".
        message (Optional[str]): An optional message for the response.
        type (Optional[str]): The type of the error.
        param (Optional[str]): An optional parameter for additional context.
        code (int): The HTTP status code for the error.
    """

    object: lowercase_string = "error"
    message: Optional[str]
    type: Optional[str] = "InternalServerError"
    param: Optional[str] = None
    code: int = HTTPStatus.INTERNAL_SERVER_ERROR.value

    @model_validator(mode="before")
    @classmethod
    def root_validator(cls, data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate and set default values for the error response data.

        Set the `type` and `message` fields based on the HTTP status code if not provided.
        The `type` is derived from the status code's phrase and suffixed with "Error" if applicable.

        Args:
            data (dict): The input data to validate and adjust.

        Returns:
            dict: The validated and potentially adjusted data.
        """
        data["type"] = data.get("type") or cls.to_pascal_case(HTTPStatus(data["code"]).phrase, suffix="Error")
        data["message"] = data.get("message") or HTTPStatus(data["code"]).description

        return data


# Schemas related to Tag


class Tag(BaseModel):
    """Tag schema with name and color."""

    name: str = Field(..., min_length=1)
    color: str = Field(..., pattern="^#[0-9A-Fa-f]{6}$")

    @field_validator("color")
    def validate_hex_color(cls, v: str) -> str:
        """Validate that color is a valid hex color code."""
        if not re.match(r"^#[0-9A-Fa-f]{6}$", v):
            raise ValueError("Color must be a valid hex color code (e.g., #FF0000)")
        return v.upper()


# Schemas related to task


class Task(Tag):
    """Task schema with name and description."""

    pass


class BudNotificationMetadata(BaseModel):
    """Recommended cluster notification metadata"""

    workflow_id: str
    subscriber_ids: str
    name: str

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/exceptions.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Defines custom exceptions to handle specific error cases gracefully."""

from functools import wraps
from typing import Any, Callable, Optional

from . import logging
from .async_utils import dispatch_async
from .types import Logger


logger = logging.get_logger(__name__)


class SuppressAndLog:
    """Suppress specified exceptions and log error messages.

    This context manager and decorator handle exceptions by suppressing them and logging an error message.
    Use it to prevent certain exceptions from propagating and ensure that relevant error information is logged.

    Attributes:
        exceptions (tuple): Tuple of exception types to be suppressed.
        logger (Optional[logging.BoundLogger]): Logger instance used to log error messages.
        default_return (Any): Default value returned when an exception is suppressed.
        log_msg (str): Custom message to log when an exception is suppressed.

    Args:
        *exceptions: Exception types to be suppressed.
        _logger (Optional[logging.BoundLogger], default=None): Logger instance for logging errors.
        default_return (Any, default=None): Default return value when an exception is suppressed.
        log_msg (str, default=None): Custom error message for logging.
    """

    def __init__(
        self,
        *exceptions: Any,
        _logger: Optional[Logger] = None,
        default_return: Any = None,
        log_msg: Optional[str] = None,
    ):
        """Initialize the SuppressAndLog instance with exception types, a logger, a default return value, and a log message.

        Args:
            *exceptions: Exception types to be suppressed.
            _logger (Optional[logging.BoundLogger], default=None): Logger instance for logging errors.
            default_return (Any, default=None): Default return value when an exception is suppressed.
            log_msg (str, default=None): Custom error message for logging.
        """
        self.exceptions = exceptions
        self.logger = _logger or logger
        self.default_return = default_return
        self.log_msg = log_msg or "An error occurred"

    def __enter__(self) -> None:
        """Prepare the context for exception suppression and logging.

        This method is a no-op for this context manager.
        """
        pass

    def __exit__(self, exc_type: type, exc_value: Exception, traceback: Any) -> bool:
        """Handle exceptions that occur within the context, log the error message, and suppress the specified exceptions.

        Args:
            exc_type (type): The type of the exception raised.
            exc_value (Exception): The exception instance raised.
            traceback (Any): The traceback object.

        Returns:
            bool: `True` to suppress the exception, `False` otherwise.
        """
        if exc_type is not None and issubclass(exc_type, self.exceptions):
            if self.logger is not None:
                self.logger.error(self.log_msg + ": %s:'%s'", exc_type.__name__, exc_value)
            return True  # Suppresses the exception

        # Do not suppress the exception if it's not one of the specified types
        return False

    def __call__(self, func: Callable[..., Any]) -> Callable[..., Any]:
        """Decorate a function to suppress specified exceptions and log errors.

        Args:
            func (Callable): The function to be decorated.

        Returns:
            Callable: The decorated function that suppresses specified exceptions and logs errors.
        """

        @wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            self.log_msg = f"An error occurred in {func.__name__}"
            result = self.default_return
            with self:
                result = dispatch_async(func, *args, **kwargs)

            return result

        return wrapper


class DatabaseException(Exception):
    """A custom exception class for database-related errors.

    This exception can be raised when database operations fail or encounter issues.

    Attributes:
        message (str): A human-readable string describing the database error.

    Args:
        message (str): The error message describing the database issue.
    """

    def __init__(self, message: str):
        """Initialize the DatabaseException with a message."""
        self.message = message
        super().__init__(self.message)

    def __str__(self):
        """Return a string representation of the database exception."""
        return f"DatabaseException: {self.message}"


class ClientException(Exception):
    """A custom exception class for client-related errors.

    This exception can be raised when client requests fail or encounter issues.

    Attributes:
        message (str): A human-readable string describing the database error.

    Args:
        message (str): The error message describing the database issue.
    """

    def __init__(self, message: str, status_code: int = 400):
        """Initialize the ClientException with a message."""
        self.message = message
        self.status_code = status_code
        super().__init__(self.message)

    def __str__(self):
        """Return a string representation of the database exception."""
        return f"ClientException: {self.message}"


class RedisException(Exception):
    """A custom exception class for Redis-related errors.

    This exception can be raised when Redis operations fail or encounter issues.

    Attributes:
        message (str): A human-readable string describing the Redis error.

    Args:
        message (str): The error message describing the Redis issue.
    """

    def __init__(self, message: str):
        """Initialize the RedisException with a message."""
        self.message = message
        super().__init__(self.message)

    def __str__(self):
        """Return a string representation of the Redis exception."""
        return f"RedisException: {self.message}"

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/db_utils.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Provides utility functions for managing database operations."""

from typing import Any, Dict, List, Optional, Tuple, Type

from fastapi import status
from sqlalchemy import BigInteger as SqlAlchemyBigInteger
from sqlalchemy import String as SqlAlchemyString
from sqlalchemy import cast, func, inspect, select
from sqlalchemy.dialects.postgresql import ARRAY as PostgresArray
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.orm import DeclarativeBase, Session
from sqlalchemy.sql import Executable

from . import logging
from .exceptions import ClientException, DatabaseException


logger = logging.get_logger(__name__)


class SessionMixin:
    """A mixin class that provides an instance of a database session.

    This mixin is designed to be used with classes that require database access.
    It initializes and stores a SQLAlchemy Session object for database operations.

    Attributes:
        session (Session): An instance of SQLAlchemy Session for database operations.

    Example:
        class MyDatabaseHandler(SessionMixin):
            def __init__(self, session):
                super().__init__(session)

            def some_db_operation(self):
                # Use self.session to perform database operations
                pass
    """

    def __init__(self, session: Session) -> None:
        """Initialize the SessionMixin with a database session.

        Args:
            session (Session): An instance of SQLAlchemy Session to be used for database operations.
        """
        self.session = session


class SQLAlchemyMixin(SessionMixin):
    """A mixin class that provides methods for database operations."""

    def add_one(self, model: object) -> object:
        """Add a single model instance to the database.

        This method adds the given model to the session, commits the transaction,
        and refreshes the model to ensure it reflects the current state in the database.

        Args:
            model (Any): The SQLAlchemy model instance to be added to the database.

        Returns:
            Any: The added and refreshed model instance.

        Raises:
            DatabaseException: If there's an error during the database operation.
        """
        try:
            self.session.add(model)
            self.session.commit()
            self.session.refresh(model)
            return model
        except (Exception, SQLAlchemyError) as e:
            self.session.rollback()
            logger.exception(f"Failed to add one model to database: {e}")
            raise DatabaseException("Unable to add model to database") from e

    def add_all(self, models: List[object]) -> List[object]:
        """Add a list of model instances to the database.

        This method adds the given model to the session, commits the transaction,
        and refreshes the model to ensure it reflects the current state in the database.

        Args:
            models (List[Any]): The SQLAlchemy model instances to be added to the database.

        Returns:
            Any: The added and refreshed model instance.

        Raises:
            DatabaseException: If there's an error during the database operation.
        """
        try:
            self.session.add_all(models)
            self.session.commit()
            return models
        except (Exception, SQLAlchemyError) as e:
            self.session.rollback()
            logger.exception(f"Failed to add all models to database: {e}")
            raise DatabaseException("Unable to add models to database") from e

    def scalar_one_or_none(self, stmt: Executable) -> object:
        """Execute a SQL statement and return a single result or None.

        This method executes the given SQL statement and returns either a single
        scalar result or None if no results are found.

        Args:
            stmt (Executable): The SQLAlchemy statement to be executed.

        Returns:
            Any: The scalar result of the query, or None if no results are found.

        Raises:
            DatabaseException: If there's an error during the database operation.
        """
        try:
            return self.session.execute(stmt).scalar_one_or_none()
        except (Exception, SQLAlchemyError) as e:
            logger.exception(f"Failed to get one model from database: {e}")
            raise DatabaseException("Unable to get model from database") from e

    def update_one(self, model: object) -> object:
        """Update a single model instance in the database.

        This method commits the current session (which should contain the updates to the model),
        and then refreshes the model to ensure it reflects the current state in the database.

        Args:
            model (Any): The SQLAlchemy model instance to be updated in the database.

        Returns:
            Any: The updated and refreshed model instance.

        Raises:
            DatabaseException: If there's an error during the database operation.
        """
        try:
            self.session.commit()
            self.session.refresh(model)
            return model
        except (Exception, SQLAlchemyError) as e:
            self.session.rollback()
            logger.exception(f"Failed to update one model in database: {e}")
            raise DatabaseException("Unable to update model in database") from e

    def delete_model(self, model: object) -> None:
        """Delete a single model instance from the database.

        This method commits the current session, and then deletes the model from the database.

        Args:
            model (Any): The SQLAlchemy model instance to be deleted from the database.

        Raises:
            DatabaseException: If there's an error during the database operation.
        """
        try:
            self.session.delete(model)
            self.session.commit()
        except (Exception, SQLAlchemyError) as e:
            self.session.rollback()
            logger.exception(f"Failed to delete one model in database: {e}")
            raise DatabaseException("Unable to delete model in database") from e

    def scalars_all(self, stmt: Executable) -> object:
        """Scalars a SQL statement and return a single result or None.

        This method executes the given SQL statement and returns the result.

        Args:
            stmt (Executable): The SQLAlchemy statement to be executed.

        Returns:
            Any: The result of the executed statement.

        Raises:
            DatabaseException: If there's an error during the database operation.
        """
        try:
            return self.session.scalars(stmt).all()
        except (Exception, SQLAlchemyError) as e:
            logger.exception(f"Failed to execute statement: {e}")
            raise DatabaseException("Unable to execute statement") from e

    def execute_all(self, stmt: Executable) -> object:
        """Execute a SQL statement and return a single result or None.

        This method executes the given SQL statement and returns the result.

        Args:
            stmt (Executable): The SQLAlchemy statement to be executed.

        Returns:
            Any: The result of the executed statement.

        Raises:
            DatabaseException: If there's an error during the database operation.
        """
        try:
            return self.session.execute(stmt).all()
        except (Exception, SQLAlchemyError) as e:
            logger.exception(f"Failed to execute statement: {e}")
            raise DatabaseException("Unable to execute statement") from e

    def execute_scalar(self, stmt: Executable) -> object:
        """Execute a SQL statement and return a single result or None.

        This method executes the given SQL statement and returns the result.

        Args:
            stmt (Executable): The SQLAlchemy statement to be executed.

        Returns:
            Any: The result of the executed statement.

        Raises:
            DatabaseException: If there's an error during the database operation.
        """
        try:
            return self.session.scalar(stmt)
        except (Exception, SQLAlchemyError) as e:
            logger.exception(f"Failed to execute scalar statement: {e}")
            raise DatabaseException("Unable to execute scalar statement") from e


class DataManagerUtils(SQLAlchemyMixin):
    """Utility class for data management operations."""

    @staticmethod
    async def validate_fields(model: Type[DeclarativeBase], fields: Dict[str, Any]) -> None:
        """Validate that the given fields exist in the SQLAlchemy model.

        Args:
            model (Type[DeclarativeBase]): The SQLAlchemy model class to validate against.
            fields (Dict[str, Any]): A dictionary of field names and their values to validate.

        Raises:
            DatabaseException: If an invalid field is found in the input.
        """
        for field in fields:
            if not hasattr(model, field):
                logger.error(f"Invalid field: '{field}' not found in {model.__name__} model")
                raise DatabaseException(f"Invalid field: '{field}' not found in {model.__name__} model")

    @staticmethod
    async def generate_search_stmt(model: Type[DeclarativeBase], fields: Dict[str, Any]) -> List[Executable]:
        """Generate search conditions for a SQLAlchemy model based on the provided fields.

        Args:
            model (Type[DeclarativeBase]): The SQLAlchemy model class to generate search conditions for.
            fields (Dict): A dictionary of field names and their values to search by.

        Returns:
            List[Executable]: A list of SQLAlchemy search conditions.
        """
        # Inspect model columns
        model_columns = inspect(model).columns

        # Initialize list to store search conditions
        search_conditions = []

        # Iterate over search fields and generate conditions
        for field, value in fields.items():
            column = getattr(model, field)

            # Check if column type is string like
            if type(model_columns[field].type) is SqlAlchemyString:
                search_conditions.append(func.lower(column).like(f"%{value.lower()}%"))
            elif type(model_columns[field].type) is PostgresArray:
                search_conditions.append(column.contains(value))
            elif type(model_columns[field].type) is SqlAlchemyBigInteger:
                search_conditions.append(cast(column, SqlAlchemyString).like(f"%{value}%"))
            else:
                search_conditions.append(column == value)

        return search_conditions

    @staticmethod
    async def generate_sorting_stmt(
        model: Type[DeclarativeBase], sort_details: List[Tuple[str, str]]
    ) -> List[Executable]:
        """Generate sorting conditions for a SQLAlchemy model based on the provided sort details.

        Args:
            model (Type[DeclarativeBase]): The SQLAlchemy model class to generate sorting conditions for.
            sort_details (List[Tuple[str, str]]): A list of tuples, where each tuple contains a field name and a direction ('asc' or 'desc').

        Returns:
            List[Executable]: A list of SQLAlchemy sorting conditions.
        """
        sort_conditions = []

        for field, direction in sort_details:
            # Check if column exists, if not, skip
            try:
                column = getattr(model, field)
            except AttributeError:
                continue

            if direction == "asc":
                sort_conditions.append(getattr(model, field))
            else:
                sort_conditions.append(getattr(model, field).desc())

        return sort_conditions

    async def insert_one(self, model: object) -> object:
        """Insert a single model instance into the database.

        This method is an alias for the `add_one` method, providing a more
        intuitive name for the insertion operation.

        Args:
            model (object): The model instance to be inserted into the database.

        Returns:
            object: The inserted model instance, potentially with updated
                attributes (e.g., auto-generated ID).

        Raises:
            Any exceptions that may be raised by the underlying `add_one` method.
        """
        return self.add_one(model)

    async def insert_all(self, models: List[object]) -> List[object]:
        """Insert a list of model instances into the database.

        This method is an alias for the `add_all` method, providing a more
        intuitive name for the insertion operation.

        Args:
            models (List[object]): The list of model instances to be inserted into the database.

        Returns:
            List[object]: The list of inserted model instances.
        """
        return self.add_all(models)

    async def retrieve_by_fields(
        self,
        model: Type[DeclarativeBase],
        fields: Dict[str, Any],
        exclude_fields: Optional[Dict[str, Any]] = None,
        missing_ok: bool = False,
        case_sensitive: bool = True,
    ) -> Optional[DeclarativeBase]:
        """Retrieve a model instance from the database based on given fields.

        This method queries the database for a model instance matching the provided fields.
        If the instance is not found and missing_ok is False, it raises an HTTPException.

        Args:
            model (Type[DeclarativeBase]): The SQLAlchemy model class to query.
            fields (Dict): A dictionary of field names and their values to filter by.
            missing_ok (bool, optional): If True, return None when no instance is found
                                         instead of raising an exception. Defaults to False.
            case_sensitive (bool, optional): If True, the search will be case-sensitive.
                                             Defaults to True.
            exclude_fields (Optional[Dict[str, Any]]): A dictionary of field names and values to exclude from the results.

        Returns:
            Optional[DeclarativeBase]: The found model instance, or None if not found and missing_ok is True.

        Raises:
            HTTPException: If the model instance is not found and missing_ok is False.
            DatabaseException: If there's an error in field validation or database operation.
        """
        await self.validate_fields(model, fields)

        # Build main query
        if case_sensitive:
            stmt = select(model).filter_by(**fields)
        else:
            conditions = []
            for field_name, value in fields.items():
                field = getattr(model, field_name)
                if isinstance(field.type, SqlAlchemyString):
                    # NOTE: didn't use ilike because of escape character issue
                    conditions.append(func.lower(cast(field, SqlAlchemyString)) == func.lower(value))
                else:
                    conditions.append(field == value)
            stmt = select(model).filter(*conditions)

        if exclude_fields is not None:
            await self.validate_fields(model, exclude_fields)
            exclude_conditions = []
            for field_name, value in exclude_fields.items():
                field = getattr(model, field_name)
                if not case_sensitive and isinstance(field.type, SqlAlchemyString):
                    exclude_conditions.append(func.lower(cast(field, SqlAlchemyString)) != func.lower(value))
                else:
                    exclude_conditions.append(field != value)
            stmt = stmt.filter(*exclude_conditions)

        db_model = self.scalar_one_or_none(stmt)

        if not missing_ok and db_model is None:
            logger.info(f"{model.__name__} not found in database")
            raise ClientException(f"{model.__name__} not found", status_code=status.HTTP_404_NOT_FOUND)

        return db_model if db_model else None

    async def update_by_fields(self, model: Type[DeclarativeBase], fields: Dict[str, Any]) -> object:
        """Update a model instance with the given fields.

        This method updates the attributes of the provided model instance with the values
        in the fields dictionary and then persists the changes to the database.

        Args:
            model (Type[DeclarativeBase]): The SQLAlchemy model instance to update.
            fields (Dict): A dictionary of field names and their new values.

        Returns:
            DeclarativeBase: The updated model instance.

        Raises:
            DatabaseException: If there's an error in field validation or database operation.
        """
        await self.validate_fields(model, fields)

        for field, value in fields.items():
            setattr(model, field, value)

        return self.update_one(model)

    async def delete_one(self, model: Type[DeclarativeBase]) -> None:
        """Delete a model instance from the database."""
        self.delete_model(model)

    async def get_all_by_fields(
        self, model: Type[DeclarativeBase], fields: Dict[str, Any], exclude_fields: Optional[Dict[str, Any]] = None
    ) -> Optional[List[DeclarativeBase]]:
        """Retrieve all model instances from database based on the given fields.

        This method queries the database for all model instances matching the provided fields.
        If no instances are found and missing_ok is False, it raises an HTTPException.

        Args:
            model (Type[DeclarativeBase]): The SQLAlchemy model class to query.
            fields (Dict): A dictionary of field names and their values to filter by.
            exclude_fields (Optional[Dict[str, Any]]): A dictionary of field names and values to exclude from the results.

        Returns:
            Optional[List[DeclarativeBase]]: A list of found model instances, or an empty list if not found
                                            and missing_ok is True.

        Raises:
            HTTPException: If no model instances are found and missing_ok is False.
            DatabaseException: If there's an error in field validation or database operation.
        """
        await self.validate_fields(model, fields)

        stmt = select(model).filter_by(**fields)

        if exclude_fields is not None:
            await self.validate_fields(model, exclude_fields)
            exclude_conditions = [getattr(model, field) != value for field, value in exclude_fields.items()]
            stmt = stmt.filter(*exclude_conditions)

        return self.scalars_all(stmt)

    async def get_count_by_fields(
        self, model: Type[DeclarativeBase], fields: Dict[str, Any], exclude_fields: Optional[Dict[str, Any]] = None
    ) -> int:
        """Get the count of model instances from database based on the given fields.

        Args:
            model (Type[DeclarativeBase]): The SQLAlchemy model class to query.
            fields (Dict): A dictionary of field names and their values to filter by.
            exclude_fields (Optional[Dict[str, Any]]): A dictionary of field names and values to exclude from the results.

        Returns:
            int: The count of model instances matching the provided fields.
        """
        await self.validate_fields(model, fields)

        stmt = select(func.count()).select_from(model).filter_by(**fields)

        if exclude_fields:
            await self.validate_fields(model, exclude_fields)
            exclude_conditions = [getattr(model, field) != value for field, value in exclude_fields.items()]
            stmt = stmt.filter(*exclude_conditions)

        return self.execute_scalar(stmt)

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/helpers.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Provides helper functions for the project."""

import os
from enum import Enum
from typing import Dict, List, Literal, Optional, Union

from huggingface_hub.utils import validate_repo_id
from huggingface_hub.utils._validators import HFValidationError

from budapp.commons import logging


logger = logging.get_logger(__name__)


def create_dynamic_enum(enum_name: str, enum_values: List[str]) -> Enum:
    """Create a dynamic Enum class from a list of values.

    This function generates an Enum class with the given name, using the provided values as enum members.
    The enum member names are created by converting the values to uppercase.

    Args:
        enum_name (str): The name of the Enum class to be created.
        enum_values (list): A list of strings representing the values for the Enum members.

    Returns:
        Enum: A dynamically created Enum class with the specified name and members.

    Raises:
        ValueError: If enum_name is not a valid identifier or enum_values is empty.

    Example:
        >>> Color = create_dynamic_enum("Color", ["red", "green", "blue"])
        >>> Color.RED
        <Color.RED: 'red'>
    """
    # creating enum dynamically from a list of values
    # converting enum name to upper assuming no spaces or special characters
    return Enum(enum_name, {val.upper(): val for val in enum_values})


def assign_random_colors_to_names(names: List[str]) -> List[Dict]:
    """Assign random colors to a list of names, trying to avoid color repetition.

    Args:
        names: List of strings to assign colors to

    Returns:
        List of dictionaries containing name and color pairs
        Example: [{"name": "example", "color": "#E57333"}]
    """
    from .constants import DropdownBackgroundColor

    result = []

    for name in names:
        result.append(
            {
                "name": name,
                "color": DropdownBackgroundColor.get_random_color(),
            }
        )

    return result


def normalize_value(value: Optional[Union[str, List, Dict]]) -> Optional[Union[str, List, Dict]]:
    """Normalize a value by handling None, empty strings, empty lists, and empty dicts.

    Args:
        value: The value to normalize

    Returns:
        - None if the value is None, empty string, empty list, or empty dict
        - Stripped string if the value is a non-empty string
        - Original value for non-empty lists and dicts
        - Original value for other types
    """
    if value is None:
        return None

    # Handle strings
    if isinstance(value, str):
        stripped_value = value.strip()
        return stripped_value if stripped_value else None

    # Handle lists
    if isinstance(value, list):
        return value if value else None

    # Handle dicts
    if isinstance(value, dict):
        return value if value else None

    # Return original value for other types
    return value


def validate_huggingface_repo_format(repo_id: str) -> bool:
    """Validate a huggingface repo id.

    Args:
        repo_id: The huggingface repo id to validate

    Returns:
        True if the repo id is valid, False otherwise
    """
    if not isinstance(repo_id, str):
        return False

    repo_id = repo_id.strip()
    if not repo_id:
        return False

    parts = repo_id.split("/")

    if len(parts) != 2:
        return False

    try:
        validate_repo_id(repo_id)
    except HFValidationError:
        return False

    return True


def validate_icon(icon: str) -> bool:
    """Validates if the provided string is either an emoji or a valid path to an icon.

    Args:
        icon (str): String to validate as emoji or path

    Returns:
        bool: True if valid emoji or existing path

    Raises:
        ValueError: If raise_exception is True and icon is invalid
    """
    from .config import app_settings
    from .constants import EMOJIS

    if not icon:
        logger.debug("No icon provided")
        return False

    try:
        if icon in EMOJIS:
            logger.debug(f"Valid emoji icon: {icon}")
            return True

        icon_path = os.path.join(app_settings.static_dir, icon)
        if os.path.exists(icon_path) and os.path.isfile(icon_path):
            logger.debug(f"Valid file icon: {icon}")
            return True

        logger.debug(f"Invalid icon: {icon}")
        return False

    except Exception as e:
        logger.error(f"Error validating icon: {e}")
        return False


def get_hardware_types(cpu_count: int, gpu_count: int, hpu_count: int) -> List[Literal["CPU", "GPU", "HPU"]]:
    """Get list of hardware types based on hardware counts.

    Args:
        cpu_count: Number of CPUs
        gpu_count: Number of GPUs
        hpu_count: Number of HPUs

    Returns:
        List of hardware types available
    """
    hardware = []
    if cpu_count > 0:
        hardware.append("CPU")
    if gpu_count > 0:
        hardware.append("GPU")
    if hpu_count > 0:
        hardware.append("HPU")

    return hardware

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/api_utils.py`:

```py
"""Utility functions and decorators for API handling."""

import asyncio
from functools import wraps
from typing import Any, Callable, Protocol, Tuple, Type, TypeVar, runtime_checkable

from pydantic import BaseModel


T = TypeVar("T")


@runtime_checkable
class PubSubAPIEndpoint(Protocol):
    is_pubsub_api: bool
    request_model: Type[BaseModel]
    __call__: Callable[..., Any]


def pubsub_api_endpoint(request_model: Type[BaseModel]) -> Callable[[Callable[..., T]], PubSubAPIEndpoint]:
    """Mark a function as a pubsub API endpoint.

    Args:
        request_model (Type[BaseModel]): Pydantic model representing the request data.

    Returns:
        Callable: Decorated function.
    """

    def decorator(func: Callable[..., Any]) -> PubSubAPIEndpoint:
        func.is_pubsub_api = True  # type: ignore
        func.request_model = request_model  # type: ignore

        @wraps(func)
        async def wrapper(*args: Tuple[Any], **kwargs: Any) -> Any:
            if asyncio.iscoroutinefunction(func):
                return await func(*args, **kwargs)
            return func(*args, **kwargs)

        return wrapper  # type: ignore

    return decorator

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/dependencies.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Contains dependency injection functions and utilities for the microservices, enabling modular and reusable components across the application."""

from collections.abc import AsyncGenerator
from typing import List
from uuid import UUID

from fastapi import Depends, HTTPException, Query, status
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from jose import JWTError, jwt
from sqlalchemy.orm import Session
from typing_extensions import Annotated

from budapp.auth.schemas import AccessTokenData
from budapp.commons import logging
from budapp.commons.config import secrets_settings
from budapp.commons.constants import JWT_ALGORITHM, TokenTypeEnum, UserStatusEnum
from budapp.commons.database import SessionLocal
from budapp.user_ops.crud import UserDataManager
from budapp.user_ops.models import User as UserModel
from budapp.user_ops.schemas import User


logger = logging.get_logger(__name__)

security = HTTPBearer()


async def get_session() -> AsyncGenerator[Session, None]:
    """Create and yield an Session for database operations.

    This function is a dependency that provides an Session for use in FastAPI
    route handlers. It ensures that the session is properly closed after use.

    Yields:
        Session: An asynchronous SQLAlchemy session.

    Raises:
        SQLAlchemyError: If there's an error creating or using the session.
    """
    session = SessionLocal()
    try:
        yield session
    finally:
        session.close()


async def get_current_user(
    token: Annotated[HTTPAuthorizationCredentials, Depends(security)],
    session: Session = Depends(get_session),
) -> User:
    """Get the current user.

    Args:
        token (HTTPAuthorizationCredentials): The token.
        session (Session): The database session.

    Returns:
        User: The current user.
    """
    # Define an exception to be raised for invalid credentials
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Invalid authentication credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        # Decode the token and verify its validity
        payload = jwt.decode(
            token.credentials,
            secrets_settings.jwt_secret_key,
            algorithms=[JWT_ALGORITHM],
        )

        # Raise an exception if the token is not an access token
        if payload.get("type") != TokenTypeEnum.ACCESS.value:
            raise credentials_exception from None

        # Extract the user ID from the payload. Raise an exception if it can't be found
        auth_id: str = payload.get("sub")
        if not auth_id:
            raise credentials_exception from None

        # Create AccessTokenData instance with user auth ID
        token_data = AccessTokenData(sub=auth_id)
    except JWTError:
        logger.info("Invalid access token found")
        # Raise an exception if there's an issue decoding the token
        raise credentials_exception from None

    # Retrieve the user from the database
    db_user = await UserDataManager(session).retrieve_by_fields(
        UserModel, {"auth_id": UUID(token_data.sub)}, missing_ok=True
    )

    # Raise an exception if the user is not found
    if not db_user:
        raise credentials_exception from None

    return db_user


async def get_current_active_user(current_user: Annotated[User, Depends(get_current_user)]) -> User:
    """Get the current active user.

    Args:
        current_user (User): The current user.

    Returns:
        User: The current active user.
    """
    if current_user.status != UserStatusEnum.ACTIVE:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Inactive user")
    return current_user


async def get_current_active_invite_user(current_user: Annotated[User, Depends(get_current_user)]) -> User:
    """Get the current active invite user.

    Args:
        current_user (User): The current user.

    Returns:
        User: The current active invite user.
    """
    if current_user.status == UserStatusEnum.DELETED:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Inactive user")
    return current_user


async def parse_ordering_fields(
    order_by: Annotated[
        str | None,
        Query(
            alias="order_by",
            description="Comma-separated list of fields. Example: field1,-field2,field3:asc,field4:desc",
        ),
    ] = None,
) -> List:
    """Parse a comma-separated list of fields with optional sorting directions and returns a list of tuples containing the field name and sorting direction.

    Args:
      order_by (Annotated[
            str | None,
            Query(
                alias="order_by",
                description="Comma-separated list of fields. Example: field1,-field2,field3:asc,field4:desc",
            ),
        ]): The `parse_ordering_fields` function takes a parameter `order_by`,
    which is a comma-separated list of fields used for ordering. Each field can
    optionally include a sorting direction (asc for ascending, desc for
    descending).

    Returns:
      A list of tuples where each tuple contains a field name and its sorting
    direction (ascending or descending) based on the input order_by string provided
    in the function parameter.
    """
    order_by_list = []

    if order_by is not None and order_by != "null":
        # Split the order_by string into individual fields
        fields = order_by.split(",")

        for field in fields:
            # Skip empty fields
            if not field.strip():
                continue

            # Split field into field name and sorting direction
            parts = field.split(":")
            field_name = parts[0].strip()

            if len(parts) == 1:
                # No sorting direction specified, default to ascending
                if field_name.startswith("-"):
                    order_by_list.append((field_name[1:], "desc"))
                else:
                    order_by_list.append((field_name, "asc"))
            else:
                # Sorting direction specified
                sort_direction = parts[1].lower().strip()
                if sort_direction == "asc" or sort_direction == "desc":
                    order_by_list.append((field_name, sort_direction))

    return order_by_list

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/commons/resiliency.py`:

```py
import asyncio
import time
from functools import wraps
from typing import Any, Awaitable, Callable, Dict, Optional, Tuple, Type, TypeVar, Union, cast

from . import logging
from .async_utils import dispatch_async


logger = logging.get_logger(__name__)

T = TypeVar("T")


class RetryWithModifiedParams(Exception):
    """Exception raised to signal that a retry should be attempted with modified parameters.

    This exception can be used to indicate that a function should be retried with different
    parameters than the original call.

    Attributes:
        message (str): The error message.
        details (dict): Additional details about the exception, defaulting to an empty dictionary.
    """

    def __init__(self, message: str, details: Optional[Dict[str, Any]] = None) -> None:
        """Initialize the RetryWithModifiedParams exception.

        Args:
            message (str): The error message.
            details (dict, optional): Additional details about the exception. Defaults to None.
        """
        super().__init__(message)
        self.details: Dict[str, Any] = details or {}


class AbortRetry(Exception):
    """Exception raised to signal that retry attempts should be aborted.

    This exception can be used to immediately stop any further retry attempts and
    propagate the exception up the call stack.
    """

    pass


def retry(
    max_attempts: Optional[int] = 3,
    max_elapsed_time: Optional[float] = None,
    delay: Optional[float] = None,
    backoff_factor: Optional[float] = 2,
    max_delay: Optional[float] = None,
    exceptions_to_retry: Union[Type[Exception], Tuple[Type[Exception], ...]] = Exception,
) -> Callable[[Callable[..., T]], Callable[..., T]]:
    """Retry a function with exponential backoff.

    Args:
        max_attempts (int, optional): Maximum number of retry attempts. Defaults to 3.
        max_elapsed_time (float, optional): Maximum total time allowed for retries. Defaults to None.
        delay (float, optional): Initial delay between retries in seconds. Defaults to None.
        backoff_factor (float, optional): Multiplier applied to delay between attempts. Defaults to 2.
        max_delay (float, optional): Maximum delay between retries in seconds. Defaults to None.
        exceptions_to_retry (Union[Type[Exception], Tuple[Type[Exception], ...]], optional):
            Exceptions that trigger a retry. Defaults to Exception.

    Returns:
        Callable: Decorated function with retry logic.
    """

    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @wraps(func)
        def sync_wrapper(*args: Any, **kwargs: Any) -> T:
            return cast(T, _retry_logic(func, False, *args, **kwargs))

        @wraps(func)
        async def async_wrapper(*args: Any, **kwargs: Any) -> T:
            return await cast(Awaitable[T], _retry_logic(func, True, *args, **kwargs))

        return cast(Callable[..., T], async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper)

    def _retry_logic(
        func: Callable[..., Union[T, Awaitable[T]]], is_async: bool, *args: Any, **kwargs: Any
    ) -> Union[T, Awaitable[T]]:
        """Implement the retry logic for both synchronous and asynchronous functions.

        Args:
            func (Callable): The function to be retried.
            is_async (bool): Whether the function is asynchronous.
            *args: Positional arguments for the function.
            **kwargs: Keyword arguments for the function.

        Returns:
            Union[T, Awaitable[T]]: The result of the function call.

        Raises:
            Exception: If the retry limit is reached.
        """
        retry_config: Dict[str, Any] = kwargs.pop("retry_config", {})
        local_max_attempts: Optional[int] = retry_config.get("max_attempts", max_attempts)
        local_max_elapsed_time: Optional[float] = retry_config.get("max_elapsed_time", max_elapsed_time)
        local_delay: Optional[float] = retry_config.get("delay", delay)
        local_backoff_factor: Optional[float] = retry_config.get("backoff_factor", backoff_factor)
        local_max_delay: Optional[float] = retry_config.get("max_delay", max_delay)
        local_exceptions_to_retry: Union[Type[Exception], Tuple[Type[Exception], ...]] = retry_config.get(
            "exceptions_to_retry", exceptions_to_retry
        )

        if local_max_attempts == 0:
            return dispatch_async(func, *args, **kwargs)

        wait_for: float = local_delay or (1 if local_backoff_factor is not None else 0)
        start_time: float = time.monotonic()

        async def async_retry() -> T:
            """Retry asynchronously."""
            nonlocal wait_for
            attempt: int = 0
            while True:
                try:
                    return await cast(Awaitable[T], func(*args, **kwargs))
                except local_exceptions_to_retry as e:
                    attempt += 1
                    elapsed_time: float = time.monotonic() - start_time

                    if isinstance(e, AbortRetry):
                        raise e from None
                    if isinstance(e, RetryWithModifiedParams) and "kwargs" in e.details:
                        kwargs.update(e.details["kwargs"])

                    if (local_max_attempts and attempt >= local_max_attempts) or (
                        local_max_elapsed_time and elapsed_time >= local_max_elapsed_time
                    ):
                        raise Exception(
                            f"Retry limit reached for {func.__name__}. "
                            f"Attempts: {attempt}, Elapsed time: {elapsed_time:.2f}s"
                        ) from None

                    logger.error(
                        f"Attempt {attempt} failed with {str(e)}. "
                        f"Retrying {func.__name__} in {wait_for:.2f} seconds..."
                    )
                    await asyncio.sleep(wait_for)
                    if local_backoff_factor is not None:
                        wait_for = min(wait_for * local_backoff_factor, local_max_delay or float("inf"))

        def sync_retry() -> T:
            """Retry synchronously."""
            nonlocal wait_for
            attempt: int = 0
            while True:
                try:
                    return cast(T, func(*args, **kwargs))
                except local_exceptions_to_retry as e:
                    attempt += 1
                    elapsed_time: float = time.monotonic() - start_time

                    if isinstance(e, AbortRetry):
                        raise e from None
                    if isinstance(e, RetryWithModifiedParams) and "kwargs" in e.details:
                        kwargs.update(e.details["kwargs"])

                    if (local_max_attempts and attempt >= local_max_attempts) or (
                        local_max_elapsed_time and elapsed_time >= local_max_elapsed_time
                    ):
                        raise Exception(
                            f"Retry limit reached for {func.__name__}. "
                            f"Attempts: {attempt}, Elapsed time: {elapsed_time:.2f}s"
                        ) from None

                    logger.error(
                        f"Attempt {attempt} failed with {str(e)}. "
                        f"Retrying {func.__name__} in {wait_for:.2f} seconds..."
                    )
                    time.sleep(wait_for)
                    if local_backoff_factor is not None:
                        wait_for = min(wait_for * local_backoff_factor, local_max_delay or float("inf"))

        return async_retry() if is_async else sync_retry()

    return decorator

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/alembic.ini`:

```ini
# A generic, single database configuration.

[alembic]
# path to migration scripts
# Use forward slashes (/) also on windows to provide an os agnostic path
script_location = %(here)s/migrations

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to migrations/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "version_path_separator" below.
# version_locations = %(here)s/bar:%(here)s/bat:migrations/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
# version_path_separator = newline
version_path_separator = os  # Use os.pathsep. Default configuration used for new projects.

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

sqlalchemy.url = driver://user:pass@localhost/dbname


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the exec runner, execute a binary
# hooks = ruff
# ruff.type = exec
# ruff.executable = %(here)s/.venv/bin/ruff
# ruff.options = --fix REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/model_ops/services.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The model ops services. Contains business logic for model ops."""

import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID, uuid4

import aiohttp
from fastapi import UploadFile, status
from pydantic import HttpUrl

from budapp.commons import logging
from budapp.commons.config import app_settings
from budapp.commons.db_utils import SessionMixin
from budapp.commons.exceptions import ClientException
from budapp.commons.helpers import assign_random_colors_to_names, normalize_value
from budapp.commons.schemas import Tag, Task
from budapp.credential_ops.crud import ProprietaryCredentialDataManager
from budapp.credential_ops.models import ProprietaryCredential as ProprietaryCredentialModel
from budapp.workflow_ops.crud import WorkflowDataManager, WorkflowStepDataManager
from budapp.workflow_ops.models import Workflow as WorkflowModel
from budapp.workflow_ops.models import WorkflowStep as WorkflowStepModel
from budapp.workflow_ops.services import WorkflowService, WorkflowStepService

from ..commons.constants import (
    APP_ICONS,
    BUD_INTERNAL_WORKFLOW,
    LICENSE_DIR,
    BaseModelRelationEnum,
    BudServeWorkflowStepEventName,
    CloudModelStatusEnum,
    CredentialTypeEnum,
    EndpointStatusEnum,
    ModelProviderTypeEnum,
    ModelSecurityScanStatusEnum,
    ModelSourceEnum,
    ModelStatusEnum,
    NotificationTypeEnum,
    VisibilityEnum,
    WorkflowStatusEnum,
    WorkflowTypeEnum,
)
from ..commons.helpers import validate_huggingface_repo_format
from ..core.schemas import NotificationPayload, NotificationResult
from ..endpoint_ops.crud import EndpointDataManager
from ..endpoint_ops.models import Endpoint as EndpointModel
from ..shared.notification_service import BudNotifyService, NotificationBuilder
from ..workflow_ops.schemas import WorkflowUtilCreate
from .crud import (
    CloudModelDataManager,
    ModelDataManager,
    ModelLicensesDataManager,
    ModelSecurityScanResultDataManager,
    PaperPublishedDataManager,
    ProviderDataManager,
)
from .models import CloudModel, Model, ModelLicenses, PaperPublished
from .models import ModelSecurityScanResult as ModelSecurityScanResultModel
from .models import Provider as ProviderModel
from .schemas import (
    CreateCloudModelWorkflowRequest,
    CreateCloudModelWorkflowResponse,
    CreateCloudModelWorkflowStepData,
    CreateCloudModelWorkflowSteps,
    CreateLocalModelWorkflowRequest,
    CreateLocalModelWorkflowSteps,
    LocalModelScanRequest,
    LocalModelScanWorkflowStepData,
    ModelArchitectureLLMConfig,
    ModelArchitectureVisionConfig,
    ModelCreate,
    ModelDetailSuccessResponse,
    ModelIssue,
    ModelLicensesCreate,
    ModelLicensesModel,
    ModelListResponse,
    ModelResponse,
    ModelSecurityScanResultCreate,
    ModelTree,
    PaperPublishedCreate,
)


logger = logging.get_logger(__name__)


class ProviderService(SessionMixin):
    """Provider service."""

    async def get_all_providers(
        self,
        offset: int = 0,
        limit: int = 10,
        filters: Dict = {},
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[ProviderModel], int]:
        """Get all providers."""
        return await ProviderDataManager(self.session).get_all_providers(offset, limit, filters, order_by, search)


class CloudModelWorkflowService(SessionMixin):
    """Model service."""

    async def add_cloud_model_workflow(self, current_user_id: UUID, request: CreateCloudModelWorkflowRequest) -> None:
        """Add a cloud model workflow."""
        # Get request data
        step_number = request.step_number
        workflow_id = request.workflow_id
        workflow_total_steps = request.workflow_total_steps
        provider_type = request.provider_type
        name = request.name
        modality = request.modality
        uri = request.uri
        tags = request.tags
        trigger_workflow = request.trigger_workflow
        provider_id = request.provider_id
        cloud_model_id = request.cloud_model_id

        current_step_number = step_number

        # Retrieve or create workflow
        workflow_create = WorkflowUtilCreate(
            workflow_type=WorkflowTypeEnum.CLOUD_MODEL_ONBOARDING,
            title="Cloud Model Onboarding",
            total_steps=workflow_total_steps,
            icon=APP_ICONS["general"]["model_mono"],
            tag="Model Onboarding",
        )
        db_workflow = await WorkflowService(self.session).retrieve_or_create_workflow(
            workflow_id, workflow_create, current_user_id
        )

        # Model source is provider type
        source = None
        if provider_id:
            db_provider = await ProviderDataManager(self.session).retrieve_by_fields(
                ProviderModel, {"id": provider_id}
            )
            source = db_provider.type.value

            # Update icon on workflow
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"icon": db_provider.icon, "title": db_provider.name},
            )

        if provider_type == ModelProviderTypeEnum.CLOUD_MODEL:
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"title": "Cloud Model"},
            )

        if cloud_model_id:
            db_cloud_model = await CloudModelDataManager(self.session).retrieve_by_fields(
                CloudModel, {"id": cloud_model_id, "status": CloudModelStatusEnum.ACTIVE}
            )

            if db_cloud_model.is_present_in_model:
                raise ClientException("Cloud model is already present in model")

            # Update title on workflow
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"title": db_cloud_model.name},
            )

        if name:
            db_model = await ModelDataManager(self.session).retrieve_by_fields(
                Model, {"name": name, "status": ModelStatusEnum.ACTIVE}, missing_ok=True
            )
            if db_model:
                raise ClientException("Model name already exists")

            # Update title on workflow
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"title": name},
            )

        # Prepare workflow step data
        workflow_step_data = CreateCloudModelWorkflowSteps(
            provider_type=provider_type,
            source=source if source else None,
            name=name,
            modality=modality,
            uri=uri,
            tags=tags,
            provider_id=provider_id,
            cloud_model_id=cloud_model_id,
        ).model_dump(exclude_none=True, exclude_unset=True, mode="json")

        # Get workflow steps
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": db_workflow.id}
        )

        # For avoiding another db call for record retrieval, storing db object while iterating over db_workflow_steps
        db_current_workflow_step = None

        if db_workflow_steps:
            await self._validate_duplicate_source_uri_model(source, uri, db_workflow_steps, current_step_number)

            for db_step in db_workflow_steps:
                # Get current workflow step
                if db_step.step_number == current_step_number:
                    db_current_workflow_step = db_step

        if db_current_workflow_step:
            logger.info(f"Workflow {db_workflow.id} step {current_step_number} already exists")

            # Update workflow step data in db
            db_workflow_step = await WorkflowStepDataManager(self.session).update_by_fields(
                db_current_workflow_step,
                {"data": workflow_step_data},
            )
            logger.info(f"Workflow {db_workflow.id} step {current_step_number} updated")
        else:
            logger.info(f"Creating workflow step {current_step_number} for workflow {db_workflow.id}")

            # Insert step details in db
            db_workflow_step = await WorkflowStepDataManager(self.session).insert_one(
                WorkflowStepModel(
                    workflow_id=db_workflow.id,
                    step_number=current_step_number,
                    data=workflow_step_data,
                )
            )

        # Update workflow current step as the highest step_number
        db_max_workflow_step_number = max(step.step_number for step in db_workflow_steps) if db_workflow_steps else 0
        workflow_current_step = max(current_step_number, db_max_workflow_step_number)
        logger.info(f"The current step of workflow {db_workflow.id} is {workflow_current_step}")

        # Create next step if workflow is triggered
        if trigger_workflow:
            # Increment step number of workflow and workflow step
            current_step_number = current_step_number + 1
            workflow_current_step = current_step_number

            # Update or create next workflow step
            db_workflow_step = await self._create_or_update_next_workflow_step(db_workflow.id, current_step_number, {})

        # Update workflow step data in db
        db_workflow = await WorkflowDataManager(self.session).update_by_fields(
            db_workflow,
            {"current_step": workflow_current_step},
        )

        # Execute workflow
        if trigger_workflow:
            logger.info("Workflow triggered")

            # TODO: Currently querying workflow steps again by ordering steps in ascending order
            # To ensure the latest step update is fetched, Consider excluding it later
            db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
                {"workflow_id": db_workflow.id}
            )

            # Define the keys required for model deployment
            keys_of_interest = [
                "source",
                "name",
                "modality",
                "uri",
                "tags",
                "provider_type",
                "provider_id",
                "cloud_model_id",
            ]

            # from workflow steps extract necessary information
            required_data = {}
            for db_workflow_step in db_workflow_steps:
                for key in keys_of_interest:
                    if key in db_workflow_step.data:
                        required_data[key] = db_workflow_step.data[key]

            # Check if all required keys are present
            required_keys = ["provider_type", "provider_id", "modality", "tags", "name", "source"]
            missing_keys = [key for key in required_keys if key not in required_data]
            if missing_keys:
                raise ClientException(f"Missing required data: {', '.join(missing_keys)}")

            # Check duplicate name exist in model
            db_model = await ModelDataManager(self.session).retrieve_by_fields(
                Model,
                {"name": required_data["name"], "status": ModelStatusEnum.ACTIVE},
                missing_ok=True,
                case_sensitive=False,
            )
            if db_model:
                raise ClientException("Model name already exists")

            # Trigger deploy model by step
            db_model = await self._execute_add_cloud_model_workflow(required_data, db_workflow.id, current_user_id)
            logger.debug(f"Successfully created model {db_model.id}")

        return db_workflow

    async def _execute_add_cloud_model_workflow(
        self, data: Dict[str, Any], workflow_id: UUID, current_user_id: UUID
    ) -> None:
        """Execute add cloud model workflow."""
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        # Latest step
        db_latest_workflow_step = db_workflow_steps[-1]

        # if cloud model id is provided, retrieve cloud model
        cloud_model_id = data.get("cloud_model_id")
        db_cloud_model = None
        if cloud_model_id:
            db_cloud_model = await CloudModelDataManager(self.session).retrieve_by_fields(
                CloudModel, {"id": cloud_model_id, "status": CloudModelStatusEnum.ACTIVE}, missing_ok=True
            )
        # Check duplicate name exist in model
        db_model = await ModelDataManager(self.session).retrieve_by_fields(
            Model,
            {"name": data["name"], "status": ModelStatusEnum.ACTIVE},
            missing_ok=True,
            case_sensitive=False,
        )
        if db_model:
            raise ClientException("Model name already exists")

        # Prepare model creation data from input
        model_data = await self._prepare_model_data(data, current_user_id, db_cloud_model)

        # Check for duplicate model
        db_model = await ModelDataManager(self.session).retrieve_by_fields(
            Model,
            {"uri": model_data.uri, "source": model_data.source, "status": ModelStatusEnum.ACTIVE},
            missing_ok=True,
        )
        if db_model:
            logger.info(f"Model {model_data.uri} with {model_data.source} already exists")
            raise ClientException("Duplicate model uri and source found")

        # Mark workflow completed
        logger.debug(f"Updating workflow status: {workflow_id}")
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})

        execution_status_data = {
            "workflow_execution_status": {
                "status": "success",
                "message": "Model successfully added to the repository",
            },
            "model_id": None,
        }
        try:
            db_model = await ModelDataManager(self.session).insert_one(
                Model(**model_data.model_dump(exclude_unset=True))
            )
        except Exception as e:
            logger.exception(f"Failed to add model to the repository {e}")
            execution_status_data["workflow_execution_status"]["status"] = "error"
            execution_status_data["workflow_execution_status"]["message"] = "Failed to add model to the repository"
            execution_status_data["model_id"] = None
            db_workflow_step = await WorkflowStepDataManager(self.session).update_by_fields(
                db_latest_workflow_step, {"data": execution_status_data}
            )
            await WorkflowDataManager(self.session).update_by_fields(
                db_workflow, {"status": WorkflowStatusEnum.FAILED, "reason": str(e)}
            )
        else:
            execution_status_data["model_id"] = str(db_model.id)
            db_workflow_step = await WorkflowStepDataManager(self.session).update_by_fields(
                db_latest_workflow_step, {"data": execution_status_data}
            )

            if db_cloud_model:
                await CloudModelDataManager(self.session).update_by_fields(
                    db_cloud_model, {"is_present_in_model": True}
                )

            # leaderboard data. TODO: Need to add service for leaderboard
            leaderboard_data = await self._get_leaderboard_data()

            # Add leader board details
            end_step_number = db_workflow_step.step_number + 1

            # Create or update next workflow step
            db_workflow_step = await self._create_or_update_next_workflow_step(
                workflow_id, end_step_number, {"leaderboard": leaderboard_data}
            )

            # Update workflow current step and status
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"current_step": end_step_number, "status": WorkflowStatusEnum.COMPLETED},
            )

            # Send notification to workflow creator
            model_icon = await ModelServiceUtil(self.session).get_model_icon(db_model)
            notification_request = (
                NotificationBuilder()
                .set_content(
                    title=db_model.name,
                    message="Added to Repository",
                    icon=model_icon,
                    result=NotificationResult(target_id=db_model.id, target_type="model").model_dump(
                        exclude_none=True, exclude_unset=True
                    ),
                )
                .set_payload(workflow_id=str(db_workflow.id), type=NotificationTypeEnum.MODEL_ONBOARDING_SUCCESS.value)
                .set_notification_request(subscriber_ids=[str(db_workflow.created_by)])
                .build()
            )
            await BudNotifyService().send_notification(notification_request)

        return db_model

    async def _validate_duplicate_source_uri_model(
        self, source: str, uri: str, db_workflow_steps: List[WorkflowStepModel], current_step_number: int
    ) -> None:
        """Validate duplicate source and uri."""
        db_step_uri = None
        db_step_source = None

        for db_step in db_workflow_steps:
            # Get current workflow step
            if db_step.step_number == current_step_number:
                db_current_workflow_step = db_step

            if "uri" in db_step.data:
                db_step_uri = db_step.data["uri"]
            if "source" in db_step.data:
                db_step_source = db_step.data["source"]

        # Check duplicate endpoint in project
        query_uri = None
        query_source = None
        if uri and db_step_source:
            # If user gives uri but source given in earlier step
            query_uri = uri
            query_source = db_step_source
        elif source and db_step_uri:
            # If user gives source but uri given in earlier step
            query_uri = db_step_uri
            query_source = source
        elif uri and source:
            # if user gives both source and uri
            query_uri = uri
            query_source = source
        elif db_step_uri and db_step_source:
            # if user gives source and uri in earlier step
            query_uri = db_step_uri
            query_source = db_step_source

        if query_uri and query_source:
            # NOTE: A model can only be deployed once in a project
            db_cloud_model = await ModelDataManager(self.session).retrieve_by_fields(
                Model,
                {
                    "uri": query_uri,
                    "source": query_source,
                    "status": ModelStatusEnum.ACTIVE,
                },
                missing_ok=True,
            )
            if db_cloud_model:
                logger.info(f"Model {query_uri} already added with {query_source}")
                raise ClientException("Duplicate model uri and source found")

    async def _create_or_update_next_workflow_step(
        self, workflow_id: UUID, step_number: int, data: Dict[str, Any]
    ) -> None:
        """Create or update next workflow step."""
        # Check for workflow step exist or not
        db_workflow_step = await WorkflowStepDataManager(self.session).retrieve_by_fields(
            WorkflowStepModel,
            {"workflow_id": workflow_id, "step_number": step_number},
            missing_ok=True,
        )

        if db_workflow_step:
            db_workflow_step = await WorkflowStepDataManager(self.session).update_by_fields(
                db_workflow_step,
                {
                    "workflow_id": workflow_id,
                    "step_number": step_number,
                    "data": data,
                },
            )
        else:
            # Create a new workflow step
            db_workflow_step = await WorkflowStepDataManager(self.session).insert_one(
                WorkflowStepModel(
                    workflow_id=workflow_id,
                    step_number=step_number,
                    data=data,
                )
            )

        return db_workflow_step

    async def _get_leaderboard_data(self) -> None:
        """Get leaderboard data."""
        return [
            {
                "position": 1,
                "model": "name",
                "IFEval": 13.2,
                "BBH": 13.2,
                "M": 1.2,
            },
            {
                "position": 1,
                "model": "name",
                "IFEval": 13.2,
                "BBH": 13.2,
                "M": 1.2,
            },
            {
                "position": 1,
                "model": "name",
                "IFEval": 13.2,
                "BBH": 13.2,
                "M": 1.2,
            },
            {
                "position": 1,
                "model": "name",
                "IFEval": 13.2,
                "BBH": 13.2,
                "M": 1.2,
            },
        ]

    async def _prepare_model_data(
        self, data: Dict[str, Any], current_user_id: UUID, db_cloud_model: Optional[CloudModel] = None
    ) -> None:
        """Prepare model data."""
        source = data.get("source")
        name = data.get("name")
        modality = data.get("modality")
        uri = data.get("uri")
        tags = data.get("tags")
        provider_type = data.get("provider_type")
        provider_id = data.get("provider_id")

        if db_cloud_model:
            model_data = ModelCreate(
                name=name,
                description=db_cloud_model.description,
                tags=tags,
                tasks=db_cloud_model.tasks,
                author=db_cloud_model.author,
                model_size=db_cloud_model.model_size,
                github_url=db_cloud_model.github_url,
                huggingface_url=db_cloud_model.huggingface_url,
                website_url=db_cloud_model.website_url,
                modality=db_cloud_model.modality,
                source=db_cloud_model.source,
                provider_type=provider_type,
                uri=db_cloud_model.uri,
                created_by=current_user_id,
                provider_id=provider_id,
            )
        else:
            model_data = ModelCreate(
                source=source,
                name=name,
                modality=modality,
                uri=uri,
                tags=tags,
                provider_type=provider_type,
                created_by=current_user_id,
                provider_id=provider_id,
            )

        return model_data

    async def get_cloud_model_workflow(self, workflow_id: UUID) -> CreateCloudModelWorkflowResponse:
        """Get cloud model workflow."""
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})

        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        if not db_workflow_steps:
            return CreateCloudModelWorkflowResponse(
                workflow_id=db_workflow.id,
                status=db_workflow.status,
                current_step=db_workflow.current_step,
                total_steps=db_workflow.total_steps,
                reason=db_workflow.reason,
                workflow_steps=CreateCloudModelWorkflowStepData(),
                code=status.HTTP_200_OK,
                object="cloud_model_workflow.get",
                message="Cloud model workflow retrieved successfully",
            )

        # Define the keys required for model deployment
        keys_of_interest = [
            "source",
            "name",
            "modality",
            "uri",
            "tags",
            "icon",
            "provider_type",
            "provider_id",
            "cloud_model_id",
            "description",
            "model_id",
            "workflow_execution_status",
            "leaderboard",
        ]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]

        provider_type = required_data.get("provider_type")
        provider_id = required_data.get("provider_id")
        cloud_model_id = required_data.get("cloud_model_id")
        model_id = required_data.get("model_id")
        workflow_execution_status = required_data.get("workflow_execution_status")
        leaderboard = required_data.get("leaderboard")

        db_provider = (
            await ProviderDataManager(self.session).retrieve_by_fields(
                ProviderModel, {"id": required_data["provider_id"]}, missing_ok=True
            )
            if "provider_id" in required_data
            else None
        )

        db_cloud_model = (
            await CloudModelDataManager(self.session).retrieve_by_fields(
                CloudModel,
                {"id": required_data["cloud_model_id"], "status": CloudModelStatusEnum.ACTIVE},
                missing_ok=True,
            )
            if "cloud_model_id" in required_data
            else None
        )

        db_model = (
            await ModelDataManager(self.session).retrieve_by_fields(
                Model, {"id": UUID(required_data["model_id"]), "status": ModelStatusEnum.ACTIVE}, missing_ok=True
            )
            if "model_id" in required_data
            else None
        )

        return CreateCloudModelWorkflowResponse(
            workflow_id=db_workflow.id,
            status=db_workflow.status,
            current_step=db_workflow.current_step,
            total_steps=db_workflow.total_steps,
            reason=db_workflow.reason,
            workflow_steps=CreateCloudModelWorkflowStepData(
                provider_type=provider_type,
                provider=db_provider,
                provider_id=provider_id,
                cloud_model=db_cloud_model,
                cloud_model_id=cloud_model_id,
                model=db_model,
                model_id=model_id,
                workflow_execution_status=workflow_execution_status,
                leaderboard=leaderboard,
            ),
            code=status.HTTP_200_OK,
            object="cloud_model_workflow.get",
            message="Cloud model workflow retrieved successfully",
        )


class LocalModelWorkflowService(SessionMixin):
    """Local model workflow service."""

    async def add_local_model_workflow(self, current_user_id: UUID, request: CreateLocalModelWorkflowRequest) -> None:
        """Add a local model workflow."""
        # Get request data
        step_number = request.step_number
        workflow_id = request.workflow_id
        workflow_total_steps = request.workflow_total_steps
        provider_type = request.provider_type
        proprietary_credential_id = request.proprietary_credential_id
        name = request.name
        uri = request.uri
        author = request.author
        tags = request.tags
        icon = request.icon
        trigger_workflow = request.trigger_workflow

        current_step_number = step_number

        # Retrieve or create workflow
        workflow_create = WorkflowUtilCreate(
            workflow_type=WorkflowTypeEnum.LOCAL_MODEL_ONBOARDING,
            title="Local Model Onboarding",
            total_steps=workflow_total_steps,
            icon=APP_ICONS["general"]["model_mono"],
            tag="Model Onboarding",
        )
        db_workflow = await WorkflowService(self.session).retrieve_or_create_workflow(
            workflow_id, workflow_create, current_user_id
        )

        # Validate proprietary credential id
        if proprietary_credential_id:
            await ProprietaryCredentialDataManager(self.session).retrieve_by_fields(
                ProprietaryCredentialModel, {"id": proprietary_credential_id}
            )

        # Validate model name to be unique
        if name:
            db_model = await ModelDataManager(self.session).retrieve_by_fields(
                Model, {"name": name, "status": ModelStatusEnum.ACTIVE}, missing_ok=True
            )
            if db_model:
                raise ClientException("Model name should be unique")

            # Update title on workflow
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"title": name},
            )

        # Add provider_id for HuggingFace provider type
        provider_id = None
        if provider_type == ModelProviderTypeEnum.HUGGING_FACE:
            db_provider = await ProviderDataManager(self.session).retrieve_by_fields(
                ProviderModel, {"type": CredentialTypeEnum.HUGGINGFACE}
            )
            provider_id = db_provider.id

            # Update icon, title on workflow
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"icon": db_provider.icon, "title": "Huggingface Model"},
            )
        elif provider_type == ModelProviderTypeEnum.URL:
            # Update icon, title on workflow
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"icon": APP_ICONS["general"]["model_mono"], "title": "URL"},
            )
        elif provider_type == ModelProviderTypeEnum.DISK:
            # Update icon, title on workflow
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"icon": APP_ICONS["general"]["model_mono"], "title": "Disk"},
            )

        # Prepare workflow step data
        workflow_step_data = CreateLocalModelWorkflowSteps(
            provider_type=provider_type,
            proprietary_credential_id=proprietary_credential_id,
            name=name,
            uri=uri,
            author=author,
            tags=tags,
            icon=icon,
            provider_id=provider_id,
        ).model_dump(exclude_none=True, exclude_unset=True, mode="json")

        # Get workflow steps
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": db_workflow.id}
        )

        # For avoiding another db call for record retrieval, storing db object while iterating over db_workflow_steps
        db_current_workflow_step = None

        # Verify hugging face uri duplication
        await self._verify_provider_type_uri_duplication(provider_type, uri, db_workflow_steps)

        if db_workflow_steps:
            for db_step in db_workflow_steps:
                # Get current workflow step
                if db_step.step_number == current_step_number:
                    db_current_workflow_step = db_step

        if db_current_workflow_step:
            logger.info(f"Workflow {db_workflow.id} step {current_step_number} already exists")

            # Update workflow step data in db
            db_workflow_step = await WorkflowStepDataManager(self.session).update_by_fields(
                db_current_workflow_step,
                {"data": workflow_step_data},
            )
            logger.info(f"Workflow {db_workflow.id} step {current_step_number} updated")
        else:
            logger.info(f"Creating workflow step {current_step_number} for workflow {db_workflow.id}")

            # Insert step details in db
            db_workflow_step = await WorkflowStepDataManager(self.session).insert_one(
                WorkflowStepModel(
                    workflow_id=db_workflow.id,
                    step_number=current_step_number,
                    data=workflow_step_data,
                )
            )

        # Update workflow current step as the highest step_number
        db_max_workflow_step_number = max(step.step_number for step in db_workflow_steps) if db_workflow_steps else 0
        workflow_current_step = max(current_step_number, db_max_workflow_step_number)
        logger.info(f"The current step of workflow {db_workflow.id} is {workflow_current_step}")

        # This will ensure workflow step number is updated to the latest step number
        db_workflow = await WorkflowDataManager(self.session).update_by_fields(
            db_workflow,
            {"current_step": workflow_current_step},
        )

        if trigger_workflow:
            # query workflow steps again to get latest data
            db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
                {"workflow_id": db_workflow.id}
            )

            # Define the keys required for model extraction
            keys_of_interest = [
                "provider_type",
                "proprietary_credential_id",  # Only required for HuggingFace (Optional)
                "name",
                "uri",
            ]

            # from workflow steps extract necessary information
            required_data = {}
            for db_workflow_step in db_workflow_steps:
                for key in keys_of_interest:
                    if key in db_workflow_step.data:
                        required_data[key] = db_workflow_step.data[key]

            # Check if all required keys are present
            required_keys = ["provider_type", "name", "uri"]
            missing_keys = [key for key in required_keys if key not in required_data]
            if missing_keys:
                raise ClientException(f"Missing required data for model extraction: {', '.join(missing_keys)}")

            try:
                # Perform model extraction
                await self._perform_model_extraction(current_step_number, required_data, current_user_id, db_workflow)
            except ClientException as e:
                raise e

        return db_workflow

    async def create_model_from_notification_event(self, payload: NotificationPayload) -> None:
        """Create a local model from notification event."""
        logger.debug("Received event for creating local model")

        # Get workflow and steps
        workflow_id = payload.workflow_id
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        # Define the keys required for endpoint creation
        keys_of_interest = [
            "provider_type",
            "name",
            "uri",
            "author",
            "tags",
            "icon",
        ]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]
        logger.debug("Collected required data from workflow steps")

        # Check for model with duplicate name
        db_model = await ModelDataManager(self.session).retrieve_by_fields(
            Model,
            {"name": required_data["name"], "status": ModelStatusEnum.ACTIVE},
            missing_ok=True,
            case_sensitive=False,
        )
        if db_model:
            logger.error(f"Unable to create model with name {required_data['name']} as it already exists")
            raise ClientException("Model name should be unique")

        model_info = payload.content.result["model_info"]
        local_path = payload.content.result["local_path"]

        # Extract and finalize tags, tasks and author
        given_tags = required_data.get("tags", [])
        extracted_tags = model_info.get("tags", []) or []
        if extracted_tags:
            # assign random colors to extracted tags
            extracted_tags = assign_random_colors_to_names(extracted_tags)

            # remove duplicate tags
            existing_tag_names = [tag["name"] for tag in given_tags]
            extracted_tags = [tag for tag in extracted_tags if tag["name"] not in existing_tag_names]

        extracted_tags.extend(given_tags)

        extracted_tasks = model_info.get("tasks", [])
        if extracted_tasks:
            extracted_tasks = assign_random_colors_to_names(extracted_tasks)

        extracted_author = required_data.get("author")
        if not extracted_author:
            extracted_author = normalize_value(model_info.get("author", None))

        # Finalize model details
        model_description = normalize_value(model_info.get("description", None))
        model_github_url = normalize_value(model_info.get("github_url", None))
        model_huggingface_url = normalize_value(model_info.get("provider_url", None))
        model_website_url = normalize_value(model_info.get("website_url", None))
        languages = normalize_value(model_info.get("languages", None))
        use_cases = normalize_value(model_info.get("use_cases", None))
        strengths = normalize_value(model_info.get("strengths", None))
        limitations = normalize_value(model_info.get("limitations", None))

        # Architecture
        model_info_architecture = model_info.get("architecture", {})
        if model_info_architecture is not None:
            model_size = normalize_value(model_info_architecture.get("num_params", None))
            model_type = normalize_value(model_info_architecture.get("type", None))
            family = normalize_value(model_info_architecture.get("family", None))
            model_weights_size = normalize_value(model_info_architecture.get("model_weights_size", None))
            kv_cache_size = normalize_value(model_info_architecture.get("kv_cache_size", None))

            # LLM Config
            model_info_text_config = model_info_architecture.get("text_config", {})
            if model_info_text_config is not None:
                text_config = ModelArchitectureLLMConfig(
                    num_layers=normalize_value(model_info_text_config.get("num_layers", None)),
                    hidden_size=normalize_value(model_info_text_config.get("hidden_size", None)),
                    intermediate_size=normalize_value(model_info_text_config.get("intermediate_size", None)),
                    context_length=normalize_value(model_info_text_config.get("context_length", None)),
                    vocab_size=normalize_value(model_info_text_config.get("vocab_size", None)),
                    torch_dtype=normalize_value(model_info_text_config.get("torch_dtype", None)),
                    num_attention_heads=normalize_value(model_info_text_config.get("num_attention_heads", None)),
                    num_key_value_heads=normalize_value(model_info_text_config.get("num_key_value_heads", None)),
                    rope_scaling=normalize_value(model_info_text_config.get("rope_scaling", None)),
                )
            else:
                text_config = None

            # Vision Config
            model_info_vision_config = model_info_architecture.get("vision_config", {})
            if model_info_vision_config is not None:
                vision_config = ModelArchitectureVisionConfig(
                    num_layers=normalize_value(model_info_vision_config.get("num_layers", None)),
                    hidden_size=normalize_value(model_info_vision_config.get("hidden_size", None)),
                    intermediate_size=normalize_value(model_info_vision_config.get("intermediate_size", None)),
                    torch_dtype=normalize_value(model_info_vision_config.get("torch_dtype", None)),
                )
            else:
                vision_config = None
        else:
            model_size = None
            model_type = None
            family = None
            model_weights_size = None
            kv_cache_size = None
            text_config = None
            vision_config = None

        # Get base model relation
        model_tree = model_info.get("model_tree", {})
        base_model_relation = await self.get_base_model_relation(model_tree)

        # Sanitize base model
        base_model = model_tree.get("base_model", [])
        if isinstance(base_model, list) and len(base_model) > 0:
            base_model = base_model
        elif isinstance(base_model, str):
            base_model = [base_model]
        base_model = normalize_value(base_model)

        # If base model is the same as the model uri, set base model to None
        if required_data["uri"] == base_model:
            base_model = None

        # Dummy Values
        # TODO: remove this after implementing actual service
        examples = [
            {
                "prompt": "Explain the concept of machine learning in simple terms.",
                "prompt_type": "string",
                "response": "Machine learning is like teaching a computer by showing it examples. Just as you learn to recognize cats by seeing many cat pictures, a computer can learn patterns from data to make predictions or decisions.",
                "response_type": "string",
            },
            {
                "prompt": "What are the key differences between AI and human intelligence?",
                "prompt_type": "string",
                "response": "AI excels at processing large amounts of data and finding patterns, while human intelligence shines in creativity, emotional understanding, and adaptability. AI learns from specific data, whereas humans can learn from few examples and apply knowledge across different contexts. Humans have consciousness and self-awareness, which AI currently lacks.",
                "response_type": "string",
            },
        ]
        minimum_requirements = {"device_name": "Xenon Dev", "core": 3, "memory": "32 GB", "RAM": "32 GB"}

        # Set provider id and icon
        provider_id = None
        icon = required_data.get("icon")
        if required_data["provider_type"] == ModelProviderTypeEnum.HUGGING_FACE.value:
            # icon is not supported for hugging face models
            # Add provider id for hugging face models to retrieve icon for frontend
            icon = None
            db_provider = await ProviderDataManager(self.session).retrieve_by_fields(
                ProviderModel, {"type": "huggingface"}
            )
            provider_id = db_provider.id

        model_data = ModelCreate(
            name=required_data["name"],
            description=model_description,
            tags=extracted_tags,
            tasks=extracted_tasks,
            github_url=model_github_url,
            huggingface_url=model_huggingface_url,
            website_url=model_website_url,
            modality=model_info["modality"],
            source=ModelSourceEnum.LOCAL,
            provider_type=required_data["provider_type"],
            uri=required_data["uri"],
            created_by=db_workflow.created_by,
            author=extracted_author,
            provider_id=provider_id,
            local_path=local_path,
            icon=icon,
            model_size=model_size,
            strengths=strengths,
            limitations=limitations,
            languages=languages,
            use_cases=use_cases,
            minimum_requirements=minimum_requirements,
            examples=examples,
            base_model=base_model,
            base_model_relation=base_model_relation,
            model_type=model_type,
            family=family,
            model_weights_size=model_weights_size,
            kv_cache_size=kv_cache_size,
            architecture_text_config=text_config,
            architecture_vision_config=vision_config,
        )

        # Create model
        db_model = await ModelDataManager(self.session).insert_one(Model(**model_data.model_dump(exclude_none=True)))
        logger.debug(f"Model created with id {db_model.id}")

        # Create papers
        extracted_papers = model_info.get("papers", [])
        if extracted_papers:
            db_papers = await self._create_papers_from_model_info(extracted_papers, db_model.id)
            logger.debug(f"Papers created for model {db_model.id}")

        # Create model licenses
        extracted_license = model_info.get("license", {})
        if extracted_license:
            db_model_licenses = await self._create_model_licenses_from_model_info(extracted_license, db_model.id)
            logger.debug(f"Model licenses created for model {db_model.id}")

        # Update to workflow step
        workflow_update_data = {
            "model_id": str(db_model.id),
            "tags": extracted_tags,
            "description": model_description,
        }

        current_step_number = db_workflow.current_step + 1
        workflow_current_step = current_step_number

        # Update or create next workflow step
        db_workflow_step = await WorkflowStepService(self.session).create_or_update_next_workflow_step(
            workflow_id, current_step_number, workflow_update_data
        )
        logger.debug(f"Workflow step updated {db_workflow_step.id}")

        # Mark workflow as completed
        logger.debug(f"Updating workflow status: {workflow_id}")
        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"status": WorkflowStatusEnum.COMPLETED, "current_step": workflow_current_step}
        )

        # Send notification to workflow creator
        model_icon = await ModelServiceUtil(self.session).get_model_icon(db_model)
        notification_request = (
            NotificationBuilder()
            .set_content(
                title=db_model.name,
                message="Added to Repository",
                icon=model_icon,
                result=NotificationResult(target_id=db_model.id, target_type="model").model_dump(
                    exclude_none=True, exclude_unset=True
                ),
            )
            .set_payload(workflow_id=str(db_workflow.id), type=NotificationTypeEnum.MODEL_ONBOARDING_SUCCESS.value)
            .set_notification_request(subscriber_ids=[str(db_workflow.created_by)])
            .build()
        )
        await BudNotifyService().send_notification(notification_request)

    async def _verify_provider_type_uri_duplication(
        self,
        provider_type: ModelProviderTypeEnum,
        uri: str,
        db_workflow_steps: Optional[List[WorkflowStepModel]] = None,
    ) -> None:
        """Verify hugging face uri duplication."""
        db_step_uri = None
        db_step_provider_type = None

        if db_workflow_steps:
            for db_step in db_workflow_steps:
                if "uri" in db_step.data:
                    db_step_uri = db_step.data["uri"]
                if "provider_type" in db_step.data:
                    db_step_provider_type = db_step.data["provider_type"]

        # Check duplicate hugging face uri
        query_uri = None
        query_provider_type = None

        if uri and db_step_provider_type:
            # If user gives uri but provider type given in earlier step
            query_uri = uri
            query_provider_type = db_step_provider_type
        elif provider_type and db_step_uri:
            # If user gives provider type but uri given in earlier step
            query_uri = db_step_uri
            query_provider_type = provider_type.value
        elif uri and provider_type:
            # If user gives both uri and provider type
            query_uri = uri
            query_provider_type = provider_type.value
        elif db_step_uri and db_step_provider_type:
            # If user gives both uri and provider type in earlier step
            query_uri = db_step_uri
            query_provider_type = db_step_provider_type

        if query_uri and query_provider_type and query_provider_type == ModelProviderTypeEnum.HUGGING_FACE.value:
            # Check uri in huggingface uri format
            is_valid_huggingface_uri = validate_huggingface_repo_format(query_uri)
            if not is_valid_huggingface_uri:
                raise ClientException("Invalid huggingface uri format")

        if query_uri and query_provider_type and query_provider_type == ModelProviderTypeEnum.URL.value:
            # Check for valid url
            try:
                HttpUrl(query_uri)
            except ValueError:
                raise ClientException("Invalid url found")

        if query_uri and query_provider_type and query_provider_type == ModelProviderTypeEnum.DISK.value:
            # Check for valid local path
            if not os.path.exists(query_uri):
                raise ClientException("Given local path does not exist")

        # Check duplicate hugging face uri
        db_model = await ModelDataManager(self.session).retrieve_by_fields(
            Model,
            {"uri": query_uri, "provider_type": query_provider_type, "status": ModelStatusEnum.ACTIVE},
            missing_ok=True,
        )
        if db_model:
            raise ClientException(f"Duplicate {query_provider_type} uri found")

    async def _perform_model_extraction(
        self, current_step_number: int, data: Dict, current_user_id: UUID, db_workflow: WorkflowModel
    ) -> None:
        """Perform model extraction."""
        # Perform model extraction request
        model_extraction_response = await self._perform_model_extraction_request(db_workflow.id, data, current_user_id)

        # Add payload dict to response
        for step in model_extraction_response["steps"]:
            step["payload"] = {}

        model_extraction_events = {
            BudServeWorkflowStepEventName.MODEL_EXTRACTION_EVENTS.value: model_extraction_response
        }

        current_step_number = current_step_number + 1
        workflow_current_step = current_step_number

        # Update or create next workflow step
        db_workflow_step = await WorkflowStepService(self.session).create_or_update_next_workflow_step(
            db_workflow.id, current_step_number, model_extraction_events
        )
        logger.debug(f"Workflow step created with id {db_workflow_step.id}")

        # Update progress in workflow
        model_extraction_response["progress_type"] = BudServeWorkflowStepEventName.MODEL_EXTRACTION_EVENTS.value
        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"progress": model_extraction_response, "current_step": workflow_current_step}
        )

    async def _perform_model_extraction_request(self, workflow_id: UUID, data: Dict, current_user_id: UUID) -> None:
        """Perform model extraction request."""
        model_extraction_endpoint = (
            f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_model_app_id}/method/model-info/extract"
        )

        # TODO: get proprietary credential from db
        proprietary_credential_id = data.get("proprietary_credential_id")

        hf_token = None
        if proprietary_credential_id:
            db_proprietary_credential = await ProprietaryCredentialDataManager(self.session).retrieve_by_fields(
                ProprietaryCredentialModel, {"id": proprietary_credential_id}
            )
            hf_token = db_proprietary_credential.other_provider_creds.get("api_key")

            # TODO: remove this after implementing token decryption, decryption not required here
            # Send decrypted token to model extraction endpoint
            hf_token = await self._get_decrypted_token(proprietary_credential_id)

        model_extraction_request = {
            "model_name": data["name"],
            "model_uri": data["uri"],
            "provider_type": data["provider_type"],
            "hf_token": hf_token,
            "notification_metadata": {
                "name": BUD_INTERNAL_WORKFLOW,
                "subscriber_ids": str(current_user_id),
                "workflow_id": str(workflow_id),
            },
            "source_topic": f"{app_settings.source_topic}",
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(model_extraction_endpoint, json=model_extraction_request) as response:
                    response_data = await response.json()
                    if response.status >= 400:
                        raise ClientException("Unable to perform model extraction")

                    return response_data
        except ClientException as e:
            raise e
        except Exception as e:
            logger.error(f"Failed to perform model extraction request: {e}")
            raise ClientException("Unable to perform model extraction") from e

    @staticmethod
    async def _get_decrypted_token(credential_id: UUID) -> str:
        """Get decrypted token."""
        # TODO: remove this function after implementing dapr decryption
        url = f"{app_settings.budserve_host}/proprietary/credentials/{credential_id}/details"

        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                response_data = await response.json()
                try:
                    decrypted_token = response_data["result"]["other_provider_creds"]["api_key"]
                    return decrypted_token
                except (KeyError, TypeError):
                    raise ClientException("Unable to get decrypted token")

    async def _create_papers_from_model_info(
        self, extracted_papers: list[dict], model_id: UUID
    ) -> List[PaperPublished]:
        """Create papers from model info."""
        # Store papers here for bulk insert
        paper_models = []

        for extracted_paper in extracted_papers:
            paper_title = normalize_value(extracted_paper.get("title", None))
            paper_authors = normalize_value(extracted_paper.get("authors", None))
            paper_url = normalize_value(extracted_paper.get("url", None))

            # Only add paper if it has title, authors or url
            if any([paper_title, paper_authors, paper_url]):
                paper_data = PaperPublishedCreate(
                    title=paper_title,
                    authors=paper_authors,
                    url=paper_url,
                    model_id=model_id,
                )
                paper_models.append(PaperPublished(**paper_data.model_dump(exclude_none=True)))

        # Insert papers in db
        return await PaperPublishedDataManager(self.session).insert_all(paper_models)

    async def _create_model_licenses_from_model_info(
        self, extracted_license: dict, model_id: UUID
    ) -> List[ModelLicenses]:
        """Create model licenses from model info."""
        license_name = normalize_value(extracted_license.get("name"))
        license_url = normalize_value(extracted_license.get("url"))
        license_faqs = normalize_value(extracted_license.get("faqs"))
        license_data = ModelLicensesCreate(
            name=license_name,
            url=license_url,
            faqs=license_faqs,
            model_id=model_id,
        )
        return await ModelLicensesDataManager(self.session).insert_one(
            ModelLicenses(**license_data.model_dump(exclude_none=True))
        )

    @staticmethod
    async def get_base_model_relation(model_tree: dict) -> Optional[BaseModelRelationEnum]:
        """Get base model relation.

        Args:
            model_tree (dict): Model tree.

        Returns:
            Optional[BaseModelRelationEnum]: Base model relation.
        """
        if model_tree.get("is_finetune"):
            return BaseModelRelationEnum.FINETUNE
        elif model_tree.get("is_adapter"):
            return BaseModelRelationEnum.ADAPTER
        elif model_tree.get("is_quantization"):
            return BaseModelRelationEnum.QUANTIZED
        elif model_tree.get("is_merge"):
            return BaseModelRelationEnum.MERGE
        else:
            return None

    async def scan_local_model_workflow(self, current_user_id: UUID, request: LocalModelScanRequest) -> WorkflowModel:
        """Scan a local model."""
        # Get request data
        step_number = request.step_number
        workflow_id = request.workflow_id
        workflow_total_steps = request.workflow_total_steps
        model_id = request.model_id
        trigger_workflow = request.trigger_workflow

        current_step_number = step_number

        # Retrieve or create workflow
        workflow_create = WorkflowUtilCreate(
            workflow_type=WorkflowTypeEnum.MODEL_SECURITY_SCAN,
            title="Model Security Scan",
            total_steps=workflow_total_steps,
            icon=APP_ICONS["general"]["model_mono"],
            tag="Model Repository",
        )
        db_workflow = await WorkflowService(self.session).retrieve_or_create_workflow(
            workflow_id, workflow_create, current_user_id
        )

        # Validate model id
        if model_id:
            db_model = await ModelDataManager(self.session).retrieve_by_fields(
                Model, {"id": model_id, "status": ModelStatusEnum.ACTIVE}
            )
            if db_model.provider_type == ModelProviderTypeEnum.CLOUD_MODEL:
                raise ClientException("Security scan is only supported for local models")

            # Update icon on workflow
            if db_model.provider_type == ModelProviderTypeEnum.HUGGING_FACE:
                db_provider = await ProviderDataManager(self.session).retrieve_by_fields(
                    ProviderModel, {"id": db_model.provider_id}
                )
                model_icon = db_provider.icon
            else:
                model_icon = db_model.icon

            # Update title, icon on workflow
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"title": db_model.name, "icon": model_icon},
            )

        # Prepare workflow step data
        workflow_step_data = LocalModelScanWorkflowStepData(
            model_id=model_id,
        ).model_dump(exclude_none=True, exclude_unset=True, mode="json")

        # Get workflow steps
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": db_workflow.id}
        )

        # For avoiding another db call for record retrieval, storing db object while iterating over db_workflow_steps
        db_current_workflow_step = None

        if db_workflow_steps:
            for db_step in db_workflow_steps:
                # Get current workflow step
                if db_step.step_number == current_step_number:
                    db_current_workflow_step = db_step

        if db_current_workflow_step:
            logger.info(f"Workflow {db_workflow.id} step {current_step_number} already exists")

            # Update workflow step data in db
            db_workflow_step = await WorkflowStepDataManager(self.session).update_by_fields(
                db_current_workflow_step,
                {"data": workflow_step_data},
            )
            logger.info(f"Workflow {db_workflow.id} step {current_step_number} updated")
        else:
            logger.info(f"Creating workflow step {current_step_number} for workflow {db_workflow.id}")

            # Insert step details in db
            db_workflow_step = await WorkflowStepDataManager(self.session).insert_one(
                WorkflowStepModel(
                    workflow_id=db_workflow.id,
                    step_number=current_step_number,
                    data=workflow_step_data,
                )
            )

        # Update workflow current step as the highest step_number
        db_max_workflow_step_number = max(step.step_number for step in db_workflow_steps) if db_workflow_steps else 0
        workflow_current_step = max(current_step_number, db_max_workflow_step_number)
        logger.info(f"The current step of workflow {db_workflow.id} is {workflow_current_step}")

        # This will ensure workflow step number is updated to the latest step number
        db_workflow = await WorkflowDataManager(self.session).update_by_fields(
            db_workflow,
            {"current_step": workflow_current_step},
        )

        if trigger_workflow:
            # query workflow steps again to get latest data
            db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
                {"workflow_id": db_workflow.id}
            )

            # Define the keys required for model security scan
            keys_of_interest = [
                "model_id",
            ]

            # from workflow steps extract necessary information
            required_data = {}
            for db_workflow_step in db_workflow_steps:
                for key in keys_of_interest:
                    if key in db_workflow_step.data:
                        required_data[key] = db_workflow_step.data[key]

            # Check if all required keys are present
            required_keys = ["model_id"]
            missing_keys = [key for key in required_keys if key not in required_data]
            if missing_keys:
                raise ClientException(f"Missing required data for model security scan: {', '.join(missing_keys)}")

            try:
                # Perform model security scan
                await self._perform_model_security_scan(current_step_number, required_data, db_workflow)
            except ClientException as e:
                # NOTE: Update workflow status to failed only for model security scan workflow. if micro-service fails,
                # For remaining workflows, if microservice fails, the workflow steps won't be created from backend
                # According to ui model_id and trigger workflow required in request body
                # But Orchestration perspective, the workflow steps can also provided separately in request body
                # So, we need to update the workflow status to failed only for model security scan workflow
                db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                    db_workflow,
                    {"status": WorkflowStatusEnum.FAILED},
                )
                logger.debug("Workflow updated with latest step")
                raise e

        return db_workflow

    async def _perform_model_security_scan(
        self, current_step_number: int, data: Dict, db_workflow: WorkflowModel
    ) -> None:
        """Perform model security scan."""
        # Retrieve workflow step
        db_workflow_step = await WorkflowDataManager(self.session).retrieve_by_fields(
            WorkflowStepModel, {"workflow_id": db_workflow.id, "step_number": current_step_number}
        )

        # Perform model security scan request
        current_user_id = db_workflow_step.workflow.created_by
        model_security_scan_response = await self._perform_model_security_scan_request(
            db_workflow.id, data, current_user_id
        )

        # Add payload dict to response
        for step in model_security_scan_response["steps"]:
            step["payload"] = {}

        # Include model security scan response in current step data
        data[BudServeWorkflowStepEventName.MODEL_SECURITY_SCAN_EVENTS.value] = model_security_scan_response

        # Update workflow step with response
        await WorkflowStepDataManager(self.session).update_by_fields(db_workflow_step, {"data": data})

        # Update progress in workflow
        model_security_scan_response["progress_type"] = BudServeWorkflowStepEventName.MODEL_SECURITY_SCAN_EVENTS.value

        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"progress": model_security_scan_response}
        )

    async def _perform_model_security_scan_request(self, workflow_id: UUID, data: Dict, current_user_id: UUID) -> None:
        """Perform model security scan request."""
        model_security_scan_endpoint = (
            f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_model_app_id}/method/model-info/scan"
        )

        # Retrieve model local path
        model_id = data.get("model_id")
        db_model = await ModelDataManager(self.session).retrieve_by_fields(
            Model, {"id": model_id, "status": ModelStatusEnum.ACTIVE}
        )
        local_path = db_model.local_path

        model_security_scan_request = {
            "model_path": local_path,
            "notification_metadata": {
                "name": BUD_INTERNAL_WORKFLOW,
                "subscriber_ids": str(current_user_id),
                "workflow_id": str(workflow_id),
            },
            "source_topic": f"{app_settings.source_topic}",
        }
        logger.debug(f"Model security scan payload: {model_security_scan_request}")

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(model_security_scan_endpoint, json=model_security_scan_request) as response:
                    response_data = await response.json()
                    if response.status >= 400:
                        raise ClientException("unable to perform model security scan request")

                    return response_data
        except ClientException as e:
            raise e
        except Exception as e:
            logger.error(f"Failed to perform model security scan request: {e}")
            raise ClientException("unable to perform model security scan request") from e

    async def create_scan_result_from_notification_event(self, payload: NotificationPayload) -> None:
        """Create a local model security scan result from notification event."""
        logger.debug("Received event for creating local model security scan result")

        # Get workflow steps
        workflow_id = payload.workflow_id
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})
        logger.debug(f"Retrieved workflow: {db_workflow.id}")

        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        # Define the keys required for endpoint creation
        keys_of_interest = [
            "model_id",
        ]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]
        logger.debug("Collected required data from workflow steps")

        # Get model
        db_model = await ModelDataManager(self.session).retrieve_by_fields(
            Model, {"id": required_data["model_id"], "status": ModelStatusEnum.ACTIVE}
        )
        local_path = db_model.local_path
        logger.debug(f"Local path: {local_path}")

        # Get model scan result
        scan_result = payload.content.result["scan_result"]
        logger.debug(f"Scan result: {scan_result}")

        # Parse necessary data from scan result
        total_issues_by_severity = scan_result.get("total_issues_by_severity", {})
        low_severity_count = total_issues_by_severity.get("LOW", 0)
        medium_severity_count = total_issues_by_severity.get("MEDIUM", 0)
        high_severity_count = total_issues_by_severity.get("HIGH", 0)
        critical_severity_count = total_issues_by_severity.get("CRITICAL", 0)

        model_issues = scan_result.get("model_issues", [])
        grouped_issues = await self._group_model_issues(model_issues, local_path)

        # Determine overall scan status
        overall_scan_status = await self.determine_overall_scan_status(
            low_severity_count, medium_severity_count, high_severity_count, critical_severity_count
        )

        # Create model security scan result
        model_security_scan_result = ModelSecurityScanResultCreate(
            model_id=db_model.id,
            status=overall_scan_status,
            total_issues=scan_result.get("total_issues", 0),
            total_scanned_files=scan_result.get("total_scanned", 0),
            total_skipped_files=scan_result.get("total_skipped_files", 0),
            scanned_files=scan_result.get("scanned_files", []),
            low_severity_count=low_severity_count,
            medium_severity_count=medium_severity_count,
            high_severity_count=high_severity_count,
            critical_severity_count=critical_severity_count,
            model_issues=grouped_issues,
        )
        logger.debug("Parsed model security scan result")

        # Check if model security scan result already exists
        db_model_security_scan_result = await ModelSecurityScanResultDataManager(self.session).retrieve_by_fields(
            ModelSecurityScanResultModel, {"model_id": db_model.id}, missing_ok=True
        )

        if db_model_security_scan_result:
            logger.debug("Model security scan result already exists. Updating it.")
            db_model_security_scan_result = await ModelSecurityScanResultDataManager(self.session).update_by_fields(
                db_model_security_scan_result,
                model_security_scan_result.model_dump(),
            )
        else:
            logger.debug("Model security scan result does not exist. Creating it.")
            db_model_security_scan_result = await ModelSecurityScanResultDataManager(self.session).insert_one(
                ModelSecurityScanResultModel(**model_security_scan_result.model_dump())
            )

        # Update workflow current step
        current_step_number = db_workflow.current_step + 1
        workflow_current_step = current_step_number

        # Update workflow step with model security scan result id
        db_workflow_step = await WorkflowStepService(self.session).create_or_update_next_workflow_step(
            db_workflow.id, current_step_number, {"security_scan_result_id": str(db_model_security_scan_result.id)}
        )

        # Update workflow current step
        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow,
            {"current_step": workflow_current_step},
        )

        # Mark scan_verified according to overall scan status
        scan_verified = True if overall_scan_status == ModelSecurityScanStatusEnum.SAFE else False

        db_model = await ModelDataManager(self.session).update_by_fields(
            db_model,
            {"scan_verified": scan_verified},
        )

        # Get dummy leaderboard data
        leaderboard_data = await self.get_leaderboard()

        # Update workflow current step
        current_step_number = db_workflow.current_step + 1
        workflow_current_step = current_step_number

        # Create workflow step to store model scan result
        db_workflow_step = await WorkflowStepService(self.session).create_or_update_next_workflow_step(
            workflow_id, current_step_number, {"leaderboard": leaderboard_data}
        )

        # Update workflow current step and status
        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow,
            {"current_step": workflow_current_step, "status": WorkflowStatusEnum.COMPLETED},
        )

        # Send notification to workflow creator
        model_icon = await ModelServiceUtil(self.session).get_model_icon(db_model)
        notification_request = (
            NotificationBuilder()
            .set_content(
                title=db_model.name,
                message="Scan is Completed",
                icon=model_icon,
                tag=overall_scan_status.value,
                result=NotificationResult(target_id=db_model.id, target_type="model").model_dump(
                    exclude_none=True, exclude_unset=True
                ),
            )
            .set_payload(workflow_id=str(db_workflow.id), type=NotificationTypeEnum.MODEL_SCAN_SUCCESS.value)
            .set_notification_request(subscriber_ids=[str(db_workflow.created_by)])
            .build()
        )
        await BudNotifyService().send_notification(notification_request)

    async def _group_model_issues(self, model_issues: list, local_path: str) -> list[ModelIssue]:
        """Group model issues by severity."""
        grouped_issues = {}

        for model_issue in model_issues:
            severity = model_issue["severity"].lower()

            # NOTE: changed this formatting to budmodel
            # Clean up the source path by removing local_path prefix
            # source = model_issue["source"]
            # if source.startswith(local_path):
            #     source = source[len(local_path) :].lstrip("/")

            # Group issues by severity
            if severity not in grouped_issues:
                grouped_issues[severity] = []
            grouped_issues[severity].append(
                ModelIssue(
                    title=model_issue["title"],
                    severity=severity,
                    description=model_issue["description"],
                    source=model_issue["source"],
                ).model_dump()
            )

        return grouped_issues

    async def get_leaderboard(self) -> dict:
        """Get leaderboard for a model."""
        return {
            "headers": {
                "evaluation_type": "Evaluation type",
                "dataset": "Data Set",
                "model_1": "Selected Model",
                "model_2": "Model 2",
            },
            "rows": [
                {"evaluation_type": "IFEval", "dataset": "Reasoning", "model_1": 65.1, "model_2": 65.1},
                {"evaluation_type": "BBH", "dataset": "MMLU", "model_1": 46.9, "model_2": 46.9},
                {"evaluation_type": "Model", "dataset": "Factuality", "model_1": 78.4, "model_2": 78.4},
                {"evaluation_type": "Tags", "dataset": "Reasoning", "model_1": 60.8, "model_2": 60.8},
            ],
        }

    @staticmethod
    async def determine_overall_scan_status(
        low_count: int, medium_count: int, high_count: int, critical_count: int
    ) -> ModelSecurityScanStatusEnum:
        """Determine the overall security scan status based on issue counts.

        Args:
            low_count: Number of low severity issues
            medium_count: Number of medium severity issues
            high_count: Number of high severity issues
            critical_count: Number of critical severity issues

        Returns:
            ModelSecurityScanStatusEnum: The overall status based on the highest severity with issues
        """
        if critical_count > 0:
            return ModelSecurityScanStatusEnum.CRITICAL
        elif high_count > 0:
            return ModelSecurityScanStatusEnum.HIGH
        elif medium_count > 0:
            return ModelSecurityScanStatusEnum.MEDIUM
        elif low_count > 0:
            return ModelSecurityScanStatusEnum.LOW
        else:
            return ModelSecurityScanStatusEnum.SAFE  # Default to SAFE if no issues are found


class CloudModelService(SessionMixin):
    """Cloud model service."""

    async def get_all_cloud_models(
        self,
        offset: int = 0,
        limit: int = 10,
        filters: Dict = {},
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[CloudModel], int]:
        """Get all cloud models."""
        db_cloud_models, count = await CloudModelDataManager(self.session).get_all_cloud_models(
            offset, limit, filters, order_by, search
        )

        # convert db_cloud_models to cloud model list response
        db_cloud_models_response = []
        for db_cloud_model in db_cloud_models:
            model_response = ModelResponse.model_validate(db_cloud_model)
            db_cloud_models_response.append(ModelListResponse(model=model_response))

        return db_cloud_models_response, count

    async def get_all_recommended_tags(
        self,
        offset: int = 0,
        limit: int = 10,
    ) -> Tuple[List[CloudModel], int]:
        """Get all cloud models."""
        return await CloudModelDataManager(self.session).get_all_recommended_tags(offset, limit)


class ModelService(SessionMixin):
    """Model service."""

    async def retrieve_model(self, model_id: UUID) -> ModelDetailSuccessResponse:
        """Retrieve model details by model ID."""
        db_model = await ModelDataManager(self.session).retrieve_by_fields(
            Model, {"id": model_id, "status": ModelStatusEnum.ACTIVE}
        )

        # Get base model relation count
        model_tree_count = await ModelDataManager(self.session).get_model_tree_count(db_model.uri)
        base_model_relation_count = {row.base_model_relation.value: row.count for row in model_tree_count}
        model_tree = ModelTree(
            adapters_count=base_model_relation_count.get(BaseModelRelationEnum.ADAPTER.value, 0),
            finetunes_count=base_model_relation_count.get(BaseModelRelationEnum.FINETUNE.value, 0),
            merges_count=base_model_relation_count.get(BaseModelRelationEnum.MERGE.value, 0),
            quantizations_count=base_model_relation_count.get(BaseModelRelationEnum.QUANTIZED.value, 0),
        )

        db_endpoint_count = await ModelDataManager(self.session).get_count_by_fields(
            EndpointModel, fields={"model_id": model_id}, exclude_fields={"status": EndpointStatusEnum.DELETED}
        )

        return ModelDetailSuccessResponse(
            model=db_model,
            model_tree=model_tree,
            scan_result=db_model.model_security_scan_result,
            endpoints_count=db_endpoint_count,
            message="model retrieved successfully",
            code=status.HTTP_200_OK,
            object="model.get",
        ).to_http_response()

    async def list_model_tags(self, name: str, offset: int = 0, limit: int = 10) -> tuple[list[Tag], int]:
        """Search model tags by name with pagination."""
        tags_result, count = await ModelDataManager(self.session).list_model_tags(name, offset, limit)
        tags = [Tag(name=row.name, color=row.color) for row in tags_result]

        return tags, count

    async def list_model_tasks(self, name: str, offset: int = 0, limit: int = 10) -> tuple[list[Task], int]:
        """Search model tasks by name with pagination."""
        tasks_result, count = await ModelDataManager(self.session).list_model_tasks(name, offset, limit)
        tasks = [Task(name=row.name, color=row.color) for row in tasks_result]

        return tasks, count

    async def get_all_active_models(
        self,
        offset: int = 0,
        limit: int = 10,
        filters: Dict = {},
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[Model], int]:
        """Get all active models."""
        filters_dict = filters

        results, count = await ModelDataManager(self.session).get_all_models(
            offset, limit, filters_dict, order_by, search
        )

        # Parse the results to model list response
        db_models_response = []
        for result in results:
            model_response = ModelResponse.model_validate(result[0])
            db_models_response.append(ModelListResponse(model=model_response, endpoints_count=result[1]))

        return db_models_response, count

    async def list_all_model_authors(
        self,
        offset: int = 0,
        limit: int = 10,
        filters: Dict = {},
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[str], int]:
        """Search author by name with pagination support."""
        filters["status"] = ModelStatusEnum.ACTIVE
        db_models, count = await ModelDataManager(self.session).list_all_model_authors(
            offset, limit, filters, order_by, search
        )
        db_authors = [model.author for model in db_models]

        return db_authors, count

    async def edit_model(self, model_id: UUID, data: Dict[str, Any], current_user_id: UUID) -> None:
        """Edit cloud model by validating and updating specific fields, and saving an uploaded file if provided."""
        logger.debug(f"edit recieved data: {data}")
        # Retrieve existing model
        db_model = await ModelDataManager(self.session).retrieve_by_fields(
            model=Model, fields={"id": model_id, "status": ModelStatusEnum.ACTIVE}
        )

        if data.get("name"):
            duplicate_model = await ModelDataManager(self.session).retrieve_by_fields(
                model=Model,
                fields={"name": data["name"], "status": ModelStatusEnum.ACTIVE},
                exclude_fields={"id": model_id},
                missing_ok=True,
                case_sensitive=False,
            )
            if duplicate_model:
                raise ClientException("Model name already exists")

        if data.get("icon") and db_model.provider_type in [
            ModelProviderTypeEnum.CLOUD_MODEL,
            ModelProviderTypeEnum.HUGGING_FACE,
        ]:
            data.pop("icon")

        # Handle file upload if provided
        # TODO: consider dapr local storage
        if data.get("license_file"):
            # If a file is provided, save it locally and update the DB with the local path
            file = data.pop("license_file")
            file_path = await self._save_uploaded_file(file)
            await self._create_or_update_license_entry(model_id, file.filename, file_path, None, current_user_id)

        elif data.get("license_url"):
            # If a license URL is provided, store the URL in the DB instead of the file path
            license_url = data.pop("license_url")
            filename = license_url.split("/")[-1]  # Extract filename from the URL
            await self._create_or_update_license_entry(
                model_id, filename if filename else "sample license", None, str(license_url), current_user_id
            )  # TODO: modify filename arg when license service implemented

        # Add papers if provided
        if isinstance(data.get("paper_urls"), list):
            paper_urls = data.pop("paper_urls")
            await self._update_papers(model_id, paper_urls)

        # Update model with validated data
        await ModelDataManager(self.session).update_by_fields(db_model, data)

    async def _save_uploaded_file(self, file: UploadFile) -> str:
        """Save uploaded file and return file path."""
        # create the license directory if not present already
        os.makedirs(os.path.join(app_settings.static_dir, LICENSE_DIR), exist_ok=True)
        file_path = os.path.join(app_settings.static_dir, LICENSE_DIR, file.filename)
        with Path(file_path).open("wb") as f:
            f.write(await file.read())
        return os.path.join(LICENSE_DIR, file.filename)

    async def _create_or_update_license_entry(
        self, model_id: UUID, filename: str, file_path: str, license_url: str, current_user_id: UUID
    ) -> None:
        """Create or update a license entry in the database."""
        # Check if a license entry with the given model_id exists
        existing_license = await ModelLicensesDataManager(self.session).retrieve_by_fields(
            ModelLicenses, fields={"model_id": model_id}, missing_ok=True
        )
        if existing_license:
            logger.debug(f"existing license: {existing_license}")
            existing_license_path = (
                os.path.join(app_settings.static_dir, existing_license.path) if existing_license.path else ""
            )
            if existing_license_path and os.path.exists(existing_license_path):
                try:
                    os.remove(existing_license_path)
                except PermissionError:
                    raise ClientException(
                        status_code=status.HTTP_403_FORBIDDEN,
                        message=f"Permission denied while accessing the file: {existing_license.name}",
                    )

            update_license_data = {
                "name": filename,
                "path": file_path if file_path else None,
                "url": license_url if license_url else None,
            }
            license_source = file_path if file_path else license_url
            await ModelLicensesDataManager(self.session).update_by_fields(existing_license, update_license_data)
            await self.fetch_license_faqs(model_id, existing_license.id, current_user_id, license_source)
        else:
            # Create a new license entry
            license_entry = ModelLicensesModel(
                id=uuid4(),
                name=filename,
                path=file_path if file_path else None,
                url=license_url if license_url else None,
                model_id=model_id,
            )
            license_source = file_path if file_path else license_url
            await ModelLicensesDataManager(self.session).insert_one(
                ModelLicenses(**license_entry.model_dump(exclude_unset=True))
            )
            await self.fetch_license_faqs(model_id, license_entry.id, current_user_id, license_source)

    async def _update_papers(self, model_id: UUID, paper_urls: list[str]) -> None:
        """Update paper entries for the given model by adding new URLs and removing old ones."""
        # Fetch existing paper URLs for the model
        existing_papers = await ModelDataManager(self.session).get_all_by_fields(
            model=PaperPublished, fields={"model_id": model_id}
        )
        existing_urls = {paper.url for paper in existing_papers}

        # Determine URLs to add and remove
        input_urls = set(paper_urls)
        urls_to_add = input_urls - existing_urls
        urls_to_remove = existing_urls - input_urls
        logger.debug(
            f"paper info: {input_urls}, existing urls: {existing_urls}, urls_to_add: {urls_to_add}, urls_to_remove: {urls_to_remove}"
        )

        # Add new paper URLs
        if urls_to_add:
            urls_to_add = [
                PaperPublished(id=uuid4(), title="Untitled Research Paper", url=str(paper_url), model_id=model_id)
                for paper_url in urls_to_add
            ]
            await PaperPublishedDataManager(self.session).insert_all(urls_to_add)

        # Remove old paper URLs
        if urls_to_remove:
            # Delete all matching entries for given URLs and model_id in one query
            await PaperPublishedDataManager(self.session).delete_paper_by_urls(
                model_id=model_id, paper_urls={"url": urls_to_remove}
            )

    async def cancel_model_deployment_workflow(self, workflow_id: UUID) -> None:
        """Cancel model deployment workflow."""
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": db_workflow.id}
        )

        # Define the keys required for endpoint creation
        keys_of_interest = [
            BudServeWorkflowStepEventName.BUDSERVE_CLUSTER_EVENTS.value,
        ]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]
        logger.debug("Collected required data from workflow steps")

        if required_data.get(BudServeWorkflowStepEventName.BUDSERVE_CLUSTER_EVENTS.value) is None:
            raise ClientException("Model deployment process has not been initiated")

        budserve_cluster_response = required_data.get(BudServeWorkflowStepEventName.BUDSERVE_CLUSTER_EVENTS.value)
        dapr_workflow_id = budserve_cluster_response.get("workflow_id")

        try:
            await self._perform_cancel_model_deployment_request(dapr_workflow_id)
        except ClientException as e:
            raise e

    async def _perform_cancel_model_deployment_request(self, workflow_id: str) -> Dict:
        """Perform cancel model deployment request to bud_cluster app.

        Args:
            workflow_id: The ID of the workflow to cancel.
        """
        cancel_model_deployment_endpoint = f"{app_settings.dapr_base_url}v1.0/invoke/{app_settings.bud_cluster_app_id}/method/deployment/cancel/{workflow_id}"

        logger.debug(f"Performing cancel model deployment request to budcluster {cancel_model_deployment_endpoint}")
        try:
            async with aiohttp.ClientSession() as session, session.post(cancel_model_deployment_endpoint) as response:
                response_data = await response.json()
                if response.status != 200:
                    logger.error(f"Failed to cancel model deployment: {response.status} {response_data}")
                    raise ClientException(
                        "Failed to cancel model deployment", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
                    )

                logger.debug("Successfully cancelled model deployment")
                return response_data
        except Exception as e:
            logger.exception(f"Failed to send cancel model deployment request: {e}")
            raise ClientException(
                "Failed to cancel model deployment", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
            ) from e

    async def delete_active_model(self, model_id: UUID) -> Model:
        db_model = await ModelDataManager(self.session).retrieve_by_fields(
            Model, fields={"id": model_id, "status": ModelStatusEnum.ACTIVE}
        )

        # Check for active endpoint
        db_endpoints = await EndpointDataManager(self.session).retrieve_by_fields(
            EndpointModel,
            fields={"model_id": model_id},
            exclude_fields={"status": EndpointStatusEnum.DELETED},
            missing_ok=True,
        )

        # Raise error if model has active endpoint
        if db_endpoints:
            raise ClientException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Cannot delete model because it has active endpoint.",
            )

        if db_model.provider_type == ModelProviderTypeEnum.CLOUD_MODEL:
            db_cloud_model = await CloudModelDataManager(self.session).retrieve_by_fields(
                CloudModel,
                fields={
                    "status": CloudModelStatusEnum.ACTIVE,
                    "source": db_model.source,
                    "uri": db_model.uri,
                    "provider_id": db_model.provider_id,
                    "is_present_in_model": True,
                },
            )
            db_cloud_model = await CloudModelDataManager(self.session).update_by_fields(
                db_cloud_model, fields={"is_present_in_model": False}
            )

        else:
            await self._perform_model_deletion_request(db_model.local_path)
            logger.debug(f"Model deletion successful for {db_model.local_path}")

        db_model = await ModelDataManager(self.session).update_by_fields(db_model, {"status": ModelStatusEnum.DELETED})

        return db_model

    async def _perform_model_deletion_request(self, local_path: str) -> None:
        """Perform model deletion request."""
        model_deletion_endpoint = (
            f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_model_app_id}/method/model-info/local-models"
        )

        params = {"path": local_path}

        try:
            async with aiohttp.ClientSession() as session:
                async with session.delete(model_deletion_endpoint, params=params) as response:
                    if response.status >= 400:
                        raise ClientException("Unable to perform model deletion")

        except ClientException as e:
            raise e
        except Exception as e:
            logger.error(f"Failed to perform model deletion request: {e}")
            raise ClientException("Unable to perform local model deletion") from e

    async def fetch_license_faqs(
        self, model_id: UUID, license_id: UUID, current_user_id: UUID, license_source: str
    ) -> WorkflowModel:
        """Fetch license faqs of a license by path or url.

        Args:
            model_id: The ID of the model corresponding to license source.
            license_source: file path or web url of license file.
        """
        db_model = await ModelDataManager(self.session).retrieve_by_fields(
            Model, fields={"id": model_id}, exclude_fields={"status": ModelStatusEnum.DELETED}
        )

        current_step_number = 1

        # Retrieve or create workflow
        workflow_create = WorkflowUtilCreate(
            workflow_type=WorkflowTypeEnum.LICENSE_FAQ_FETCH,
            title="Model License FAQS",
            total_steps=current_step_number,
            icon=db_model.icon,
            tag="Model License FAQS",
            visibility=VisibilityEnum.INTERNAL,
        )
        db_workflow = await WorkflowService(self.session).retrieve_or_create_workflow(
            workflow_id=None, workflow_data=workflow_create, current_user_id=current_user_id
        )
        logger.debug(f"model license faq {db_workflow.id} created")

        # Perform license faq fetch request to bud_model app
        try:
            bud_model_response = await self._perform_license_faq_fetch_request(
                license_source, current_user_id, db_workflow.id
            )
        except ClientException as e:
            await WorkflowDataManager(self.session).update_by_fields(
                db_workflow, {"status": WorkflowStatusEnum.FAILED}
            )
            raise e

        # Add payload dict to response
        for step in bud_model_response["steps"]:
            step["payload"] = {}

        license_faq_events = {
            BudServeWorkflowStepEventName.LICENSE_FAQ_EVENTS.value: bud_model_response,
            "license_id": str(license_id),
            "model_id": str(model_id),
        }

        # Insert step details in db
        db_workflow_step = await WorkflowStepDataManager(self.session).insert_one(
            WorkflowStepModel(
                workflow_id=db_workflow.id,
                step_number=current_step_number,
                data=license_faq_events,
            )
        )
        logger.debug(f"Created workflow step {current_step_number} for workflow {db_workflow.id}")

        # Update progress in workflow
        bud_model_response["progress_type"] = BudServeWorkflowStepEventName.LICENSE_FAQ_EVENTS.value
        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"progress": bud_model_response, "current_step": current_step_number}
        )

        return db_workflow

    async def _perform_license_faq_fetch_request(
        self, license_source: str, current_user_id: UUID, workflow_id: UUID
    ) -> Dict:
        """Perform license faqs fetch request to bud_model app.

        Args:
            license_source: The source of license, can be file path or url.
        """
        license_faq_fetch_endpoint = (
            f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_model_app_id}/method/model-info/license-faq"
        )

        payload = {
            "license_source": str(license_source),
            "notification_metadata": {
                "name": BUD_INTERNAL_WORKFLOW,
                "subscriber_ids": str(current_user_id),
                "workflow_id": str(workflow_id),
            },
            "source_topic": f"{app_settings.source_topic}",
        }

        logger.debug(f"Performing license faqs fetch request to budmodel {payload}")
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(license_faq_fetch_endpoint, json=payload) as response:
                    response_data = await response.json()
                    if response.status != 200:
                        logger.error(f"Failed to fetch license faqs: {response.status} {response_data}")
                        raise ClientException(
                            "Failed to fetch license faqs", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
                        )

                    logger.debug(f"Successfully fetched license faqs from budmodel{response_data}")
                    return response_data
        except Exception as e:
            logger.exception(f"Failed to send license faqs fetch request: {e}")
            raise ClientException(
                "Failed to fetch license faqs", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
            ) from e

    async def update_license_faqs_from_notification_event(self, payload: NotificationPayload) -> None:
        """Update license faqs from notification event."""
        logger.debug("Received event for fetching license faqs")

        # Get workflow and steps
        workflow_id = payload.workflow_id
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        # Define the keys required for updating faqs
        keys_of_interest = ["model_id", "license_id"]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]

        logger.debug("Collected required data from workflow steps")

        # Retrieve cluster from db
        db_license = await ModelLicensesDataManager(self.session).retrieve_by_fields(
            ModelLicenses,
            fields={"id": required_data["license_id"], "model_id": required_data["model_id"]},
            missing_ok=True,
        )
        logger.debug(f"license retrieved successfully: {db_license.id}")

        # update faqs
        faqs = payload.content.result["faqs"]
        db_license = await ModelLicensesDataManager(self.session).update_by_fields(db_license, {"faqs": faqs})
        logger.debug(f"updated FAQs for license {db_license.id}")

        # Mark workflow as completed
        await WorkflowDataManager(self.session).update_by_fields(db_workflow, {"status": WorkflowStatusEnum.COMPLETED})
        logger.debug(f"Workflow {db_workflow.id} marked as completed")


class ModelServiceUtil(SessionMixin):
    """Model util service."""

    async def get_model_icon(self, db_model: Model) -> Optional[str]:
        """Get model icon.

        Args:
            db_model: The model to get the icon for.

        Returns:
            The model icon.
        """
        if db_model.provider_type in [ModelProviderTypeEnum.CLOUD_MODEL, ModelProviderTypeEnum.HUGGING_FACE]:
            return db_model.provider.icon
        else:
            return db_model.icon

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/model_ops/models.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The model ops package, containing essential business logic, services, and routing configurations for the model ops."""

from datetime import datetime
from typing import List, Optional
from uuid import UUID, uuid4

from sqlalchemy import BigInteger, Boolean, DateTime, Enum, ForeignKey, Integer, String, Uuid
from sqlalchemy.dialects.postgresql import ARRAY as PG_ARRAY
from sqlalchemy.dialects.postgresql import ENUM as PG_ENUM
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import Mapped, mapped_column, relationship
from sqlalchemy.sql import false as sa_false

from budapp.commons.constants import (
    BaseModelRelationEnum,
    CloudModelStatusEnum,
    CredentialTypeEnum,
    ModalityEnum,
    ModelProviderTypeEnum,
    ModelSecurityScanStatusEnum,
    ModelStatusEnum,
)
from budapp.commons.database import Base


class Model(Base):
    """Model for a AI model."""

    __tablename__ = "model"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    name: Mapped[str] = mapped_column(String, nullable=False)
    description: Mapped[str] = mapped_column(String, nullable=True)
    tags: Mapped[list[dict]] = mapped_column(JSONB, nullable=True)
    tasks: Mapped[list[dict]] = mapped_column(JSONB, nullable=True)
    author: Mapped[str] = mapped_column(String, nullable=True)
    model_size: Mapped[Optional[int]] = mapped_column(BigInteger, nullable=True)
    icon: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    github_url: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    huggingface_url: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    bud_verified: Mapped[bool] = mapped_column(Boolean, default=False, server_default=sa_false())
    scan_verified: Mapped[bool] = mapped_column(Boolean, default=False, nullable=True)
    eval_verified: Mapped[bool] = mapped_column(Boolean, default=False, server_default=sa_false())
    strengths: Mapped[list[str]] = mapped_column(PG_ARRAY(String), nullable=True)
    limitations: Mapped[list[str]] = mapped_column(PG_ARRAY(String), nullable=True)
    languages: Mapped[list[str]] = mapped_column(PG_ARRAY(String), nullable=True)
    use_cases: Mapped[list[str]] = mapped_column(PG_ARRAY(String), nullable=True)
    minimum_requirements: Mapped[dict] = mapped_column(JSONB, nullable=True)
    examples: Mapped[list[dict]] = mapped_column(JSONB, nullable=True)
    base_model: Mapped[list[str]] = mapped_column(PG_ARRAY(String), nullable=True)
    base_model_relation: Mapped[str] = mapped_column(
        Enum(
            BaseModelRelationEnum,
            name="base_model_relation_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=True,
    )
    model_type: Mapped[str] = mapped_column(String, nullable=True)
    family: Mapped[str] = mapped_column(String, nullable=True)
    model_weights_size: Mapped[Optional[int]] = mapped_column(BigInteger, nullable=True)
    kv_cache_size: Mapped[Optional[int]] = mapped_column(BigInteger, nullable=True)
    architecture_text_config: Mapped[dict] = mapped_column(JSONB, nullable=True)
    architecture_vision_config: Mapped[dict] = mapped_column(JSONB, nullable=True)
    website_url: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    status: Mapped[str] = mapped_column(
        Enum(
            ModelStatusEnum,
            name="model_status_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=False,
        default=ModelStatusEnum.ACTIVE,
    )
    modality: Mapped[str] = mapped_column(
        Enum(
            ModalityEnum,
            name="modality_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=False,
    )
    source: Mapped[str] = mapped_column(String, nullable=False)
    provider_type: Mapped[str] = mapped_column(
        Enum(
            ModelProviderTypeEnum,
            name="model_provider_type_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=False,
    )
    uri: Mapped[str] = mapped_column(String, nullable=False)
    local_path: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    provider_id: Mapped[Optional[UUID]] = mapped_column(ForeignKey("provider.id"), nullable=True)
    created_by: Mapped[UUID] = mapped_column(ForeignKey("user.id"), nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    endpoints: Mapped[list["Endpoint"]] = relationship(back_populates="model")
    # benchmarks: Mapped[list["Benchmark"]] = relationship(back_populates="model")
    created_user: Mapped["User"] = relationship(back_populates="created_models", foreign_keys=[created_by])
    paper_published: Mapped[List["PaperPublished"]] = relationship("PaperPublished", back_populates="model")
    model_licenses: Mapped["ModelLicenses"] = relationship("ModelLicenses", back_populates="model")
    provider: Mapped[Optional["Provider"]] = relationship("Provider", back_populates="models")
    model_security_scan_result: Mapped["ModelSecurityScanResult"] = relationship(
        "ModelSecurityScanResult", back_populates="model"
    )


class PaperPublished(Base):
    """Model for Paper Published."""

    __tablename__ = "paper_published"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    title: Mapped[str] = mapped_column(String, nullable=True)
    authors: Mapped[list[str]] = mapped_column(PG_ARRAY(String), nullable=True)
    url: Mapped[str] = mapped_column(String, nullable=True)
    model_id: Mapped[UUID] = mapped_column(ForeignKey("model.id"), nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    model: Mapped["Model"] = relationship("Model", back_populates="paper_published")


class ModelLicenses(Base):
    """Model for a AI model licenses."""

    __tablename__ = "model_licenses"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    name: Mapped[str] = mapped_column(String, nullable=True)
    url: Mapped[str] = mapped_column(String, nullable=True)
    path: Mapped[str] = mapped_column(String, nullable=True)
    faqs: Mapped[list[dict]] = mapped_column(JSONB, nullable=True)
    model_id: Mapped[UUID] = mapped_column(ForeignKey("model.id"), nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    model: Mapped["Model"] = relationship("Model", back_populates="model_licenses")


class Provider(Base):
    """Model for a AI model provider."""

    __tablename__ = "provider"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    name: Mapped[str] = mapped_column(String, nullable=False)
    type: Mapped[str] = mapped_column(
        Enum(
            CredentialTypeEnum,
            name="credential_type_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=False,
    )
    description: Mapped[str] = mapped_column(String, nullable=True)
    icon: Mapped[Optional[str]] = mapped_column(String, nullable=False)

    models: Mapped[Optional[list["Model"]]] = relationship("Model", back_populates="provider")
    cloud_models: Mapped[list["CloudModel"]] = relationship("CloudModel", back_populates="provider")


class CloudModel(Base):
    """Model for a AI cloud model."""

    __tablename__ = "cloud_model"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    name: Mapped[str] = mapped_column(String, nullable=False)
    description: Mapped[str] = mapped_column(String, nullable=True)
    tags: Mapped[list[dict]] = mapped_column(JSONB, nullable=True)
    tasks: Mapped[list[dict]] = mapped_column(JSONB, nullable=True)
    author: Mapped[str] = mapped_column(String, nullable=True)
    model_size: Mapped[Optional[int]] = mapped_column(BigInteger, nullable=True)
    github_url: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    huggingface_url: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    website_url: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    status: Mapped[str] = mapped_column(
        Enum(
            CloudModelStatusEnum,
            name="cloud_model_status_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=False,
        default=CloudModelStatusEnum.ACTIVE,
    )
    modality: Mapped[str] = mapped_column(
        PG_ENUM(
            ModalityEnum,
            name="modality_enum",
            values_callable=lambda x: [e.value for e in x],
            create_type=False,
        ),
        nullable=False,
    )
    source: Mapped[str] = mapped_column(String, nullable=False)
    provider_type: Mapped[str] = mapped_column(
        PG_ENUM(
            ModelProviderTypeEnum,
            name="model_provider_type_enum",
            values_callable=lambda x: [e.value for e in x],
            create_type=False,
        ),
        nullable=False,
    )
    uri: Mapped[str] = mapped_column(String, nullable=False)
    provider_id: Mapped[UUID] = mapped_column(ForeignKey("provider.id"), nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    is_present_in_model: Mapped[bool] = mapped_column(Boolean, default=False, nullable=False)

    provider: Mapped[Optional["Provider"]] = relationship("Provider", back_populates="cloud_models")


class ModelSecurityScanResult(Base):
    """Model for a AI model security scan result."""

    __tablename__ = "model_security_scan_result"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    model_id: Mapped[UUID] = mapped_column(ForeignKey("model.id"), nullable=False)
    status: Mapped[str] = mapped_column(
        PG_ENUM(
            ModelSecurityScanStatusEnum,
            name="model_security_scan_status_enum",
            values_callable=lambda x: [e.value for e in x],
            create_type=False,
        ),
        nullable=False,
    )
    total_issues: Mapped[int] = mapped_column(Integer, nullable=False)
    total_scanned_files: Mapped[int] = mapped_column(Integer, nullable=False)
    total_skipped_files: Mapped[int] = mapped_column(Integer, nullable=False)
    scanned_files: Mapped[list[str]] = mapped_column(PG_ARRAY(String), nullable=False)
    low_severity_count: Mapped[int] = mapped_column(Integer, nullable=False)
    medium_severity_count: Mapped[int] = mapped_column(Integer, nullable=False)
    high_severity_count: Mapped[int] = mapped_column(Integer, nullable=False)
    critical_severity_count: Mapped[int] = mapped_column(Integer, nullable=False)
    model_issues: Mapped[dict] = mapped_column(JSONB, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    model: Mapped["Model"] = relationship("Model", back_populates="model_security_scan_result")

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/model_ops/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, containing essential business logic, services, and routing configurations for the model ops."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/model_ops/schemas.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Contains core Pydantic schemas used for data validation and serialization within the model ops services."""

import re
from datetime import datetime
from typing import List, Literal, Optional, Tuple

from fastapi import UploadFile
from pydantic import (
    UUID4,
    BaseModel,
    ConfigDict,
    Field,
    HttpUrl,
    field_serializer,
    field_validator,
    model_validator,
)

from budapp.commons.constants import (
    BaseModelRelationEnum,
    CredentialTypeEnum,
    ModalityEnum,
    ModelProviderTypeEnum,
    ModelSecurityScanStatusEnum,
    ModelSourceEnum,
    WorkflowStatusEnum,
)
from budapp.commons.schemas import PaginatedSuccessResponse, SuccessResponse, Tag, Task
from budapp.user_ops.schemas import UserInfo

from ..commons.helpers import validate_icon


class ProviderFilter(BaseModel):
    """Provider filter schema."""

    name: str | None = None


class Provider(BaseModel):
    """Provider schema."""

    model_config = ConfigDict(from_attributes=True)

    id: UUID4
    name: str
    description: str
    type: CredentialTypeEnum
    icon: str


class ProviderResponse(PaginatedSuccessResponse):
    """Provider response schema."""

    model_config = ConfigDict(extra="ignore")

    providers: list[Provider] = []


class CloudModel(BaseModel):
    """Cloud model schema."""

    model_config = ConfigDict(from_attributes=True, protected_namespaces=())

    id: UUID4
    name: str
    description: str | None = None
    modality: ModalityEnum
    source: CredentialTypeEnum
    provider_type: ModelProviderTypeEnum
    uri: str
    model_size: int | None = None
    tags: list[Tag] | None = None
    tasks: list[Tag] | None = None
    is_present_in_model: bool = False


class PaperPublishedModel(BaseModel):
    """Paper Published Model Schema"""

    id: UUID4
    title: str | None = None
    authors: list[str] | None = None
    url: str | None = None
    model_id: UUID4

    class Config:
        orm_mode = True
        from_attributes = True


class PaperPublishedModelEditRequest(BaseModel):
    """Paper Published Edit Model Schema"""

    id: UUID4 | None = None
    title: str | None = None
    url: str


class ModelLicensesModel(BaseModel):
    """Model Licenses Model Schema"""

    id: UUID4
    name: str | None = None
    url: str | None = None
    path: str | None = None
    faqs: list[dict] | None = None
    model_id: UUID4

    class Config:
        orm_mode = True
        from_attributes = True


# Model related schemas


class ModelBase(BaseModel):
    """Base model schema."""

    model_config = ConfigDict(from_attributes=True, protected_namespaces=())

    name: str
    description: Optional[str] = None
    tags: Optional[List[Tag]] = None
    tasks: Optional[List[Tag]] = None
    github_url: Optional[str] = None
    huggingface_url: Optional[str] = None
    website_url: Optional[str] = None


class Model(ModelBase):
    """Model schema."""

    id: UUID4
    icon: str | None = None
    modality: ModalityEnum
    source: ModelSourceEnum
    provider_type: ModelProviderTypeEnum
    uri: str
    model_size: Optional[int] = None
    created_by: Optional[UUID4] = None
    author: Optional[str] = None
    created_at: datetime
    modified_at: datetime
    provider: Provider | None = None


class ModelArchitectureLLMConfig(BaseModel):
    """Model architecture schema."""

    num_layers: int | None = None
    hidden_size: int | None = None
    intermediate_size: int | None = None
    context_length: int | None = None
    vocab_size: int | None = None
    torch_dtype: str | None = None
    num_attention_heads: int | None = None
    num_key_value_heads: int | None = None
    rope_scaling: dict | None = None


class ModelArchitectureVisionConfig(BaseModel):
    """Model architecture schema."""

    num_layers: int | None = None
    hidden_size: int | None = None
    intermediate_size: int | None = None
    torch_dtype: str | None = None


class ModelCreate(ModelBase):
    """Schema for creating a new AI Model."""

    modality: ModalityEnum
    source: str
    provider_type: ModelProviderTypeEnum
    uri: str
    model_size: Optional[int] = None
    created_by: UUID4
    author: Optional[str] = None
    provider_id: UUID4 | None = None
    local_path: str | None = None
    strengths: list[str] | None = None
    limitations: list[str] | None = None
    languages: list[str] | None = None
    use_cases: list[str] | None = None
    minimum_requirements: dict | None = None
    examples: list[dict] | None = None
    base_model: list[str] | None = None
    base_model_relation: BaseModelRelationEnum | None = None
    model_type: str | None = None
    family: str | None = None
    model_weights_size: int | None = None
    kv_cache_size: int | None = None
    architecture_text_config: ModelArchitectureLLMConfig | None = None
    architecture_vision_config: ModelArchitectureVisionConfig | None = None
    scan_verified: bool | None = None


class ModelDetailResponse(BaseModel):
    """Response schema for model details."""

    model_config = ConfigDict(from_attributes=True, protected_namespaces=())

    id: UUID4
    name: str
    description: str | None = None
    tags: list[Tag] | None = None
    tasks: list[Task] | None = None
    author: str | None = None
    model_size: int | None = None
    icon: str | None = None
    github_url: str | None = None
    huggingface_url: str | None = None
    website_url: str | None = None
    bud_verified: bool = False
    scan_verified: bool | None = None
    eval_verified: bool = False
    strengths: list[str] | None = None
    limitations: list[str] | None = None
    languages: list[str] | None = None
    use_cases: list[str] | None = None
    minimum_requirements: dict | None = None
    examples: list[dict] | None = None
    base_model: list[str] | None = None
    model_type: str | None = None
    family: str | None = None
    model_weights_size: int | None = None
    kv_cache_size: int | None = None
    architecture_text_config: ModelArchitectureLLMConfig | None = None
    architecture_vision_config: ModelArchitectureVisionConfig | None = None
    modality: ModalityEnum
    source: str
    provider_type: ModelProviderTypeEnum
    uri: str
    paper_published: list[PaperPublishedModel] | None = None
    model_licenses: ModelLicensesModel | None = None
    provider: Provider | None = None
    created_at: datetime


class ModelTree(BaseModel):
    """Model tree schema."""

    adapters_count: int = 0
    finetunes_count: int = 0
    merges_count: int = 0
    quantizations_count: int = 0


# Schemas related to Model Security Scan Results


class ModelIssue(BaseModel):
    """Model issue schema."""

    title: str
    severity: str
    description: str
    source: str


class ModelSecurityScanResultCreate(BaseModel):
    """Model security scan result create schema."""

    model_id: UUID4
    status: ModelSecurityScanStatusEnum
    total_issues: int
    total_scanned_files: int
    total_skipped_files: int
    scanned_files: list[str]
    low_severity_count: int
    medium_severity_count: int
    high_severity_count: int
    critical_severity_count: int
    model_issues: dict


class ModelSecurityScanResult(ModelSecurityScanResultCreate):
    """Model security scan result schema."""

    model_config = ConfigDict(from_attributes=True, protected_namespaces=())

    id: UUID4
    created_at: datetime
    modified_at: datetime


class ModelDetailSuccessResponse(SuccessResponse):
    """Model detail success response schema."""

    model: ModelDetailResponse
    scan_result: ModelSecurityScanResult | None = None
    eval_result: dict | None = None  # TODO: integrate actual eval result
    model_tree: ModelTree
    endpoints_count: int


class CreateCloudModelWorkflowRequest(BaseModel):
    """Cloud model workflow request schema."""

    workflow_id: UUID4 | None = None
    workflow_total_steps: int | None = None
    step_number: int = Field(..., gt=0)
    trigger_workflow: bool = False
    provider_type: ModelProviderTypeEnum | None = None
    provider_id: UUID4 | None = None
    name: str | None = None
    modality: ModalityEnum | None = None
    uri: str | None = None
    tags: list[Tag] | None = None
    cloud_model_id: UUID4 | None = None

    @model_validator(mode="after")
    def validate_fields(self) -> "CreateCloudModelWorkflowRequest":
        """Validate the fields of the request."""
        if self.workflow_id is None and self.workflow_total_steps is None:
            raise ValueError("workflow_total_steps is required when workflow_id is not provided")

        if self.workflow_id is not None and self.workflow_total_steps is not None:
            raise ValueError("workflow_total_steps and workflow_id cannot be provided together")

        # Check if at least one of the other fields is provided
        other_fields = [
            self.provider_type,
            self.provider_id,
            self.modality,
            self.uri,
            self.tags,
            self.name,
        ]
        required_fields = ["provider_type", "provider_id", "modality", "uri", "tags", "name"]
        if not any(other_fields):
            # Allow if cloud model id is explicitly provided
            input_data = self.model_dump(exclude_unset=True)
            if "cloud_model_id" in input_data:
                return self
            raise ValueError(f"At least one of {', '.join(required_fields)} is required when workflow_id is provided")

        return self


class CreateLocalModelWorkflowRequest(BaseModel):
    """Local model workflow request schema."""

    workflow_id: UUID4 | None = None
    workflow_total_steps: int | None = None
    step_number: int = Field(..., gt=0)
    trigger_workflow: bool = False
    provider_type: ModelProviderTypeEnum | None = None
    proprietary_credential_id: UUID4 | None = None
    name: str | None = None
    uri: str | None = None
    author: str | None = None
    tags: list[Tag] | None = None
    icon: str | None = None

    @model_validator(mode="after")
    def validate_fields(self) -> "CreateLocalModelWorkflowRequest":
        """Validate the fields of the request."""
        if self.workflow_id is None and self.workflow_total_steps is None:
            raise ValueError("workflow_total_steps is required when workflow_id is not provided")

        if self.workflow_id is not None and self.workflow_total_steps is not None:
            raise ValueError("workflow_total_steps and workflow_id cannot be provided together")

        # Validate proprietary_credential_id based on provider_type
        if (
            self.provider_type is not None
            and self.provider_type != ModelProviderTypeEnum.HUGGING_FACE
            and self.proprietary_credential_id is not None
        ):
            raise ValueError("proprietary_credential_id should be None for non-HuggingFace providers")

        if (
            self.provider_type is not None
            and self.provider_type == ModelProviderTypeEnum.HUGGING_FACE
            and self.icon is not None
        ):
            raise ValueError("Icon is not supported for HuggingFace models")

        # Validate provider type
        if self.provider_type and self.provider_type == ModelProviderTypeEnum.CLOUD_MODEL:
            raise ValueError("Cloud model provider type not supported for local model workflow")

        return self

    @field_validator("icon", mode="before")
    @classmethod
    def icon_validate(cls, value: str | None) -> str | None:
        """Validate the icon."""
        if value is not None and not validate_icon(value):
            raise ValueError("invalid icon")
        return value


class CreateLocalModelWorkflowSteps(BaseModel):
    """Create cluster workflow step data schema."""

    provider_type: ModelProviderTypeEnum | None = None
    proprietary_credential_id: UUID4 | None = None
    name: str | None = None
    icon: str | None = None
    uri: str | None = None
    author: str | None = None
    tags: list[Tag] | None = None
    provider_id: UUID4 | None


class EditModel(BaseModel):
    name: str | None = Field(None, min_length=1, max_length=100)
    description: str | None = Field(None, max_length=400)
    tags: List[Tag] | None = None
    tasks: List[Task] | None = None
    icon: str | None = None
    paper_urls: List[HttpUrl] | None = None
    github_url: HttpUrl | None = None
    huggingface_url: HttpUrl | None = None
    website_url: HttpUrl | None = None
    license_file: UploadFile | None = None
    license_url: HttpUrl | None = None

    @field_validator("name", mode="before")
    def validate_name(cls, value: Optional[str]) -> Optional[str]:
        if value is not None:
            value = value.strip()
            if len(value) == 0:
                raise ValueError("Model name cannot be empty or only whitespace.")
        return value

    @field_validator("icon", mode="before")
    @classmethod
    def icon_validate(cls, value: str | None) -> str | None:
        """Validate the icon."""
        if value is not None and not validate_icon(value):
            raise ValueError("invalid icon")
        return value

    @model_validator(mode="before")
    def validate_license(cls, values):
        license_file = values.get("license_file")
        license_url = values.get("license_url")

        # Ensure only one of license_file or license_url is provided
        if license_file and license_url:
            raise ValueError("Please provide either a license file or a license URL, but not both.")

        if license_file:
            filename = license_file.filename
            allowed_extensions = ["pdf", "txt", "doc", "docx", "md"]

            if not filename or "." not in filename:
                raise ValueError("File does not have a valid extension")

            # Get the file extension from the filename
            file_extension = filename.split(".")[-1].lower()

            # Check if the file extension is in the allowed list
            if file_extension not in allowed_extensions:
                raise ValueError("Invalid file extension for license file")
        return values

    @field_serializer("github_url", "huggingface_url", "website_url", "license_url")
    def str_url(self, url: HttpUrl | None) -> str:
        return str(url) if url else None

    @field_serializer("paper_urls")
    def str_paper_urls(self, urls: List[HttpUrl] | None) -> List[str]:
        return [str(url) for url in urls] if urls else urls


class ModelResponse(BaseModel):
    """Model response schema."""

    model_config = ConfigDict(from_attributes=True, protected_namespaces=())

    id: UUID4
    name: str
    author: str | None = None
    modality: ModalityEnum
    source: str
    uri: str
    created_user: UserInfo | None = None
    model_size: int | None = None
    tasks: list[Task] | None = None
    tags: list[Tag] | None = None
    icon: str | None = None
    description: str | None = None
    provider_type: ModelProviderTypeEnum
    created_at: datetime
    modified_at: datetime
    provider: Provider | None = None
    is_present_in_model: bool | None = None


class ModelListResponse(BaseModel):
    """Model list response schema."""

    model: ModelResponse
    endpoints_count: int | None = None


class ModelPaginatedResponse(PaginatedSuccessResponse):
    """Model paginated response schema."""

    models: list[ModelListResponse] = []


class ModelFilter(BaseModel):
    """Filter model schema for filtering models based on specific criteria."""

    model_config = ConfigDict(protected_namespaces=())

    name: str | None = None
    source: CredentialTypeEnum | None = None
    model_size_min: int | None = Field(None, ge=0, le=500)
    model_size_max: int | None = Field(None, ge=0, le=500)
    provider_type: ModelProviderTypeEnum | None = None
    table_source: Literal["cloud_model", "model"] = "cloud_model"
    base_model: str | None = None
    base_model_relation: BaseModelRelationEnum | None = None

    @field_validator("source")
    @classmethod
    def change_to_string(cls, v: CredentialTypeEnum | None) -> str | None:
        """Convert the source enum value to a string."""
        return v.value if v else None

    @field_validator("model_size_min", "model_size_max")
    @classmethod
    def convert_to_billions(cls, v: Optional[int]) -> Optional[int]:
        """Convert the input value to billions."""
        if v is not None:
            return v * 1000000000  # Convert to billions
        return v


# Cloud model related schemas


class CreateCloudModelWorkflowSteps(BaseModel):
    """Cloud model workflow step data schema."""

    provider_type: ModelProviderTypeEnum | None = None
    source: str | None = None
    name: str | None = None
    modality: ModalityEnum | None = None
    uri: str | None = None
    tags: list[Tag] | None = None
    icon: str | None = None
    provider_id: UUID4 | None = None
    cloud_model_id: UUID4 | None = None


class CreateCloudModelWorkflowStepData(BaseModel):
    """Cloud model workflow step data schema."""

    model_config = ConfigDict(from_attributes=True, protected_namespaces=())

    provider_type: ModelProviderTypeEnum | None = None
    provider: Provider | None = None
    cloud_model: CloudModel | None = None
    cloud_model_id: UUID4 | None = None
    provider_id: UUID4 | None = None
    model_id: UUID4 | None = None
    model: Model | None = None
    workflow_execution_status: dict | None = None
    leaderboard: list | None = None


class CreateCloudModelWorkflowResponse(SuccessResponse):
    """Add Cloud Model Workflow Response."""

    workflow_id: UUID4
    status: WorkflowStatusEnum
    current_step: int
    total_steps: int
    reason: str | None = None
    workflow_steps: CreateCloudModelWorkflowStepData | None = None


class CloudModelFilter(BaseModel):
    """Cloud model filter schema."""

    model_config = ConfigDict(protected_namespaces=())

    source: CredentialTypeEnum | None = None
    modality: ModalityEnum | None = None
    model_size: int | None = None
    name: str | None = None

    @field_validator("source")
    def change_to_string(cls, v: CredentialTypeEnum | None) -> str | None:
        """Change the source to a string."""
        return v.value if v else None


class CloudModelResponse(PaginatedSuccessResponse):
    """Cloud model response schema."""

    model_config = ConfigDict(extra="ignore")

    cloud_models: list[CloudModel] = []


class TagWithCount(BaseModel):
    """Tag with count schema."""

    name: str
    color: str = Field(..., pattern="^#[0-9A-Fa-f]{6}$")
    count: int

    @field_validator("color")
    def validate_hex_color(cls, v: str) -> str:
        """Validate that color is a valid hex color code."""
        if not re.match(r"^#[0-9A-Fa-f]{6}$", v):
            raise ValueError("Color must be a valid hex color code (e.g., #FF0000)")
        return v.upper()  # Normalize to uppercase


class RecommendedTagsResponse(PaginatedSuccessResponse):
    """Recommended tags response schema."""

    tags: List[TagWithCount] = []

    @field_validator("tags", mode="before")
    def validate_tags(cls, v: List[Tuple[str, str, int]]) -> List[TagWithCount]:
        """Convert tuples to TagWithCount objects."""
        return [TagWithCount(name=tag[0], color=tag[1], count=tag[2]) for tag in v]


class TagsListResponse(PaginatedSuccessResponse):
    """Response schema for tags list."""

    tags: List[Tag] = Field(..., description="List of matching tags")


class TasksListResponse(PaginatedSuccessResponse):
    """Response schema for tasks list."""

    tasks: List[Task] = []


class ModelAuthorResponse(PaginatedSuccessResponse):
    """Response schema for searching tags by name."""

    authors: List[str] = Field(..., description="List of matching authors")


class ModelAuthorFilter(BaseModel):
    """Filter schema for model authors."""

    author: str | None = None


# Schemas related to Paper Published


class PaperPublishedCreate(BaseModel):
    """Paper Published Create Schema."""

    title: str | None = None
    authors: list[str] | None = None
    url: str | None = None
    model_id: UUID4


# Schemas related to Model Licenses


class ModelLicensesCreate(BaseModel):
    """Model Licenses Create Schema."""

    name: str | None = None
    url: str | None = None
    path: str | None = None
    faqs: list[dict] | None = None
    model_id: UUID4


# Local model related schemas


class LocalModelScanRequest(BaseModel):
    """Local model scan request schema."""

    workflow_id: UUID4 | None = None
    workflow_total_steps: int | None = None
    step_number: int = Field(..., gt=0)
    trigger_workflow: bool = False
    model_id: UUID4 | None = None

    @model_validator(mode="after")
    def validate_fields(self) -> "LocalModelScanRequest":
        """Validate the fields of the request."""
        if self.workflow_id is None and self.workflow_total_steps is None:
            raise ValueError("workflow_total_steps is required when workflow_id is not provided")

        if self.workflow_id is not None and self.workflow_total_steps is not None:
            raise ValueError("workflow_total_steps and workflow_id cannot be provided together")

        # Check if at least one of the other fields is provided
        other_fields = [self.model_id]
        required_fields = ["model_id"]
        if not any(other_fields):
            raise ValueError(f"At least one of {', '.join(required_fields)} is required")

        return self


class LocalModelScanWorkflowStepData(BaseModel):
    """Local model scan workflow step data schema."""

    model_id: UUID4 | None


class CancelDeploymentWorkflowRequest(BaseModel):
    """Cancel deployment workflow request schema."""

    workflow_id: UUID4

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/model_ops/model_routes.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The model ops package, containing essential business logic, services, and routing configurations for the model ops."""

import json
from json.decoder import JSONDecodeError
from typing import List, Optional, Union
from uuid import UUID

from fastapi import APIRouter, Depends, Form, Query, UploadFile, status
from fastapi.exceptions import RequestValidationError
from pydantic import ValidationError
from sqlalchemy.orm import Session
from typing_extensions import Annotated

from budapp.commons import logging
from budapp.commons.constants import ModalityEnum
from budapp.commons.dependencies import (
    get_current_active_user,
    get_session,
    parse_ordering_fields,
)
from budapp.commons.exceptions import ClientException
from budapp.commons.schemas import ErrorResponse, SuccessResponse
from budapp.user_ops.schemas import User
from budapp.workflow_ops.schemas import RetrieveWorkflowDataResponse
from budapp.workflow_ops.services import WorkflowService

from .schemas import (
    CancelDeploymentWorkflowRequest,
    CreateCloudModelWorkflowRequest,
    CreateCloudModelWorkflowResponse,
    CreateLocalModelWorkflowRequest,
    EditModel,
    LocalModelScanRequest,
    ModelAuthorFilter,
    ModelAuthorResponse,
    ModelDetailSuccessResponse,
    ModelFilter,
    ModelPaginatedResponse,
    ProviderFilter,
    ProviderResponse,
    RecommendedTagsResponse,
    TagsListResponse,
    TasksListResponse,
)
from .services import (
    CloudModelService,
    CloudModelWorkflowService,
    LocalModelWorkflowService,
    ModelService,
    ProviderService,
)


logger = logging.get_logger(__name__)

model_router = APIRouter(prefix="/models", tags=["model"])


@model_router.get(
    "/",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": ModelPaginatedResponse,
            "description": "Successfully list all models",
        },
    },
    description="List all models",
)
async def list_all_models(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    filters: Annotated[ModelFilter, Depends()],
    author: List[str] = Query(default=[]),
    modality: List[ModalityEnum] = Query(default=[]),
    tags: List[str] = Query(default=[]),
    tasks: List[str] = Query(default=[]),
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=0),
    order_by: Optional[List[str]] = Depends(parse_ordering_fields),
    search: bool = False,
) -> Union[ModelPaginatedResponse, ErrorResponse]:
    """List all models."""
    # Calculate offset
    offset = (page - 1) * limit

    # Convert UserFilter to dictionary
    filters_dict = filters.model_dump(exclude_none=True, exclude={"table_source"})

    # Update filters_dict only for non-empty lists
    filter_updates = {"tags": tags, "tasks": tasks, "author": author, "modality": modality}
    filters_dict.update({k: v for k, v in filter_updates.items() if v})

    # Perform router level validation
    if filters.table_source == "cloud_model" and filter_updates["author"]:
        return ErrorResponse(
            code=status.HTTP_400_BAD_REQUEST,
            message="Author is not allowed for cloud models.",
        ).to_http_response()

    if filters.table_source == "cloud_model" and (filters.base_model or filters.base_model_relation):
        return ErrorResponse(
            code=status.HTTP_400_BAD_REQUEST,
            message="Base model and base model relation are not allowed for cloud models.",
        ).to_http_response()

    if filters.model_size_min and filters.model_size_max and filters.model_size_min > filters.model_size_max:
        return ErrorResponse(
            code=status.HTTP_400_BAD_REQUEST,
            message="Model size min is greater than model size max.",
        ).to_http_response()

    try:
        if filters.table_source == "cloud_model":
            db_models, count = await CloudModelService(session).get_all_cloud_models(
                offset, limit, filters_dict, order_by, search
            )
        else:
            db_models, count = await ModelService(session).get_all_active_models(
                offset, limit, filters_dict, order_by, search
            )
    except ClientException as e:
        logger.exception(f"Failed to get all models: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to get all models: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to get all cloud models"
        ).to_http_response()

    return ModelPaginatedResponse(
        models=db_models,
        total_record=count,
        page=page,
        limit=limit,
        object="models.list",
        code=status.HTTP_200_OK,
    ).to_http_response()


@model_router.get(
    "/providers",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": ProviderResponse,
            "description": "Successfully list all providers",
        },
    },
    description="List all model providers",
)
async def list_providers(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    filters: ProviderFilter = Depends(),
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=0),
    order_by: Optional[List[str]] = Depends(parse_ordering_fields),
    search: bool = False,
) -> Union[ProviderResponse, ErrorResponse]:
    """List all model providers."""
    # Calculate offset
    offset = (page - 1) * limit

    # Convert UserFilter to dictionary
    filters_dict = filters.model_dump(exclude_none=True)

    try:
        db_providers, count = await ProviderService(session).get_all_providers(
            offset, limit, filters_dict, order_by, search
        )
    except Exception as e:
        logger.exception(f"Failed to get all providers: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to get all providers"
        ).to_http_response()

    return ProviderResponse(
        providers=db_providers,
        total_record=count,
        page=page,
        limit=limit,
        object="providers.list",
        code=status.HTTP_200_OK,
    ).to_http_response()


@model_router.post(
    "/cloud-model-workflow",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": RetrieveWorkflowDataResponse,
            "description": "Successfully add cloud model workflow",
        },
    },
    description="Add cloud model workflow",
)
async def add_cloud_model_workflow(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    request: CreateCloudModelWorkflowRequest,
) -> Union[RetrieveWorkflowDataResponse, ErrorResponse]:
    """Add cloud model workflow."""
    try:
        db_workflow = await CloudModelWorkflowService(session).add_cloud_model_workflow(
            current_user_id=current_user.id,
            request=request,
        )

        return await WorkflowService(session).retrieve_workflow_data(db_workflow.id)
    except ClientException as e:
        logger.exception(f"Failed to add cloud model workflow: {e}")
        return ErrorResponse(code=status.HTTP_400_BAD_REQUEST, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to add cloud model workflow: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to add cloud model workflow"
        ).to_http_response()


@model_router.post(
    "/local-model-workflow",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": RetrieveWorkflowDataResponse,
            "description": "Successfully add local model workflow",
        },
    },
    description="Add local model workflow",
)
async def add_local_model_workflow(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    request: CreateLocalModelWorkflowRequest,
) -> Union[RetrieveWorkflowDataResponse, ErrorResponse]:
    """Add local model workflow."""
    try:
        db_workflow = await LocalModelWorkflowService(session).add_local_model_workflow(
            current_user_id=current_user.id,
            request=request,
        )

        return await WorkflowService(session).retrieve_workflow_data(db_workflow.id)
    except ClientException as e:
        logger.exception(f"Failed to add local model workflow: {e}")
        return ErrorResponse(code=status.HTTP_400_BAD_REQUEST, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to add local model workflow: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to add local model workflow"
        ).to_http_response()


@model_router.patch(
    "/{model_id}",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": CreateCloudModelWorkflowResponse,
            "description": "Successfully edited cloud model",
        },
    },
    description="Edit cloud model",
)
async def edit_model(
    model_id: UUID,
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    name: str | None = Form(None),
    description: str | None = Form(None),
    tags: str | None = Form(None),
    tasks: str | None = Form(None),
    icon: str | None = Form(None),
    paper_urls: str | None = Form(None),
    github_url: str | None = Form(None),
    huggingface_url: str | None = Form(None),
    website_url: str | None = Form(None),
    license_file: UploadFile | None = None,
    license_url: str | None = Form(None),
) -> Union[SuccessResponse, ErrorResponse]:
    """Edit cloud model with file upload"""
    logger.debug(
        f"Received data: name={name}, description={description}, tags={tags}, tasks={tasks}, paper_urls={paper_urls}, github_url={github_url}, huggingface_url={huggingface_url}, website_url={website_url}, license_file={license_file}, license_url={license_url}"
    )

    try:
        if isinstance(tags, str) and len(tags) == 0:
            tags = []
        else:
            tags = json.loads(tags) if tags else None

        if isinstance(tasks, str) and len(tasks) == 0:
            tasks = []
        else:
            tasks = json.loads(tasks) if tasks else None

        if isinstance(tags, str) and len(paper_urls) == 0:
            paper_urls = []
        elif isinstance(paper_urls, str) and len(paper_urls) > 0:
            # Split the first element into a list of URLs and validate each URL in loop
            paper_urls = [url.strip() for url in paper_urls.split(",")]

        edit_model = EditModel(
            name=name if name else None,
            description=description if description else None,
            tags=tags if isinstance(tags, list) else None,
            tasks=tasks if isinstance(tasks, list) else None,
            icon=icon if icon else None,
            paper_urls=paper_urls if isinstance(paper_urls, list) else None,
            github_url=github_url if github_url else None,
            huggingface_url=huggingface_url if huggingface_url else None,
            website_url=website_url if website_url else None,
            license_url=license_url if license_url else None,
            license_file=license_file if license_file else None,
        )

        # Pass file and edit_model data to your service
        await ModelService(session).edit_model(
            current_user_id=current_user.id,
            model_id=model_id,
            data=edit_model.model_dump(exclude_none=True, exclude_unset=True),
        )

        return SuccessResponse(message="Cloud model edited successfully", code=status.HTTP_200_OK).to_http_response()
    except ClientException as e:
        logger.exception(f"Failed to edit cloud model: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except ValidationError as e:
        logger.exception(f"ValidationErrors: {str(e)}")
        raise RequestValidationError(e.errors())
    except JSONDecodeError as e:
        logger.exception(f"Failed to edit cloud model: {e}")
        return ErrorResponse(
            code=status.HTTP_422_UNPROCESSABLE_ENTITY, message="Failed to edit cloud model. Invalid input format."
        ).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to edit cloud model: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to edit cloud model"
        ).to_http_response()


# @model_router.get(
#     "/cloud-model-workflow/{workflow_id}",
#     responses={
#         status.HTTP_500_INTERNAL_SERVER_ERROR: {
#             "model": ErrorResponse,
#             "description": "Service is unavailable due to server error",
#         },
#         status.HTTP_400_BAD_REQUEST: {
#             "model": ErrorResponse,
#             "description": "Service is unavailable due to client error",
#         },
#         status.HTTP_200_OK: {
#             "model": CreateCloudModelWorkflowResponse,
#             "description": "Successfully add cloud model workflow",
#         },
#     },
#     description="Get cloud model workflow",
# )
# async def get_cloud_model_workflow(
#     current_user: Annotated[User, Depends(get_current_active_user)],
#     session: Annotated[Session, Depends(get_session)],
#     workflow_id: UUID,
# ) -> Union[CreateCloudModelWorkflowResponse, ErrorResponse]:
#     """Get cloud model workflow."""
#     try:
#         return await CloudModelWorkflowService(session).get_cloud_model_workflow(workflow_id)
#     except ClientException as e:
#         logger.exception(f"Failed to get cloud model workflow: {e}")
#         return ErrorResponse(code=status.HTTP_400_BAD_REQUEST, message=e.message).to_http_response()
#     except Exception as e:
#         logger.exception(f"Failed to get cloud model workflow: {e}")
#         return ErrorResponse(
#             code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to get cloud model workflow"
#         ).to_http_response()


# @model_router.get(
#     "/cloud-models",
#     responses={
#         status.HTTP_500_INTERNAL_SERVER_ERROR: {
#             "model": ErrorResponse,
#             "description": "Service is unavailable due to server error",
#         },
#         status.HTTP_400_BAD_REQUEST: {
#             "model": ErrorResponse,
#             "description": "Service is unavailable due to client error",
#         },
#         status.HTTP_200_OK: {
#             "model": ProviderResponse,
#             "description": "Successfully list all providers",
#         },
#     },
#     description="List all cloud models",
# )
# async def list_cloud_models(
#     current_user: Annotated[User, Depends(get_current_active_user)],
#     session: Annotated[Session, Depends(get_session)],
#     filters: Annotated[CloudModelFilter, Depends()],
#     tags: List[str] = Query(default_factory=list),
#     tasks: List[str] = Query(default_factory=list),
#     page: int = Query(1, ge=1),
#     limit: int = Query(10, ge=0),
#     order_by: Optional[List[str]] = Depends(parse_ordering_fields),
#     search: bool = False,
# ) -> Union[CloudModelResponse, ErrorResponse]:
#     """List all cloud models."""
#     # Calculate offset
#     offset = (page - 1) * limit

#     # Convert UserFilter to dictionary
#     filters_dict = filters.model_dump(exclude_none=True)
#     if tags:
#         filters_dict["tags"] = tags
#     if tasks:
#         filters_dict["tasks"] = tasks

#     try:
#         db_models, count = await CloudModelService(session).get_all_cloud_models(
#             offset, limit, filters_dict, order_by, search
#         )
#     except Exception as e:
#         logger.exception(f"Failed to get all cloud models: {e}")
#         return ErrorResponse(
#             code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to get all cloud models"
#         ).to_http_response()

#     return CloudModelResponse(
#         cloud_models=db_models,
#         total_record=count,
#         page=page,
#         limit=limit,
#         object="cloud_models.list",
#         code=status.HTTP_200_OK,
#     ).to_http_response()


@model_router.get(
    "/cloud-models/recommended-tags",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": RecommendedTagsResponse,
            "description": "Successfully list all recommended tags",
        },
    },
    description="List all cloud model recommended tags",
)
async def list_cloud_model_recommended_tags(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=0),
) -> Union[RecommendedTagsResponse, ErrorResponse]:
    """List all most used tags."""
    # Calculate offset
    offset = (page - 1) * limit

    try:
        db_tags, count = await CloudModelService(session).get_all_recommended_tags(offset, limit)
    except Exception as e:
        logger.exception(f"Failed to get all recommended tags: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to get all recommended tags"
        ).to_http_response()

    return RecommendedTagsResponse(
        tags=db_tags,
        total_record=count,
        page=page,
        limit=limit,
        object="recommended_tags.list",
        code=status.HTTP_200_OK,
    ).to_http_response()


@model_router.get(
    "/tags",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": TagsListResponse,
            "description": "Successfully searched tags by name",
        },
    },
    description="Search model tags by name with pagination",
)
async def list_model_tags(
    session: Annotated[Session, Depends(get_session)],
    name: Optional[str] = Query(default=None),
    current_user: User = Depends(get_current_active_user),
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=1),
) -> Union[TagsListResponse, ErrorResponse]:
    """List tags by name with pagination support."""
    offset = (page - 1) * limit

    try:
        db_tags, count = await ModelService(session).list_model_tags(name or "", offset, limit)
    except Exception as e:
        return ErrorResponse(code=status.HTTP_500_INTERNAL_SERVER_ERROR, message=str(e)).to_http_response()

    return TagsListResponse(
        tags=db_tags,
        total_record=count,
        page=page,
        limit=limit,
        object="tags.search",
        code=status.HTTP_200_OK,
    ).to_http_response()


@model_router.get(
    "/tasks",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": TasksListResponse,
            "description": "Successfully listed tasks",
        },
    },
    description="Search model tags by name with pagination",
)
async def list_model_tasks(
    session: Annotated[Session, Depends(get_session)],
    name: Optional[str] = Query(default=None),
    current_user: User = Depends(get_current_active_user),
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=1),
) -> Union[TasksListResponse, ErrorResponse]:
    """List tasks by name with pagination support."""
    offset = (page - 1) * limit

    try:
        db_tasks, count = await ModelService(session).list_model_tasks(name or "", offset, limit)
    except Exception as e:
        return ErrorResponse(code=status.HTTP_500_INTERNAL_SERVER_ERROR, message=str(e)).to_http_response()

    return TasksListResponse(
        tasks=db_tasks,
        total_record=count,
        page=page,
        limit=limit,
        object="tasks.list",
        code=status.HTTP_200_OK,
    ).to_http_response()


@model_router.get(
    "/authors",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": ModelAuthorResponse,
            "description": "Successfully searched author by name",
        },
    },
    description="Search model author by name with pagination",
)
async def list_all_model_authors(
    session: Annotated[Session, Depends(get_session)],
    filters: Annotated[ModelAuthorFilter, Depends()],
    current_user: User = Depends(get_current_active_user),
    search: bool = False,
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=0),
    order_by: Optional[List[str]] = Depends(parse_ordering_fields),
) -> Union[ModelAuthorResponse, ErrorResponse]:
    """Search author by name with pagination support."""
    offset = (page - 1) * limit

    filters_dict = filters.model_dump(exclude_none=True)

    try:
        db_authors, count = await ModelService(session).list_all_model_authors(
            offset, limit, filters_dict, order_by, search
        )
    except Exception as e:
        return ErrorResponse(code=status.HTTP_500_INTERNAL_SERVER_ERROR, message=str(e)).to_http_response()

    return ModelAuthorResponse(
        authors=db_authors,
        total_record=count,
        page=page,
        limit=limit,
        object="author.list",
        code=status.HTTP_200_OK,
    ).to_http_response()


@model_router.get(
    "/{model_id}",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_404_NOT_FOUND: {
            "model": ErrorResponse,
            "description": "Model not found",
        },
        status.HTTP_200_OK: {
            "model": ModelDetailSuccessResponse,
            "description": "Successfully retrieved model details",
        },
    },
    description="Retrieve details of a model by ID",
)
async def retrieve_model(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    model_id: UUID,
) -> Union[ModelDetailSuccessResponse, ErrorResponse]:
    """Retrieve details of a model by its ID."""
    try:
        return await ModelService(session).retrieve_model(model_id)
    except ClientException as e:
        logger.exception(f"Failed to get model details: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to get model details: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            message="Failed to retrieve model details",
        ).to_http_response()


@model_router.post(
    "/security-scan",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": RetrieveWorkflowDataResponse,
            "description": "Successfully scan local model",
        },
    },
    description="Scan local model",
)
async def scan_local_model_workflow(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    request: LocalModelScanRequest,
) -> Union[RetrieveWorkflowDataResponse, ErrorResponse]:
    """Scan local model."""
    try:
        db_workflow = await LocalModelWorkflowService(session).scan_local_model_workflow(
            current_user_id=current_user.id,
            request=request,
        )

        return await WorkflowService(session).retrieve_workflow_data(db_workflow.id)
    except ClientException as e:
        logger.exception(f"Failed to scan local model: {e}")
        return ErrorResponse(code=status.HTTP_400_BAD_REQUEST, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to scan local model: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to scan local model"
        ).to_http_response()


@model_router.post(
    "/cancel-deployment",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": SuccessResponse,
            "description": "Successfully cancel model deployment",
        },
    },
    description="Cancel model deployment",
)
async def cancel_model_deployment(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    cancel_request: CancelDeploymentWorkflowRequest,
) -> Union[SuccessResponse, ErrorResponse]:
    """Cancel model deployment."""
    try:
        await ModelService(session).cancel_model_deployment_workflow(cancel_request.workflow_id)
        return SuccessResponse(
            message="Model deployment cancelled successfully",
            code=status.HTTP_200_OK,
            object="model.cancel_deployment",
        )
    except ClientException as e:
        logger.exception(f"Failed to cancel model deployment: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to cancel model deployment: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to cancel model deployment"
        ).to_http_response()


@model_router.delete(
    "/{model_id}",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_404_NOT_FOUND: {
            "model": ErrorResponse,
            "description": "Model not found",
        },
        status.HTTP_200_OK: {
            "model": SuccessResponse,
            "description": "Successfully deleted model",
        },
    },
    description="Delete an active model from the database",
)
async def delete_model(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    model_id: UUID,
) -> Union[SuccessResponse, ErrorResponse]:
    """Delete a model by its ID."""
    _ = await ModelService(session).delete_active_model(model_id)
    logger.debug(f"Model deleted: {model_id}")

    return SuccessResponse(message="Model deleted successfully", code=status.HTTP_200_OK, object="model.delete")

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/model_ops/crud.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The crud package, containing essential business logic, services, and routing configurations for the model ops."""

from typing import Any, Dict, List, Tuple, Optional
from uuid import UUID

from sqlalchemy import and_, desc, func, or_, select
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.exc import SQLAlchemyError

from budapp.commons import logging
from budapp.commons.db_utils import DataManagerUtils
from budapp.commons.constants import ModelStatusEnum, CloudModelStatusEnum
from budapp.commons.exceptions import DatabaseException
from budapp.endpoint_ops.models import Endpoint
from budapp.model_ops.models import CloudModel, Model, PaperPublished
from budapp.model_ops.models import Provider as ProviderModel
from budapp.commons.constants import EndpointStatusEnum


logger = logging.get_logger(__name__)


class ProviderDataManager(DataManagerUtils):
    """Data manager for the Provider model."""

    async def get_all_providers_by_type(self, provider_types: List[str]) -> List[ProviderModel]:
        """Get all providers from the database."""
        stmt = select(ProviderModel).filter(ProviderModel.type.in_(provider_types))
        return self.scalars_all(stmt)

    async def get_all_providers(
        self,
        offset: int = 0,
        limit: int = 10,
        filters: Dict[str, Any] = {},
        order_by: List[Tuple[str, str]] = [],
        search: bool = False,
    ) -> Tuple[List[ProviderModel], int]:
        """Get all providers from the database."""
        await self.validate_fields(ProviderModel, filters)

        # Generate statements according to search or filters
        if search:
            search_conditions = await self.generate_search_stmt(ProviderModel, filters)
            stmt = select(ProviderModel).filter(or_(*search_conditions))
            count_stmt = select(func.count()).select_from(ProviderModel).filter(and_(*search_conditions))
        else:
            stmt = select(ProviderModel).filter_by(**filters)
            count_stmt = select(func.count()).select_from(ProviderModel).filter_by(**filters)

        # Calculate count before applying limit and offset
        count = self.execute_scalar(count_stmt)

        # Apply limit and offset
        stmt = stmt.limit(limit).offset(offset)

        # Apply sorting
        if order_by:
            sort_conditions = await self.generate_sorting_stmt(ProviderModel, order_by)
            stmt = stmt.order_by(*sort_conditions)

        result = self.scalars_all(stmt)

        return result, count


class PaperPublishedDataManager(DataManagerUtils):
    """Data manager for the PaperPublished model."""

    async def delete_paper_by_urls(self, model_id: UUID, paper_urls: Optional[Dict[str, List[Any]]] = None) -> None:
        """
        Delete multiple model instances based on the model id and paper urls.
        """
        try:
            # Build the query with filters
            query = self.session.query(PaperPublished).filter_by(**{"model_id": model_id})

            # Add paper_urls
            if paper_urls:
                for key, values in paper_urls.items():
                    query = query.filter(getattr(PaperPublished, key).in_(values))

            # Delete records
            query.delete(synchronize_session=False)

            # Commit the transaction
            self.session.commit()
            logger.debug(f"Successfully deleted records from {PaperPublished.__name__} with paper_urls: {paper_urls}")
        except (Exception, SQLAlchemyError) as e:
            # Rollback the transaction on error
            self.session.rollback()
            logger.exception(f"Failed to delete records from {PaperPublished.__name__}: {e}")
            raise DatabaseException(f"Unable to delete records from {PaperPublished.__name__}") from e


class ModelDataManager(DataManagerUtils):
    """Data manager for the Model model."""

    async def list_model_tags(
        self,
        search_value: str = "",
        offset: int = 0,
        limit: int = 10,
    ) -> Tuple[List[Model], int]:
        """Search tags by name with pagination, or fetch all tags if no search value is provided."""
        # Ensure only valid JSON arrays are processed
        tags_subquery = (
            select(func.jsonb_array_elements(Model.tags).label("tag"))
            .where(Model.status == ModelStatusEnum.ACTIVE)
            .where(Model.tags.is_not(None))  # Exclude null tags
            .where(func.jsonb_typeof(Model.tags) == "array")  # Ensure tags is a JSON array
        ).subquery()

        # Extract name and color as jsonb
        distinct_tags_query = (
            select(
                func.jsonb_extract_path_text(tags_subquery.c.tag, "name").label("name"),
                func.jsonb_extract_path_text(tags_subquery.c.tag, "color").label("color"),
            )
            .where(func.jsonb_typeof(tags_subquery.c.tag) == "object")  # Ensure valid JSONB objects
            .where(func.jsonb_extract_path_text(tags_subquery.c.tag, "name").is_not(None))  # Valid names
            .where(func.jsonb_extract_path_text(tags_subquery.c.tag, "color").is_not(None))  # Valid colors
        ).subquery()

        # Apply DISTINCT to get unique tags by name, selecting the first color
        distinct_on_name_query = (
            select(
                distinct_tags_query.c.name,
                distinct_tags_query.c.color,
            )
            .distinct(distinct_tags_query.c.name)
            .order_by(distinct_tags_query.c.name, distinct_tags_query.c.color)  # Ensure deterministic order
        )

        # Apply search filter if provided
        if search_value:
            distinct_on_name_query = distinct_on_name_query.where(distinct_tags_query.c.name.ilike(f"{search_value}%"))

        # Add pagination
        distinct_tags_with_pagination = distinct_on_name_query.offset(offset).limit(limit)

        # Execute the paginated query
        tags_result = self.session.execute(distinct_tags_with_pagination)

        # Count total distinct tag names
        distinct_count_query = (
            select(func.count(func.distinct(distinct_tags_query.c.name)))
            .where(func.jsonb_typeof(tags_subquery.c.tag) == "object")  # Ensure valid JSONB objects
            .where(func.jsonb_extract_path_text(tags_subquery.c.tag, "name").is_not(None))  # Valid names
            .where(func.jsonb_extract_path_text(tags_subquery.c.tag, "color").is_not(None))  # Valid colors
        )

        # Apply search filter to the count query
        if search_value:
            distinct_count_query = distinct_count_query.where(
                func.jsonb_extract_path_text(tags_subquery.c.tag, "name").ilike(f"{search_value}%")
            )

        # Execute the count query
        distinct_count_result = self.session.execute(distinct_count_query)
        total_count = distinct_count_result.scalar()

        return tags_result, total_count

    async def list_model_tasks(
        self,
        search_value: str = "",
        offset: int = 0,
        limit: int = 10,
    ) -> Tuple[List[Model], int]:
        """Search tasks by name with pagination, or fetch all tasks if no search value is provided."""
        # Ensure only valid JSON arrays are processed
        tasks_subquery = (
            select(func.jsonb_array_elements(Model.tasks).label("task"))
            .where(Model.status == ModelStatusEnum.ACTIVE)
            .where(Model.tasks.is_not(None))  # Exclude null tasks
            .where(func.jsonb_typeof(Model.tasks) == "array")  # Ensure tasks is a JSON array
        ).subquery()

        # Extract name and color as jsonb
        distinct_tasks_query = (
            select(
                func.jsonb_extract_path_text(tasks_subquery.c.task, "name").label("name"),
                func.jsonb_extract_path_text(tasks_subquery.c.task, "color").label("color"),
            )
            .where(func.jsonb_typeof(tasks_subquery.c.task) == "object")  # Ensure valid JSONB objects
            .where(func.jsonb_extract_path_text(tasks_subquery.c.task, "name").is_not(None))  # Valid names
            .where(func.jsonb_extract_path_text(tasks_subquery.c.task, "color").is_not(None))  # Valid colors
        ).subquery()

        # Apply DISTINCT to get unique tasks by name, selecting the first color
        distinct_on_name_query = (
            select(
                distinct_tasks_query.c.name,
                distinct_tasks_query.c.color,
            )
            .distinct(distinct_tasks_query.c.name)
            .order_by(distinct_tasks_query.c.name, distinct_tasks_query.c.color)  # Ensure deterministic order
        )

        # Apply search filter if provided
        if search_value:
            distinct_on_name_query = distinct_on_name_query.where(
                distinct_tasks_query.c.name.ilike(f"{search_value}%")
            )

        # Add pagination
        distinct_tasks_with_pagination = distinct_on_name_query.offset(offset).limit(limit)

        # Execute the paginated query
        tasks_result = self.session.execute(distinct_tasks_with_pagination)

        # Count total distinct task names
        distinct_count_query = (
            select(func.count(func.distinct(distinct_tasks_query.c.name)))
            .where(func.jsonb_typeof(tasks_subquery.c.task) == "object")  # Ensure valid JSONB objects
            .where(func.jsonb_extract_path_text(tasks_subquery.c.task, "name").is_not(None))  # Valid names
            .where(func.jsonb_extract_path_text(tasks_subquery.c.task, "color").is_not(None))  # Valid colors
        )

        # Apply search filter to the count query
        if search_value:
            distinct_count_query = distinct_count_query.where(
                func.jsonb_extract_path_text(tasks_subquery.c.task, "name").ilike(f"{search_value}%")
            )

        # Execute the count query
        distinct_count_result = self.session.execute(distinct_count_query)
        total_count = distinct_count_result.scalar()

        return tasks_result, total_count

    async def get_all_models(
        self,
        offset: int,
        limit: int,
        filters: Dict = {},
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[Model], int]:
        """List all models in the database."""
        # Convert base_model to list if it is a string
        base_model = filters.pop("base_model", None)
        base_model = [base_model] if base_model else None

        # Tags and tasks are not filterable
        # Also remove from filters dict
        explicit_conditions = []
        json_filters = {"tags": filters.pop("tags", []), "tasks": filters.pop("tasks", [])}
        explicit_filters = {
            "modality": filters.pop("modality", []),
            "author": filters.pop("author", []),
            "model_size_min": filters.pop("model_size_min", None),
            "model_size_max": filters.pop("model_size_max", None),
            "base_model": base_model,
        }

        # Validate the remaining filters
        await self.validate_fields(Model, filters)

        if json_filters["tags"]:
            # Either TagA or TagB exist in tag field
            tag_conditions = or_(
                *[Model.tags.cast(JSONB).contains([{"name": tag_name}]) for tag_name in json_filters["tags"]]
            )
            explicit_conditions.append(tag_conditions)

        if json_filters["tasks"]:
            # Either TaskA or TaskB exist in task field
            task_conditions = or_(
                *[Model.tasks.cast(JSONB).contains([{"name": task_name}]) for task_name in json_filters["tasks"]]
            )
            explicit_conditions.append(task_conditions)

        if explicit_filters["modality"]:
            # Check any of modality present in the field
            modality_condition = Model.modality.in_(explicit_filters["modality"])
            explicit_conditions.append(modality_condition)

        if explicit_filters["author"]:
            # Check any of author present in the field
            author_condition = Model.author.in_(explicit_filters["author"])
            explicit_conditions.append(author_condition)

        if explicit_filters["base_model"]:
            # Check any of base_model present in the field
            base_model_condition = Model.base_model.contains(explicit_filters["base_model"])
            explicit_conditions.append(base_model_condition)

        if explicit_filters["model_size_min"] is not None or explicit_filters["model_size_max"] is not None:
            # Add model size range condition
            size_conditions = []
            if explicit_filters["model_size_min"] is not None:
                size_conditions.append(Model.model_size >= explicit_filters["model_size_min"])
            if explicit_filters["model_size_max"] is not None:
                size_conditions.append(Model.model_size <= explicit_filters["model_size_max"])
            size_condition = and_(*size_conditions)
            explicit_conditions.append(size_condition)

        # Generate statements according to search or filters
        if search:
            search_conditions = await self.generate_search_stmt(Model, filters)
            stmt = (
                select(
                    Model,
                    func.count(Endpoint.id)
                    .filter(Endpoint.status != EndpointStatusEnum.DELETED)
                    .label("endpoints_count"),
                )
                .select_from(Model)
                .filter(or_(*search_conditions, *explicit_conditions))
                .filter(Model.status == ModelStatusEnum.ACTIVE)
                .outerjoin(Endpoint, Endpoint.model_id == Model.id)
                .group_by(Model.id)
            )
            count_stmt = (
                select(func.count())
                .select_from(Model)
                .filter(or_(*search_conditions, *explicit_conditions))
                .filter(Model.status == ModelStatusEnum.ACTIVE)
            )
        else:
            stmt = (
                select(
                    Model,
                    func.count(Endpoint.id)
                    .filter(Endpoint.status != EndpointStatusEnum.DELETED)
                    .label("endpoints_count"),
                )
                .select_from(Model)
                .filter_by(**filters)
                .where(and_(*explicit_conditions))
                .filter(Model.status == ModelStatusEnum.ACTIVE)
                .outerjoin(Endpoint, Endpoint.model_id == Model.id)
                .group_by(Model.id)
            )
            count_stmt = (
                select(func.count())
                .select_from(Model)
                .filter_by(**filters)
                .where(and_(*explicit_conditions))
                .filter(Model.status == ModelStatusEnum.ACTIVE)
            )

        # Calculate count before applying limit and offset
        count = self.execute_scalar(count_stmt)

        # Apply limit and offset
        stmt = stmt.limit(limit).offset(offset)

        # Apply sorting
        if order_by:
            sort_conditions = await self.generate_sorting_stmt(Model, order_by)
            stmt = stmt.order_by(*sort_conditions)

        result = self.execute_all(stmt)

        return result, count

    async def list_all_model_authors(
        self,
        offset: int = 0,
        limit: int = 10,
        filters: Dict[str, Any] = {},
        order_by: List[Tuple[str, str]] = [],
        search: bool = False,
    ) -> Tuple[List[Model], int]:
        """Get all authors from the database."""
        await self.validate_fields(Model, filters)

        # Generate statements according to search or filters
        if search:
            search_conditions = await self.generate_search_stmt(Model, filters)
            stmt = (
                select(Model)
                .distinct(Model.author)
                .filter(and_(*search_conditions, Model.author.is_not(None), Model.status == ModelStatusEnum.ACTIVE))
            )
            count_stmt = select(func.count().label("count")).select_from(
                select(Model.author)
                .distinct()
                .filter(and_(*search_conditions, Model.author.is_not(None), Model.status == ModelStatusEnum.ACTIVE))
                .alias("distinct_authors")
            )
        else:
            stmt = (
                select(Model)
                .distinct(Model.author)
                .filter_by(**filters)
                .filter(Model.author.is_not(None), Model.status == ModelStatusEnum.ACTIVE)
            )
            count_stmt = select(func.count().label("count")).select_from(
                select(Model.author)
                .distinct()
                .filter(Model.author.is_not(None), Model.status == ModelStatusEnum.ACTIVE)
                .alias("distinct_authors")
            )

        # Calculate count before applying limit and offset
        count = self.execute_scalar(count_stmt)

        # Apply limit and offset
        stmt = stmt.limit(limit).offset(offset)

        # Apply sorting
        if order_by:
            sort_conditions = await self.generate_sorting_stmt(Model, order_by)
            stmt = stmt.order_by(*sort_conditions)

        result = self.scalars_all(stmt)

        return result, count

    async def get_model_tree_count(self, uri: str) -> List[dict]:
        """Get the model tree count."""
        stmt = (
            select(Model.base_model_relation, func.count(Model.id).label("count"))
            .filter(
                Model.base_model.contains([uri]),
                Model.status == ModelStatusEnum.ACTIVE,
                Model.base_model_relation.is_not(None),
            )
            .group_by(Model.base_model_relation)
        )

        return self.execute_all(stmt)


class CloudModelDataManager(DataManagerUtils):
    """Data manager for the CloudModel model."""

    async def get_all_cloud_models_by_source_uris(self, provider: str, uris: List[str]) -> List[CloudModel]:
        """Get all cloud models from the database."""
        stmt = select(CloudModel).filter(CloudModel.uri.in_(uris), CloudModel.source == provider)
        return self.scalars_all(stmt)

    async def get_all_cloud_models(
        self,
        offset: int = 0,
        limit: int = 10,
        filters: Dict[str, Any] = {},
        order_by: List[Tuple[str, str]] = [],
        search: bool = False,
    ) -> Tuple[List[CloudModel], int]:
        """Get all cloud models from the database."""
        # Tags and tasks are not filterable
        # Also remove from filters dict
        explicit_conditions = []
        json_filters = {"tags": filters.pop("tags", []), "tasks": filters.pop("tasks", [])}
        explicit_filters = {
            "modality": filters.pop("modality", []),
            "author": filters.pop("author", []),
            "model_size_min": filters.pop("model_size_min", None),
            "model_size_max": filters.pop("model_size_max", None),
        }

        # Validate the remaining filters
        await self.validate_fields(CloudModel, filters)

        if json_filters["tags"]:
            # Either TagA or TagB exist in tag field
            tag_conditions = or_(
                *[CloudModel.tags.cast(JSONB).contains([{"name": tag_name}]) for tag_name in json_filters["tags"]]
            )
            explicit_conditions.append(tag_conditions)

        if json_filters["tasks"]:
            # Either TaskA or TaskB exist in task field
            task_conditions = or_(
                *[CloudModel.tasks.cast(JSONB).contains([{"name": task_name}]) for task_name in json_filters["tasks"]]
            )
            explicit_conditions.append(task_conditions)

        if explicit_filters["modality"]:
            # Check any of modality present in the field
            modality_condition = CloudModel.modality.in_(explicit_filters["modality"])
            explicit_conditions.append(modality_condition)

        if explicit_filters["author"]:
            # Check any of author present in the field
            author_condition = CloudModel.modality.in_(explicit_filters["author"])
            explicit_conditions.append(author_condition)

        if explicit_filters["model_size_min"] is not None or explicit_filters["model_size_max"] is not None:
            # Add model size range condition
            size_conditions = []
            if explicit_filters["model_size_min"] is not None:
                size_conditions.append(CloudModel.model_size >= explicit_filters["model_size_min"])
            if explicit_filters["model_size_max"] is not None:
                size_conditions.append(CloudModel.model_size <= explicit_filters["model_size_max"])
            size_condition = and_(*size_conditions)
            explicit_conditions.append(size_condition)

        # Generate statements according to search or filters
        if search:
            search_conditions = await self.generate_search_stmt(CloudModel, filters)
            stmt = (
                select(CloudModel)
                .filter(and_(or_(*search_conditions), CloudModel.status == CloudModelStatusEnum.ACTIVE))
                .where(or_(*explicit_conditions))
            )
            count_stmt = (
                select(func.count())
                .select_from(CloudModel)
                .filter(and_(or_(*search_conditions), CloudModel.status == CloudModelStatusEnum.ACTIVE))
                .where(or_(*explicit_conditions))
            )
        else:
            stmt = (
                select(CloudModel)
                .filter_by(**filters)
                .where(and_(*explicit_conditions))
                .filter(CloudModel.status == CloudModelStatusEnum.ACTIVE)
            )
            count_stmt = (
                select(func.count())
                .select_from(CloudModel)
                .filter_by(**filters)
                .where(and_(*explicit_conditions))
                .filter(CloudModel.status == CloudModelStatusEnum.ACTIVE)
            )

        # Calculate count before applying limit and offset
        count = self.execute_scalar(count_stmt)

        # Apply limit and offset
        stmt = stmt.limit(limit).offset(offset)

        # Apply sorting
        if order_by:
            sort_conditions = await self.generate_sorting_stmt(CloudModel, order_by)
            stmt = stmt.order_by(*sort_conditions)

        result = self.scalars_all(stmt)

        return result, count

    async def get_all_recommended_tags(
        self,
        offset: int = 0,
        limit: int = 10,
    ) -> Tuple[List[CloudModel], int]:
        """Get all recommended tags from the database."""
        stmt = (
            (
                select(
                    func.jsonb_array_elements(CloudModel.tags).op("->>")("name").label("name"),
                    func.jsonb_array_elements(CloudModel.tags).op("->>")("color").label("color"),
                    func.count().label("count"),
                )
                .select_from(CloudModel)
                .where(CloudModel.tags.is_not(None), CloudModel.status == CloudModelStatusEnum.ACTIVE)
                .group_by(
                    func.jsonb_array_elements(CloudModel.tags).op("->>")("name"),
                    func.jsonb_array_elements(CloudModel.tags).op("->>")("color"),
                )
            )
            .union_all(
                select(
                    func.jsonb_array_elements(CloudModel.tasks).op("->>")("name").label("name"),
                    func.jsonb_array_elements(CloudModel.tasks).op("->>")("color").label("color"),
                    func.count().label("count"),
                )
                .select_from(CloudModel)
                .where(CloudModel.tasks.is_not(None))
                .group_by(
                    func.jsonb_array_elements(CloudModel.tasks).op("->>")("name"),
                    func.jsonb_array_elements(CloudModel.tasks).op("->>")("color"),
                )
            )
            .order_by(desc("count"), "name")
            .offset(offset)
            .limit(limit)
        )

        count_stmt = select(func.count()).select_from(stmt)

        count = self.execute_scalar(count_stmt)

        result = self.execute_all(stmt)

        return result, count


class ModelLicensesDataManager(DataManagerUtils):
    """Data manager for the ModelLicenses model."""

    pass


class ModelSecurityScanResultDataManager(DataManagerUtils):
    """Data manager for the ModelSecurityScanResult model."""

    pass

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/user_ops/services.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The user ops package, containing essential business logic, services, and routing configurations for the user ops."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/user_ops/models.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The models package, containing the database models for the user ops."""

from datetime import datetime
from uuid import UUID, uuid4

from sqlalchemy import Boolean, DateTime, Enum, Integer, String, Uuid
from sqlalchemy.orm import Mapped, mapped_column, relationship

from budapp.cluster_ops.models import Cluster
from budapp.commons.constants import UserRoleEnum, UserStatusEnum
from budapp.commons.database import Base
from budapp.endpoint_ops.models import Endpoint
from budapp.model_ops.models import Model
from budapp.project_ops.models import Project, project_user_association


class User(Base):
    """User model."""

    __tablename__ = "user"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    auth_id: Mapped[UUID] = mapped_column(Uuid, unique=True, default=uuid4)
    name: Mapped[str] = mapped_column(String, nullable=False)
    email: Mapped[str] = mapped_column(String, unique=True, nullable=False)
    role: Mapped[str] = mapped_column(
        Enum(
            UserRoleEnum,
            name="user_role_enum",
            values_callable=lambda x: [e.value for e in x],
        )
    )
    status: Mapped[str] = mapped_column(
        Enum(
            UserStatusEnum,
            name="user_status_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        default=UserStatusEnum.INVITED.value,
    )
    password: Mapped[str] = mapped_column(String, nullable=False)
    is_superuser: Mapped[bool] = mapped_column(Boolean, default=False)
    is_reset_password: Mapped[bool] = mapped_column(Boolean, default=True)
    color: Mapped[str] = mapped_column(String, nullable=False)
    first_login: Mapped[bool] = mapped_column(Boolean, default=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    is_subscriber: Mapped[bool] = mapped_column(Boolean, default=False)
    reset_password_attempt: Mapped[int] = mapped_column(Integer, default=0)

    permission: Mapped["Permission"] = relationship(back_populates="user")  # one-to-one
    created_models: Mapped[list[Model]] = relationship(back_populates="created_user")

    # TODO: uncomment when implement individual fields
    # benchmarks: Mapped[list["Benchmark"]] = relationship(back_populates="user")
    # benchmark_results: Mapped[list["BenchmarkResult"]] = relationship(back_populates="user")
    projects: Mapped[list[Project]] = relationship(secondary=project_user_association, back_populates="users")
    # project_permissions: Mapped[list[ProjectPermission]] = relationship(
    #     back_populates="user"
    # )
    created_projects: Mapped[list[Project]] = relationship(back_populates="created_user")
    created_clusters: Mapped[list[Cluster]] = relationship(back_populates="created_user")
    created_endpoints: Mapped[list[Endpoint]] = relationship(back_populates="created_user")

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/user_ops/user_routes.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The model ops package, containing essential business logic, services, and routing configurations for the user ops."""

from typing import Union

from fastapi import APIRouter, Depends, status
from sqlalchemy.orm import Session
from typing_extensions import Annotated

from budapp.commons import logging
from budapp.commons.dependencies import get_current_active_invite_user, get_session
from budapp.commons.schemas import ErrorResponse
from budapp.user_ops.schemas import User

from .schemas import UserResponse


logger = logging.get_logger(__name__)

user_router = APIRouter(prefix="/users", tags=["user"])


@user_router.get(
    "/me",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": UserResponse,
            "description": "Successfully get current user",
        },
    },
    description="Get current user",
)
async def get_current_user(
    current_user: Annotated[User, Depends(get_current_active_invite_user)],
    session: Annotated[Session, Depends(get_session)],
) -> Union[UserResponse, ErrorResponse]:
    """Get current user."""
    try:
        logger.debug(f"Active user retrieved: {current_user.email}")
        return UserResponse(
            object="user.me", code=status.HTTP_200_OK, message="Successfully get current user", user=current_user
        ).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to get current user: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to get current user"
        ).to_http_response()

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/user_ops/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, containing essential business logic, services, and routing configurations for the user ops."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/user_ops/schemas.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The schemas package, containing the schemas for the user ops."""

from datetime import datetime

from pydantic import UUID4, BaseModel, ConfigDict, EmailStr, Field

from budapp.commons.constants import UserRoleEnum, UserStatusEnum
from budapp.commons.schemas import SuccessResponse


class UserBase(BaseModel):
    """Base user schema."""

    name: str = Field(min_length=1, max_length=100)
    email: EmailStr = Field(min_length=1, max_length=100)


class UserInfo(UserBase):
    """User response to client schema."""

    model_config = ConfigDict(from_attributes=True)

    id: UUID4
    color: str
    role: UserRoleEnum


class User(UserInfo):
    """User schema."""

    model_config = ConfigDict(from_attributes=True)

    auth_id: UUID4
    password: str
    status: UserStatusEnum
    created_at: datetime
    modified_at: datetime


class UserResponse(SuccessResponse):
    """User response to client schema."""

    model_config = ConfigDict(from_attributes=True)

    user: UserInfo

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/user_ops/crud.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The crud package, containing essential business logic, services, and routing configurations for the user ops."""

from budapp.commons import logging
from budapp.commons.db_utils import DataManagerUtils


logger = logging.get_logger(__name__)


class UserDataManager(DataManagerUtils):
    """Data manager for the User model."""

    pass

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/credential_ops/models.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The credential ops package, containing essential business logic, services, and routing configurations for the credential ops."""

from datetime import UTC, datetime
from uuid import UUID, uuid4

from sqlalchemy import DateTime, Enum, ForeignKey, Float, String, Uuid
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import Mapped, mapped_column, relationship

from budapp.commons.constants import CredentialTypeEnum
from budapp.commons.database import Base


class ProprietaryCredential(Base):
    """Proprietary model creds at global level : Credential model."""

    __tablename__ = "proprietary_credential"
    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    name: Mapped[str] = mapped_column(String, nullable=False)
    user_id: Mapped[UUID] = mapped_column(ForeignKey("user.id", ondelete="CASCADE"), nullable=False)
    type: Mapped[str] = mapped_column(
        Enum(
            CredentialTypeEnum,
            name="proprietary_credential_type_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=False,
    )

    # placeholder for api base, project, organization, etc.
    other_provider_creds: Mapped[dict] = mapped_column(JSONB, nullable=True)

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.now(UTC))
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.now(UTC), onupdate=datetime.now(UTC))

    endpoints: Mapped[list["Endpoint"]] = relationship("Endpoint", back_populates="credential")


class Credential(Base):
    """Project API Keys : Credential model"""

    __tablename__ = "credential"
    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    user_id: Mapped[UUID] = mapped_column(ForeignKey("user.id", ondelete="CASCADE"), nullable=False)
    key: Mapped[str] = mapped_column(String, nullable=False, unique=True)
    project_id: Mapped[UUID] = mapped_column(ForeignKey("project.id", ondelete="CASCADE"), nullable=True)
    expiry: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    max_budget: Mapped[float] = mapped_column(Float, nullable=True)

    # placeholder for per model budgets : {"model_id": "budget"}
    model_budgets: Mapped[dict] = mapped_column(JSONB, nullable=True)
    
    last_used_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    
    name: Mapped[str] = mapped_column(String, nullable=False)
    hashed_key: Mapped[str] = mapped_column(String, nullable=True)

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    project: Mapped["Project"] = relationship("Project", foreign_keys=[project_id])



```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/credential_ops/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, containing essential business logic, services, and routing configurations for the credential ops."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/credential_ops/schemas.py`:

```py
from datetime import datetime

from pydantic import BaseModel, ConfigDict, model_validator

from budapp.commons import logging
from budapp.commons.schemas import CloudEventBase


logger = logging.get_logger(__name__)


class CredentialUpdatePayload(BaseModel):
    hashed_key: str
    last_used_at: datetime

class CredentialUpdateRequest(CloudEventBase):
    """Request to update the credential last used at time."""

    model_config = ConfigDict(extra="allow")

    payload: CredentialUpdatePayload

    @model_validator(mode="before")
    def log_credential_update(cls, data):
        """Log the credential update hits for debugging purposes."""
        # TODO: remove this function after Debugging
        logger.info("================================================")
        logger.info("Received hit in credentials/update:")
        logger.info(f"{data}")
        logger.info("================================================")
        return data

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/credential_ops/credential_routes.py`:

```py
from typing import Annotated

from fastapi import APIRouter, Depends, status
from sqlalchemy.orm import Session

from budapp.commons import logging
from budapp.commons.dependencies import get_session
from budapp.commons.exceptions import ClientException

from ..commons.api_utils import pubsub_api_endpoint
from ..commons.schemas import ErrorResponse, SuccessResponse
from .crud import CredentialDataManager
from .models import Credential
from .schemas import CredentialUpdateRequest


logger = logging.get_logger(__name__)

credential_router = APIRouter(prefix="/credentials", tags=["credential"])


@credential_router.post("/update")
@pubsub_api_endpoint(request_model=CredentialUpdateRequest)
async def update_credential(
    credential_update_request: CredentialUpdateRequest,
    session: Annotated[Session, Depends(get_session)],
):
    """Update the credential last used at time."""
    logger.debug("Received request to subscribe to bud-serve-app credential update")
    try:
        payload = credential_update_request.payload
        logger.debug(f"Update CredentialReceived payload: {payload}")
        db_credential = await CredentialDataManager(session).retrieve_by_fields(Credential, {"hashed_key": payload.hashed_key})
        db_last_used_at = db_credential.last_used_at
        if db_last_used_at is None or db_last_used_at < payload.last_used_at:
            await CredentialDataManager(session).update_by_fields(db_credential, {"last_used_at": payload.last_used_at})
        return SuccessResponse(message="Credential updated successfully").to_http_response()
    except ClientException as e:
        logger.exception(f"Failed to execute credential update: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to update credential: {e}")
        return ErrorResponse(code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to update credential").to_http_response()

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/credential_ops/crud.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The crud package, containing essential business logic, services, and routing configurations for the credential ops."""

from budapp.commons import logging
from budapp.commons.db_utils import DataManagerUtils


logger = logging.get_logger(__name__)


class CredentialDataManager(DataManagerUtils):
    """Data manager for the Credential model."""

    pass


class ProprietaryCredentialDataManager(DataManagerUtils):
    """Data manager for the ProprietaryCredential model."""

    pass

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The main package initializer. This package provides tools, utilities, and best practices for developing Python microservices using Dapr."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/shared/http_client.py`:

```py
from enum import Enum
from typing import Any, AsyncIterator, Callable, Dict, Iterable, Optional, Union

import aiohttp
import ujson as json
from fastapi.responses import Response, StreamingResponse


class RESTMethods(Enum):
    """Enumeration of HTTP REST methods.

    This enum defines the standard HTTP methods used in RESTful APIs.

    Attributes:
        GET (str): Represents the HTTP GET method, used for retrieving resources.
        POST (str): Represents the HTTP POST method, used for creating new resources.
        DELETE (str): Represents the HTTP DELETE method, used for deleting resources.
        PUT (str): Represents the HTTP PUT method, used for updating existing resources.
    """

    GET = "GET"
    POST = "POST"
    DELETE = "DELETE"
    PUT = "PUT"


class AsyncHTTPClient:
    """An asynchronous HTTP client for making API requests.

    This class provides methods for sending HTTP requests asynchronously using aiohttp.
    It supports various HTTP methods, request parameters, and streaming responses.

    Attributes:
        timeout (aiohttp.ClientTimeout): The timeout for HTTP requests.
        connector (aiohttp.TCPConnector): The TCP connector for managing connections.
        session (Optional[aiohttp.ClientSession]): The aiohttp client session.
    """

    __slots__ = ("timeout", "connector", "session")

    def __init__(self, timeout: Optional[int] = None, max_connections: int = 100) -> None:
        """Initialize the AsyncHTTPClient.

        Args:
            timeout (Optional[int]): The timeout for HTTP requests in seconds.
            max_connections (int): The maximum number of connections to keep in the pool.
        """
        self.timeout = aiohttp.ClientTimeout(timeout)
        self.connector = aiohttp.TCPConnector(limit=max_connections)
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self) -> "AsyncHTTPClient":
        """Enter the runtime context and create a new client session.

        Returns:
            AsyncHTTPClient: The instance of the client.
        """
        self.session = aiohttp.ClientSession(timeout=self.timeout, connector=self.connector, json_serialize=json.dumps)
        return self

    async def __aexit__(self, exc_type: Optional[type], exc_val: Optional[Exception], exc_tb: Optional[Any]) -> None:
        """Exit the runtime context and close the client session."""
        if self.session:
            await self.session.close()

    async def send_request(
        self,
        method: str,
        url: str,
        data: Union[bytes, Iterable[bytes], Dict[str, Any], None] = None,
        json: Optional[Dict[str, Any]] = None,
        params: Union[str, Dict[str, Any], None] = None,
        headers: Optional[Dict[str, str]] = None,
        cookies: Optional[Dict[str, str]] = None,
        auth: Optional[Callable[..., Any]] = None,
        follow_redirects: bool = False,
        raise_for_status: bool = True,
        redirect_response: bool = False,
        streaming: bool = False,
    ) -> Union[Response, StreamingResponse]:
        """Send an HTTP request asynchronously.

        Args:
            method (str): The HTTP method to use.
            url (str): The URL to send the request to.
            data (Union[bytes, Iterable[bytes], Dict[str, Any], None]): The request body.
            json (Optional[Dict[str, Any]]): JSON data to send in the request body.
            params (Union[str, Dict[str, Any], None]): Query parameters to append to the URL.
            headers (Optional[Dict[str, str]]): Additional headers to send with the request.
            cookies (Optional[Dict[str, str]]): Cookies to send with the request.
            auth (Optional[Callable[..., Any]]): Callable to enable authentication.
            follow_redirects (bool): Whether to follow redirects.
            raise_for_status (bool): Whether to raise an exception for non-2xx status codes.
            redirect_response (bool): Whether to return a redirect response.
            streaming (bool): Whether to return a streaming response.

        Returns:
            Union[Response, StreamingResponse]: The HTTP response.

        Raises:
            AssertionError: If an invalid HTTP method is provided.
        """
        assert hasattr(RESTMethods, method.upper()), f"{method} is not a valid REST method."
        method = RESTMethods(method.upper()).value

        if self.session is None:
            raise RuntimeError("Session is not initialized. Use AsyncHTTPClient as a context manager.")

        async with self.session.request(
            method,
            url,
            data=data,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            auth=auth,
            allow_redirects=follow_redirects,
        ) as client_response:
            if raise_for_status:
                client_response.raise_for_status()

            if streaming and not redirect_response:
                return self._stream_response(client_response)
            else:
                return await self._build_response(client_response, streaming=streaming, incl_headers=False)

    async def send_streaming_request(self, *args: Any, **kwargs: Any) -> Union[Response, StreamingResponse]:
        """Send a streaming HTTP request.

        This method is a wrapper around send_request with streaming set to True.

        Args:
            *args: Positional arguments to pass to send_request.
            **kwargs: Keyword arguments to pass to send_request.

        Returns:
            Union[Response, StreamingResponse]: The HTTP response.
        """
        kwargs["streaming"] = True
        return await self.send_request(*args, **kwargs)

    async def redirect_streaming_request(self, *args: Any, **kwargs: Any) -> Union[Response, StreamingResponse]:
        """Send a streaming HTTP request that allows redirects.

        This method is a wrapper around send_request with streaming and redirect_response set to True.

        Args:
            *args: Positional arguments to pass to send_request.
            **kwargs: Keyword arguments to pass to send_request.

        Returns:
            Union[Response, StreamingResponse]: The HTTP response.
        """
        kwargs["streaming"] = True
        kwargs["redirect_response"] = True
        return await self.send_request(*args, **kwargs)

    @staticmethod
    async def _build_response(
        client_response: aiohttp.ClientResponse,
        streaming: bool = False,
        incl_headers: bool = False,
    ) -> Union[Response, StreamingResponse]:
        """Build a FastAPI response from an aiohttp ClientResponse.

        Args:
            client_response (aiohttp.ClientResponse): The aiohttp response to convert.
            streaming (bool): Whether to return a streaming response.
            incl_headers (bool): Whether to include headers in the response.

        Returns:
            Union[Response, StreamingResponse]: The FastAPI response.
        """
        content_type = client_response.headers.get("content-type", None)
        if incl_headers:
            response_headers = {k: v for k, v in client_response.headers.items() if k.lower() != "transfer-encoding"}
        else:
            response_headers = {}

        if content_type is not None:
            response_headers["Content-Type"] = content_type

        if not streaming:
            content = await client_response.content.read()
            return Response(
                content=content,
                status_code=client_response.status,
                headers=response_headers or None,
            )
        else:
            # FIXME: Streaming is getting stuck and not getting any results.
            return StreamingResponse(
                content=client_response.content.iter_any(),
                status_code=client_response.status,
                # media_type="text/event-stream",
                headers=response_headers or None,
                # background=BackgroundTask(client_response.close),
            )

    @staticmethod
    async def _stream_response(client_response: aiohttp.ClientResponse) -> AsyncIterator[bytes]:
        """Stream the response content.

        Args:
            client_response (aiohttp.ClientResponse): The aiohttp response to stream.

        Yields:
            bytes: Chunks of the response content.
        """
        async for chunk in client_response.content:
            yield chunk

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/shared/notification_service.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Provides shared functions for managing notification service."""

from typing import Any, Dict, List, Optional, Union

import aiohttp

from ..commons import logging
from ..commons.config import app_settings
from ..commons.constants import (
    BUD_NOTIFICATION_WORKFLOW,
    NotificationCategory,
    NotificationStatus,
)
from ..core.schemas import NotificationContent, NotificationPayload, NotificationRequest


logger = logging.get_logger(__name__)


class NotificationBuilder:
    """Builder class for notification."""

    def __init__(self):
        """Initialize the builder."""
        self.content = None
        self.payload = None
        self.notification_request = None

    def set_content(
        self,
        *,
        title: Optional[str] = None,
        message: Optional[str] = None,
        icon: Optional[str] = None,
        tag: Optional[str] = None,
        result: Optional[Dict[str, Any]] = None,
        status: NotificationStatus = NotificationStatus.COMPLETED,
    ) -> "NotificationBuilder":
        """Set the content for the notification."""
        self.content = NotificationContent(
            title=title, message=message, icon=icon, tag=tag, status=status, result=result
        )
        return self

    def set_payload(
        self,
        *,
        category: NotificationCategory = NotificationCategory.INAPP,
        type: str = None,
        source: str = app_settings.source_topic,
        workflow_id: str = None,
    ) -> "NotificationBuilder":
        """Set the payload for the notification."""
        self.payload = NotificationPayload(
            category=category, type=type, source=source, content=self.content, workflow_id=workflow_id
        )
        return self

    def set_notification_request(
        self, *, subscriber_ids: Union[str, List[str]], name: str = BUD_NOTIFICATION_WORKFLOW
    ) -> "NotificationBuilder":
        """Build the notification request."""
        self.notification_request = NotificationRequest.model_construct(
            name=name, subscriber_ids=subscriber_ids, payload=self.payload
        )
        return self

    def build(self) -> NotificationRequest:
        """Build the notification request."""
        notification = self.notification_request
        self.reset()
        return notification

    def reset(self) -> "NotificationBuilder":
        """Reset the builder."""
        self.content = None
        self.payload = None
        self.notification_request = None
        return self


class BudNotifyService:
    """Service for sending notifications."""

    def __init__(self):
        """Initialize the notification service."""
        self.notification_endpoint = (
            f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_notify_app_id}/method/notifications"
        )

    async def send_notification(self, notification: NotificationRequest) -> dict:
        """Send a notification.

        Args:
            notification (NotificationRequest): The notification to send

        Returns:
            dict: The response from the notification service
        """
        payload = notification.model_dump(exclude_none=True, mode="json")
        logger.debug(f"Sending notification with payload {payload}")

        try:
            async with aiohttp.ClientSession() as session, session.post(
                self.notification_endpoint, json=payload
            ) as response:
                response_data = await response.json()
                if response.status != 200:
                    logger.error(f"Failed to send notification: {response.status} {response_data}")

                logger.debug("Successfully sent notification")
                return response_data
        except Exception as e:
            logger.exception(f"Failed to send notification: {e}")

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/shared/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The shared package, containing shared services and components that are used across multiple microservices within the project."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/shared/dapr_service.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Provides utility functions and wrappers for interacting with Dapr components, including service invocation, pub/sub, and state management."""

import uuid
from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union

import ujson as json
from aiohttp import ClientConnectionError, ClientError
from dapr.clients import DaprClient
from dapr.clients.grpc._state import Concurrency, Consistency, StateOptions
from dapr.clients.grpc.client import ConfigurationResponse
from dapr.conf import settings as dapr_settings

from budapp.commons import logging
from budapp.commons.config import app_settings, secrets_settings
from budapp.commons.exceptions import SuppressAndLog
from budapp.commons.resiliency import retry

from .http_client import AsyncHTTPClient


logger = logging.get_logger(__name__)


class ServiceRegistrationException(Exception):
    """Exception raised when there is an error during service registration.

    This exception is used to indicate that an attempt to register a service
    with Dapr or a service registry has failed. It can be used to catch and
    handle specific registration-related errors in the application.
    """

    pass


class DaprService(DaprClient):
    """A service class for interacting with Dapr, providing methods for syncing configurations and secrets.

    Inherits from:
        DaprClient: Base class for Dapr client operations.

    Args:
        api_method_invocation_protocol (Optional[str]): The protocol used for API method invocation.
            Defaults to the value in `app_settings.dapr_api_method_invocation_protocol`.
        health_timeout (Optional[int]): Timeout for health checks. Defaults to the value in
            `app_settings.dapr_health_timeout`.
        **kwargs: Additional keyword arguments passed to the `DaprClient` constructor.
    """

    def __init__(
        self,
        dapr_http_port: Optional[int] = app_settings.dapr_http_port,
        dapr_api_token: Optional[str] = secrets_settings.dapr_api_token,
        api_method_invocation_protocol: Optional[str] = app_settings.dapr_api_method_invocation_protocol,
        health_timeout: Optional[int] = app_settings.dapr_health_timeout,
        **kwargs: Any,
    ):
        """Initialize the DaprService with optional API method invocation protocol and health timeout.

        Args:
            api_method_invocation_protocol (Optional[str]): The protocol for API method invocation.
            health_timeout (Optional[int]): Timeout for health checks.
            **kwargs: Additional keyword arguments for the `DaprClient` initialization.
        """
        settings_updates = {
            "DAPR_HTTP_PORT": dapr_http_port,
            "DAPR_API_TOKEN": dapr_api_token,
            "DAPR_API_METHOD_INVOCATION_PROTOCOL": api_method_invocation_protocol,
            "DAPR_HEALTH_TIMEOUT": health_timeout,
        }

        for attr, value in settings_updates.items():
            if value is not None:
                setattr(dapr_settings, attr, value)

        super().__init__(**kwargs)

    @retry(
        max_attempts=10,
        delay=1,
        backoff_factor=2,
        exceptions_to_retry=(ClientError, ClientConnectionError),
    )
    async def register_service(self) -> None:
        """Register the service with Dapr and retrieve metadata.

        This method attempts to register the service by fetching metadata from the Dapr sidecar,
        parsing the component information, and saving it to the state store. It will retry up to
        10 times with exponential backoff in case of connection errors.

        Returns:
            None

        Raises:
            ServiceRegistrationException: If the service registration fails after all retry attempts
                or if there's an error in parsing the metadata.

        Note:
            This method updates app_settings with the names of various components (configstore,
            secretstore, statestore, pubsub) and related information (pubsub topic, dead letter topic)
            based on the metadata received from Dapr.
        """
        async with AsyncHTTPClient(timeout=100) as client:
            response = await client.send_request(
                "GET",
                f"http://localhost:{app_settings.dapr_http_port}/v1.0/metadata",
                headers={"dapr-api-token": secrets_settings.dapr_api_token}
                if secrets_settings.dapr_api_token is not None
                else None,
                raise_for_status=False,
            )
            body = response.body.decode()
            if response.status_code != 200:
                raise ServiceRegistrationException(
                    f"Service registration failed with metadata resolution error <{response.status_code}:{body}>."
                ) from None

            metadata = json.loads(body)

        service_info: Dict[str, Optional[str]] = {
            "app_name": metadata["id"],
            "configstore": None,
            "secretstore": None,
            "statestore": None,
            "pubsub": None,
            "topic": None,
            "deadletter": None,
        }
        try:
            for component in metadata["components"]:
                if component["type"].startswith("configuration."):
                    service_info["configstore"] = component["name"]
                    app_settings.configstore_name = component["name"]
                elif component["type"].startswith("secretstores."):
                    service_info["secretstore"] = component["name"]
                    app_settings.secretstore_name = component["name"]
                elif component["type"].startswith("state."):
                    service_info["statestore"] = component["name"]
                    app_settings.statestore_name = component["name"]

            for subscription in metadata["subscriptions"]:
                service_info["pubsub"] = subscription["pubsubname"]
                service_info["topic"] = subscription["topic"]
                service_info["deadletter"] = subscription["deadLetterTopic"]

                app_settings.pubsub_name = subscription["pubsubname"]
                app_settings.pubsub_topic = subscription["topic"]
                app_settings.dead_letter_topic = subscription["deadLetterTopic"]
        except KeyError as e:
            raise ServiceRegistrationException(
                f"Service registration failed with metadata parse error {str(e)}."
            ) from None

        assert service_info["statestore"], "statestore is not configured."

        failures = 0
        while failures <= 5:
            try:
                await self.save_to_statestore(
                    f"__metadata__{service_info['app_name']}",
                    service_info,
                    store_name=service_info["statestore"],
                    concurrency="first_write",
                    consistency="strong",
                )
                logger.info("Service registration successful.")
                return
            except Exception as e:
                logger.exception(f"Service registration failed with error {str(e)}.")
                failures += 1

        raise ServiceRegistrationException("Service registration failed.")

    def get_service_metadata_by_id(self, app_id: str, store_name: Optional[str] = None) -> Dict[str, Any]:
        """Retrieve service metadata for a given application ID from the state store.

        Args:
            app_id (str): The ID of the application to retrieve metadata for.
            store_name (Optional[str]): The name of the state store to use. If not provided,
                                        it will use the default store name from app_settings.

        Returns:
            dict: The service metadata for the specified application ID.

        Raises:
            AssertionError: If the state store is not configured.

        Note:
            This method assumes that the metadata is stored with a key format of "__metadata__{app_id}".
        """
        store_name = store_name or app_settings.statestore_name
        assert store_name, "statestore is not configured."
        resp = self.get_state(store_name=store_name, key=f"__metadata__{app_id}")
        return json.loads(resp.data.decode("utf-8"))  # type: ignore

    @SuppressAndLog(Exception, _logger=logger, default_return=({}, None))
    async def sync_configurations(
        self,
        keys: Union[str, List[str]],
        store_name: Optional[str] = None,
        subscription_callback: Optional[Callable[[str, ConfigurationResponse], None]] = None,
    ) -> Tuple[Dict[str, Any], Optional[str]]:
        """Sync configurations from the specified config store and optionally subscribe to configuration changes.

        Args:
            keys (str | List[str]): The configuration keys to sync.
            store_name (Optional[str]): The name of the configuration store. Defaults to `app_settings.configstore_name`.
            subscription_callback (Optional[Callable[[str, ConfigurationResponse], None]]): Optional callback
                function for handling configuration updates. If provided, will subscribe to configuration changes.

        Returns:
            Tuple[Dict[str, Any], Optional[str]]: A tuple containing a dictionary of configurations and an optional
            subscription ID. The dictionary maps keys to their corresponding configuration values.
        """
        store_name = store_name or app_settings.configstore_name
        assert store_name, "configstore is not configured."
        config: Dict[str, Any] = {}
        if store_name:
            keys = [keys] if isinstance(keys, str) else keys
            try:
                configuration = self.get_configuration(store_name=store_name, keys=keys, config_metadata={})
                logger.info("Found %d/%d configurations, syncing...", len(configuration.items), len(keys))
                config = {key: configuration.items[key].value for key in configuration.items}
            except Exception as e:
                logger.exception("Failed to get configurations: %s", str(e))

        sub_id: Optional[str] = None
        if store_name and subscription_callback is not None:
            try:
                # FIXME: subscription gets stopped with the following message when the app receives a request
                #  configstore configuration watcher for keys ['fastapi_soa.debug'] stopped.
                sub_id = self.subscribe_configuration(
                    store_name=store_name, keys=keys, handler=subscription_callback, config_metadata={}
                )
                logger.debug("Successfully subscribed to config store with subscription id: %s", sub_id)
            except Exception as e:
                logger.exception("Failed to subscribe to config store: %s", str(e))

        return config, sub_id

    @SuppressAndLog(Exception, _logger=logger, default_return={})
    async def sync_secrets(
        self,
        keys: Union[str, List[str]],
        store_name: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Sync secrets from the specified secret store.

        Args:
            keys (str | List[str]): The secret keys to sync.
            store_name (str): The name of the secret store. Defaults to `app_settings.secretstore_name`.

        Returns:
            Dict[str, Any]: A dictionary of secrets where each key maps to its corresponding secret value.
        """
        store_name = store_name or app_settings.secretstore_name
        assert store_name, "secretstore is not configured."
        secrets: Dict[str, Any] = {}
        if store_name:
            keys = [keys] if isinstance(keys, str) else keys
            for key in keys:
                try:
                    secrets[key] = self.get_secret(store_name=store_name, key=key).secret.get(key)
                except Exception as e:
                    logger.error("Failed to get secret: %s", str(e))

            logger.info("Found %d/%d secrets, syncing...", len(secrets), len(keys))

        return secrets

    async def unsync_configurations(self, sub_id: str, store_name: Optional[str] = None) -> bool:
        """Unsubscribe from configuration updates and stop syncing.

        Args:
            store_name (str): The name of the configuration store.
            sub_id (str): The subscription ID to unsubscribe from.

        Returns:
            bool: True if successfully unsubscribed, False otherwise.
        """
        store_name = store_name or app_settings.configstore_name
        is_success = False

        if sub_id:
            try:
                is_success = self.unsubscribe_configuration(store_name=store_name, id=sub_id)
                logger.debug("Unsubscribed successfully? %s", is_success)
            except Exception as e:
                logger.exception("Failed to unsubscribe from config store: %s", str(e))

        return is_success

    async def save_to_statestore(
        self,
        key: str,
        value: Union[Dict[str, Any], str],
        etag: Optional[str] = None,
        store_name: Optional[str] = None,
        concurrency: Literal["first_write", "last_write", "unspecified"] = "unspecified",
        consistency: Literal["eventual", "strong", "unspecified"] = "unspecified",
        ttl: Optional[int] = None,
        skip_etag_if_unset: bool = False,
    ) -> None:
        """Save a key-value pair to the state store.

        Args:
            key (str): The key to save the value under.
            value (Union[Dict[str, Any], str]): The value to save. Can be a dictionary or a string.
            etag (Optional[str]): The etag for optimistic concurrency control. If None and skip_etag_if_unset is False, it will be fetched.
            store_name (Optional[str]): The name of the state store. If None, uses the default from app settings.
            concurrency (Literal["first_write", "last_write", "unspecified"]): The concurrency mode for the operation.
            consistency (Literal["eventual", "strong", "unspecified"]): The consistency mode for the operation.
            ttl (Optional[int]): Time-to-live for the state in seconds.
            skip_etag_if_unset (bool): If True, skips fetching the etag when it's not provided.

        Raises:
            AssertionError: If the state store is not configured or if invalid concurrency or consistency options are provided.
        """
        store_name = store_name or app_settings.statestore_name
        assert store_name, "statestore is not configured."
        if etag is None and not skip_etag_if_unset:
            resp = self.get_state(store_name=store_name, key=key)
            etag = resp.etag

        assert concurrency is None or hasattr(
            Concurrency, concurrency
        ), f"{concurrency} is not a valid concurrency, choose from (first_write, last_write)"
        assert consistency is None or hasattr(
            Consistency, consistency
        ), f"{consistency} is not a valid consistency, choose from (eventual, strong)"

        concurrency = concurrency or "unspecified"
        consistency = consistency or "unspecified"
        state_options = StateOptions(
            concurrency=getattr(Concurrency, concurrency), consistency=getattr(Consistency, consistency)
        )

        state_metadata = {}
        if ttl is not None:
            state_metadata["ttlInSeconds"] = str(ttl)
        if isinstance(value, dict):
            value = json.dumps(value)
            state_metadata["contentType"] = "application/json"

        self.save_state(store_name, key, value, etag, state_options, state_metadata)

    async def publish_to_topic(
        self,
        data: Dict[str, Any],
        pubsub_name: Optional[str] = None,
        target_topic_name: Optional[str] = None,
        target_name: Optional[str] = None,
        source_topic_name: Optional[str] = None,
        source_name: str = app_settings.name,
        event_type: Optional[str] = None,
    ) -> str:
        """Publish data to a specified pubsub topic.

        Args:
            data (Dict[str, Any]): The data to publish.
            pubsub_name (Optional[str]): The name of the pubsub component. If not provided, uses the default from app settings.
            target_topic_name (Optional[str]): The name of the topic to publish to. Either this or target_name must be provided.
            target_name (Optional[str]): The name of the target service. Used to resolve the topic name if target_topic_name is not provided.
            source_topic_name (Optional[str]): The name of the source topic. If not provided, uses the default from app settings.
            source_name (str): The app name of the source event. Defaults to the value of `app_settings.name`.
            event_type (Optional[str]): The type of the event. If provided, it will be included in the CloudEvent metadata.

        Returns:
            str: The ID of the published CloudEvent.

        Raises:
            DaprInternalError: If there's an error while publishing the event.
            AssertionError: If neither target_topic_name nor target_name is provided, or if pubsub is not configured.

        Note:
            - If 'workflow' is not in the data and event_type is provided, event_type is used as the workflow.
        """
        assert target_topic_name or target_name, "Either target_topic_name or target_name is required."
        if target_topic_name is None:
            metadata = self.get_service_metadata_by_id(str(target_name))
            target_topic_name = metadata.get("topic") if isinstance(metadata, dict) else None
            assert target_topic_name, f"Failed to resolve pubsub topic for {target_name}"

        pubsub_name = pubsub_name or app_settings.pubsub_name
        source_topic_name = source_topic_name or app_settings.pubsub_topic
        assert pubsub_name, "pubsub is not configured."

        event_id = str(uuid.uuid4())
        publish_metadata = {
            "cloudevent.id": event_id,
            "cloudevent.source": source_name,
            "cloudevent.type": event_type,
        }
        publish_metadata = {k: v for k, v in publish_metadata.items() if v is not None}
        data.update({"source": source_name, "source_topic_name": source_topic_name})
        if data.get("workflow") is None and event_type is not None:
            data["workflow"] = event_type

        self.publish_event(
            pubsub_name=pubsub_name,
            topic_name=target_topic_name,
            data=json.dumps(data),
            data_content_type="application/cloudevents+json",
            publish_metadata=publish_metadata,
        )

        logger.debug("Published to pubsub topic %s/%s", pubsub_name, target_topic_name)

        return event_id

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/shared/singleton.py`:

```py
from threading import Lock


class SingletonMeta(type):
    """This is a thread-safe implementation of Singleton.

    Ref: https://refactoring.guru/design-patterns/singleton/python/example#example-1
    """

    _instances = {}
    _lock: Lock = Lock()
    """We now have a lock object that will be used to synchronize threads during
    first access to the Singleton.
    """

    def __call__(cls, *args, **kwargs):
        """Possible changes to the value of the `__init__` argument do not affect the returned instance."""
        with cls._lock:
            if cls not in cls._instances:
                instance = super().__call__(*args, **kwargs)
                cls._instances[cls] = instance
        return cls._instances[cls]

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/shared/redis_service.py`:

```py
from typing import Optional, Union

import redis.asyncio as aioredis
from redis.typing import AbsExpiryT, EncodableT, ExpiryT, KeyT, PatternT, ResponseT

from ..commons import logging
from ..commons.config import secrets_settings
from ..commons.exceptions import RedisException
from .singleton import SingletonMeta


logger = logging.get_logger(__name__)


class RedisSingleton(metaclass=SingletonMeta):
    """Redis singleton class."""

    _redis_client: Optional[aioredis.Redis] = None

    def __init__(self):
        """Initialize the Redis singleton."""
        if not self._redis_client:
            pool = aioredis.ConnectionPool.from_url(secrets_settings.redis_url)
            self._redis_client = aioredis.Redis.from_pool(pool)

    async def __aenter__(self):
        """Enter the context manager."""
        return self._redis_client

    async def __aexit__(self, exc_type, exc_value, traceback):
        """Exit the context manager."""
        if self._redis_client:
            await self._redis_client.aclose()


class RedisService:
    """Redis service class."""

    def __init__(self):
        """Initialize the Redis service."""
        self.redis_singleton = RedisSingleton()

    async def set(
        self,
        name: KeyT,
        value: EncodableT,
        ex: Union[ExpiryT, None] = None,
        px: Union[ExpiryT, None] = None,
        nx: bool = False,
        xx: bool = False,
        keepttl: bool = False,
        get: bool = False,
        exat: Union[AbsExpiryT, None] = None,
        pxat: Union[AbsExpiryT, None] = None,
    ) -> ResponseT:
        """Set a key-value pair in Redis."""
        async with self.redis_singleton as redis:
            try:
                return await redis.set(name, value, ex, px, nx, xx, keepttl, get, exat, pxat)
            except Exception as e:
                logger.exception(f"Error setting Redis key: {e}")
                raise RedisException(f"Error setting Redis key {name}") from e

    async def get(self, name: KeyT) -> ResponseT:
        """Get a value from Redis."""
        async with self.redis_singleton as redis:
            try:
                return await redis.get(name)
            except Exception as e:
                logger.exception(f"Error getting Redis key: {e}")
                raise RedisException(f"Error getting Redis key {name}") from e

    async def keys(self, pattern: PatternT, **kwargs) -> ResponseT:
        """Get all keys matching the pattern."""
        async with self.redis_singleton as redis:
            try:
                return await redis.keys(pattern, **kwargs)
            except Exception as e:
                logger.exception(f"Error getting Redis keys: {e}")
                raise RedisException("Error getting Redis keys") from e

    async def delete(self, *names: KeyT) -> ResponseT:
        """Delete a key from Redis."""
        async with self.redis_singleton as redis:
            try:
                return await redis.delete(*names)
            except Exception as e:
                logger.exception(f"Error deleting Redis key: {e}")
                raise RedisException(f"Error deleting Redis key {names}") from e

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/metric_ops/services.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The metric ops services. Contains business logic for metric ops."""

from typing import Dict
from uuid import UUID

import aiohttp
from fastapi import status

from budapp.commons import logging
from budapp.commons.config import app_settings
from budapp.commons.db_utils import SessionMixin
from budapp.commons.exceptions import ClientException

from ..cluster_ops.crud import ClusterDataManager
from ..cluster_ops.models import Cluster as ClusterModel
from ..commons.constants import (
    ClusterStatusEnum,
    EndpointStatusEnum,
    ModelProviderTypeEnum,
    ModelStatusEnum,
    ProjectStatusEnum,
)
from ..endpoint_ops.crud import EndpointDataManager
from ..endpoint_ops.models import Endpoint as EndpointModel
from ..model_ops.crud import ModelDataManager
from ..model_ops.models import Model
from ..project_ops.crud import ProjectDataManager
from ..project_ops.models import Project as ProjectModel
from .schemas import (
    CountAnalyticsRequest,
    CountAnalyticsResponse,
    DashboardStatsResponse,
    PerformanceAnalyticsRequest,
    PerformanceAnalyticsResponse,
)


logger = logging.get_logger(__name__)


class MetricService(SessionMixin):
    """Metric service."""

    async def get_request_count_analytics(
        self,
        request: CountAnalyticsRequest,
    ) -> CountAnalyticsResponse:
        """Get request count analytics."""
        bud_metric_response = await self._perform_request_count_analytics(request)

        return CountAnalyticsResponse(
            code=status.HTTP_200_OK,
            object="request.count.analytics",
            message="Successfully fetched request count analytics",
            overall_metrics=bud_metric_response["overall_metrics"],
            concurrency_metrics=bud_metric_response["concurrency_metrics"],
            queuing_time_metrics=bud_metric_response["queuing_time_metrics"],
            global_metrics=bud_metric_response["global_metrics"],
        )

    async def get_request_performance_analytics(
        self,
        request: PerformanceAnalyticsRequest,
    ) -> PerformanceAnalyticsResponse:
        """Get request performance analytics."""
        bud_metric_response = await self._perform_request_performance_analytics(request)

        return PerformanceAnalyticsResponse(
            code=status.HTTP_200_OK,
            object="request.performance.analytics",
            message="Successfully fetched request performance analytics",
            ttft_metrics=bud_metric_response["ttft_metrics"],
            latency_metrics=bud_metric_response["latency_metrics"],
            throughput_metrics=bud_metric_response["throughput_metrics"],
        )

    @staticmethod
    async def _perform_request_count_analytics(
        metric_request: CountAnalyticsRequest,
    ) -> Dict:
        """Get request count analytics."""
        request_count_analytics_endpoint = f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_metrics_app_id}/method/metrics/analytics/request-counts"

        logger.debug(
            f"Performing request count analytics request to bud_metric {metric_request.model_dump(exclude_none=True, exclude_unset=True, mode='json')}"
        )
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    request_count_analytics_endpoint,
                    json=metric_request.model_dump(exclude_none=True, exclude_unset=True, mode="json"),
                ) as response:
                    response_data = await response.json()
                    if response.status != status.HTTP_200_OK:
                        logger.error(f"Failed to get request count analytics: {response.status} {response_data}")
                        raise ClientException(
                            "Failed to get request count analytics", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
                        )

                    logger.debug("Successfully get request count analytics from budmetric")
                    return response_data
        except Exception as e:
            logger.exception(f"Failed to send request count analytics request: {e}")
            raise ClientException(
                "Failed to get request count analytics", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
            ) from e

    @staticmethod
    async def _perform_request_performance_analytics(
        metric_request: PerformanceAnalyticsRequest,
    ) -> Dict:
        """Get request performance analytics."""
        request_performance_analytics_endpoint = f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_metrics_app_id}/method/metrics/analytics/request-performance"

        logger.debug(
            f"Performing request performance analytics request to bud_metric {metric_request.model_dump(exclude_none=True, exclude_unset=True, mode='json')}"
        )
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    request_performance_analytics_endpoint,
                    json=metric_request.model_dump(exclude_none=True, exclude_unset=True, mode="json"),
                ) as response:
                    response_data = await response.json()
                    if response.status != status.HTTP_200_OK:
                        logger.error(f"Failed to get request performance analytics: {response.status} {response_data}")
                        raise ClientException(
                            "Failed to get request performance analytics",
                            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                        )

                    logger.debug("Successfully get request performance analytics from budmetric")
                    return response_data
        except Exception as e:
            logger.exception(f"Failed to send request performance analytics request: {e}")
            raise ClientException(
                "Failed to get request performance analytics", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
            ) from e

    async def get_dashboard_stats(self, user_id: UUID) -> DashboardStatsResponse:
        """Fetch dashboard statistics for the given user."""
        db_total_model_count = await ModelDataManager(self.session).get_count_by_fields(
            Model, fields={"status": ModelStatusEnum.ACTIVE}
        )
        db_cloud_model_count = await ModelDataManager(self.session).get_count_by_fields(
            Model,
            fields={
                "status": ModelStatusEnum.ACTIVE,
                "provider_type": ModelProviderTypeEnum.CLOUD_MODEL,
            },
        )
        db_local_model_count = await ModelDataManager(self.session).get_count_by_fields(
            Model,
            fields={"status": ModelStatusEnum.ACTIVE},
            exclude_fields={"provider_type": ModelProviderTypeEnum.CLOUD_MODEL},
        )
        db_total_endpoint_count = await EndpointDataManager(self.session).get_count_by_fields(
            EndpointModel, fields={}, exclude_fields={"status": EndpointStatusEnum.DELETED}
        )
        db_running_endpoint_count = await EndpointDataManager(self.session).get_count_by_fields(
            EndpointModel, fields={"status": EndpointStatusEnum.RUNNING}
        )

        db_total_clusters = await ClusterDataManager(self.session).get_count_by_fields(
            ClusterModel, fields={}, exclude_fields={"status": ClusterStatusEnum.DELETED}
        )

        _, db_inactive_clusters = await ClusterDataManager(self.session).get_inactive_clusters()

        db_project_count = await ProjectDataManager(self.session).get_count_by_fields(
            ProjectModel, fields={"status": ProjectStatusEnum.ACTIVE}
        )

        db_total_project_users = ProjectDataManager(self.session).get_unique_user_count_in_all_projects()

        db_dashboard_stats = {
            "total_model_count": db_total_model_count,
            "cloud_model_count": db_cloud_model_count,
            "local_model_count": db_local_model_count,
            "total_projects": db_project_count,
            "total_project_users": db_total_project_users,
            "total_endpoints_count": db_total_endpoint_count,
            "running_endpoints_count": db_running_endpoint_count,
            "total_clusters": db_total_clusters,
            "inactive_clusters": db_inactive_clusters,
        }

        db_dashboard_stats = DashboardStatsResponse(
            code=status.HTTP_200_OK,
            object="dashboard.count",
            message="Successfully fetched dashboard count statistics",
            **db_dashboard_stats,
        )

        return db_dashboard_stats

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/metric_ops/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, containing essential business logic, services, and routing configurations for the metric ops."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/metric_ops/schemas.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Contains core Pydantic schemas used for data validation and serialization within the metric ops services."""

from datetime import datetime
from typing import Any, Dict, Literal

from pydantic import UUID4, BaseModel, model_validator

from ..commons.schemas import SuccessResponse


class BaseAnalyticsRequest(BaseModel):
    """Base analytics request schema."""

    frequency: Literal["hourly", "daily", "weekly", "monthly", "quarterly", "yearly"]
    filter_by: Literal["project", "model", "endpoint"]
    filter_conditions: list[UUID4] | None = None
    project_id: UUID4 | None = None
    model_id: UUID4 | None = None
    from_date: datetime
    to_date: datetime | None = None
    top_k: int | None = None


class CountAnalyticsRequest(BaseAnalyticsRequest):
    """Request count analytics request schema."""

    metrics: Literal["global", "overall", "concurrency", "queuing_time"] | None = None

    @model_validator(mode="before")
    def validate_filter_by(cls, data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate that global metrics don't have filter conditions."""
        if data.get("metrics") == "global" and data.get("filter_conditions"):
            raise ValueError("global metrics doesn't support filter_conditions, use overall metrics instead")
        return data


class CountAnalyticsResponse(SuccessResponse):
    """Request count analytics response schema."""

    overall_metrics: dict | None = None
    concurrency_metrics: dict | None = None
    queuing_time_metrics: dict | None = None
    global_metrics: dict | None = None


class PerformanceAnalyticsRequest(BaseAnalyticsRequest):
    """Request performance analytics request schema."""

    metrics: Literal["ttft", "latency", "throughput"] | None = None


class PerformanceAnalyticsResponse(SuccessResponse):
    """Request performance analytics response schema."""

    ttft_metrics: dict | None = None
    latency_metrics: dict | None = None
    throughput_metrics: dict | None = None


class DashboardStatsResponse(SuccessResponse):
    """Dashboard stats response schema."""

    total_model_count: int
    cloud_model_count: int
    local_model_count: int
    total_projects: int
    total_project_users: int
    total_endpoints_count: int
    running_endpoints_count: int
    total_clusters: int
    inactive_clusters: int

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/metric_ops/metric_routes.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The metric ops package, containing essential business logic, services, and routing configurations for the metric ops."""

from typing import Union

from fastapi import APIRouter, Depends, status
from sqlalchemy.orm import Session
from typing_extensions import Annotated

from budapp.commons import logging
from budapp.commons.dependencies import (
    get_current_active_user,
    get_session,
)
from budapp.commons.exceptions import ClientException
from budapp.commons.schemas import ErrorResponse
from budapp.user_ops.schemas import User

from .schemas import (
    CountAnalyticsRequest,
    CountAnalyticsResponse,
    PerformanceAnalyticsRequest,
    PerformanceAnalyticsResponse,
    DashboardStatsResponse,
)
from .services import MetricService


logger = logging.get_logger(__name__)

metric_router = APIRouter(prefix="/metrics", tags=["metric"])


@metric_router.post(
    "/analytics/request-counts",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": CountAnalyticsResponse,
            "description": "Successfully get request count analytics",
        },
    },
    description="Get request count analytics",
)
async def get_request_count_analytics(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    metric_request: CountAnalyticsRequest,
) -> Union[CountAnalyticsResponse, ErrorResponse]:
    """Get request count analytics."""
    try:
        return await MetricService(session).get_request_count_analytics(metric_request)
    except ClientException as e:
        logger.exception(f"Failed to get request count analytics: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to get request count analytics: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to get request count analytics"
        ).to_http_response()


@metric_router.post(
    "/analytics/request-performance",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": PerformanceAnalyticsResponse,
            "description": "Successfully get request performance analytics",
        },
    },
    description="Get request performance analytics",
)
async def get_request_performance_analytics(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    metric_request: PerformanceAnalyticsRequest,
) -> Union[PerformanceAnalyticsResponse, ErrorResponse]:
    """Get request performance analytics."""
    try:
        return await MetricService(session).get_request_performance_analytics(metric_request)
    except ClientException as e:
        logger.exception(f"Failed to get request performance analytics: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to get request performance analytics: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to get request performance analytics"
        ).to_http_response()


@metric_router.get(
    "/count",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": DashboardStatsResponse,
            "description": "Successfully retrieved dashboard statistics",
        },
    },
    description="Retrieve the dashboard statistics, including counts for models, projects, endpoints, and clusters.",
)
async def get_dashboard_stats(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
) -> DashboardStatsResponse:
    """
    Retrieves the dashboard statistics, including counts for models, projects, endpoints, and clusters.

    Args:
        current_user (User): The current authenticated user making the request.
        session (Session): The database session used for querying data.

    Returns:
        DashboardStatsResponse: An object containing aggregated statistics for the dashboard,
        such as model counts, project counts, endpoint counts, and cluster counts.
    """
    try:
        return await MetricService(session).get_dashboard_stats(current_user.id)
    except ClientException as e:
        logger.exception(f"Failed to fetch dashboard statistics: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to fetch dashboard statistics: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to fetch dashboard statistics"
        ).to_http_response()

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/permissions/models.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, containing essential business logic, services, and routing configurations for the permissions."""

import json
from datetime import datetime
from typing import List, Union
from uuid import UUID, uuid4

from sqlalchemy import DateTime, ForeignKey, String, Uuid
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy.orm import Mapped, mapped_column, relationship

from budapp.commons.database import Base
from budapp.user_ops.models import User


class Permission(Base):
    """Permission model."""

    __tablename__ = "permission"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    user_id: Mapped[UUID] = mapped_column(ForeignKey("user.id", ondelete="CASCADE"), nullable=False)
    auth_id: Mapped[UUID] = mapped_column(Uuid, nullable=False)
    scopes: Mapped[str] = mapped_column(String, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    user: Mapped["User"] = relationship(back_populates="permission")  # one-to-one

    @hybrid_property
    def scopes_list(self) -> Union[List[str], None]:
        """Get the scopes as a list of strings."""
        if not self.scopes:
            return []
        return json.loads(self.scopes)

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/permissions/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, containing essential business logic, services, and routing configurations for the permissions."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/permissions/crud.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, containing essential business logic, services, and routing configurations for the permissions."""

from budapp.commons import logging
from budapp.commons.db_utils import DataManagerUtils

logger = logging.get_logger(__name__)


class PermissionDataManager(DataManagerUtils):
    """Data manager for the Permission model."""

    pass

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/endpoint_ops/services.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The endpoint ops services. Contains business logic for endpoint ops."""

import json
from datetime import datetime, timezone
from typing import Dict, List, Tuple
from uuid import UUID

import aiohttp
from fastapi import status

from budapp.commons import logging
from budapp.commons.db_utils import SessionMixin
from budapp.commons.schemas import BudNotificationMetadata
from budapp.project_ops.crud import ProjectDataManager
from budapp.project_ops.models import Project as ProjectModel

from ..cluster_ops.crud import ClusterDataManager
from ..cluster_ops.models import Cluster as ClusterModel
from ..cluster_ops.services import ClusterService
from ..commons.config import app_settings
from ..commons.constants import (
    APP_ICONS,
    BUD_INTERNAL_WORKFLOW,
    BudServeWorkflowStepEventName,
    EndpointStatusEnum,
    ModelProviderTypeEnum,
    NotificationTypeEnum,
    WorkflowStatusEnum,
    WorkflowTypeEnum,
)
from ..commons.exceptions import ClientException, RedisException
from ..core.schemas import NotificationPayload, NotificationResult
from ..model_ops.crud import ProviderDataManager
from ..model_ops.models import Provider as ProviderModel
from ..model_ops.services import ModelService, ModelServiceUtil
from ..shared.notification_service import BudNotifyService, NotificationBuilder
from ..shared.redis_service import RedisService
from ..workflow_ops.crud import WorkflowDataManager, WorkflowStepDataManager
from ..workflow_ops.models import Workflow as WorkflowModel
from ..workflow_ops.models import WorkflowStep as WorkflowStepModel
from ..workflow_ops.schemas import WorkflowUtilCreate
from ..workflow_ops.services import WorkflowService, WorkflowStepService
from .crud import EndpointDataManager
from .models import Endpoint as EndpointModel
from .schemas import AddWorkerRequest, AddWorkerWorkflowStepData, EndpointCreate, ModelClusterDetail, WorkerInfoFilter


logger = logging.get_logger(__name__)


class EndpointService(SessionMixin):
    """Endpoint service."""

    async def get_all_endpoints(
        self,
        project_id: UUID,
        offset: int = 0,
        limit: int = 10,
        filters: Dict = {},
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[EndpointModel], int]:
        """Get all active endpoints."""
        if search:
            # Only include name, exclude other filters
            # Otherwise it will perform global search on all fields
            filters.pop("status", None)

        # Validate project_id
        await ProjectDataManager(self.session).retrieve_by_fields(ProjectModel, {"id": project_id})

        return await EndpointDataManager(self.session).get_all_active_endpoints(
            project_id, offset, limit, filters, order_by, search
        )

    async def delete_endpoint(self, endpoint_id: UUID, current_user_id: UUID) -> WorkflowModel:
        """Delete an endpoint by its ID."""
        db_endpoint = await EndpointDataManager(self.session).retrieve_by_fields(
            EndpointModel, {"id": endpoint_id}, exclude_fields={"status": EndpointStatusEnum.DELETED}
        )

        if db_endpoint.status == EndpointStatusEnum.DELETING:
            raise ClientException("Deployment is already deleting")

        if db_endpoint.model.provider_type in [ModelProviderTypeEnum.HUGGING_FACE, ModelProviderTypeEnum.CLOUD_MODEL]:
            db_provider = await ProviderDataManager(self.session).retrieve_by_fields(
                ProviderModel, {"id": db_endpoint.model.provider_id}
            )
            model_icon = db_provider.icon
        else:
            model_icon = db_endpoint.model.icon

        current_step_number = 1

        # Retrieve or create workflow
        workflow_create = WorkflowUtilCreate(
            workflow_type=WorkflowTypeEnum.ENDPOINT_DELETION,
            title=db_endpoint.name,
            total_steps=current_step_number,
            icon=model_icon,
            tag=db_endpoint.project.name,
        )
        db_workflow = await WorkflowService(self.session).retrieve_or_create_workflow(
            workflow_id=None, workflow_data=workflow_create, current_user_id=current_user_id
        )
        logger.debug(f"Delete endpoint workflow {db_workflow.id} created")

        try:
            # Perform delete endpoint request to bud_cluster app
            bud_cluster_response = await self._perform_bud_cluster_delete_endpoint_request(
                db_endpoint.cluster.cluster_id, db_endpoint.namespace, current_user_id, db_workflow.id
            )
        except ClientException as e:
            await WorkflowDataManager(self.session).update_by_fields(
                db_workflow, {"status": WorkflowStatusEnum.FAILED}
            )
            raise e

        # Add payload dict to response
        for step in bud_cluster_response["steps"]:
            step["payload"] = {}

        delete_endpoint_workflow_id = bud_cluster_response.get("workflow_id")
        delete_endpoint_events = {
            BudServeWorkflowStepEventName.DELETE_ENDPOINT_EVENTS.value: bud_cluster_response,
            "delete_endpoint_workflow_id": delete_endpoint_workflow_id,
            "endpoint_id": str(db_endpoint.id),
        }

        # Insert step details in db
        db_workflow_step = await WorkflowStepDataManager(self.session).insert_one(
            WorkflowStepModel(
                workflow_id=db_workflow.id,
                step_number=current_step_number,
                data=delete_endpoint_events,
            )
        )
        logger.debug(f"Created workflow step {current_step_number} for workflow {db_workflow.id}")

        # Update progress in workflow
        bud_cluster_response["progress_type"] = BudServeWorkflowStepEventName.DELETE_ENDPOINT_EVENTS.value
        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"progress": bud_cluster_response, "current_step": current_step_number}
        )

        # Update endpoint status to deleting
        await EndpointDataManager(self.session).update_by_fields(db_endpoint, {"status": EndpointStatusEnum.DELETING})
        logger.debug(f"Endpoint {db_endpoint.id} status updated to {EndpointStatusEnum.DELETING.value}")

        return db_workflow

    async def _perform_bud_cluster_delete_endpoint_request(
        self, bud_cluster_id: UUID, namespace: str, current_user_id: UUID, workflow_id: UUID
    ) -> Dict:
        """Perform delete endpoint request to bud_cluster app.

        Args:
            bud_cluster_id: The ID of the cluster being served by the endpoint to delete.
            namespace: The namespace of the cluster endpoint to delete.
        """
        delete_endpoint_url = (
            f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_cluster_app_id}/method/deployment/delete"
        )

        payload = {
            "cluster_id": str(bud_cluster_id),
            "namespace": namespace,
            "notification_metadata": {
                "name": BUD_INTERNAL_WORKFLOW,
                "subscriber_ids": str(current_user_id),
                "workflow_id": str(workflow_id),
            },
            "source_topic": f"{app_settings.source_topic}",
        }

        logger.debug(f"Performing delete endpoint request to budcluster {payload}")
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(delete_endpoint_url, json=payload) as response:
                    response_data = await response.json()
                    if response.status != 200:
                        logger.error(f"Failed to delete endpoint: {response.status} {response_data}")
                        raise ClientException("Failed to delete endpoint")

                    logger.debug("Successfully deleted endpoint from budcluster")
                    return response_data
        except Exception as e:
            logger.exception(f"Failed to send delete endpoint request: {e}")
            raise ClientException("Failed to delete endpoint") from e

    async def delete_endpoint_from_notification_event(self, payload: NotificationPayload) -> None:
        """Delete a endpoint in database.

        Args:
            payload: The payload to delete the endpoint with.

        Raises:
            ClientException: If the endpoint already exists.
        """
        logger.debug("Received event for deleting endpoint")

        # Get workflow and steps
        workflow_id = payload.workflow_id
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        # Define the keys required for endpoint deletion
        keys_of_interest = [
            "endpoint_id",
        ]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]

        logger.debug("Collected required data from workflow steps")

        # Retrieve endpoint from db
        db_endpoint = await EndpointDataManager(self.session).retrieve_by_fields(
            EndpointModel, {"id": required_data["endpoint_id"]}, exclude_fields={"status": EndpointStatusEnum.DELETED}
        )
        logger.debug(f"Endpoint retrieved successfully: {db_endpoint.id}")

        # Mark endpoint as deleted
        db_endpoint = await EndpointDataManager(self.session).update_by_fields(
            db_endpoint, {"status": EndpointStatusEnum.DELETED}
        )
        logger.debug(f"Endpoint {db_endpoint.id} marked as deleted")

        # Delete endpoint details with pattern “router_config:*:<endpoint_name>“,
        try:
            redis_service = RedisService()
            endpoint_redis_keys = await redis_service.keys(f"router_config:*:{db_endpoint.name}")
            endpoint_redis_keys_count = await redis_service.delete(*endpoint_redis_keys)
            logger.debug(f"Deleted endpoint data from redis: {endpoint_redis_keys_count} keys")
        except (RedisException, Exception) as e:
            logger.error(f"Failed to delete endpoint details from redis: {e}")

        # Mark workflow as completed
        await WorkflowDataManager(self.session).update_by_fields(db_workflow, {"status": WorkflowStatusEnum.COMPLETED})
        logger.debug(f"Workflow {db_workflow.id} marked as completed")

        # Send notification to workflow creator
        model_icon = await ModelServiceUtil(self.session).get_model_icon(db_endpoint.model)
        notification_request = (
            NotificationBuilder()
            .set_content(
                title=db_endpoint.name,
                message="Deployment Deleted",
                icon=model_icon,
                result=NotificationResult(target_id=db_endpoint.project.id, target_type="project").model_dump(
                    exclude_none=True, exclude_unset=True
                ),
            )
            .set_payload(workflow_id=str(db_workflow.id), type=NotificationTypeEnum.DEPLOYMENT_DELETION_SUCCESS.value)
            .set_notification_request(subscriber_ids=[str(db_workflow.created_by)])
            .build()
        )
        await BudNotifyService().send_notification(notification_request)

    async def create_endpoint_from_notification_event(self, payload: NotificationPayload) -> None:
        """Create an endpoint in database.

        Args:
            payload: The payload to create the endpoint with.
        """
        logger.debug("Received event for creating endpoint")

        # Get namespace and deployment URL from event
        namespace = payload.content.result.get("namespace")
        deployment_url = payload.content.result["result"]["deployment_url"]
        credential_id = payload.content.result.get("credential_id")
        number_of_nodes = payload.content.result.get("number_of_nodes")
        total_replicas = payload.content.result["deployment_status"]["replicas"]["total"]

        # Calculate the active replicas with status "Running"
        active_replicas = sum(
            1
            for worker in payload.content.result["deployment_status"]["worker_data_list"]
            if worker["status"] == "Running"
        )
        if not namespace or not deployment_url:
            logger.warning("Namespace or deployment URL is missing from event")
            return

        # Get workflow steps
        workflow_id = payload.workflow_id
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        # Define the keys required for endpoint creation
        keys_of_interest = [
            "model_id",
            "project_id",
            "cluster_id",  # bud_cluster_id
            "endpoint_name",
            "deploy_config",
        ]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]

        logger.debug("Collected required data from workflow steps")

        # Get cluster id
        db_cluster = await ClusterDataManager(self.session).retrieve_by_fields(
            ClusterModel, {"cluster_id": required_data["cluster_id"]}, missing_ok=True
        )

        if not db_cluster:
            logger.error(f"Cluster with id {required_data['cluster_id']} not found")
            return

        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})

        # Check duplicate name exist in endpoints
        db_endpoint = await EndpointDataManager(self.session).retrieve_by_fields(
            EndpointModel,
            fields={"name": required_data["endpoint_name"], "project_id": required_data["project_id"]},
            exclude_fields={"status": EndpointStatusEnum.DELETED},
            missing_ok=True,
            case_sensitive=False,
        )
        if db_endpoint:
            logger.error(
                f"An endpoint with name {required_data['endpoint_name']} already exists in project: {required_data['project_id']}"
            )
            return

        # Create endpoint in database
        endpoint_data = EndpointCreate(
            model_id=required_data["model_id"],
            project_id=required_data["project_id"],
            cluster_id=db_cluster.id,
            bud_cluster_id=required_data["cluster_id"],
            name=required_data["endpoint_name"],
            url=deployment_url,
            namespace=namespace,
            status=EndpointStatusEnum.RUNNING,
            created_by=db_workflow.created_by,
            status_sync_at=datetime.now(tz=timezone.utc),
            credential_id=credential_id,
            number_of_nodes=number_of_nodes,
            active_replicas=active_replicas,
            total_replicas=total_replicas,
            deployment_config=required_data["deploy_config"],
        )

        db_endpoint = await EndpointDataManager(self.session).insert_one(
            EndpointModel(**endpoint_data.model_dump(exclude_unset=True, exclude_none=True))
        )
        logger.debug(f"Endpoint created successfully: {db_endpoint.id}")

        # Update endpoint details as next step
        # Update current step number
        current_step_number = db_workflow.current_step + 1
        workflow_current_step = current_step_number

        # Update or create next workflow step
        endpoint_details = {"endpoint_details": payload.content.result}
        db_workflow_step = await WorkflowStepService(self.session).create_or_update_next_workflow_step(
            db_workflow.id, current_step_number, endpoint_details
        )
        logger.debug(f"Upsert workflow step {db_workflow_step.id} for storing endpoint details")

        # Mark workflow as completed
        logger.debug(f"Marking workflow as completed: {workflow_id}")
        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"status": WorkflowStatusEnum.COMPLETED, "current_step": workflow_current_step}
        )

        # Send notification to workflow creator
        model_icon = await ModelServiceUtil(self.session).get_model_icon(db_endpoint.model)
        notification_request = (
            NotificationBuilder()
            .set_content(
                title=db_endpoint.name,
                message="Deployment is Done",
                icon=model_icon,
                result=NotificationResult(target_id=db_endpoint.id, target_type="endpoint").model_dump(
                    exclude_none=True, exclude_unset=True
                ),
            )
            .set_payload(workflow_id=str(db_workflow.id), type=NotificationTypeEnum.DEPLOYMENT_SUCCESS.value)
            .set_notification_request(subscriber_ids=[str(db_workflow.created_by)])
            .build()
        )
        await BudNotifyService().send_notification(notification_request)

        # Create request to trigger endpoint status update periodic task
        is_cloud_model = db_endpoint.model.provider_type == ModelProviderTypeEnum.CLOUD_MODEL

        await self._perform_endpoint_status_update_request(
            db_endpoint.bud_cluster_id, db_endpoint.namespace, is_cloud_model
        )

        return db_endpoint

    async def _perform_endpoint_status_update_request(
        self, cluster_id: UUID, namespace: str, is_cloud_model: bool
    ) -> Dict:
        """Perform update endpoint status request to bud_cluster app.

        Args:
            cluster_id: The ID of the cluster to update.
            namespace: The namespace of the cluster to update.
            current_user_id: The ID of the current user.
        """
        update_cluster_endpoint = f"{app_settings.dapr_base_url}v1.0/invoke/{app_settings.bud_cluster_app_id}/method/deployment/update-deployment-status"

        try:
            payload = {
                "deployment_name": namespace,
                "cluster_id": str(cluster_id),
                "cloud_model": is_cloud_model,
            }
            logger.debug(
                f"Performing update endpoint status request. payload: {payload}, endpoint: {update_cluster_endpoint}"
            )
            async with aiohttp.ClientSession() as session, session.post(
                update_cluster_endpoint, json=payload
            ) as response:
                response_data = await response.json()
                if response.status != 200:
                    logger.error(f"Failed to update endpoint status: {response.status} {response_data}")
                    raise ClientException(
                        "Failed to update endpoint status", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
                    )

                logger.debug("Successfully updated endpoint status")
                return response_data
        except Exception as e:
            logger.exception(f"Failed to send update endpoint status request: {e}")
            raise ClientException(
                "Failed to update endpoint status", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
            ) from e

    async def update_endpoint_status_from_notification_event(self, payload: NotificationPayload) -> None:
        """Update an endpoint status in database.

        Args:
            payload: The payload to update the endpoint status with.

        Raises:
            ClientException: If the endpoint already exists.
        """
        logger.debug("Received event for updating endpoint status")

        # Get endpoint from db
        logger.debug(
            f"Retrieving endpoint with bud_cluster_id: {payload.content.result['cluster_id']} and namespace: {payload.content.result['deployment_name']}"
        )
        total_replicas = len(payload.content.result["worker_data_list"])
        logger.debug(f"Number of workers : {total_replicas}")

        # Calculate the active replicas with status "Running"
        active_replicas = sum(
            1 for worker in payload.content.result["worker_data_list"] if worker["status"] == "Running"
        )
        logger.debug(f"active replicas with status 'Running': {active_replicas}")

        db_endpoint = await EndpointDataManager(self.session).retrieve_by_fields(
            EndpointModel,
            {
                "bud_cluster_id": payload.content.result["cluster_id"],
                "namespace": payload.content.result["deployment_name"],
            },
            exclude_fields={"status": EndpointStatusEnum.DELETED},
        )
        logger.debug(f"Endpoint retrieved successfully: {db_endpoint.id}")

        # Check if endpoint is already in deleting state
        if db_endpoint.status == EndpointStatusEnum.DELETING:
            logger.error("Endpoint %s is already in deleting state", db_endpoint.id)
            raise ClientException("Endpoint is already in deleting state")

        # Update cluster status
        endpoint_status = await self._get_endpoint_status(payload.content.result["status"])
        db_endpoint = await EndpointDataManager(self.session).update_by_fields(
            db_endpoint,
            {
                "status": endpoint_status,
                "total_replicas": total_replicas,
                "active_replicas": active_replicas,
            },
        )
        logger.debug(
            f"Endpoint {db_endpoint.id} status updated to {endpoint_status} and total replicas to {total_replicas}"
        )

    @staticmethod
    async def _get_endpoint_status(status: str) -> EndpointStatusEnum:
        """Get the endpoint status from the payload.

        Args:
            status: The status to get the endpoint status from.

        Returns:
            EndpointStatusEnum: The endpoint status.
        """
        if status == "ready":
            return EndpointStatusEnum.RUNNING
        elif status == "pending":
            return EndpointStatusEnum.PENDING
        elif status == "ingress_failed" or status == "failed":
            return EndpointStatusEnum.UNHEALTHY
        else:
            logger.error(f"Unknown endpoint status: {status}")
            raise ClientException(f"Unknown endpoint status: {status}")

    async def get_endpoint_workers(
        self,
        endpoint_id: UUID,
        filters: WorkerInfoFilter,
        refresh: bool,
        page: int,
        limit: int,
        order_by: List[str],
        search: bool,
    ) -> dict:
        """Get endpoint workers."""
        db_endpoint = await EndpointDataManager(self.session).retrieve_by_fields(EndpointModel, {"id": endpoint_id})
        get_workers_endpoint = (
            f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_cluster_app_id}/method/deployment/worker-info"
        )
        filters_dict = filters.model_dump(exclude_none=True)
        payload = {
            "namespace": db_endpoint.namespace,
            "cluster_id": str(db_endpoint.bud_cluster_id),
            "page": page,
            "limit": limit,
            "order_by": order_by or [],
            "search": str(search).lower(),
            "refresh": str(refresh).lower(),
        }
        logger.info(f"Services : payload: {payload}")
        payload.update(filters_dict)
        headers = {
            "accept": "application/json",
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(get_workers_endpoint, params=payload, headers=headers) as response:
                response_data = await response.json()
                if response.status != 200:
                    error_message = response_data.get("message", "Failed to get endpoint workers")
                    logger.error(f"Failed to get endpoint workers: {error_message}")
                    raise ClientException(error_message)

                logger.debug("Successfully retrieved endpoint workers")
                return response_data

    async def get_endpoint_worker_detail(self, endpoint_id: UUID, worker_id: UUID) -> dict:
        """Get endpoint worker detail."""
        _ = await EndpointDataManager(self.session).retrieve_by_fields(EndpointModel, {"id": endpoint_id})
        get_worker_detail_endpoint = f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_cluster_app_id}/method/deployment/worker-info/{worker_id}"
        headers = {
            "accept": "application/json",
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(get_worker_detail_endpoint, headers=headers) as response:
                response_data = await response.json()
                if response.status != 200:
                    error_message = response_data.get("message", "Failed to get endpoint worker detail")
                    logger.error(f"Failed to get endpoint worker detail: {error_message}")
                    raise ClientException(error_message)

                logger.debug("Successfully retrieved endpoint worker detail")
                return response_data

    async def get_model_cluster_detail(self, endpoint_id: UUID) -> ModelClusterDetail:
        """Get model cluster detail."""
        db_endpoint = await EndpointDataManager(self.session).retrieve_by_fields(EndpointModel, {"id": endpoint_id})
        model_id = db_endpoint.model_id
        model_detail_json_response = await ModelService(self.session).retrieve_model(model_id)
        model_detail = json.loads(model_detail_json_response.body.decode("utf-8"))
        cluster_id = db_endpoint.cluster_id
        cluster_detail = await ClusterService(self.session).get_cluster_details(cluster_id)
        return ModelClusterDetail(
            id=db_endpoint.id,
            name=db_endpoint.name,
            status=db_endpoint.status,
            model=model_detail["model"],
            cluster=cluster_detail,
            deployment_config=db_endpoint.deployment_config,
        )

    async def delete_worker_from_notification_event(self, payload: NotificationPayload) -> None:
        """Delete a worker in database.

        Args:
            payload: The payload to delete the worker with.

        Raises:
            ClientException: If the worker already exists.
        """
        logger.debug("Received event for deleting worker")

        # Get workflow and steps
        workflow_id = payload.workflow_id
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        # Define the keys required for worker deletion
        keys_of_interest = [
            "endpoint_id",
            "worker_id",
            "worker_name",
        ]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]

        logger.debug("Collected required data from workflow steps")

        db_endpoint = await EndpointDataManager(self.session).retrieve_by_fields(
            EndpointModel, {"id": required_data["endpoint_id"]}, exclude_fields={"status": EndpointStatusEnum.DELETED}
        )
        logger.debug(f"Endpoint retrieved successfully: {db_endpoint.id}")

        # Calculate concurrent request per replica and reduce it
        deployment_config = db_endpoint.deployment_config
        concurrent_requests = deployment_config["concurrent_requests"]
        total_replicas = db_endpoint.total_replicas
        concurrent_request_per_replica = concurrent_requests / total_replicas
        logger.debug(
            f"Total replicas: {total_replicas}, concurrent requests: {concurrent_requests}, concurrent request per replica: {concurrent_request_per_replica}"
        )

        updated_concurrent_requests = concurrent_requests - concurrent_request_per_replica
        updated_replica_count = total_replicas - 1
        deployment_config["concurrent_requests"] = updated_concurrent_requests
        logger.debug(
            f"Updated replica count: {updated_replica_count}, Updated concurrent requests: {updated_concurrent_requests}"
        )

        self.session.refresh(db_endpoint)
        # Update endpoint with deploy config and updated replica count
        db_endpoint = await EndpointDataManager(self.session).update_by_fields(
            db_endpoint, {"deployment_config": deployment_config, "total_replicas": updated_replica_count}
        )

        # Mark workflow as completed
        await WorkflowDataManager(self.session).update_by_fields(db_workflow, {"status": WorkflowStatusEnum.COMPLETED})
        logger.debug(f"Workflow {db_workflow.id} marked as completed")

        # Send notification to workflow creator
        model_icon = await ModelServiceUtil(self.session).get_model_icon(db_endpoint.model)
        notification_request = (
            NotificationBuilder()
            .set_content(
                title=db_endpoint.name,
                message="Worker Deleted",
                icon=model_icon,
                result=NotificationResult(target_id=db_endpoint.project.id, target_type="project").model_dump(
                    exclude_none=True, exclude_unset=True
                ),
            )
            .set_payload(workflow_id=str(db_workflow.id), type=NotificationTypeEnum.DEPLOYMENT_DELETION_SUCCESS.value)
            .set_notification_request(subscriber_ids=[str(db_workflow.created_by)])
            .build()
        )
        await BudNotifyService().send_notification(notification_request)

    async def _perform_endpoint_worker_delete_request(
        self, worker_id: UUID, workflow_id: UUID, current_user_id: UUID
    ) -> Dict:
        """Perform update endpoint status request to bud_cluster app.

        Args:
            cluster_id: The ID of the cluster to update.
            namespace: The namespace of the cluster to update.
            current_user_id: The ID of the current user.
        """
        delete_worker_endpoint = (
            f"{app_settings.dapr_base_url}v1.0/invoke/{app_settings.bud_cluster_app_id}/method/deployment/worker-info"
        )

        try:
            notification_metadata = BudNotificationMetadata(
                workflow_id=str(workflow_id),
                subscriber_ids=str(current_user_id),
                name=BUD_INTERNAL_WORKFLOW,
            )
            payload = {
                "worker_id": str(worker_id),
                "notification_metadata": notification_metadata.model_dump(mode="json"),
                "source_topic": f"{app_settings.source_topic}",
            }
            logger.debug(
                f"Performing update endpoint status request. payload: {payload}, endpoint: {delete_worker_endpoint}"
            )
            async with aiohttp.ClientSession() as session, session.delete(
                delete_worker_endpoint, json=payload
            ) as response:
                response_data = await response.json()
                if response.status != 200:
                    logger.error(f"Failed to delete worker: {response.status} {response_data}")
                    error_message = response_data.get("message", "Failed to delete worker")
                    raise ClientException(error_message, status_code=response.status)

                logger.debug("Successfully deleted worker")
                return response_data
        except ClientException as e:
            raise e
        except Exception as e:
            logger.exception(f"Failed to send delete worker request: {e}")
            raise ClientException("Failed to delete worker", status_code=status.HTTP_500_INTERNAL_SERVER_ERROR) from e

    async def delete_endpoint_worker(
        self, endpoint_id: UUID, worker_id: UUID, worker_name: str, current_user_id: UUID
    ) -> None:
        """Delete a endpoint worker by its ID."""
        # To check if endpoint exists
        db_endpoint = await EndpointDataManager(self.session).retrieve_by_fields(
            EndpointModel, {"id": endpoint_id}, exclude_fields={"status": EndpointStatusEnum.DELETED}
        )
        if not db_endpoint:
            logger.error(f"Endpoint with id {endpoint_id} not found")
            raise ClientException(f"Endpoint with id {endpoint_id} not found")

        if db_endpoint.model.provider_type in [ModelProviderTypeEnum.HUGGING_FACE, ModelProviderTypeEnum.CLOUD_MODEL]:
            db_provider = await ProviderDataManager(self.session).retrieve_by_fields(
                ProviderModel, {"id": db_endpoint.model.provider_id}
            )
            model_icon = db_provider.icon
        else:
            model_icon = db_endpoint.model.icon

        current_step_number = 1

        # Retrieve or create workflow
        workflow_create = WorkflowUtilCreate(
            workflow_type=WorkflowTypeEnum.ENDPOINT_WORKER_DELETION,
            title=worker_name,
            total_steps=current_step_number,
            icon=model_icon,
            tag=db_endpoint.project.name,
        )
        db_workflow = await WorkflowService(self.session).retrieve_or_create_workflow(
            workflow_id=None, workflow_data=workflow_create, current_user_id=current_user_id
        )
        logger.debug(f"Delete worker workflow {db_workflow.id} created")

        try:
            # Perform delete endpoint request to bud_cluster app
            bud_cluster_response = await self._perform_endpoint_worker_delete_request(
                worker_id, db_workflow.id, current_user_id
            )
        except ClientException as e:
            await WorkflowDataManager(self.session).update_by_fields(
                db_workflow, {"status": WorkflowStatusEnum.FAILED}
            )
            raise e

        # Add payload dict to response
        for step in bud_cluster_response["steps"]:
            step["payload"] = {}

        delete_worker_workflow_id = bud_cluster_response.get("workflow_id")
        delete_worker_events = {
            BudServeWorkflowStepEventName.DELETE_WORKER_EVENTS.value: bud_cluster_response,
            "delete_worker_workflow_id": delete_worker_workflow_id,
            "endpoint_id": str(db_endpoint.id),
            "worker_id": str(worker_id),
            "worker_name": worker_name,
        }

        # Insert step details in db
        db_workflow_step = await WorkflowStepDataManager(self.session).insert_one(
            WorkflowStepModel(
                workflow_id=db_workflow.id,
                step_number=current_step_number,
                data=delete_worker_events,
            )
        )
        logger.debug(f"Created workflow step {current_step_number} for workflow {db_workflow.id}")

        # Update progress in workflow
        bud_cluster_response["progress_type"] = BudServeWorkflowStepEventName.DELETE_WORKER_EVENTS.value
        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"progress": bud_cluster_response, "current_step": current_step_number}
        )

        return db_workflow

    async def add_worker_to_endpoint_workflow(self, current_user_id: UUID, request: AddWorkerRequest) -> WorkflowModel:
        """Add worker to endpoint workflow."""
        # Get request data
        step_number = request.step_number
        workflow_id = request.workflow_id
        workflow_total_steps = request.workflow_total_steps
        trigger_workflow = request.trigger_workflow
        endpoint_id = request.endpoint_id
        additional_concurrency = request.additional_concurrency
        current_step_number = step_number

        # Retrieve or create workflow
        workflow_create = WorkflowUtilCreate(
            workflow_type=WorkflowTypeEnum.ADD_WORKER_TO_ENDPOINT,
            title="Add Worker to Deployment",
            total_steps=workflow_total_steps,
            icon=APP_ICONS["general"]["deployment_mono"],
            tag="Deployment",
        )
        db_workflow = await WorkflowService(self.session).retrieve_or_create_workflow(
            workflow_id, workflow_create, current_user_id
        )

        # Validate endpoint id
        project_id = None
        if endpoint_id:
            db_endpoint = await EndpointDataManager(self.session).retrieve_by_fields(
                EndpointModel, {"id": endpoint_id}, exclude_fields={"status": EndpointStatusEnum.DELETED}
            )

            # Get icon from provider or model
            if db_endpoint.model.provider_type in [
                ModelProviderTypeEnum.CLOUD_MODEL,
                ModelProviderTypeEnum.HUGGING_FACE,
            ]:
                db_provider = await ProviderDataManager(self.session).retrieve_by_fields(
                    ProviderModel, {"id": db_endpoint.model.provider_id}
                )
                model_icon = db_provider.icon
            else:
                model_icon = db_endpoint.model.icon

            # Update title, icon and tag on workflow
            db_workflow = await WorkflowDataManager(self.session).update_by_fields(
                db_workflow,
                {"title": db_endpoint.name, "icon": model_icon, "tag": db_endpoint.project.name},
            )

            # Assign project_id
            project_id = db_endpoint.project_id

        # Prepare workflow step data
        workflow_step_data = AddWorkerWorkflowStepData(
            endpoint_id=endpoint_id,
            additional_concurrency=additional_concurrency,
        ).model_dump(exclude_none=True, exclude_unset=True, mode="json")

        # NOTE: If endpoint_id is provided, then need to add project_id to workflow step data
        # Required for frontend integration
        if endpoint_id:
            workflow_step_data["project_id"] = str(project_id)

        # Get workflow steps
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": db_workflow.id}
        )

        # For avoiding another db call for record retrieval, storing db object while iterating over db_workflow_steps
        db_current_workflow_step = None

        if db_workflow_steps:
            for db_step in db_workflow_steps:
                # Get current workflow step
                if db_step.step_number == current_step_number:
                    db_current_workflow_step = db_step

        if db_current_workflow_step:
            logger.info(f"Workflow {db_workflow.id} step {current_step_number} already exists")

            # Update workflow step data in db
            db_workflow_step = await WorkflowStepDataManager(self.session).update_by_fields(
                db_current_workflow_step,
                {"data": workflow_step_data},
            )
            logger.info(f"Workflow {db_workflow.id} step {current_step_number} updated")
        else:
            logger.info(f"Creating workflow step {current_step_number} for workflow {db_workflow.id}")

            # Insert step details in db
            db_workflow_step = await WorkflowStepDataManager(self.session).insert_one(
                WorkflowStepModel(
                    workflow_id=db_workflow.id,
                    step_number=current_step_number,
                    data=workflow_step_data,
                )
            )

        # Update workflow current step as the highest step_number
        db_max_workflow_step_number = max(step.step_number for step in db_workflow_steps) if db_workflow_steps else 0
        workflow_current_step = max(current_step_number, db_max_workflow_step_number)
        logger.info(f"The current step of workflow {db_workflow.id} is {workflow_current_step}")

        # This will ensure workflow step number is updated to the latest step number
        db_workflow = await WorkflowDataManager(self.session).update_by_fields(
            db_workflow,
            {"current_step": workflow_current_step},
        )

        # Perform bud simulation
        if additional_concurrency:
            db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
                {"workflow_id": db_workflow.id}
            )

            # Define the keys required for model security scan
            keys_of_interest = [
                "endpoint_id",
                "additional_concurrency",
            ]

            # from workflow steps extract necessary information
            required_data = {}
            for db_workflow_step in db_workflow_steps:
                for key in keys_of_interest:
                    if key in db_workflow_step.data:
                        required_data[key] = db_workflow_step.data[key]

            # Check if all required keys are present
            required_keys = ["endpoint_id", "additional_concurrency"]
            missing_keys = [key for key in required_keys if key not in required_data]
            if missing_keys:
                raise ClientException(f"Missing required data for bud simulation: {', '.join(missing_keys)}")

            # Perform add worker bud simulation
            try:
                await self._perform_add_worker_simulation(
                    current_step_number, required_data, db_workflow, current_user_id
                )
            except ClientException as e:
                raise e

        # Trigger workflow
        if trigger_workflow:
            # query workflow steps again to get latest data
            db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
                {"workflow_id": db_workflow.id}
            )

            # Define the keys required for model extraction
            keys_of_interest = [
                "endpoint_id",
                "additional_concurrency",
                "simulator_id",
                "deploy_config",
            ]

            # from workflow steps extract necessary information
            required_data = {}
            for db_workflow_step in db_workflow_steps:
                for key in keys_of_interest:
                    if key in db_workflow_step.data:
                        required_data[key] = db_workflow_step.data[key]

            # Check if all required keys are present
            required_keys = ["endpoint_id", "additional_concurrency", "simulator_id", "deploy_config"]
            missing_keys = [key for key in required_keys if key not in required_data]
            if missing_keys:
                raise ClientException(f"Missing required data for add worker to deployment: {', '.join(missing_keys)}")

            try:
                # Perform add worker deployment
                await self._perform_add_worker_to_deployment(
                    current_step_number, required_data, db_workflow, current_user_id
                )
            except ClientException as e:
                raise e

        return db_workflow

    async def _perform_add_worker_simulation(
        self, current_step_number: int, data: Dict, db_workflow: WorkflowModel, current_user_id: UUID
    ) -> None:
        """Perform bud simulation."""
        db_endpoint = await EndpointDataManager(self.session).retrieve_by_fields(
            EndpointModel, {"id": data["endpoint_id"]}, exclude_fields={"status": EndpointStatusEnum.DELETED}
        )

        # Create request payload
        deployment_config = db_endpoint.deployment_config
        payload = {
            "pretrained_model_uri": db_endpoint.model.uri,
            "input_tokens": deployment_config["avg_context_length"],
            "output_tokens": deployment_config["avg_sequence_length"],
            "concurrency": data["additional_concurrency"],
            "cluster_id": str(db_endpoint.cluster.cluster_id),
            "notification_metadata": {
                "name": BUD_INTERNAL_WORKFLOW,
                "subscriber_ids": str(current_user_id),
                "workflow_id": str(db_workflow.id),
            },
            "source_topic": f"{app_settings.source_topic}",
        }
        if db_endpoint.model.provider_type == ModelProviderTypeEnum.CLOUD_MODEL:
            payload["target_ttft"] = 0
            payload["target_throughput_per_user"] = 0
            payload["target_e2e_latency"] = 0
            payload["is_proprietary_model"] = True
        else:
            payload["target_ttft"] = deployment_config["ttft"][0] if deployment_config["ttft"] else None
            payload["target_throughput_per_user"] = (
                deployment_config["per_session_tokens_per_sec"][1]
                if deployment_config["per_session_tokens_per_sec"]
                else None
            )
            payload["target_e2e_latency"] = (
                deployment_config["e2e_latency"][0] if deployment_config["e2e_latency"] else None
            )
            payload["is_proprietary_model"] = False

        # Perform bud simulation request
        bud_simulation_response = await self._perform_bud_simulation_request(payload)

        # Add payload dict to response
        for step in bud_simulation_response["steps"]:
            step["payload"] = {}

        simulator_id = bud_simulation_response.get("workflow_id")

        # NOTE: Dependency with recommended cluster api (GET /clusters/recommended/{workflow_id})
        # NOTE: Replace concurrent_requests with additional_concurrency
        # Required to compare with concurrent_requests in simulator response
        deployment_config["concurrent_requests"] = data["additional_concurrency"]
        bud_simulation_events = {
            "simulator_id": simulator_id,
            BudServeWorkflowStepEventName.BUD_SIMULATOR_EVENTS.value: bud_simulation_response,
            "deploy_config": deployment_config,
            "model_id": str(db_endpoint.model.id),
        }

        # Increment current step number
        current_step_number = current_step_number + 1
        workflow_current_step = current_step_number

        # Update or create next workflow step
        db_workflow_step = await WorkflowStepService(self.session).create_or_update_next_workflow_step(
            db_workflow.id, current_step_number, bud_simulation_events
        )
        logger.debug(f"Workflow step created with id {db_workflow_step.id}")

        # Update progress in workflow
        bud_simulation_response["progress_type"] = BudServeWorkflowStepEventName.BUD_SIMULATOR_EVENTS.value
        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"progress": bud_simulation_response, "current_step": workflow_current_step}
        )

    async def _perform_bud_simulation_request(self, payload: Dict) -> Dict:
        """Perform bud simulation request."""
        bud_simulation_endpoint = (
            f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_simulator_app_id}/method/simulator/run"
        )
        logger.debug(f"payload for bud simulation on add worker to endpoint : {payload}")

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(bud_simulation_endpoint, json=payload) as response:
                    response_data = await response.json()
                    if response.status >= 400:
                        raise ClientException("Unable to perform bud simulation")

                    return response_data
        except ClientException as e:
            raise e
        except Exception as e:
            logger.error(f"Failed to perform bud simulation request: {e}")
            raise ClientException("Unable to perform bud simulation") from e

    async def _perform_add_worker_to_deployment(
        self, current_step_number: int, data: Dict, db_workflow: WorkflowModel, current_user_id: UUID
    ) -> None:
        """Perform add worker to deployment."""
        # Get endpoint
        db_endpoint = await EndpointDataManager(self.session).retrieve_by_fields(
            EndpointModel, {"id": data["endpoint_id"]}, exclude_fields={"status": EndpointStatusEnum.DELETED}
        )
        deployment_config = db_endpoint.deployment_config

        # Model URI selection as per lite-llm updates
        db_model = db_endpoint.model
        credential_id = db_endpoint.credential_id
        if db_model.provider_type == ModelProviderTypeEnum.CLOUD_MODEL:
            model_uri = db_model.uri
            model_source = db_model.source
            if model_uri.startswith(f"{model_source}/"):
                model_uri = model_uri.removeprefix(f"{model_source}/")
            deploy_model_uri = model_uri if not credential_id else f"{model_source}/{model_uri}"
        else:
            deploy_model_uri = db_model.local_path

        add_worker_payload = {
            "cluster_id": str(db_endpoint.bud_cluster_id),
            "simulator_id": data["simulator_id"],
            "endpoint_name": db_endpoint.name,
            "model": deploy_model_uri,
            "concurrency": data["additional_concurrency"],
            "input_tokens": deployment_config["avg_context_length"],
            "output_tokens": deployment_config["avg_sequence_length"],
            "target_throughput_per_user": deployment_config["per_session_tokens_per_sec"][1]
            if deployment_config.get("per_session_tokens_per_sec")
            else None,
            "target_ttft": deployment_config["ttft"][0] if deployment_config.get("ttft") else None,
            "target_e2e_latency": deployment_config["e2e_latency"][0]
            if deployment_config.get("e2e_latency")
            else None,
            "credential_id": str(db_endpoint.credential_id) if db_endpoint.credential_id else None,
            "existing_deployment_namespace": db_endpoint.namespace,
            "notification_metadata": {
                "name": BUD_INTERNAL_WORKFLOW,
                "subscriber_ids": str(current_user_id),
                "workflow_id": str(db_workflow.id),
            },
            "source_topic": f"{app_settings.source_topic}",
        }

        # Perform add worker to deployment request
        add_worker_response = await self._perform_add_worker_to_deployment_request(add_worker_payload)

        # Add payload dict to response
        for step in add_worker_response["steps"]:
            step["payload"] = {}

        add_worker_events = {BudServeWorkflowStepEventName.BUDSERVE_CLUSTER_EVENTS.value: add_worker_response}

        current_step_number = current_step_number + 1
        workflow_current_step = current_step_number

        # Update or create next workflow step
        db_workflow_step = await WorkflowStepService(self.session).create_or_update_next_workflow_step(
            db_workflow.id, current_step_number, add_worker_events
        )
        logger.debug(f"Workflow step created with id {db_workflow_step.id}")

        # Update progress in workflow
        add_worker_response["progress_type"] = BudServeWorkflowStepEventName.BUDSERVE_CLUSTER_EVENTS.value
        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"progress": add_worker_response, "current_step": workflow_current_step}
        )

    @staticmethod
    async def _perform_add_worker_to_deployment_request(payload: Dict) -> Dict:
        """Perform add worker to deployment request."""
        add_worker_endpoint = (
            f"{app_settings.dapr_base_url}/v1.0/invoke/{app_settings.bud_cluster_app_id}/method/deployment"
        )
        logger.debug(f"payload for add worker to deployment : {payload}")

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(add_worker_endpoint, json=payload) as response:
                    response_data = await response.json()
                    if response.status >= 400:
                        raise ClientException("Unable to perform add worker to deployment")

                    return response_data
        except ClientException as e:
            raise e
        except Exception as e:
            logger.error(f"Failed to perform add worker to deployment request: {e}")
            raise ClientException("Unable to perform add worker to deployment") from e

    async def add_worker_from_notification_event(self, payload: NotificationPayload) -> None:
        """Add worker from notification event."""
        # Get workflow and workflow steps
        workflow_id = payload.workflow_id
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})
        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        # Define the keys required for endpoint creation
        keys_of_interest = [
            "endpoint_id",
        ]

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]

        logger.debug("Collected required data from workflow steps")

        # Get endpoint id
        db_endpoint = await EndpointDataManager(self.session).retrieve_by_fields(
            EndpointModel,
            {"id": required_data["endpoint_id"]},
            exclude_fields={"status": EndpointStatusEnum.DELETED},
            missing_ok=True,
        )

        if not db_endpoint:
            logger.error(f"Endpoint with id {required_data['endpoint_id']} not found")
            return

        # Add concurrency to existing deployment config
        deployment_config = db_endpoint.deployment_config
        logger.debug(f"Existing deployment config: {deployment_config}")

        existing_concurrency = deployment_config["concurrent_requests"]
        additional_concurrency = payload.content.result.get("result", {}).get("concurrency", 0)
        deployment_config["concurrent_requests"] = existing_concurrency + additional_concurrency

        # Get total replicas
        total_replicas = payload.content.result["deployment_status"]["replicas"]["total"]
        logger.debug(f"Total replicas: {total_replicas}")

        self.session.refresh(db_endpoint)
        db_endpoint = await EndpointDataManager(self.session).update_by_fields(
            db_endpoint, {"deployment_config": deployment_config, "total_replicas": total_replicas}
        )
        logger.debug(f"Updated deployment config: {deployment_config}")

        # Update current step number
        current_step_number = db_workflow.current_step + 1
        workflow_current_step = current_step_number

        execution_status_data = {
            "workflow_execution_status": {
                "status": "success",
                "message": "Deployment successfully updated with additional concurrency",
            },
        }
        # Update or create next workflow step
        db_workflow_step = await WorkflowStepService(self.session).create_or_update_next_workflow_step(
            db_workflow.id, current_step_number, execution_status_data
        )
        logger.debug(f"Upsert workflow step {db_workflow_step.id} for storing endpoint details")

        # Mark workflow as completed
        logger.debug(f"Marking workflow as completed: {workflow_id}")
        await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"status": WorkflowStatusEnum.COMPLETED, "current_step": workflow_current_step}
        )

        # Send notification to workflow creator
        model_icon = await ModelServiceUtil(self.session).get_model_icon(db_endpoint.model)

        notification_request = (
            NotificationBuilder()
            .set_content(
                title=db_endpoint.name,
                message="Worker Added",
                icon=model_icon,
                result=NotificationResult(target_id=db_endpoint.id, target_type="endpoint").model_dump(
                    exclude_none=True, exclude_unset=True
                ),
            )
            .set_payload(workflow_id=str(db_workflow.id), type=NotificationTypeEnum.DEPLOYMENT_SUCCESS.value)
            .set_notification_request(subscriber_ids=[str(db_workflow.created_by)])
            .build()
        )
        await BudNotifyService().send_notification(notification_request)

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/endpoint_ops/models.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The endpoint ops package, containing essential business logic, services, and routing configurations for the endpoint ops."""

import json
from datetime import datetime
from typing import Optional
from uuid import UUID, uuid4

from sqlalchemy import Boolean, DateTime, Enum, ForeignKey, Integer, String, Uuid
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy.orm import Mapped, mapped_column, relationship

from budapp.cluster_ops.models import Cluster
from budapp.commons.constants import EndpointStatusEnum
from budapp.commons.database import Base
from budapp.model_ops.models import Model
from budapp.project_ops.models import Project


class Endpoint(Base):
    """Endpoint model."""

    __tablename__ = "endpoint"
    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    name: Mapped[str] = mapped_column(String, nullable=False)
    project_id: Mapped[UUID] = mapped_column(ForeignKey("project.id", ondelete="CASCADE"), nullable=False)
    model_id: Mapped[UUID] = mapped_column(ForeignKey("model.id", ondelete="CASCADE"), nullable=False)
    cache_enabled: Mapped[bool] = mapped_column(Boolean, default=False)
    cache_config: Mapped[str] = mapped_column(String, nullable=True)
    cluster_id: Mapped[UUID] = mapped_column(ForeignKey("cluster.id", ondelete="CASCADE"), nullable=False)
    bud_cluster_id: Mapped[UUID] = mapped_column(Uuid, nullable=False)
    url: Mapped[str] = mapped_column(String, nullable=False)
    namespace: Mapped[str] = mapped_column(String, nullable=False)
    created_by: Mapped[UUID] = mapped_column(ForeignKey("user.id"), nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    status: Mapped[str] = mapped_column(
        Enum(
            EndpointStatusEnum,
            name="endpoint_status_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=False,
    )
    credential_id: Mapped[Optional[UUID]] = mapped_column(ForeignKey("proprietary_credential.id"), nullable=True)
    status_sync_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    model_configuration: Mapped[dict] = mapped_column(JSONB, nullable=True)
    active_replicas: Mapped[int] = mapped_column(Integer, nullable=False)
    total_replicas: Mapped[int] = mapped_column(Integer, nullable=False)
    number_of_nodes: Mapped[int] = mapped_column(Integer, nullable=False)
    deployment_config: Mapped[dict] = mapped_column(JSONB, nullable=True)

    model: Mapped[Model] = relationship("Model", back_populates="endpoints", foreign_keys=[model_id])
    # worker: Mapped[Worker] = relationship(
    #     "Worker",
    #     back_populates="endpoint",
    #     cascade="all, delete",
    #     passive_deletes=True,
    # )
    project: Mapped[Project] = relationship("Project", back_populates="endpoints", foreign_keys=[project_id])

    cluster: Mapped[Cluster] = relationship("Cluster", back_populates="endpoints", foreign_keys=[cluster_id])
    created_user: Mapped["User"] = relationship(back_populates="created_endpoints", foreign_keys=[created_by])
    credential: Mapped[Optional["ProprietaryCredential"]] = relationship(
        "ProprietaryCredential", back_populates="endpoints"
    )

    @hybrid_property
    def cache_config_dict(self):
        if not self.cache_config:
            return {}
        return json.loads(self.cache_config)

    def to_dict(self):
        return {
            "id": str(self.id),
            "status": self.status,
            "project_id": str(self.project_id),
            "model_id": str(self.model_id),
            "cache_enabled": self.cache_enabled,
            "cache_config": self.cache_config_dict,
            "cluster_id": str(self.cluster_id),
            "url": self.url,
            "namespace": self.namespace,
            "replicas": self.replicas,
            "created_at": self.created_at.isoformat(),
            "modified_at": self.modified_at.isoformat(),
        }

    @classmethod
    def from_dict(cls, data: dict):
        return cls(
            id=data.get("id"),
            status=data.get("status"),
            project_id=UUID(data.get("project_id")),
            model_id=UUID(data.get("model_id")),
            cache_enabled=data.get("cache_enabled"),
            cache_config=json.dumps(data.get("cache_config")),
            cluster_id=UUID(data.get("cluster_id")),
            url=data.get("url"),
            namespace=data.get("namespace"),
            replicas=data.get("replicas"),
            created_at=datetime.fromisoformat(data.get("created_at")),
            modified_at=datetime.fromisoformat(data.get("modified_at")),
        )

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/endpoint_ops/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, containing essential business logic, services, and routing configurations for the endpoint ops."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/endpoint_ops/endpoint_routes.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The endpoint ops package, containing essential business logic, services, and routing configurations for the endpoint ops."""

from typing import List, Optional, Union
from uuid import UUID

from fastapi import APIRouter, Depends, Query, status
from sqlalchemy.orm import Session
from typing_extensions import Annotated

from budapp.commons import logging
from budapp.commons.dependencies import (
    get_current_active_user,
    get_session,
    parse_ordering_fields,
)
from budapp.commons.exceptions import ClientException
from budapp.user_ops.schemas import User

from ..commons.schemas import ErrorResponse, SuccessResponse
from ..workflow_ops.schemas import RetrieveWorkflowDataResponse
from ..workflow_ops.services import WorkflowService
from .schemas import (
    AddWorkerRequest,
    DeleteWorkerRequest,
    EndpointFilter,
    EndpointPaginatedResponse,
    ModelClusterDetailResponse,
    WorkerDetailResponse,
    WorkerInfoFilter,
    WorkerInfoResponse,
)
from .services import EndpointService


logger = logging.get_logger(__name__)

endpoint_router = APIRouter(prefix="/endpoints", tags=["endpoint"])


@endpoint_router.get(
    "/",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": EndpointPaginatedResponse,
            "description": "Successfully list all endpoints",
        },
    },
    description="List all endpoints. \n\n order_by fields are: name, status, created_at, modified_at, cluster_name, model_name, modality",
)
async def list_all_endpoints(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    filters: Annotated[EndpointFilter, Depends()],
    project_id: UUID = Query(description="List endpoints by project id"),
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=0),
    order_by: Optional[List[str]] = Depends(parse_ordering_fields),
    search: bool = False,
) -> Union[EndpointPaginatedResponse, ErrorResponse]:
    """List all endpoints."""
    # Calculate offset
    offset = (page - 1) * limit

    # Construct filters
    filters_dict = filters.model_dump(exclude_none=True, exclude_unset=True)

    try:
        db_endpoints, count = await EndpointService(session).get_all_endpoints(
            project_id, offset, limit, filters_dict, order_by, search
        )
    except ClientException as e:
        logger.exception(f"Failed to get all endpoints: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to get all endpoints: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to get all endpoints"
        ).to_http_response()

    return EndpointPaginatedResponse(
        endpoints=db_endpoints,
        total_record=count,
        page=page,
        limit=limit,
        object="endpoints.list",
        code=status.HTTP_200_OK,
        message="Successfully list all endpoints",
    ).to_http_response()


@endpoint_router.post(
    "/{endpoint_id}/delete-workflow",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Invalid request parameters",
        },
        status.HTTP_200_OK: {
            "model": SuccessResponse,
            "description": "Successfully executed delete endpoint workflow",
        },
    },
    description="Delete an endpoint by ID",
)
async def delete_endpoint(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    endpoint_id: UUID,
) -> Union[SuccessResponse, ErrorResponse]:
    """Delete a endpoint by its ID."""
    try:
        db_workflow = await EndpointService(session).delete_endpoint(endpoint_id, current_user.id)
        logger.debug(f"Endpoint deleting initiated with workflow id: {db_workflow.id}")
        return SuccessResponse(
            message="Deployment deleting initiated successfully",
            code=status.HTTP_200_OK,
            object="endpoint.delete",
        )
    except ClientException as e:
        logger.exception(f"Failed to delete endpoint: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to delete endpoint: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            message="Failed to delete endpoint",
        ).to_http_response()


@endpoint_router.get(
    "/{endpoint_id}/workers",
    responses={
        status.HTTP_200_OK: {
            "model": WorkerInfoResponse,
            "description": "Successfully get endpoint detail",
        },
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Failed to get endpoint workers",
        },
        status.HTTP_404_NOT_FOUND: {
            "model": ErrorResponse,
            "description": "Endpoint not found",
        },
    },
)
async def get_endpoint_workers(
    endpoint_id: UUID,
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    filters: Annotated[WorkerInfoFilter, Depends()],
    refresh: bool = Query(False),  # noqa: B008
    page: int = Query(1, ge=1),  # noqa: B008
    limit: int = Query(10, ge=0),  # noqa: B008
    order_by: Optional[List[str]] = Query(None),  # noqa: B008
    search: bool = Query(False),  # noqa: B008
) -> Union[WorkerInfoResponse, ErrorResponse]:
    """Get endpoint workers."""
    try:
        workers = await EndpointService(session).get_endpoint_workers(
            endpoint_id, filters, refresh, page, limit, order_by, search
        )
        response = WorkerInfoResponse(**workers)
    except ClientException as e:
        logger.exception(f"Failed to get endpoint workers: {e}")
        response = ErrorResponse(message=e.message, code=e.status_code)
    except Exception as e:
        logger.exception(f"Failed to get endpoint workers: {e}")
        response = ErrorResponse(message="Failed to get endpoint workers", code=status.HTTP_500_INTERNAL_SERVER_ERROR)
    return response.to_http_response()


@endpoint_router.get(
    "/{endpoint_id}/workers/{worker_id}",
    responses={
        status.HTTP_200_OK: {
            "model": WorkerDetailResponse,
            "description": "Successfully get endpoint detail",
        },
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Failed to get endpoint workers",
        },
        status.HTTP_404_NOT_FOUND: {
            "model": ErrorResponse,
            "description": "Worker not found",
        },
    },
)
async def get_endpoint_worker_detail(
    endpoint_id: UUID,
    worker_id: UUID,
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
) -> Union[WorkerDetailResponse, ErrorResponse]:
    """Get endpoint workers."""
    try:
        worker_detail = await EndpointService(session).get_endpoint_worker_detail(endpoint_id, worker_id)
        response = WorkerDetailResponse(**worker_detail)
    except ClientException as e:
        logger.exception(f"Failed to get endpoint worker detail: {e}")
        response = ErrorResponse(message=e.message, code=e.status_code)
    except Exception as e:
        logger.exception(f"Failed to get endpoint worker detail: {e}")
        response = ErrorResponse(
            message="Failed to get endpoint worker detail", code=status.HTTP_500_INTERNAL_SERVER_ERROR
        )
    return response.to_http_response()


@endpoint_router.get(
    "/{endpoint_id}/model-cluster-detail",
    responses={
        status.HTTP_200_OK: {
            "model": ModelClusterDetailResponse,
            "description": "Successfully get model cluster detail",
        },
        status.HTTP_404_NOT_FOUND: {
            "model": ErrorResponse,
            "description": "Endpoint not found",
        },
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Failed to get model cluster detail",
        },
    },
)
async def get_model_cluster_detail(
    endpoint_id: UUID,
    _: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
) -> Union[ModelClusterDetailResponse, ErrorResponse]:
    """Get model cluster detail."""
    try:
        model_cluster_detail = await EndpointService(session).get_model_cluster_detail(endpoint_id)
        response = ModelClusterDetailResponse(
            object="endpoint.detail",
            result=model_cluster_detail,
            message="Successfully fetched model cluster detail for the deployment.",
        )
    except ClientException as e:
        logger.exception(f"Failed to get model cluster detail: {e}")
        response = ErrorResponse(message=e.message, code=e.status_code)
    except Exception as e:
        logger.exception(f"Failed to get model cluster detail: {e}")
        response = ErrorResponse(
            message="Failed to get model cluster detail", code=status.HTTP_500_INTERNAL_SERVER_ERROR
        )
    return response.to_http_response()


@endpoint_router.post(
    "/delete-worker",
    responses={
        status.HTTP_200_OK: {
            "model": SuccessResponse,
            "description": "Successfully deleted deploymentworker",
        },
        status.HTTP_404_NOT_FOUND: {
            "model": ErrorResponse,
            "description": "Worker not found",
        },
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Failed to delete deployment worker",
        },
    },
)
async def delete_endpoint_worker(
    request: DeleteWorkerRequest,
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
) -> Union[SuccessResponse, ErrorResponse]:
    """Delete a endpoint worker by its ID."""
    try:
        db_workflow = await EndpointService(session).delete_endpoint_worker(request.endpoint_id, request.worker_id, request.worker_name, current_user.id)
        logger.debug(f"Endpoint deleting initiated with workflow id: {db_workflow.id}")
        response = SuccessResponse(
            message="Worker deleting initiated successfully",
            code=status.HTTP_200_OK,
            object="worker.delete",
        )
    except ClientException as e:
        logger.exception(f"Failed to get endpoint worker detail: {e}")
        response = ErrorResponse(message=e.message, code=e.status_code)
    except Exception as e:
        logger.exception(f"Failed to get endpoint worker detail: {e}")
        response = ErrorResponse(message="Failed to get endpoint worker detail", code=status.HTTP_500_INTERNAL_SERVER_ERROR)
    return response.to_http_response()


@endpoint_router.post(
    "/add-worker",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": RetrieveWorkflowDataResponse,
            "description": "Successfully add worker",
        },
    },
    description="Add worker to endpoint",
)
async def add_worker_to_endpoint(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    request: AddWorkerRequest,
) -> Union[RetrieveWorkflowDataResponse, ErrorResponse]:
    """Add worker to endpoint."""
    try:
        db_workflow = await EndpointService(session).add_worker_to_endpoint_workflow(
            current_user_id=current_user.id,
            request=request,
        )

        return await WorkflowService(session).retrieve_workflow_data(db_workflow.id)
    except ClientException as e:
        logger.exception(f"Failed to add worker to endpoint: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to add worker to endpoint: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to add worker to endpoint"
        ).to_http_response()


```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/endpoint_ops/schemas.py`:

```py
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Contains core Pydantic schemas used for data validation and serialization within the core services."""

from datetime import datetime
from enum import Enum
from typing import Optional
from uuid import UUID

from pydantic import UUID4, BaseModel, ConfigDict, Field, model_validator

from budapp.cluster_ops.schemas import ClusterResponse
from budapp.commons.constants import EndpointStatusEnum
from budapp.commons.schemas import PaginatedSuccessResponse, SuccessResponse
from budapp.model_ops.schemas import ModelDetailResponse, ModelResponse


# Endpoint schemas


class EndpointCreate(BaseModel):
    """Create endpoint schema."""

    project_id: UUID4
    model_id: UUID4
    cluster_id: UUID4
    bud_cluster_id: UUID4
    name: str
    url: str
    namespace: str
    status: EndpointStatusEnum
    created_by: UUID4
    status_sync_at: datetime
    credential_id: UUID4 | None
    active_replicas: int
    total_replicas: int
    number_of_nodes: int
    deployment_config: dict | None


class EndpointFilter(BaseModel):
    """Filter endpoint schema for filtering endpoints based on specific criteria."""

    name: str | None = None
    status: EndpointStatusEnum | None = None


class EndpointResponse(BaseModel):
    """Endpoint response schema."""

    model_config = ConfigDict(from_attributes=True)

    id: UUID4
    name: str
    status: EndpointStatusEnum
    deployment_config: dict
    created_at: datetime
    modified_at: datetime


class EndpointListResponse(BaseModel):
    """Endpoint list response schema."""

    model_config = ConfigDict(from_attributes=True)

    id: UUID4
    name: str
    status: EndpointStatusEnum
    model: ModelResponse
    cluster: ClusterResponse
    created_at: datetime
    modified_at: datetime


class EndpointPaginatedResponse(PaginatedSuccessResponse):
    """Endpoint paginated response schema."""

    endpoints: list[EndpointListResponse] = []


class WorkerInfoFilter(BaseModel):
    """Filter for worker info."""

    status: str | None = None
    hardware: str | None = None
    utilization_min: int | None = None
    utilization_max: int | None = None


class DeploymentStatusEnum(str, Enum):
    READY = "ready"
    PENDING = "pending"
    INGRESS_FAILED = "ingress_failed"
    FAILED = "failed"


class WorkerData(BaseModel):
    """Worker data."""

    cluster_id: Optional[UUID] = None
    namespace: Optional[str] = None
    name: str
    status: str
    node_name: str
    device_name: str
    utilization: Optional[str] = None
    hardware: str
    uptime: str
    last_restart_datetime: Optional[datetime] = None
    last_updated_datetime: Optional[datetime] = None
    created_datetime: datetime
    node_ip: str
    cores: int
    memory: str
    deployment_status: Optional[DeploymentStatusEnum] = None
    concurrency: int


class WorkerInfo(WorkerData):
    """Worker info."""

    model_config = ConfigDict(orm_mode=True, from_attributes=True)

    id: UUID


class WorkerInfoResponse(PaginatedSuccessResponse):
    """Response body for getting worker info."""

    model_config = ConfigDict(extra="allow")

    workers: list[WorkerInfo]


class WorkerDetailResponse(SuccessResponse):
    """Worker detail response."""

    model_config = ConfigDict(extra="allow")

    worker: WorkerInfo


class ModelClusterDetail(BaseModel):
    """Model cluster detail."""

    model_config = ConfigDict(extra="allow")

    id: UUID
    name: str
    status: str
    model: ModelDetailResponse
    cluster: ClusterResponse
    deployment_config: dict


class ModelClusterDetailResponse(SuccessResponse):
    """Model cluster detail response."""

    model_config = ConfigDict(extra="allow")

    result: ModelClusterDetail


class AddWorkerRequest(BaseModel):
    """Add worker request."""

    workflow_id: UUID4 | None = None
    workflow_total_steps: int | None = None
    step_number: int = Field(..., gt=0)
    trigger_workflow: bool = False
    endpoint_id: UUID4 | None = None
    additional_concurrency: int | None = Field(None, gt=0)

    @model_validator(mode="after")
    def validate_fields(self) -> "AddWorkerRequest":
        """Validate the fields of the request."""
        if self.workflow_id is None and self.workflow_total_steps is None:
            raise ValueError("workflow_total_steps is required when workflow_id is not provided")

        if self.workflow_id is not None and self.workflow_total_steps is not None:
            raise ValueError("workflow_total_steps and workflow_id cannot be provided together")

        return self


class AddWorkerWorkflowStepData(BaseModel):
    """Add worker workflow step data."""

    endpoint_id: UUID4 | None = None
    additional_concurrency: int | None = None


class DeleteWorkerRequest(BaseModel):
    """Delete worker request."""

    endpoint_id: UUID4
    worker_id: UUID4
    worker_name: str

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/endpoint_ops/crud.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The crud package, containing essential business logic, services, and routing configurations for the endpoint ops."""

from typing import Any, Dict, List, Tuple
from uuid import UUID

from sqlalchemy import and_, asc, desc, distinct, func, or_, select

from budapp.cluster_ops.models import Cluster as ClusterModel
from budapp.commons import logging
from budapp.commons.constants import EndpointStatusEnum
from budapp.commons.db_utils import DataManagerUtils
from budapp.model_ops.models import Model as Model

from ..project_ops.models import Project as ProjectModel
from .models import Endpoint as EndpointModel


logger = logging.get_logger(__name__)


class EndpointDataManager(DataManagerUtils):
    """Data manager for the Endpoint model."""

    async def get_all_active_endpoints(
        self,
        project_id: UUID,
        offset: int = 0,
        limit: int = 10,
        filters: Dict[str, Any] = {},
        order_by: List[Tuple[str, str]] = [],
        search: bool = False,
    ) -> Tuple[List[EndpointModel], int]:
        """Get all active endpoints from the database."""
        await self.validate_fields(EndpointModel, filters)

        # explicit conditions for order by model_name, cluster_name, modality
        explicit_conditions = []
        for field in order_by:
            if field[0] == "model_name":
                sorting_stmt = await self.generate_sorting_stmt(
                    Model,
                    [
                        ("name", field[1]),
                    ],
                )
                explicit_conditions.append(sorting_stmt[0])
            elif field[0] == "cluster_name":
                sorting_stmt = await self.generate_sorting_stmt(
                    ClusterModel,
                    [
                        ("name", field[1]),
                    ],
                )
                explicit_conditions.append(sorting_stmt[0])
            elif field[0] == "modality":
                sorting_stmt = await self.generate_sorting_stmt(
                    Model,
                    [
                        ("modality", field[1]),
                    ],
                )
                explicit_conditions.append(sorting_stmt[0])

        # Generate statements according to search or filters
        if search:
            search_conditions = await self.generate_search_stmt(EndpointModel, filters)
            stmt = (
                select(EndpointModel)
                .join(Model)
                .join(ClusterModel)
                .filter(or_(*search_conditions))
                .filter(
                    and_(EndpointModel.status != EndpointStatusEnum.DELETED, EndpointModel.project_id == project_id)
                )
            )
            count_stmt = (
                select(func.count())
                .select_from(EndpointModel)
                .join(Model)
                .join(ClusterModel)
                .filter(and_(*search_conditions))
                .filter(
                    and_(EndpointModel.status != EndpointStatusEnum.DELETED, EndpointModel.project_id == project_id)
                )
            )
        else:
            stmt = select(EndpointModel).join(Model).join(ClusterModel)
            count_stmt = select(func.count()).select_from(EndpointModel).join(Model).join(ClusterModel)
            for key, value in filters.items():
                stmt = stmt.filter(getattr(EndpointModel, key) == value)
                count_stmt = count_stmt.filter(getattr(EndpointModel, key) == value)
            stmt = stmt.filter(
                and_(EndpointModel.status != EndpointStatusEnum.DELETED, EndpointModel.project_id == project_id)
            )
            count_stmt = count_stmt.filter(
                and_(EndpointModel.status != EndpointStatusEnum.DELETED, EndpointModel.project_id == project_id)
            )

        # Calculate count before applying limit and offset
        count = self.execute_scalar(count_stmt)

        # Apply limit and offset
        stmt = stmt.limit(limit).offset(offset)

        # Apply sorting
        if order_by:
            sort_conditions = await self.generate_sorting_stmt(EndpointModel, order_by)
            # Extend sort conditions with explicit conditions
            sort_conditions.extend(explicit_conditions)
            stmt = stmt.order_by(*sort_conditions)

        result = self.scalars_all(stmt)

        return result, count

    async def get_all_endpoints_in_cluster(
        self, cluster_id: UUID, offset: int, limit: int, filters: Dict[str, Any], order_by: List[str], search: bool
    ) -> Tuple[List[EndpointModel], int, int, int]:
        """Get all endpoints in a cluster."""
        await self.validate_fields(EndpointModel, filters)

        # Base conditions
        base_conditions = [
            EndpointModel.cluster_id == cluster_id,
            EndpointModel.status != EndpointStatusEnum.DELETED,
        ]

        if search:
            search_conditions = await self.generate_search_stmt(EndpointModel, filters)

            stmt = (
                select(
                    EndpointModel,
                    ProjectModel.name.label("project_name"),
                    Model.name.label("model_name"),
                    EndpointModel.total_replicas.label("total_workers"),
                    EndpointModel.active_replicas.label("active_workers"),
                )
                .join(ProjectModel, ProjectModel.id == EndpointModel.project_id)
                .join(Model, Model.id == EndpointModel.model_id)
                .filter(*base_conditions)
                .filter(and_(*search_conditions))
                .group_by(EndpointModel.id, ProjectModel.name, Model.name)
            )

            count_stmt = (
                select(func.count(distinct(EndpointModel.id)))
                .select_from(EndpointModel)
                .filter(*base_conditions)
                .filter(and_(*search_conditions))
            )
        else:
            filter_conditions = [getattr(EndpointModel, field) == value for field, value in filters.items()]
            stmt = (
                select(
                    EndpointModel,
                    ProjectModel.name.label("project_name"),
                    Model.name.label("model_name"),
                    EndpointModel.total_replicas.label("total_workers"),
                    EndpointModel.active_replicas.label("active_workers"),
                )
                .join(ProjectModel, ProjectModel.id == EndpointModel.project_id)
                .join(Model, Model.id == EndpointModel.model_id)
                .filter(*base_conditions)
                .filter(*filter_conditions)
                .group_by(EndpointModel.id, ProjectModel.name, Model.name)
            )

            count_stmt = (
                select(func.count(distinct(EndpointModel.id)))
                .select_from(EndpointModel)
                .filter(*base_conditions)
                .filter(*filter_conditions)
            )

        # Get count
        count = self.execute_scalar(count_stmt)

        # Apply sorting and limit/offset
        stmt = stmt.limit(limit).offset(offset)

        if order_by:
            sort_conditions = await self.generate_sorting_stmt(EndpointModel, order_by)

            # Handle sorting for project_name, model_name, and total_workers
            for field, direction in order_by:
                sort_func = asc if direction == "asc" else desc
                if field == "project_name":
                    stmt = stmt.order_by(sort_func("project_name"))
                elif field == "model_name":
                    stmt = stmt.order_by(sort_func("model_name"))
                elif field == "total_workers":
                    stmt = stmt.order_by(sort_func("total_workers"))
                elif field == "active_workers":
                    stmt = stmt.order_by(sort_func("active_workers"))

            stmt = stmt.order_by(*sort_conditions)

        result = self.session.execute(stmt)

        return result, count

    async def get_cluster_count_details(self, cluster_id: UUID) -> Tuple[int, int, int, int]:
        """
        Retrieve cluster statistics including:
        - Total endpoints count (excluding deleted ones)
        - Running endpoints count
        - Sum of active replicas (workers)
        - Sum of total replicas (workers)

        Args:
            cluster_id (UUID): The ID of the cluster.

        Returns:
            Tuple[int, int, int, int]:
            (total_endpoints_count, running_endpoints_count, active_workers_count, total_workers_count)
        """
        query = select(
            func.count().filter(EndpointModel.status != EndpointStatusEnum.DELETED).label("total_endpoints"),
            func.count().filter(EndpointModel.status == EndpointStatusEnum.RUNNING).label("running_endpoints"),
            func.coalesce(
                func.sum(EndpointModel.active_replicas).filter(EndpointModel.status != EndpointStatusEnum.DELETED), 0
            ).label("active_workers"),
            func.coalesce(
                func.sum(EndpointModel.total_replicas).filter(EndpointModel.status != EndpointStatusEnum.DELETED), 0
            ).label("total_workers"),
        ).where(EndpointModel.cluster_id == cluster_id)

        result = self.session.execute(query)
        total_endpoints, running_endpoints, active_replicas, total_replicas = result.fetchone()

        return total_endpoints, running_endpoints, active_replicas, total_replicas

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/__about__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""Contains metadata about the package, including version information and author details."""

__version__ = "budapp@0.0.1"

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/project_ops/services.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The project ops services. Contains business logic for project ops."""

from typing import Any, Dict, List, Tuple
from uuid import UUID

from budapp.commons import logging
from budapp.commons.db_utils import SessionMixin
from budapp.commons.exceptions import ClientException

from ..cluster_ops.crud import ClusterDataManager
from ..commons.constants import ProjectStatusEnum
from ..commons.helpers import get_hardware_types
from .crud import ProjectDataManager
from .models import Project as ProjectModel
from .schemas import ProjectClusterListResponse, ProjectResponse


logger = logging.get_logger(__name__)


class ProjectService(SessionMixin):
    """Project service."""

    async def edit_project(self, project_id: UUID, data: Dict[str, Any]) -> ProjectResponse:
        """Edit project by validating and updating specific fields."""
        # Retrieve existing model
        db_project = await ProjectDataManager(self.session).retrieve_by_fields(
            model=ProjectModel,
            fields={"id": project_id, "status": ProjectStatusEnum.ACTIVE},
        )

        if "name" in data:
            duplicate_project = await ProjectDataManager(self.session).retrieve_by_fields(
                model=ProjectModel,
                fields={"name": data["name"], "status": ProjectStatusEnum.ACTIVE},
                exclude_fields={"id": project_id},
                missing_ok=True,
                case_sensitive=False,
            )
            if duplicate_project:
                raise ClientException("Project name already exists")

        db_project = await ProjectDataManager(self.session).update_by_fields(db_project, data)

        return db_project

    async def get_all_clusters_in_project(
        self, project_id: UUID, offset: int, limit: int, filters: Dict[str, Any], order_by: List[str], search: bool
    ) -> Tuple[List[ProjectClusterListResponse], int]:
        """Get all clusters in a project."""
        db_results, count = await ClusterDataManager(self.session).get_all_clusters_in_project(
            project_id, offset, limit, filters, order_by, search
        )

        result = []
        for db_result in db_results:
            db_cluster = db_result[0]
            endpoint_count = db_result[1]
            total_nodes = db_result[2]
            total_replicas = db_result[3]
            result.append(
                ProjectClusterListResponse(
                    id=db_cluster.id,
                    name=db_cluster.name,
                    endpoint_count=endpoint_count,
                    hardware_type=get_hardware_types(db_cluster.cpu_count, db_cluster.gpu_count, db_cluster.hpu_count),
                    node_count=total_nodes,
                    worker_count=total_replicas,
                    status=db_cluster.status,
                    created_at=db_cluster.created_at,
                    modified_at=db_cluster.modified_at,
                )
            )

        return result, count

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/project_ops/models.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The project ops package, containing essential business logic, services, and routing configurations for the project ops."""

from datetime import datetime
from uuid import UUID, uuid4

from sqlalchemy import Boolean, Column, DateTime, ForeignKey, String, Table, Uuid, Enum
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import Mapped, mapped_column, relationship

from budapp.commons.database import Base
from ..commons.constants import ProjectStatusEnum

project_user_association = Table(
    "project_user_association",
    Base.metadata,
    Column("project_id", Uuid, ForeignKey("project.id"), primary_key=True),
    Column("user_id", Uuid, ForeignKey("user.id"), primary_key=True),
)


class Project(Base):
    """Project model."""

    __tablename__ = "project"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    name: Mapped[str] = mapped_column(String, nullable=False)
    description: Mapped[str] = mapped_column(String)
    tags: Mapped[list[dict]] = mapped_column(JSONB, nullable=True)
    icon: Mapped[str] = mapped_column(String, nullable=True)
    status: Mapped[str] = mapped_column(
        Enum(
            ProjectStatusEnum,
            name="project_status_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=False,
        default=ProjectStatusEnum.ACTIVE,
    )
    benchmark: Mapped[bool] = mapped_column(Boolean, default=False)
    created_by: Mapped[UUID] = mapped_column(ForeignKey("user.id"), nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    users: Mapped[list["User"]] = relationship("User", secondary=project_user_association, back_populates="projects")
    endpoints: Mapped[list["Endpoint"]] = relationship(
        "Endpoint",
        back_populates="project",
    )
    # project_permissions: Mapped[list[ProjectPermission]] = relationship(back_populates="project", cascade="all, delete")
    created_user: Mapped["User"] = relationship(back_populates="created_projects", foreign_keys=[created_by])

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/project_ops/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, containing essential business logic, services, and routing configurations for the project ops."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/project_ops/schemas.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------


"""Contains core Pydantic schemas used for data validation and serialization within the project ops services."""

from datetime import datetime
from typing import List, Literal

from pydantic import (
    UUID4,
    BaseModel,
    ConfigDict,
    Field,
    field_validator,
)

from budapp.commons.schemas import PaginatedSuccessResponse, SuccessResponse, Tag

from ..commons.constants import ClusterStatusEnum
from ..commons.helpers import validate_icon


class ProjectBase(BaseModel):
    name: str
    description: str | None = None
    tags: List[Tag] | None = None
    icon: str | None = None


class EditProjectRequest(BaseModel):
    name: str | None = Field(None, min_length=1, max_length=100)
    description: str | None = Field(None, max_length=400)
    tags: List[Tag] | None = None
    icon: str | None = None

    @field_validator("name", mode="before")
    @classmethod
    def validate_name(cls, value: str | None) -> str | None:
        """Ensure the name is not empty or only whitespace."""
        if value is not None and not value.strip():
            raise ValueError("Project name cannot be empty or only whitespace.")
        return value

    @field_validator("icon", mode="before")
    @classmethod
    def icon_validate(cls, value: str | None) -> str | None:
        """Validate the icon."""
        if value is not None and not validate_icon(value):
            raise ValueError("invalid icon")
        return value


class ProjectResponse(ProjectBase):
    """Project response to client schema"""

    model_config = ConfigDict(from_attributes=True)

    id: UUID4


class SingleProjectResponse(SuccessResponse):
    project: ProjectResponse


class ProjectClusterListResponse(BaseModel):
    """Project cluster list response schema."""

    model_config = ConfigDict(from_attributes=True)

    id: UUID4
    name: str
    endpoint_count: int
    hardware_type: list[Literal["CPU", "GPU", "HPU"]]
    node_count: int
    worker_count: int
    status: ClusterStatusEnum
    created_at: datetime
    modified_at: datetime


class ProjectClusterPaginatedResponse(PaginatedSuccessResponse):
    """Project cluster paginated response schema."""

    clusters: list[ProjectClusterListResponse] = []


class ProjectClusterFilter(BaseModel):
    """Filter project cluster schema for filtering clusters based on specific criteria."""

    name: str | None = None
    status: ClusterStatusEnum | None = None

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/project_ops/project_routes.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The project ops package, containing essential business logic, services, and routing configurations for the project ops."""

from typing import List, Optional, Union
from uuid import UUID

from fastapi import APIRouter, Depends, Query, status
from sqlalchemy.orm import Session
from typing_extensions import Annotated

from budapp.commons import logging
from budapp.commons.dependencies import (
    get_current_active_user,
    get_session,
    parse_ordering_fields,
)
from budapp.commons.exceptions import ClientException
from budapp.commons.schemas import ErrorResponse
from budapp.user_ops.schemas import User

from .schemas import EditProjectRequest, ProjectClusterFilter, ProjectClusterPaginatedResponse, SingleProjectResponse
from .services import ProjectService


logger = logging.get_logger(__name__)

project_router = APIRouter(prefix="/projects", tags=["project"])


@project_router.patch(
    "/{project_id}",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": SingleProjectResponse,
            "description": "Successfully edited project",
        },
    },
    description="Edit project",
)
async def edit_project(
    project_id: UUID,
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    edit_project: EditProjectRequest,
) -> Union[SingleProjectResponse, ErrorResponse]:
    """Edit project"""
    try:
        db_project = await ProjectService(session).edit_project(
            project_id=project_id, data=edit_project.model_dump(exclude_unset=True, exclude_none=True)
        )
        return SingleProjectResponse(
            project=db_project,
            message="Project details updated successfully",
            code=status.HTTP_200_OK,
            object="project.edit",
        )
    except ClientException as e:
        logger.exception(f"Failed to edit project: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to edit project: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to edit project"
        ).to_http_response()


@project_router.get(
    "/{project_id}/clusters",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": ProjectClusterPaginatedResponse,
            "description": "Successfully list all clusters in a project",
        },
    },
    description="List all clusters in a project.\n\nOrder by values are: name, endpoint_count, hardware_type, node_count, worker_count, status, created_at, modified_at",
)
async def list_all_clusters(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    filters: Annotated[ProjectClusterFilter, Depends()],
    project_id: UUID,
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=0),
    order_by: Optional[List[str]] = Depends(parse_ordering_fields),
    search: bool = False,
) -> Union[ProjectClusterPaginatedResponse, ErrorResponse]:
    """List all clusters in a project."""
    # Calculate offset
    offset = (page - 1) * limit

    # Construct filters
    filters_dict = filters.model_dump(exclude_none=True, exclude_unset=True)

    try:
        result, count = await ProjectService(session).get_all_clusters_in_project(
            project_id, offset, limit, filters_dict, order_by, search
        )
    except ClientException as e:
        logger.exception(f"Failed to get all clusters: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to get all clusters: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to get all clusters"
        ).to_http_response()

    return ProjectClusterPaginatedResponse(
        clusters=result,
        total_record=count,
        page=page,
        limit=limit,
        object="project.clusters.list",
        code=status.HTTP_200_OK,
        message="Successfully list all clusters in a project",
    ).to_http_response()

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/project_ops/crud.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The crud package, containing essential business logic, services, and routing configurations for the project ops."""

from uuid import UUID
from typing import Tuple
from sqlalchemy import func, distinct, select

from budapp.commons import logging
from budapp.commons.db_utils import DataManagerUtils
from .models import project_user_association, Project
from ..commons.constants import ProjectStatusEnum, UserStatusEnum
from ..user_ops.models import User

logger = logging.get_logger(__name__)


class ProjectDataManager(DataManagerUtils):
    """Data manager for the Project model."""

    def get_unique_user_count_in_all_projects(self) -> int:
        """
        Get the count of unique users across all active projects.

        Returns:
            int: Count of unique users in all active projects.
        """
        unique_users_stmt = (
            select(func.count(distinct(project_user_association.c.user_id)))
            .join(Project, project_user_association.c.project_id == Project.id)
            .join(User, project_user_association.c.user_id == User.id)
            .where(
                Project.status == ProjectStatusEnum.ACTIVE,
                User.status != UserStatusEnum.DELETED,
            )
        )
        return self.scalar_one_or_none(unique_users_stmt) or 0

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/main.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The main entry point for the application, initializing the FastAPI app and setting up the application's lifespan management, including configuration and secret syncs."""

import asyncio
from contextlib import asynccontextmanager
from typing import Any, AsyncIterator

from fastapi import APIRouter, FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.openapi.utils import get_openapi
from fastapi.staticfiles import StaticFiles

from .auth import auth_routes
from .cluster_ops import cluster_routes
from .credential_ops import credential_routes
from .commons import logging
from .commons.config import app_settings
from .commons.constants import Environment
from .core import common_routes, meta_routes, notify_routes
from .endpoint_ops import endpoint_routes
from .initializers.seeder import seeders
from .metric_ops import metric_routes
from .model_ops import model_routes
from .project_ops import project_routes
from .user_ops import user_routes
from .workflow_ops import workflow_routes


logger = logging.get_logger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncIterator[None]:
    """Manage the lifespan of the FastAPI application, including scheduling periodic syncs of configurations and secrets.

    This context manager starts a background task that periodically syncs configurations and secrets from
    their respective stores if they are configured. The sync intervals are randomized between 90% and 100%
    of the maximum sync interval specified in the application settings. The task is canceled upon exiting the
    context.

    Args:
        app (FastAPI): The FastAPI application instance.

    Yields:
        None: Yields control back to the context where the lifespan management is performed.
    """

    async def schedule_secrets_and_config_sync() -> None:
        from random import randint

        await asyncio.sleep(3)

        await meta_routes.register_service()
        while True:
            await meta_routes.sync_configurations()
            await meta_routes.sync_secrets()

            await asyncio.sleep(
                randint(
                    int(app_settings.max_sync_interval * 0.9),
                    app_settings.max_sync_interval,
                )
            )

    if app_settings.configstore_name or app_settings.secretstore_name:
        task = asyncio.create_task(schedule_secrets_and_config_sync())
    else:
        task = None

    for seeder_name, seeder in seeders.items():
        try:
            await seeder().seed()
            logger.info(f"Seeded {seeder_name} seeder successfully.")
        except Exception as e:
            logger.error(f"Failed to seed {seeder_name}. Error: {e}")

    yield

    if task is not None:
        try:
            task.cancel()
        except asyncio.CancelledError:
            logger.exception("Failed to cleanup config & store sync.")


app = FastAPI(
    title=app_settings.name,
    description=app_settings.description,
    version=app_settings.version,
    root_path=app_settings.api_root,
    lifespan=lifespan,
    openapi_url=None if app_settings.env == Environment.PRODUCTION else "/openapi.json",
)

# Serve static files
app.mount("/static", StaticFiles(directory=app_settings.static_dir), name="static")

# Set all CORS enabled origins
if app_settings.cors_origins:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[str(origin).strip("/") for origin in app_settings.cors_origins],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

internal_router = APIRouter()
internal_router.include_router(auth_routes.auth_router)
internal_router.include_router(cluster_routes.cluster_router)
internal_router.include_router(common_routes.common_router)
internal_router.include_router(credential_routes.credential_router)
internal_router.include_router(endpoint_routes.endpoint_router)
internal_router.include_router(meta_routes.meta_router)
internal_router.include_router(metric_routes.metric_router)
internal_router.include_router(model_routes.model_router)
internal_router.include_router(notify_routes.notify_router)
internal_router.include_router(user_routes.user_router)
internal_router.include_router(workflow_routes.workflow_router)
internal_router.include_router(project_routes.project_router)

app.include_router(internal_router)


# Override schemas for Swagger documentation
app.openapi_schema = None  # Clear the cached schema


def custom_openapi() -> Any:
    """Customize the OpenAPI schema for Swagger documentation.

    This function modifies the OpenAPI schema to include both API and PubSub models for routes that are marked as PubSub API endpoints.
    This approach allows the API to handle both direct API calls and PubSub events using the same endpoint, while providing clear documentation for API users in the Swagger UI.
    """
    if app.openapi_schema:
        return app.openapi_schema

    openapi_schema = get_openapi(
        title=app.title,
        version=app.version,
        description=app.description,
        routes=app.routes,
    )

    for route in app.routes:
        if hasattr(route, "endpoint") and hasattr(route.endpoint, "is_pubsub_api"):
            request_model = route.endpoint.request_model
            path = route.path
            method = list(route.methods)[0].lower()

            pubsub_model = request_model.create_pubsub_model()
            api_model = request_model.create_api_model()

            openapi_schema["components"]["schemas"][pubsub_model.__name__] = pubsub_model.model_json_schema()
            openapi_schema["components"]["schemas"][api_model.__name__] = api_model.model_json_schema()

            openapi_schema["components"]["schemas"][request_model.__name__] = {
                "oneOf": [
                    {"$ref": f"#/components/schemas/{api_model.__name__}"},
                    {"$ref": f"#/components/schemas/{pubsub_model.__name__}"},
                ]
            }

            openapi_schema["paths"][path][method]["requestBody"]["content"]["application/json"]["schema"] = {
                "$ref": f"#/components/schemas/{api_model.__name__}"
            }

    app.openapi_schema = openapi_schema
    return app.openapi_schema


app.openapi = custom_openapi

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/workflow_ops/services.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The workflow ops services. Contains business logic for workflow ops."""

from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID

from fastapi import status

from budapp.commons import logging
from budapp.commons.constants import (
    WORKFLOW_DELETE_MESSAGES,
    BudServeWorkflowStepEventName,
    WorkflowStatusEnum,
    VisibilityEnum,
)
from budapp.commons.db_utils import SessionMixin
from budapp.commons.exceptions import ClientException
from budapp.model_ops.crud import (
    CloudModelDataManager,
    ModelDataManager,
    ModelSecurityScanResultDataManager,
    ProviderDataManager,
)
from budapp.model_ops.models import CloudModel, Model
from budapp.model_ops.models import ModelSecurityScanResult as ModelSecurityScanResultModel
from budapp.model_ops.models import Provider as ProviderModel
from budapp.workflow_ops.models import Workflow as WorkflowModel
from budapp.workflow_ops.models import WorkflowStep as WorkflowStepModel

from ..endpoint_ops.crud import EndpointDataManager
from ..endpoint_ops.models import Endpoint as EndpointModel
from ..project_ops.crud import ProjectDataManager
from ..project_ops.models import Project as ProjectModel
from .crud import WorkflowDataManager, WorkflowStepDataManager
from .schemas import RetrieveWorkflowDataResponse, RetrieveWorkflowStepData, WorkflowUtilCreate


logger = logging.get_logger(__name__)


class WorkflowService(SessionMixin):
    """Workflow service."""

    async def retrieve_workflow_data(self, workflow_id: UUID) -> RetrieveWorkflowDataResponse:
        """Retrieve workflow data."""
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(WorkflowModel, {"id": workflow_id})

        db_workflow_steps = await WorkflowStepDataManager(self.session).get_all_workflow_steps(
            {"workflow_id": workflow_id}
        )

        # Extract required data from workflow steps
        required_data = await self._extract_required_data_from_workflow_steps(db_workflow_steps)

        # Parse workflow step data response
        return await self._parse_workflow_step_data_response(required_data, db_workflow)

    async def _extract_required_data_from_workflow_steps(
        self, db_workflow_steps: List[WorkflowStepModel]
    ) -> Dict[str, Any]:
        """Get required data from workflow steps.

        Args:
            db_workflow_steps: List of workflow steps.

        Returns:
            Dict of required data.
        """
        # Define the keys required data retrieval
        keys_of_interest = await self._get_keys_of_interest()

        # from workflow steps extract necessary information
        required_data = {}
        for db_workflow_step in db_workflow_steps:
            for key in keys_of_interest:
                if key in db_workflow_step.data:
                    required_data[key] = db_workflow_step.data[key]

        return required_data

    async def _parse_workflow_step_data_response(
        self, required_data: Dict[str, Any], db_workflow: WorkflowModel
    ) -> RetrieveWorkflowDataResponse:
        """Parse workflow step data response.

        Args:
            required_data: Dict of required data.
            db_workflow: DB workflow.

        Returns:
            RetrieveWorkflowDataResponse: Retrieve workflow data response.
        """
        if required_data:
            # Collect necessary info according to required data
            provider_type = required_data.get("provider_type")
            provider_id = required_data.get("provider_id")
            cloud_model_id = required_data.get("cloud_model_id")
            model_id = required_data.get("model_id")
            workflow_execution_status = required_data.get("workflow_execution_status")
            leaderboard = required_data.get("leaderboard")
            name = required_data.get("name")
            ingress_url = required_data.get("ingress_url")
            create_cluster_events = required_data.get(BudServeWorkflowStepEventName.CREATE_CLUSTER_EVENTS.value)
            delete_cluster_events = required_data.get(BudServeWorkflowStepEventName.DELETE_CLUSTER_EVENTS.value)
            delete_endpoint_events = required_data.get(BudServeWorkflowStepEventName.DELETE_ENDPOINT_EVENTS.value)
            model_extraction_events = required_data.get(BudServeWorkflowStepEventName.MODEL_EXTRACTION_EVENTS.value)
            bud_serve_cluster_events = required_data.get(BudServeWorkflowStepEventName.BUDSERVE_CLUSTER_EVENTS.value)
            model_security_scan_events = required_data.get(
                BudServeWorkflowStepEventName.MODEL_SECURITY_SCAN_EVENTS.value
            )
            bud_simulator_events = required_data.get(BudServeWorkflowStepEventName.BUD_SIMULATOR_EVENTS.value)
            security_scan_result_id = required_data.get("security_scan_result_id")
            icon = required_data.get("icon")
            uri = required_data.get("uri")
            author = required_data.get("author")
            tags = required_data.get("tags")
            description = required_data.get("description")
            additional_concurrency = required_data.get("additional_concurrency")

            db_provider = (
                await ProviderDataManager(self.session).retrieve_by_fields(
                    ProviderModel, {"id": required_data["provider_id"]}, missing_ok=True
                )
                if "provider_id" in required_data
                else None
            )

            db_cloud_model = (
                await CloudModelDataManager(self.session).retrieve_by_fields(
                    CloudModel, {"id": required_data["cloud_model_id"]}, missing_ok=True
                )
                if "cloud_model_id" in required_data
                else None
            )

            db_model = (
                await ModelDataManager(self.session).retrieve_by_fields(
                    Model, {"id": UUID(required_data["model_id"])}, missing_ok=True
                )
                if "model_id" in required_data
                else None
            )

            db_model_security_scan_result = (
                await ModelSecurityScanResultDataManager(self.session).retrieve_by_fields(
                    ModelSecurityScanResultModel, {"id": UUID(security_scan_result_id)}, missing_ok=True
                )
                if "security_scan_result_id" in required_data
                else None
            )

            db_endpoint = (
                await EndpointDataManager(self.session).retrieve_by_fields(
                    EndpointModel, {"id": UUID(required_data["endpoint_id"])}, missing_ok=True
                )
                if "endpoint_id" in required_data
                else None
            )

            db_project = (
                await ProjectDataManager(self.session).retrieve_by_fields(
                    ProjectModel, {"id": UUID(required_data["project_id"])}, missing_ok=True
                )
                if "project_id" in required_data
                else None
            )

            workflow_steps = RetrieveWorkflowStepData(
                provider_type=provider_type if provider_type else None,
                provider=db_provider if db_provider else None,
                provider_id=provider_id if provider_id else None,
                cloud_model=db_cloud_model if db_cloud_model else None,
                cloud_model_id=cloud_model_id if cloud_model_id else None,
                model=db_model if db_model else None,
                model_id=model_id if model_id else None,
                workflow_execution_status=workflow_execution_status if workflow_execution_status else None,
                leaderboard=leaderboard if leaderboard else None,
                name=name if name else None,
                icon=icon if icon else None,
                ingress_url=ingress_url if ingress_url else None,
                create_cluster_events=create_cluster_events if create_cluster_events else None,
                uri=uri if uri else None,
                author=author if author else None,
                tags=tags if tags else None,
                model_extraction_events=model_extraction_events if model_extraction_events else None,
                description=description if description else None,
                security_scan_result_id=security_scan_result_id if security_scan_result_id else None,
                model_security_scan_events=model_security_scan_events if model_security_scan_events else None,
                bud_serve_cluster_events=bud_serve_cluster_events if bud_serve_cluster_events else None,
                security_scan_result=db_model_security_scan_result if db_model_security_scan_result else None,
                delete_cluster_events=delete_cluster_events if delete_cluster_events else None,
                delete_endpoint_events=delete_endpoint_events if delete_endpoint_events else None,
                endpoint=db_endpoint if db_endpoint else None,
                additional_concurrency=additional_concurrency if additional_concurrency else None,
                bud_simulator_events=bud_simulator_events if bud_simulator_events else None,
                project=db_project if db_project else None,
            )
        else:
            workflow_steps = RetrieveWorkflowStepData()

        return RetrieveWorkflowDataResponse(
            workflow_id=db_workflow.id,
            status=db_workflow.status,
            current_step=db_workflow.current_step,
            total_steps=db_workflow.total_steps,
            reason=db_workflow.reason,
            workflow_steps=workflow_steps,
            code=status.HTTP_200_OK,
            object="workflow.get",
            message="Workflow data retrieved successfully",
        )

    @staticmethod
    async def _get_keys_of_interest() -> List[str]:
        """Get keys of interest as per different workflows."""
        workflow_keys = {
            "add_cloud_model": [
                "source",
                "name",
                "modality",
                "uri",
                "tags",
                "icon",
                "provider_type",
                "provider_id",
                "cloud_model_id",
                "description",
                "model_id",
                "workflow_execution_status",
                "leaderboard",
            ],
            "create_cluster": [
                "name",
                "icon",
                "ingress_url",
                BudServeWorkflowStepEventName.CREATE_CLUSTER_EVENTS.value,
            ],
            "add_local_model": [
                "name",
                "uri",
                "author",
                "tags",
                "icon",
                "provider_type",
                "provider_id",
                BudServeWorkflowStepEventName.MODEL_EXTRACTION_EVENTS.value,
                "model_id",
                "description",
            ],
            "scan_local_model": [
                "model_id",
                "security_scan_result_id",
                "leaderboard",
                BudServeWorkflowStepEventName.MODEL_SECURITY_SCAN_EVENTS.value,
            ],
            "delete_cluster": [
                BudServeWorkflowStepEventName.DELETE_CLUSTER_EVENTS.value,
            ],
            "delete_endpoint": [
                BudServeWorkflowStepEventName.DELETE_ENDPOINT_EVENTS.value,
            ],
            "add_worker_to_endpoint": [
                BudServeWorkflowStepEventName.BUD_SIMULATOR_EVENTS.value,
                BudServeWorkflowStepEventName.BUDSERVE_CLUSTER_EVENTS.value,
                "endpoint_id",
                "additional_concurrency",
                "cluster_id",
                "project_id",
            ],
        }

        # Combine all lists using set union
        all_keys = set().union(*workflow_keys.values())

        return list(all_keys)

    async def retrieve_or_create_workflow(
        self, workflow_id: Optional[UUID], workflow_data: WorkflowUtilCreate, current_user_id: UUID
    ) -> None:
        """Retrieve or create workflow."""
        workflow_data = workflow_data.model_dump(exclude_none=True, exclude_unset=True)

        if workflow_id:
            db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(
                WorkflowModel, {"id": workflow_id}
            )

            if db_workflow.status != WorkflowStatusEnum.IN_PROGRESS:
                logger.error(f"Workflow {workflow_id} is not in progress")
                raise ClientException("Workflow is not in progress")

            if db_workflow.created_by != current_user_id:
                logger.error(f"User {current_user_id} is not the creator of workflow {workflow_id}")
                raise ClientException("User is not authorized to perform this action")
        elif "total_steps" in workflow_data:
            db_workflow = await WorkflowDataManager(self.session).insert_one(
                WorkflowModel(**workflow_data, created_by=current_user_id),
            )
        else:
            raise ClientException("Either workflow_id or total_steps should be provided")

        return db_workflow

    async def mark_workflow_as_completed(self, workflow_id: UUID, current_user_id: UUID) -> WorkflowModel:
        """Mark workflow as completed."""
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(
            WorkflowModel, {"id": workflow_id, "created_by": current_user_id}
        )
        logger.debug(f"Workflow found: {db_workflow.id}")

        # Update status to completed only if workflow is not failed
        if db_workflow.status == WorkflowStatusEnum.FAILED:
            logger.error(f"Workflow {workflow_id} is failed")
            raise ClientException("Workflow is failed")

        return await WorkflowDataManager(self.session).update_by_fields(
            db_workflow, {"status": WorkflowStatusEnum.COMPLETED}
        )

    async def delete_workflow(self, workflow_id: UUID, current_user_id: UUID) -> None:
        """Delete workflow."""
        db_workflow = await WorkflowDataManager(self.session).retrieve_by_fields(
            WorkflowModel, {"id": workflow_id, "created_by": current_user_id}
        )

        if db_workflow.status != WorkflowStatusEnum.IN_PROGRESS:
            logger.error("Unable to delete failed or completed workflow")
            raise ClientException("Workflow is not in progress state")

        # Define success messages for different workflow types
        success_response = WORKFLOW_DELETE_MESSAGES.get(db_workflow.workflow_type, "Workflow deleted successfully")

        # Delete workflow
        await WorkflowDataManager(self.session).delete_one(db_workflow)

        return success_response

    async def get_all_active_workflows(
        self,
        offset: int = 0,
        limit: int = 10,
        filters: Dict = {},
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[WorkflowModel], int]:
        """Get all active worflows."""
        filters_dict = filters

        # Filter by in progress status
        filters_dict["status"] = WorkflowStatusEnum.IN_PROGRESS
        filters_dict["visibility"] = VisibilityEnum.PUBLIC

        return await WorkflowDataManager(self.session).get_all_workflows(offset, limit, filters_dict, order_by, search)


class WorkflowStepService(SessionMixin):
    """Workflow step service."""

    async def create_or_update_next_workflow_step(
        self, workflow_id: UUID, step_number: int, data: Dict[str, Any]
    ) -> None:
        """Create or update next workflow step."""
        # Check for workflow step exist or not
        db_workflow_step = await WorkflowStepDataManager(self.session).retrieve_by_fields(
            WorkflowStepModel,
            {"workflow_id": workflow_id, "step_number": step_number},
            missing_ok=True,
        )

        if db_workflow_step:
            db_workflow_step = await WorkflowStepDataManager(self.session).update_by_fields(
                db_workflow_step,
                {
                    "workflow_id": workflow_id,
                    "step_number": step_number,
                    "data": data,
                },
            )
        else:
            # Create a new workflow step
            db_workflow_step = await WorkflowStepDataManager(self.session).insert_one(
                WorkflowStepModel(
                    workflow_id=workflow_id,
                    step_number=step_number,
                    data=data,
                )
            )

        return db_workflow_step

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/workflow_ops/models.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The workflow ops package, containing essential business logic, services, and routing configurations for the workflow ops."""

from datetime import datetime
from typing import Any, Dict, List, Optional, Union
from uuid import UUID, uuid4

from sqlalchemy import DateTime, Enum, ForeignKey, Integer, String, Uuid
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import Mapped, mapped_column, relationship

from budapp.commons.database import Base

from ..commons.constants import WorkflowStatusEnum, WorkflowTypeEnum, VisibilityEnum


class Workflow(Base):
    """Workflow model."""

    __tablename__ = "workflow"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    created_by: Mapped[UUID] = mapped_column(ForeignKey("user.id"), nullable=False, index=True)
    status: Mapped[str] = mapped_column(
        Enum(
            WorkflowStatusEnum,
            name="workflow_status_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        default=WorkflowStatusEnum.IN_PROGRESS.value,
    )
    workflow_type: Mapped[str] = mapped_column(
        Enum(
            WorkflowTypeEnum,
            name="workflow_type_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        nullable=False,
    )
    title: Mapped[str] = mapped_column(String, nullable=True)
    icon: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    tag: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    progress: Mapped[Union[Dict[str, Any], List[Any]]] = mapped_column(JSONB, nullable=True)
    current_step: Mapped[int] = mapped_column(Integer, default=0)
    total_steps: Mapped[int] = mapped_column(Integer, nullable=False)
    reason: Mapped[str] = mapped_column(String, nullable=True)
    visibility: Mapped[str] = mapped_column(
        Enum(
            VisibilityEnum,
            name="visibility_enum",
            values_callable=lambda x: [e.value for e in x],
        ),
        default=VisibilityEnum.PUBLIC,
    )
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    steps: Mapped[list["WorkflowStep"]] = relationship(
        "WorkflowStep",
        back_populates="workflow",
        cascade="all, delete-orphan",
    )


class WorkflowStep(Base):
    """Workflow step model."""

    __tablename__ = "workflow_step"

    id: Mapped[UUID] = mapped_column(Uuid, primary_key=True, default=uuid4)
    workflow_id: Mapped[UUID] = mapped_column(ForeignKey("workflow.id"), nullable=False, index=True)
    step_number: Mapped[int] = mapped_column(Integer, nullable=False)
    data: Mapped[dict] = mapped_column(JSONB, nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    workflow: Mapped[Workflow] = relationship("Workflow", back_populates="steps")

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/workflow_ops/workflow_routes.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The workflow ops package, containing essential business logic, services, and routing configurations for the workflow ops."""

from typing import List, Optional, Union
from uuid import UUID

from fastapi import APIRouter, Depends, Query, status
from sqlalchemy.orm import Session
from typing_extensions import Annotated

from budapp.commons import logging
from budapp.commons.dependencies import get_current_active_user, get_session, parse_ordering_fields
from budapp.commons.exceptions import ClientException
from budapp.commons.schemas import ErrorResponse, SuccessResponse
from budapp.user_ops.schemas import User

from .schemas import RetrieveWorkflowDataResponse, WorkflowFilter, WorkflowListResponse, WorkflowResponse
from .services import WorkflowService


logger = logging.get_logger(__name__)

workflow_router = APIRouter(prefix="/workflows", tags=["workflow"])


@workflow_router.get(
    "",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": WorkflowListResponse,
            "description": "Successfully listed all workflows",
        },
    },
    description="List all workflows",
)
async def list_active_workflows(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    filters: WorkflowFilter = Depends(),
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=0),
    order_by: Optional[List[str]] = Depends(parse_ordering_fields),
    search: bool = False,
) -> Union[WorkflowListResponse, ErrorResponse]:
    """List all workflows."""
    offset = (page - 1) * limit

    filters_dict = filters.model_dump(exclude_none=True)
    filters_dict["created_by"] = current_user.id

    try:
        db_workflows, count = await WorkflowService(session).get_all_active_workflows(
            offset, limit, filters_dict, order_by, search
        )
        return WorkflowListResponse(
            workflows=db_workflows,
            total_record=count,
            page=page,
            limit=limit,
            object="workflow.list",
            code=status.HTTP_200_OK,
        ).to_http_response()
    except ClientException as e:
        logger.exception(f"Failed to list workflows: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to list workflows: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to list workflows"
        ).to_http_response()


@workflow_router.get(
    "/{workflow_id}",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_200_OK: {
            "model": RetrieveWorkflowDataResponse,
            "description": "Successfully retrieve workflow data",
        },
    },
    description="Retrieve workflow data",
)
async def retrieve_workflow_data(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    workflow_id: UUID,
) -> Union[RetrieveWorkflowDataResponse, ErrorResponse]:
    """Retrieve workflow data."""
    try:
        return await WorkflowService(session).retrieve_workflow_data(workflow_id)
    except ClientException as e:
        logger.exception(f"Failed to retrieve workflow data: {e}")
        return ErrorResponse(code=status.HTTP_400_BAD_REQUEST, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to retrieve workflow data: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to retrieve workflow data"
        ).to_http_response()


@workflow_router.patch(
    "/{workflow_id}",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_404_NOT_FOUND: {
            "model": ErrorResponse,
            "description": "Resource not found",
        },
        status.HTTP_200_OK: {
            "model": WorkflowResponse,
            "description": "Successfully mark workflow as completed",
        },
    },
    description="Mark workflow as completed",
)
async def mark_workflow_as_completed(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    workflow_id: UUID,
) -> Union[WorkflowResponse, ErrorResponse]:
    """Mark workflow as completed."""
    try:
        db_workflow = await WorkflowService(session).mark_workflow_as_completed(workflow_id, current_user.id)
        return WorkflowResponse(
            id=db_workflow.id,
            total_steps=db_workflow.total_steps,
            status=db_workflow.status,
            current_step=db_workflow.current_step,
            reason=db_workflow.reason,
            code=status.HTTP_200_OK,
            object="workflow.get",
            message="Workflow marked as completed",
        ).to_http_response()
    except ClientException as e:
        logger.exception(f"Failed to mark workflow as completed: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to mark workflow as completed: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to mark workflow as completed"
        ).to_http_response()


@workflow_router.delete(
    "/{workflow_id}",
    responses={
        status.HTTP_500_INTERNAL_SERVER_ERROR: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to server error",
        },
        status.HTTP_400_BAD_REQUEST: {
            "model": ErrorResponse,
            "description": "Service is unavailable due to client error",
        },
        status.HTTP_404_NOT_FOUND: {
            "model": ErrorResponse,
            "description": "Resource not found",
        },
        status.HTTP_200_OK: {
            "model": SuccessResponse,
            "description": "Successfully delete workflow",
        },
    },
    description="Delete workflow",
)
async def delete_workflow(
    current_user: Annotated[User, Depends(get_current_active_user)],
    session: Annotated[Session, Depends(get_session)],
    workflow_id: UUID,
) -> Union[SuccessResponse, ErrorResponse]:
    """Delete workflow."""
    try:
        success_response = await WorkflowService(session).delete_workflow(workflow_id, current_user.id)
        return SuccessResponse(
            code=status.HTTP_200_OK,
            object="workflow.delete",
            message=success_response,
        ).to_http_response()
    except ClientException as e:
        logger.exception(f"Failed to delete workflow: {e}")
        return ErrorResponse(code=e.status_code, message=e.message).to_http_response()
    except Exception as e:
        logger.exception(f"Failed to delete workflow: {e}")
        return ErrorResponse(
            code=status.HTTP_500_INTERNAL_SERVER_ERROR, message="Failed to delete workflow"
        ).to_http_response()

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/workflow_ops/schemas.py`:

```py
from datetime import datetime

from pydantic import UUID4, BaseModel, ConfigDict, Field

from budapp.commons.schemas import PaginatedSuccessResponse, SuccessResponse, Tag
from budapp.model_ops.schemas import CloudModel, Model, ModelSecurityScanResult, Provider

from ..commons.constants import ModelProviderTypeEnum, WorkflowStatusEnum, WorkflowTypeEnum, VisibilityEnum
from ..endpoint_ops.schemas import EndpointResponse
from ..project_ops.schemas import ProjectResponse


class RetrieveWorkflowStepData(BaseModel):
    """Workflow step data schema."""

    model_config = ConfigDict(from_attributes=True, protected_namespaces=())

    provider_type: ModelProviderTypeEnum | None = None
    provider: Provider | None = None
    cloud_model: CloudModel | None = None
    cloud_model_id: UUID4 | None = None
    provider_id: UUID4 | None = None
    model_id: UUID4 | None = None
    model: Model | None = None
    workflow_execution_status: dict | None = None
    leaderboard: list | dict | None = None
    name: str | None = None
    ingress_url: str | None = None
    create_cluster_events: dict | None = None
    delete_cluster_events: dict | None = None
    delete_endpoint_events: dict | None = None
    model_security_scan_events: dict | None = None
    bud_simulator_events: dict | None = None
    icon: str | None = None
    uri: str | None = None
    author: str | None = None
    tags: list[Tag] | None = None
    model_extraction_events: dict | None = None
    description: str | None = None
    security_scan_result_id: UUID4 | None = None
    security_scan_result: ModelSecurityScanResult | None = None
    endpoint: EndpointResponse | None = None
    additional_concurrency: int | None = None
    bud_serve_cluster_events: dict | None = None
    project: ProjectResponse | None = None


class RetrieveWorkflowDataResponse(SuccessResponse):
    """Retrieve Workflow Data Response."""

    workflow_id: UUID4
    status: WorkflowStatusEnum
    current_step: int
    total_steps: int
    reason: str | None = None
    workflow_steps: RetrieveWorkflowStepData | None = None


class WorkflowResponse(SuccessResponse):
    """Workflow response schema."""

    model_config = ConfigDict(
        populate_by_name=True,
    )

    id: UUID4 = Field(alias="workflow_id")
    total_steps: int = Field(..., gt=0)
    status: WorkflowStatusEnum
    current_step: int
    reason: str | None = None


class Workflow(BaseModel):
    """Workflow schema."""

    model_config = ConfigDict(from_attributes=True, protected_namespaces=())

    id: UUID4
    title: str | None = None
    icon: str | None = None
    tag: str | None = None
    progress: dict | None = None
    workflow_type: WorkflowTypeEnum
    total_steps: int = Field(..., gt=0)
    status: WorkflowStatusEnum
    current_step: int
    reason: str | None = None
    created_at: datetime
    modified_at: datetime


class WorkflowListResponse(PaginatedSuccessResponse):
    """Workflow list response schema."""

    model_config = ConfigDict(from_attributes=True, protected_namespaces=())

    workflows: list[Workflow]


class WorkflowFilter(BaseModel):
    """Workflow filter schema."""

    workflow_type: WorkflowTypeEnum | None = None


class WorkflowUtilCreate(BaseModel):
    """Workflow create schema."""

    workflow_type: WorkflowTypeEnum
    title: str
    icon: str | None = None
    total_steps: int | None = None
    tag: str | None = None
    visibility: VisibilityEnum = VisibilityEnum.PUBLIC

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/workflow_ops/crud.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The crud package, containing essential business logic, services, and routing configurations for the workflow ops."""

from typing import Dict, List, Tuple

from sqlalchemy import and_, func, select

from budapp.commons import logging
from budapp.commons.db_utils import DataManagerUtils

from .models import Workflow as WorkflowModel
from .models import WorkflowStep as WorkflowStepModel


logger = logging.get_logger(__name__)


class WorkflowDataManager(DataManagerUtils):
    """Data manager for the Workflow model."""

    async def get_all_workflows(
        self,
        offset: int,
        limit: int,
        filters: Dict = {},  # endpoint count need to consider in future
        order_by: List = [],
        search: bool = False,
    ) -> Tuple[List[WorkflowModel], int]:
        """List all workflows from the database."""
        await self.validate_fields(WorkflowModel, filters)

        # Generate statements based on search or filters
        if search:
            search_conditions = await self.generate_search_stmt(WorkflowModel, filters)
            stmt = select(WorkflowModel).filter(and_(*search_conditions))
            count_stmt = select(func.count()).select_from(WorkflowModel).filter(and_(*search_conditions))
        else:
            stmt = select(WorkflowModel).filter_by(**filters)
            count_stmt = select(func.count()).select_from(WorkflowModel).filter_by(**filters)

        # Calculate count before applying limit and offset
        count = self.execute_scalar(count_stmt)

        # Apply limit and offset
        stmt = stmt.limit(limit).offset(offset)

        # Apply sorting
        if order_by:
            sort_conditions = await self.generate_sorting_stmt(WorkflowModel, order_by)
            stmt = stmt.order_by(*sort_conditions)

        result = self.scalars_all(stmt)

        return result, count


class WorkflowStepDataManager(DataManagerUtils):
    """Data manager for the WorkflowStep model."""

    async def get_all_workflow_steps(self, filters: dict) -> List[WorkflowStepModel]:
        """Get all workflow steps from the database."""
        stmt = select(WorkflowStepModel).filter_by(**filters).order_by(WorkflowStepModel.step_number)
        return self.scalars_all(stmt)

    async def get_all_workflow_steps_by_data(self, data_key: str, workflow_id: str) -> List[WorkflowStepModel]:
        """Get all workflow steps from the database by data key and workflow id."""
        stmt = (
            select(WorkflowStepModel)
            .filter(
                WorkflowStepModel.data.op("->>")(data_key).isnot(None), WorkflowStepModel.workflow_id == workflow_id
            )
            .order_by(WorkflowStepModel.step_number)
        )
        return self.scalars_all(stmt)

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/initializers/provider_seeder.py`:

```py
import json
import os
from typing import Any, Dict

from sqlalchemy.orm import Session

from budapp.commons import logging
from budapp.commons.constants import ModelSourceEnum
from budapp.commons.database import engine
from budapp.model_ops.crud import ProviderDataManager
from budapp.model_ops.models import Provider as ProviderModel

from .base_seeder import BaseSeeder


logger = logging.get_logger(__name__)

# current file path
CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))

# seeder file path
PROVIDERS_SEEDER_FILE_PATH = os.path.join(CURRENT_FILE_PATH, "data", "providers_seeder.json")

MODEL_SOURCES = [member.value for member in ModelSourceEnum]

class ProviderSeeder(BaseSeeder):
    """Seeder for the Provider model."""

    async def seed(self) -> None:
        """Seed providers to the database."""
        with Session(engine) as session:
            try:
                await self._seed_providers(session)
            except Exception as e:
                logger.exception(f"Failed to seed providers: {e}")

    @staticmethod
    async def _seed_providers(session: Session) -> None:
        """Seed providers to the database."""
        providers_data = await ProviderSeeder._get_providers_data()
        logger.debug(f"Found {len(providers_data)} providers in the seeder file")

        providers_data_keys = [each for each in MODEL_SOURCES if each in list(providers_data.keys())]

        all_providers = await ProviderDataManager(session).get_all_providers_by_type(providers_data_keys)
        logger.debug(f"Found {len(all_providers)} providers in the database")

        for provider in all_providers:
            values = {
                "name": providers_data[provider.type.value]["name"],
                "description": providers_data[provider.type.value]["description"],
                "icon": providers_data[provider.type.value]["icon"],
            }
            await ProviderDataManager(session).update_by_fields(provider, values)

            # Remove the provider from the data after it has been seeded
            providers_data.pop(provider.type.value)

        if providers_data:
            logger.debug(f"Found {len(providers_data)} new providers")
            create_providers_data = []
            for provider in providers_data:
                if providers_data[provider]["type"] in MODEL_SOURCES:
                    create_providers_data.append(ProviderModel(**providers_data[provider]))

            db_providers = await ProviderDataManager(session).insert_all(create_providers_data)
            logger.debug(f"Seeded {len(db_providers)} new providers")

    @staticmethod
    async def _get_providers_data() -> Dict[str, Any]:
        """Get providers data from the database."""
        try:
            with open(PROVIDERS_SEEDER_FILE_PATH, "r") as file:
                return json.load(file)
        except FileNotFoundError as e:
            raise FileNotFoundError(f"File not found: {PROVIDERS_SEEDER_FILE_PATH}") from e

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/initializers/user_seeder.py`:

```py
import json

from sqlalchemy.orm import Session

from budapp.commons import logging
from budapp.commons.config import app_settings, secrets_settings
from budapp.commons.constants import (
    PermissionEnum,
    UserColorEnum,
    UserRoleEnum,
    UserStatusEnum,
)
from budapp.commons.database import engine
from budapp.commons.security import HashManager
from budapp.permissions.crud import PermissionDataManager
from budapp.permissions.models import Permission as PermissionModel
from budapp.user_ops.crud import UserDataManager
from budapp.user_ops.models import User as UserModel

from .base_seeder import BaseSeeder


logger = logging.get_logger(__name__)


class UserSeeder(BaseSeeder):
    """User seeder."""

    async def seed(self) -> None:
        """Seed admin user to the database."""
        with Session(engine) as session:
            try:
                await self._create_manage_super_user(session)
            except Exception as e:
                logger.error(f"Failed to create super user. Error: {e}")

    @staticmethod
    async def _create_manage_super_user(session: Session) -> None:
        """Create super user if it doesn't exist. Add permissions to super user if it doesn't added."""
        # Check whether super user exists or not
        db_user = await UserDataManager(session).retrieve_by_fields(
            UserModel,
            {"email": app_settings.superuser_email, "status": UserStatusEnum.ACTIVE, "is_superuser": True},
            missing_ok=True,
        )

        if not db_user:
            # Create super user
            salted_password = app_settings.superuser_password + secrets_settings.password_salt
            hashed_password = await HashManager().get_hash(salted_password)
            super_user = UserModel(
                name="admin",
                email=app_settings.superuser_email,
                password=hashed_password,
                is_superuser=True,
                color=UserColorEnum.get_random_color(),
                is_reset_password=False,
                first_login=True,
                status=UserStatusEnum.ACTIVE.value,
                role=UserRoleEnum.SUPER_ADMIN.value,
            )
            db_user = await UserDataManager(session).insert_one(super_user)
            logger.debug("Inserted super user in database")

            # Add permissions to super user
            scopes = PermissionEnum.get_global_permissions()
            permissions = PermissionModel(
                user_id=super_user.id,
                auth_id=super_user.auth_id,
                scopes=json.dumps(scopes),
            )
            db_permissions = await PermissionDataManager(session).insert_one(permissions)
            logger.debug("Inserted permissions to super user in database")
        else:
            logger.debug("Found super user in database")
            scopes = PermissionEnum.get_global_permissions()

            # Check whether super user permissions are added or not
            db_permissions = await PermissionDataManager(session).retrieve_by_fields(
                PermissionModel,
                {"user_id": db_user.id, "auth_id": db_user.auth_id},
                missing_ok=True,
            )

            if db_permissions:
                logger.debug("Found permissions of super user in database")

                # Update super user permissions
                db_permissions = await PermissionDataManager(session).update_by_fields(
                    db_permissions, {"scopes": json.dumps(scopes)}
                )

                logger.debug("Updated permissions of super user in database")
            else:
                logger.debug("Permissions of super user not found in database")
                permissions = PermissionModel(
                    user_id=db_user.id,
                    auth_id=db_user.auth_id,
                    scopes=json.dumps(scopes),
                )

                # Add permissions to super user
                db_permissions = await PermissionDataManager(session).insert_one(permissions)
                logger.debug("Added permissions to super user")

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/initializers/__init__.py`:

```py
#  -----------------------------------------------------------------------------
#  Copyright (c) 2024 Bud Ecosystem Inc.
#  #
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  #
#      http://www.apache.org/licenses/LICENSE-2.0
#  #
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  -----------------------------------------------------------------------------

"""The core package, populate the application's data structures and settings during the initialization process."""

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/initializers/icon_seeder.py`:

```py
import os

from sqlalchemy.orm import Session

from budapp.commons import logging
from budapp.commons.config import app_settings
from budapp.commons.database import engine
from budapp.core.crud import IconDataManager
from budapp.core.models import Icon as IconModel
from budapp.core.schemas import IconCreate, IconUpdate

from .base_seeder import BaseSeeder


logger = logging.get_logger(__name__)

ALLOWED_EXTENSIONS = ("png",)
ICON_DIR_NAME = os.path.basename(app_settings.icon_dir)


class IconSeeder(BaseSeeder):
    """Seeder for the Icon model."""

    async def seed(self) -> None:
        """Seed icons to the database."""
        with Session(engine) as session:
            try:
                await self._seed_icons(session)
            except Exception as e:
                logger.exception(f"Failed to seed icons: {e}")

    @staticmethod
    async def _process_icon_name(filename: str) -> str:
        """Process icon name to be used as the name of the icon
        - Remove file extension
        - Replace underscores with spaces
        - Convert to title case (camel case for multi-word names)
        """
        name = os.path.splitext(filename)[0]
        name = name.replace("_", " ")
        return name.title()

    async def _seed_icons(self, session: Session) -> None:
        """Seed icons into the database"""
        # Store new icons as a list for bulk creation
        icons_to_create = []

        for category in os.listdir(app_settings.icon_dir):
            category_dir = os.path.join(app_settings.icon_dir, category)

            # Extract icon files from the category directory
            if os.path.isdir(category_dir):
                logger.debug(f"Extracting icons from {category}")

                for icon_file in os.listdir(category_dir):
                    # Check if the file is an icon
                    if icon_file.endswith(ALLOWED_EXTENSIONS):
                        file_path = os.path.join(ICON_DIR_NAME, category, icon_file)
                        name = await self._process_icon_name(icon_file)

                        db_icon = await IconDataManager(session).retrieve_by_fields(
                            IconModel, {"file_path": file_path}, missing_ok=True
                        )

                        if db_icon:
                            icon_update_data = IconUpdate(name=name, category=category).model_dump(
                                exclude_unset=True, exclude_none=True
                            )

                            # Update icon in the database
                            db_updated_icon = await IconDataManager(session).update_by_fields(
                                db_icon, icon_update_data
                            )
                            logger.debug(f"Updated icon {db_updated_icon.id}")
                        else:
                            icon_create_data = IconCreate(name=name, category=category, file_path=file_path)

                            # Add to list of icons to create
                            icons_to_create.append(IconModel(**icon_create_data.model_dump()))

                # Bulk create icons with batch size of 100
                BATCH_SIZE = 100
                for i in range(0, len(icons_to_create), BATCH_SIZE):
                    batch = icons_to_create[i : i + BATCH_SIZE]
                    await IconDataManager(session).insert_all(batch)
                    logger.debug(f"Created {len(batch)} icons")

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/initializers/template_seeder.py`:

```py
import json
import os
from typing import Any, Dict

from sqlalchemy.orm import Session

from budapp.commons import logging
from budapp.commons.database import engine

from budapp.core.models import ModelTemplate
from budapp.core.crud import ModelTemplateDataManager
from budapp.core.schemas import ModelTemplateCreate, ModelTemplateUpdate

from .base_seeder import BaseSeeder


logger = logging.get_logger(__name__)

# current file path
CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))

# seeder file path
TEMPLATES_SEEDER_FILE_PATH = os.path.join(CURRENT_FILE_PATH, "data", "template_seeder.json")

class TemplateSeeder(BaseSeeder):
    """Seeder for the Provider model."""

    async def seed(self) -> None:
        """Seed providers to the database."""
        with Session(engine) as session:
            try:
                await self._seed_templates(session)
            except Exception as e:
                logger.exception(f"Failed to seed templates: {e}")

    @staticmethod
    async def _seed_templates(session: Session) -> None:
        """Seed templates into the database. Updates existing templates if they already exist."""

        # Get all existing templates from database
        existing_db_templates = []
        offset = 0
        limit = 100

        while True:
            db_templates, count = await ModelTemplateDataManager(
                session
            ).get_all_model_templates(offset=offset, limit=limit)

            if not db_templates:
                break

            existing_db_templates.extend(db_templates)
            offset += limit

            logger.info(
                f"Fetched {count} templates. Total templates found: {len(existing_db_templates)}"
            )

            if count < limit:
                break

            logger.info(
                f"Finished fetching templates. Total templates found: {len(existing_db_templates)}"
            )

        template_seeder_data= await TemplateSeeder._get_templates_data()
        # Store new templates model for bulk creation
        template_data_to_seed = []

        # Map template seeder data by template type for quick lookup
        template_seeder_data_mapping = {
            template["template_type"]: template for template in template_seeder_data
        }

        # Update existing template with seeder data
        for db_template in existing_db_templates:
            if db_template.template_type.value in template_seeder_data_mapping:
                update_template_data = ModelTemplateUpdate(
                    **template_seeder_data_mapping[db_template.template_type.value]
                )
                db_updated_template = await ModelTemplateDataManager(
                    session
                ).update_by_fields(
                    db_template, update_template_data.model_dump(exclude_unset=True)
                )
                logger.debug(f"Updated template: {db_updated_template.template_type}")

                # Remove the updated template from the mapping
                del template_seeder_data_mapping[db_template.template_type.value]

        # Remaining templates are new and need to be created
        for template_type, template_data in template_seeder_data_mapping.items():
            # Store new template data for bulk creation
            create_template_data = ModelTemplateCreate(**template_data)
            template_data_to_seed.append(
                ModelTemplate(**create_template_data.model_dump(exclude_unset=True))
            )
            logger.info(f"Added template: {template_type} to seed")

        # Create new templates in the database
        created_templates = await ModelTemplateDataManager(
            session
        ).insert_all(template_data_to_seed)
        logger.info(f"Created {len(created_templates)} templates")

    @staticmethod
    async def _get_templates_data() -> Dict[str, Any]:
        """Get providers data from the database."""
        try:
            with open(TEMPLATES_SEEDER_FILE_PATH, "r") as file:
                return json.load(file)
        except FileNotFoundError as e:
            raise FileNotFoundError(f"File not found: {TEMPLATES_SEEDER_FILE_PATH}") from e

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/initializers/seeder.py`:

```py
from budapp.initializers.cloud_model_seeder import CloudModelSeeder
from budapp.initializers.icon_seeder import IconSeeder
from budapp.initializers.provider_seeder import ProviderSeeder
from budapp.initializers.user_seeder import UserSeeder
from budapp.initializers.template_seeder import TemplateSeeder


seeders = {
    "user": UserSeeder,
    "provider": ProviderSeeder,
    "cloud_model": CloudModelSeeder,
    "icon": IconSeeder,
    "template": TemplateSeeder,
}

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/initializers/base_seeder.py`:

```py
from abc import ABC, abstractmethod


class BaseSeeder(ABC):
    """Base seeder class."""

    @abstractmethod
    async def seed(self) -> None:
        """Seed the database."""
        pass

```

`/Users/rahulvramesh/bud-v2/bud-serve-app/budapp/initializers/cloud_model_seeder.py`:

```py
import json
import os
from typing import Any, Dict

from sqlalchemy.orm import Session

from budapp.commons import logging
from budapp.commons.config import app_settings
from budapp.commons.constants import ModelProviderTypeEnum, ModelSourceEnum, UserStatusEnum
from budapp.commons.database import engine
from budapp.model_ops.crud import CloudModelDataManager, ProviderDataManager
from budapp.model_ops.models import CloudModel
from budapp.model_ops.models import Provider as ProviderModel
from budapp.user_ops.crud import UserDataManager
from budapp.user_ops.models import User as UserModel

from .base_seeder import BaseSeeder


logger = logging.get_logger(__name__)

# current file path
CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))

# seeder file path
CLOUD_MODEL_SEEDER_FILE_PATH = os.path.join(CURRENT_FILE_PATH, "data", "cloud_model_seeder.json")


MODEL_SOURCES = [member.value for member in ModelSourceEnum]


class CloudModelSeeder(BaseSeeder):
    """Cloud model seeder."""

    async def seed(self):
        """Seed the cloud models."""
        with Session(engine) as session:
            try:
                await self._seed_cloud_models(session)
            except Exception as e:
                logger.exception(f"Failed to seed cloud models: {e}")

    @staticmethod
    async def _seed_cloud_models(session: Session) -> None:
        """Seed the cloud models."""
        db_user = await UserDataManager(session).retrieve_by_fields(
            UserModel,
            {"email": app_settings.superuser_email, "status": UserStatusEnum.ACTIVE, "is_superuser": True},
            missing_ok=True,
        )

        if db_user:
            cloud_model_data = await CloudModelSeeder._get_cloud_model_data()
            logger.debug(f"Seeding cloud models for {len(cloud_model_data)} providers")

            for provider, model_data in cloud_model_data.items():
                if provider not in MODEL_SOURCES:
                    continue
                db_provider = await ProviderDataManager(session).retrieve_by_fields(
                    ProviderModel,
                    {"type": provider},
                    missing_ok=True,
                )
                # URI as key and details as value
                provider_model = {}
                for model in model_data["models"]:
                    provider_model[model["uri"]] = model

                # Check if the models already exist
                existing_models = await CloudModelDataManager(session).get_all_cloud_models_by_source_uris(
                    provider, list(provider_model.keys())
                )
                logger.debug(f"Found {len(existing_models)} existing models for provider {provider}. Updating...")

                # Update the existing models
                for existing_model in existing_models:
                    update_data = {
                        "name": provider_model[existing_model.uri]["name"],
                        "modality": provider_model[existing_model.uri]["modality"],
                        "source": provider_model[existing_model.uri]["source"],
                        "uri": provider_model[existing_model.uri]["uri"],
                        "provider_type": ModelProviderTypeEnum.CLOUD_MODEL.value,
                        "provider_id": db_provider.id,
                    }

                    # Update existing model in the database
                    await CloudModelDataManager(session).update_by_fields(existing_model, update_data)

                    # Remove the model from the provider_model
                    del provider_model[existing_model.uri]

                # Bulk insert the new models
                new_models = [
                    CloudModel(
                        **model, provider_type=ModelProviderTypeEnum.CLOUD_MODEL.value, provider_id=db_provider.id
                    )
                    for model in provider_model.values()
                ]
                await CloudModelDataManager(session).insert_all(new_models)
                logger.debug(f"Seeded {len(new_models)} new models for provider {provider}")
        else:
            logger.error("Super user not found. Skipping cloud model seeding.")

    @staticmethod
    async def _get_cloud_model_data() -> Dict[str, Any]:
        """Get cloud_model data from the database."""
        try:
            with open(CLOUD_MODEL_SEEDER_FILE_PATH, "r") as file:
                return json.load(file)
        except FileNotFoundError as e:
            raise FileNotFoundError(f"File not found: {CLOUD_MODEL_SEEDER_FILE_PATH}") from e

```