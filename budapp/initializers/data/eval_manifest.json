{
  "manifest_version": "2.0.0",
  "last_updated": "2025-01-04T12:00:00Z",
  "schema_version": "1.0",
  "repository": {
    "name": "Bud Evaluation Datasets",
    "description": "Official evaluation datasets for model testing - 324 datasets from OpenCompass",
    "maintainer": "Bud Ecosystem",
    "base_url": "https://eval-datasets.bud.eco/v2",
    "bundle_url": null,
    "bundle_checksum": null,
    "bundle_size_mb": null
  },
  "version_info": {
    "current_version": "2025.01.04",
    "previous_versions": [
      {
        "version": "2024.01.01",
        "deprecated": false,
        "migration_required": false
      }
    ]
  },
  "traits": {
    "version": "1.0.0",
    "checksum": "sha256:abc123",
    "url": "traits/traits_v1.json",
    "count": 15,
    "definitions": [
      {
        "name": "Strong Reasoning",
        "description": "Advanced logical and analytical thinking capabilities",
        "icon": "icons/traits/strong_reasoning.png"
      },
      {
        "name": "Multimodal",
        "description": "Ability to process and understand multiple types of data (text, image, etc.)",
        "icon": "icons/traits/multimodal.png"
      },
      {
        "name": "Examination",
        "description": "Performance on standardized tests and academic assessments",
        "icon": "icons/traits/examination.png"
      },
      {
        "name": "Reasoning",
        "description": "General logical thinking and problem-solving abilities",
        "icon": "icons/traits/reasoning.png"
      },
      {
        "name": "Knowledge",
        "description": "Factual information and domain expertise",
        "icon": "icons/traits/knowledge.png"
      },
      {
        "name": "Code",
        "description": "Programming and software development capabilities",
        "icon": "icons/traits/code.png"
      },
      {
        "name": "Language",
        "description": "Natural language understanding and generation",
        "icon": "icons/traits/language.png"
      },
      {
        "name": "Math",
        "description": "Mathematical computation and reasoning",
        "icon": "icons/traits/math.png"
      },
      {
        "name": "Understanding",
        "description": "Comprehension and interpretation of complex information",
        "icon": "icons/traits/understanding.png"
      },
      {
        "name": "Safety",
        "description": "Alignment with safety guidelines and responsible AI practices",
        "icon": "icons/traits/safety.png"
      },
      {
        "name": "Long-Context",
        "description": "Handling and reasoning over extended text passages",
        "icon": "icons/traits/long_context.png"
      },
      {
        "name": "Agent",
        "description": "Autonomous task completion and tool usage",
        "icon": "icons/traits/agent.png"
      },
      {
        "name": "Creation",
        "description": "Creative generation of content, code, or solutions",
        "icon": "icons/traits/creation.png"
      },
      {
        "name": "Instruct",
        "description": "Following instructions and completing directed tasks",
        "icon": "icons/traits/instruct.png"
      },
      {
        "name": "Other",
        "description": "Miscellaneous capabilities not covered by specific categories",
        "icon": "icons/traits/other.png"
      }
    ]
  },
  "datasets": {
    "opencompass": {
      "version": "2.0.0",
      "license": "Various - See individual dataset licenses",
      "source": "OpenCompass Evaluation Platform",
      "checksum": "sha256:placeholder_opencompass_v2",
      "count": 324,
      "datasets": [
        {
          "id": "opencompass_1694",
          "name": "MaritimeBench",
          "version": "1.0.0",
          "description": "MaritimeBench builds a scientific, fair maritime knowledge assessment system. With 1,888 MCQs based on industry standards, we evaluate models' capabilities across shipping domains.",
          "url": "opencompass/opencompass_1694.jsonl",
          "size_mb": 1.9,
          "checksum": "sha256:placeholder_opencompass_1694",
          "sample_count": 1888,
          "traits": [
            "Examination",
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "English",
            "domain": "Maritime",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 1000,
            "estimated_output_tokens": 500
          },
          "original_data": {
            "id": "1694",
            "language": [
              "English",
              "Chinese"
            ],
            "domains": [
              "Maritime",
              "Logistics"
            ],
            "concepts": [
              "Navigation",
              "Shipping Law"
            ],
            "humans_vs_llm_qualifications": [
              "Expert",
              "Novice"
            ],
            "task_type": [
              "Multiple Choice",
              "Knowledge Assessment"
            ],
            "modalities": [
              "text"
            ],
            "sample_questions_answers": {
              "examples": [
                {
                  "question": "What is the maximum draft allowed for a vessel entering the Suez Canal?",
                  "options": [
                    "A) 20.1 meters",
                    "B) 22.5 meters",
                    "C) 24.0 meters",
                    "D) 26.8 meters"
                  ],
                  "correct_answer": "C) 24.0 meters",
                  "explanation": "The Suez Canal Authority sets the maximum draft at 24.0 meters for vessels transiting the canal."
                },
                {
                  "question": "According to MARPOL Annex VI, what is the global sulfur limit for marine fuel oil?",
                  "options": [
                    "A) 0.1%",
                    "B) 0.5%",
                    "C) 1.0%",
                    "D) 3.5%"
                  ],
                  "correct_answer": "B) 0.5%",
                  "explanation": "Since January 1, 2020, the global sulfur limit for marine fuel oil is 0.5% m/m as per MARPOL Annex VI."
                }
              ],
              "total_questions": 1888,
              "question_format": "Multiple choice questions with 4 options each",
              "difficulty_levels": [
                "Beginner",
                "Intermediate",
                "Advanced",
                "Expert"
              ]
            },
            "advantages_disadvantages": {
              "advantages": [
                "Comprehensive coverage of maritime industry knowledge",
                "Based on authoritative industry standards and regulations",
                "Large dataset with 1,888 high-quality questions",
                "Bilingual support (English and Chinese)",
                "Covers both theoretical knowledge and practical applications",
                "Regular updates to reflect current maritime regulations"
              ],
              "disadvantages": [
                "Limited to multiple-choice format only",
                "Requires domain expertise for proper evaluation",
                "May not cover emerging maritime technologies",
                "Cultural bias towards certain maritime regions",
                "No practical simulation or hands-on assessment"
              ]
            },
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              },
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "èˆªè¿",
                "en": "èˆªè¿"
              },
              {
                "cn": "çŸ¥è¯†",
                "en": "çŸ¥è¯†"
              },
              {
                "cn": "æµ·è¿",
                "en": "æµ·è¿"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 4,
            "githubLink": "",
            "paperLink": "",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "067099",
              "name": "wangxiangyu",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/067099-bd016c2a-6b89-4ba3-aaa1-6cc12a7ca88f.png",
              "nickname": "Hi-Dolphin"
            },
            "lookNum": "274",
            "top": true,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-22 11:41:39",
            "supportOnlineEval": false,
            "updateDate": "2025-04-22 11:41:39",
            "createDate": "2025-04-01 09:13:05",
            "desc": {
              "cn": "MaritimeBench è‡´åŠ›äºæ„å»ºä¸€å¥—ç§‘å­¦ã€å…¬å¹³ä¸”ä¸¥è°¨çš„èˆªè¿çŸ¥è¯†è¯„ä¼°ä½“ç³»ã€‚åŸºäºè¡Œä¸šæƒå¨æ ‡å‡†ï¼Œæˆ‘ä»¬æŒç»­ç»´æŠ¤å¹¶æ›´æ–°é«˜è´¨é‡çš„èˆªè¿æ•°æ®é›†â€”â€”å…¶ä¸­åŒ…å«1,888é“å®¢è§‚é€‰æ‹©é¢˜ï¼ˆMCQæ ¼å¼ï¼‰ï¼Œä»¥å…¨é¢ã€å¤šç»´åº¦åœ°é‡åŒ–æ¨¡å‹åœ¨èˆªè¿å„é¢†åŸŸçš„èƒ½åŠ›è¡¨ç°ã€‚",
              "en": "MaritimeBench builds a scientific, fair maritime knowledge assessment system. With 1,888 MCQs based on industry standards, we evaluate models' capabilities across shipping domains."
            }
          }
        },
        {
          "id": "opencompass_1558",
          "name": "MM-AlignBench",
          "version": "1.0.0",
          "description": "A benchmark for evaluating MLLMs' alignment with human preferences. It includes 252 high-quality, human-annotated samples with diverse image types and open-ended questions. Modeled after Arena-style benchmarks, it uses GPT-4o as the judge model and Claude-Sonnet-3 as the reference model.",
          "url": "opencompass/opencompass_1558.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1558",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1558",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/PhoenixZ810/OmniAlign-V",
            "paperLink": "https://arxiv.org/abs/2502.18411",
            "officialWebsiteLink": "https://phoenixz810.github.io/OmniAlign-V/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "825",
            "top": true,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-05 09:55:00",
            "supportOnlineEval": false,
            "updateDate": "2025-03-05 09:55:00",
            "createDate": "2025-03-05 09:54:39",
            "desc": {
              "cn": "ç”¨äºè¯„ä¼° MLLM ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§çš„åŸºå‡†ã€‚å®ƒåŒ…å« 252 ä¸ªé«˜è´¨é‡ã€äººç±»æ ‡æ³¨çš„æ ·æœ¬ï¼Œå…·æœ‰ä¸åŒçš„å›¾åƒç±»å‹å’Œå¼€æ”¾å¼é—®é¢˜ã€‚å®ƒä»¿ç…§ Arena é£æ ¼çš„åŸºå‡†ï¼Œä½¿ç”¨ GPT-4o ä½œä¸ºè¯„åˆ¤æ¨¡å‹ï¼ŒClaude-Sonnet-3 ä½œä¸ºå‚è€ƒæ¨¡å‹ã€‚",
              "en": "A benchmark for evaluating MLLMs' alignment with human preferences. It includes 252 high-quality, human-annotated samples with diverse image types and open-ended questions. Modeled after Arena-style benchmarks, it uses GPT-4o as the judge model and Claude-Sonnet-3 as the reference model."
            }
          }
        },
        {
          "id": "opencompass_1509",
          "name": "MVBench",
          "version": "1.0.0",
          "description": "MVBench can test MLLMs' temporal understanding in the dynamic video tasks. It covers 20 challenging video tasks that cannot be effectively solved with a single frame.",
          "url": "opencompass/opencompass_1509.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1509",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1509",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/OpenGVLab/Ask-Anything",
            "paperLink": "https://arxiv.org/abs/2311.17005",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "423",
            "top": true,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-02-19 14:28:53",
            "supportOnlineEval": false,
            "updateDate": "2025-02-19 14:28:53",
            "createDate": "2025-02-17 15:17:00",
            "desc": {
              "cn": "MVBenchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨åŠ¨æ€è§†é¢‘ä»»åŠ¡ä¸­çš„æ—¶é—´ç†è§£èƒ½åŠ›ï¼Œç”±20ä¸ªå•å¸§å†…å®¹æ— æ³•æœ‰æ•ˆè§£å†³çš„æŒ‘æˆ˜æ€§çš„è§†é¢‘ä»»åŠ¡ç»„æˆã€‚",
              "en": "MVBench can test MLLMs' temporal understanding in the dynamic video tasks. It covers 20 challenging video tasks that cannot be effectively solved with a single frame."
            }
          }
        },
        {
          "id": "opencompass_1375",
          "name": "VBench",
          "version": "1.0.0",
          "description": "VBench is a comprehensive benchmark evaluates video generation quality. It comprises 16 dimensions in video generation, and also provides a dataset of human preference annotations. ",
          "url": "opencompass/opencompass_1375.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1375",
          "sample_count": 1000,
          "traits": [
            "Creation",
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1375",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "åˆ›ä½œ",
                "en": "Creation"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Vchitect/VBench",
            "paperLink": "https://arxiv.org/abs/2311.17982",
            "officialWebsiteLink": "https://vchitect.github.io/VBench-project/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "440",
            "top": true,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:23:36",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:23:36",
            "createDate": "2025-01-10 16:36:24",
            "desc": {
              "cn": "VBenchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è§†é¢‘ç”Ÿæˆè´¨é‡ï¼ŒåŒ…å«16ä¸ªè§†é¢‘ç”Ÿæˆç»´åº¦åŠ1ä¸ªäººç±»åå¥½æ³¨é‡Šæ•°æ®é›†ã€‚",
              "en": "VBench is a comprehensive benchmark evaluates video generation quality. It comprises 16 dimensions in video generation, and also provides a dataset of human preference annotations. "
            }
          }
        },
        {
          "id": "opencompass_1397",
          "name": "LiveMathBench",
          "version": "1.0.0",
          "description": "LiveMathBench can capture LLM capabilities in complex reasoning tasks, including challenging latest question sets from various mathematical competitions. ",
          "url": "opencompass/opencompass_1397.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1397",
          "sample_count": 1000,
          "traits": [
            "Math",
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1397",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              },
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 3,
            "githubLink": "https://github.com/open-compass/GPassK",
            "paperLink": "https://arxiv.org/abs/2412.13147",
            "officialWebsiteLink": "https://open-compass.github.io/GPassK/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "903",
            "top": true,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-16 20:30:57",
            "supportOnlineEval": false,
            "updateDate": "2025-01-16 20:30:57",
            "createDate": "2025-01-15 18:21:51",
            "desc": {
              "cn": "LiveMathBenchç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†æ–¹é¢çš„è¡¨ç°ï¼Œç”±æå…·æŒ‘æˆ˜æ€§çš„ç°ä»£æ•°å­¦é—®é¢˜ç»„æˆã€‚",
              "en": "LiveMathBench can capture LLM capabilities in complex reasoning tasks, including challenging latest question sets from various mathematical competitions. "
            }
          }
        },
        {
          "id": "opencompass_1370",
          "name": "MathVision",
          "version": "1.0.0",
          "description": "MathVision measures multimodal mathematical reasoning capabilities through a meticulously curated collection of 3,040 high-quality mathematical problems spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty.",
          "url": "opencompass/opencompass_1370.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1370",
          "sample_count": 1000,
          "traits": [
            "Math",
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1370",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              },
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/mathllm/MATH-V",
            "paperLink": "https://arxiv.org/abs/2402.14804",
            "officialWebsiteLink": "https://mathllm.github.io/mathvision/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "564",
            "top": true,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-10 18:23:49",
            "supportOnlineEval": false,
            "updateDate": "2025-01-10 18:23:49",
            "createDate": "2025-01-10 18:19:40",
            "desc": {
              "cn": "MathVisionç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œç”±æ¶µç›–16ä¸ªæ•°å­¦é¢†åŸŸã€è·¨è¶Š5ä¸ªéš¾åº¦çº§åˆ«çš„3040ä¸ªé«˜è´¨é‡æ•°å­¦é—®é¢˜ç»„æˆã€‚",
              "en": "MathVision measures multimodal mathematical reasoning capabilities through a meticulously curated collection of 3,040 high-quality mathematical problems spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty."
            }
          }
        },
        {
          "id": "opencompass_1371",
          "name": "MathVerse",
          "version": "1.0.0",
          "description": "MathVerse is intended for evaluating MLLMs' visual math problem-solving, containing 2,612 high-quality, multi-subject math problems with diagrams. ",
          "url": "opencompass/opencompass_1371.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1371",
          "sample_count": 1000,
          "traits": [
            "Math",
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1371",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              },
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/ZrrSkywalker/MathVerse",
            "paperLink": "https://arxiv.org/abs/2403.14624",
            "officialWebsiteLink": "https://mathverse-cuhk.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "287",
            "top": true,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-10 18:00:01",
            "supportOnlineEval": false,
            "updateDate": "2025-01-10 18:00:01",
            "createDate": "2025-01-10 14:29:47",
            "desc": {
              "cn": "MathVerseç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è§†è§‰æ•°å­¦é—®é¢˜è§£å†³èƒ½åŠ›ï¼ŒåŒ…å«2612ä¸ªé«˜è´¨é‡ã€å¤šä¸»é¢˜çš„æ•°å­¦é—®é¢˜ã€‚",
              "en": "MathVerse is intended for evaluating MLLMs' visual math problem-solving, containing 2,612 high-quality, multi-subject math problems with diagrams. "
            }
          }
        },
        {
          "id": "opencompass_1374",
          "name": "DynaMath",
          "version": "1.0.0",
          "description": "DynaMath is a dynamic visual math benchmark designed for in-depth assessment of VLMs. It includes 501 high-quality, multi-topic seed questions, each represented as a Python program enabling the automatic generation of a much larger set of concrete questions.",
          "url": "opencompass/opencompass_1374.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1374",
          "sample_count": 1000,
          "traits": [
            "Math",
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1374",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              },
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/DynaMath/DynaMath",
            "paperLink": "https://arxiv.org/abs/2411.00836",
            "officialWebsiteLink": "https://dynamath.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "218",
            "top": true,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-10 18:25:13",
            "supportOnlineEval": false,
            "updateDate": "2025-01-10 18:25:13",
            "createDate": "2025-01-10 18:24:27",
            "desc": {
              "cn": " DynaMathç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ•°å­¦èƒ½åŠ›ï¼ŒåŒ…æ‹¬501ä¸ªé«˜è´¨é‡ã€å¤šä¸»é¢˜çš„ç§å­é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½ä»¥Pythonç¨‹åºè¡¨ç¤ºï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¤§é‡å…·ä½“çš„å¤šæ ·åŒ–é—®é¢˜ã€‚",
              "en": "DynaMath is a dynamic visual math benchmark designed for in-depth assessment of VLMs. It includes 501 high-quality, multi-topic seed questions, each represented as a Python program enabling the automatic generation of a much larger set of concrete questions."
            }
          }
        },
        {
          "id": "opencompass_1178",
          "name": "MathVista",
          "version": "1.0.0",
          "description": "MathVista is a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets.",
          "url": "opencompass/opencompass_1178.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1178",
          "sample_count": 1000,
          "traits": [
            "Math",
            "Multimodal",
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1178",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              },
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/lupantech/MathVista",
            "paperLink": "https://arxiv.org/pdf/2310.02255",
            "officialWebsiteLink": "https://mathvista.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "347",
            "top": true,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-10 18:23:44",
            "supportOnlineEval": false,
            "updateDate": "2025-01-10 18:23:44",
            "createDate": "2025-01-10 18:23:16",
            "desc": {
              "cn": "MathVistaç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ•°å­¦èƒ½åŠ›ï¼Œç»“åˆäº†ä¸°å¯Œçš„æ•°å­¦å’Œè§†è§‰ä»»åŠ¡æŒ‘æˆ˜ã€‚å®ƒç”±6141ä¸ªç¤ºä¾‹ç»„æˆï¼Œæ¥è‡ª28ä¸ªæ¶‰åŠæ•°å­¦çš„ç°æœ‰å¤šæ¨¡æ€æ•°æ®é›†å’Œ3ä¸ªæ–°åˆ›å»ºçš„æ•°æ®é›†ã€‚",
              "en": "MathVista is a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets."
            }
          }
        },
        {
          "id": "opencompass_1253",
          "name": "BigCodeBench",
          "version": "1.0.0",
          "description": "BigCodeBench is a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. ",
          "url": "opencompass/opencompass_1253.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1253",
          "sample_count": 1000,
          "traits": [
            "Code",
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1253",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              },
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/bigcode-project/bigcodebench/",
            "paperLink": "https://arxiv.org/abs/2406.15877",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "725",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-30 16:30:39",
            "supportOnlineEval": false,
            "updateDate": "2024-12-30 16:30:39",
            "createDate": "2024-12-30 16:24:34",
            "desc": {
              "cn": "BigCodeBenchç”¨äºè¯„ä¼°LLMçš„ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒ…å«1140ä¸ªå¯ä»¥è°ƒç”¨139ä¸ªåº“å’Œ7ä¸ªåŸŸçš„å¤šä¸ªå‡½æ•°æ¥å®Œæˆçš„ç»†ç²’åº¦ä»»åŠ¡ã€‚",
              "en": "BigCodeBench is a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. "
            }
          }
        },
        {
          "id": "opencompass_539",
          "name": "BBH",
          "version": "1.0.0",
          "description": "BIG-Bench Hard (BBH) is a subset of the BIG-Bench, a diverse evaluation suite for language models. BBH focuses on a suite of 23 challenging tasks from BIG-Bench that were found to be beyond the capabilities of current language models.",
          "url": "opencompass/opencompass_539.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_539",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "539",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [
              {
                "cn": "reasoning",
                "en": "reasoning"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/suzgunmirac/BIG-Bench-Hard",
            "paperLink": "https://arxiv.org/pdf/2210.09261.pdf",
            "officialWebsiteLink": "https://github.com/suzgunmirac/BIG-Bench-Hard",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "17087",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:31:41",
            "supportOnlineEval": true,
            "updateDate": "2024-09-12 19:31:41",
            "createDate": "2024-09-12 19:26:57",
            "desc": {
              "cn": "BIG Bench-Hardï¼ˆBBHï¼‰æ˜¯BIG Benchçš„ä¸€ä¸ªå­é›†ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”¨äºè¯­è¨€æ¨¡å‹çš„å¤šæ ·åŒ–è¯„ä¼°å¥—ä»¶ã€‚BBHä¸“æ³¨äºBIG Benchçš„23é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡è¢«å‘ç°è¶…å‡ºäº†å½“å‰è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚",
              "en": "BIG-Bench Hard (BBH) is a subset of the BIG-Bench, a diverse evaluation suite for language models. BBH focuses on a suite of 23 challenging tasks from BIG-Bench that were found to be beyond the capabilities of current language models."
            }
          }
        },
        {
          "id": "opencompass_534",
          "name": "MATH",
          "version": "1.0.0",
          "description": "MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution.",
          "url": "opencompass/opencompass_534.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_534",
          "sample_count": 1000,
          "traits": [
            "Math"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "534",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [
              {
                "cn": "math",
                "en": "math"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/hendrycks/math",
            "paperLink": "https://arxiv.org/pdf/2103.03874.pdf",
            "officialWebsiteLink": "https://huggingface.co/datasets/hendrycks/competition_math",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "12831",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:31:29",
            "supportOnlineEval": true,
            "updateDate": "2024-09-12 19:31:29",
            "createDate": "2024-09-12 19:28:19",
            "desc": {
              "cn": "MATH æ˜¯ä¸€ä¸ªåŒ…å« 12,500 ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç«èµ›æ•°å­¦é—®é¢˜çš„æ–°æ•°æ®é›†ã€‚ MATH ä¸­çš„æ¯ä¸ªé—®é¢˜éƒ½æœ‰å®Œæ•´çš„åˆ†æ­¥è§£å†³æ–¹æ¡ˆã€‚",
              "en": "MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution."
            }
          }
        },
        {
          "id": "opencompass_542",
          "name": "LongBench",
          "version": "1.0.0",
          "description": "LongBench is a benchmark for bilingual, multitask, and comprehensive assessment of long context understanding capabilities of large language models.",
          "url": "opencompass/opencompass_542.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_542",
          "sample_count": 1000,
          "traits": [
            "Long-Context"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "542",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸª‘",
            "dimensions": [
              {
                "cn": "é•¿æ–‡æœ¬",
                "en": "Long-Context"
              }
            ],
            "subDimensions": [
              {
                "cn": "long-context",
                "en": "long-context"
              }
            ],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/THUDM/LongBench",
            "paperLink": "https://arxiv.org/pdf/2308.14508.pdf",
            "officialWebsiteLink": "https://huggingface.co/datasets/THUDM/LongBench",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "8734",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:32:08",
            "supportOnlineEval": false,
            "updateDate": "2024-09-12 19:32:08",
            "createDate": "2024-09-12 19:26:26",
            "desc": {
              "cn": "LongBench æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡ã€ä¸­è‹±åŒè¯­ã€é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹é•¿æ–‡æœ¬ç†è§£èƒ½åŠ›çš„è¯„æµ‹åŸºå‡†ã€‚",
              "en": "LongBench is a benchmark for bilingual, multitask, and comprehensive assessment of long context understanding capabilities of large language models."
            }
          }
        },
        {
          "id": "opencompass_535",
          "name": "GSM8K",
          "version": "1.0.0",
          "description": "GSM8K is a dataset of 8,500 high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7,500 training problems and 1,000 test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ âˆ’ Ã— Ã·) to reach the final answer.",
          "url": "opencompass/opencompass_535.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_535",
          "sample_count": 1000,
          "traits": [
            "Math"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "535",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [
              {
                "cn": "math",
                "en": "math"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/openai/grade-school-math",
            "paperLink": "https://arxiv.org/pdf/2110.14168.pdf",
            "officialWebsiteLink": "https://huggingface.co/datasets/gsm8k",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "8452",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:31:32",
            "supportOnlineEval": true,
            "updateDate": "2024-09-12 19:31:32",
            "createDate": "2024-09-12 19:27:59",
            "desc": {
              "cn": "GSM8K æ˜¯ä¸€ä¸ªåŒ…å« 8,500 ä¸ªé«˜è´¨é‡ã€è¯­è¨€å¤šæ ·åŒ–çš„å°å­¦æ•°å­¦å•è¯é—®é¢˜çš„æ•°æ®é›†ï¼Œç”±äººç±»é—®é¢˜ç¼–å†™è€…åˆ›å»ºã€‚è¯¥æ•°æ®é›†åˆ†ä¸º 7,500 ä¸ªè®­ç»ƒé—®é¢˜å’Œ 1,000 ä¸ªæµ‹è¯•é—®é¢˜ã€‚è¿™äº›é—®é¢˜çš„è§£é¢˜æ­¥éª¤åœ¨ 2 åˆ° 8 æ­¥ä¹‹é—´ï¼Œè§£é¢˜è¿‡ç¨‹ä¸»è¦æ¶‰åŠä½¿ç”¨åŸºæœ¬ç®—æœ¯è¿ç®—ï¼ˆ+ - Ã— Ã·ï¼‰è¿›è¡Œä¸€è¿ä¸²çš„åŸºæœ¬è®¡ç®—ï¼Œä»è€Œå¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚ä¸€ä¸ªèªæ˜çš„åˆä¸­ç”Ÿåº”è¯¥èƒ½å¤Ÿè§£å†³æ¯ä¸€ä¸ªé—®é¢˜ã€‚å®ƒå¯ç”¨äºå¤šæ­¥æ•°å­¦æ¨ç†ã€‚",
              "en": "GSM8K is a dataset of 8,500 high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7,500 training problems and 1,000 test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ âˆ’ Ã— Ã·) to reach the final answer."
            }
          }
        },
        {
          "id": "opencompass_512",
          "name": "TriviaQA",
          "version": "1.0.0",
          "description": "TriviaqQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaqQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions.",
          "url": "opencompass/opencompass_512.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_512",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "512",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/mandarjoshi90/triviaqa",
            "paperLink": "https://arxiv.org/abs/1705.03551",
            "officialWebsiteLink": "http://nlp.cs.washington.edu/triviaqa/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "5894",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": true,
            "updateDate": "2024-08-02 09:45:49",
            "createDate": "2024-01-11 14:10:43",
            "desc": {
              "cn": "TriviaqQAæ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡65ä¸‡ä¸ªé—®é¢˜-ç­”æ¡ˆ-è¯æ®ä¸‰å…ƒç»„ã€‚å…¶åŒ…æ‹¬95Kä¸ªé—®ç­”å¯¹ï¼Œç”±å†·çŸ¥è¯†çˆ±å¥½è€…å’Œç‹¬ç«‹æ”¶é›†çš„äº‹å®æ€§æ–‡æ¡£æ’°å†™ï¼Œå¹³å‡æ¯ä¸ªé—®é¢˜6ä¸ªï¼Œä¸ºå›ç­”é—®é¢˜æä¾›é«˜è´¨é‡çš„è¿œç¨‹ç›‘ç£ã€‚\n",
              "en": "TriviaqQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaqQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions."
            }
          }
        },
        {
          "id": "opencompass_895",
          "name": "Fin-Eva",
          "version": "1.0.0",
          "description": "Ant Group and Shanghai University of Finance and Economics jointly launched the financial benchmarkï¼ŒFin-Eva Version 1.0, covering multiple financial scenarios and subjects such as wealth management, insurance, investment research. The number of this benchmark's questions reaches 13,000+.",
          "url": "opencompass/opencompass_895.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_895",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "895",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ“ˆ",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [
              {
                "cn": "knowledge;Finance",
                "en": "knowledge;Finance"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 4,
            "githubLink": "https://github.com/alipay/financial_evaluation_dataset",
            "paperLink": "",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "50074087",
              "name": "Ant_Group",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50074087-492c3cdd-1529-4e49-84ad-8cc2f8c86893.png",
              "nickname": "èš‚èšé›†å›¢"
            },
            "lookNum": "5368",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:40:45",
            "supportOnlineEval": false,
            "updateDate": "2024-09-12 19:40:45",
            "createDate": "2024-09-12 19:36:26",
            "desc": {
              "cn": "èš‚èšé›†å›¢ã€ä¸Šæµ·è´¢ç»å¤§å­¦è”åˆæ¨å‡ºé‡‘èè¯„æµ‹é›†Fin-Eva Version 1.0ï¼Œè¦†ç›–è´¢å¯Œç®¡ç†ã€ä¿é™©ã€æŠ•èµ„ç ”ç©¶ç­‰å¤šä¸ªé‡‘èåœºæ™¯ä»¥åŠé‡‘èä¸“ä¸šä¸»é¢˜å­¦ç§‘ï¼Œæ€»è¯„æµ‹é¢˜æ•°ç›®è¾¾åˆ°13,000+ã€‚",
              "en": "Ant Group and Shanghai University of Finance and Economics jointly launched the financial benchmarkï¼ŒFin-Eva Version 1.0, covering multiple financial scenarios and subjects such as wealth management, insurance, investment research. The number of this benchmark's questions reaches 13,000+."
            }
          }
        },
        {
          "id": "opencompass_540",
          "name": "T-Eval",
          "version": "1.0.0",
          "description": "T-Eval evaluates the tool utilization capabilities of LLMs and decomposing them into instruction following, planning, reasoning, retrieval, understanding, and review.",
          "url": "opencompass/opencompass_540.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_540",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "540",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ—½",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 3,
            "githubLink": "https://github.com/open-compass/T-Eval",
            "paperLink": "https://arxiv.org/abs/2312.14033",
            "officialWebsiteLink": "https://open-compass.github.io/T-Eval/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "5298",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-01-25 15:15:04",
            "createDate": "2024-01-25 15:15:04",
            "desc": {
              "cn": "T-Eval è¯„ä¼°äº† LLM çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œå¹¶å°†å…¶åˆ†è§£ä¸ºæŒ‡ä»¤éµå¾ªã€è§„åˆ’ã€æ¨ç†ã€æ£€ç´¢ã€ç†è§£å’Œå®¡æŸ¥ç­‰å­èƒ½åŠ›",
              "en": "T-Eval evaluates the tool utilization capabilities of LLMs and decomposing them into instruction following, planning, reasoning, retrieval, understanding, and review."
            }
          }
        },
        {
          "id": "opencompass_541",
          "name": "L-Eval",
          "version": "1.0.0",
          "description": "L-Eval is a comprehensive Long Context Language Models (LCLMs) evaluation suite with 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3kï½200k tokens).",
          "url": "opencompass/opencompass_541.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_541",
          "sample_count": 1000,
          "traits": [
            "Long-Context"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "541",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ¦¾",
            "dimensions": [
              {
                "cn": "é•¿æ–‡æœ¬",
                "en": "Long-Context"
              }
            ],
            "subDimensions": [
              {
                "cn": "long-context",
                "en": "long-context"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/OpenLMLab/LEval",
            "paperLink": "https://arxiv.org/pdf/2307.11088",
            "officialWebsiteLink": "https://huggingface.co/datasets/L4NLP/LEval",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "4908",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:32:04",
            "supportOnlineEval": false,
            "updateDate": "2024-09-12 19:32:04",
            "createDate": "2024-09-12 19:26:40",
            "desc": {
              "cn": "L-Eval æ˜¯ä¸€ä¸ªå…¨é¢çš„é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ï¼ˆLCLMsï¼‰è¯„ä¼°å¥—ä»¶ï¼ŒåŒ…æ‹¬ 20 ä¸ªå­ä»»åŠ¡ã€508 ä¸ªé•¿æ–‡æ¡£å’Œè¶…è¿‡ 2,000 ä¸ªäººå·¥æ ‡è®°çš„æŸ¥è¯¢-å“åº”å¯¹ã€‚å®ƒæ¶µç›–äº†å¤šç§é—®ç­”é£æ ¼ã€é¢†åŸŸå’Œè¾“å…¥é•¿åº¦ï¼ˆ3,000 è‡³ 200,000 ä¸ª tokenï¼‰ã€‚",
              "en": "L-Eval is a comprehensive Long Context Language Models (LCLMs) evaluation suite with 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3kï½200k tokens)."
            }
          }
        },
        {
          "id": "opencompass_496",
          "name": "C-Eval",
          "version": "1.0.0",
          "description": "C-Eval is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels.",
          "url": "opencompass/opencompass_496.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_496",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "496",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/SJTU-LIT/ceval",
            "paperLink": "https://arxiv.org/abs/2305.08322",
            "officialWebsiteLink": "https://cevalbenchmark.com/index.html",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "4636",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": true,
            "updateDate": "2024-01-11 14:09:22",
            "createDate": "2024-01-11 14:09:22",
            "desc": {
              "cn": "C-Eval æ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„ä¼°å¥—ä»¶ã€‚å®ƒåŒ…å«äº†13948ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–äº†52ä¸ªä¸åŒçš„å­¦ç§‘å’Œå››ä¸ªéš¾åº¦çº§åˆ«ã€‚",
              "en": "C-Eval is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels."
            }
          }
        },
        {
          "id": "opencompass_498",
          "name": "MMLU",
          "version": "1.0.0",
          "description": "MMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics. The granularity and breadth of the subjects makes the benchmark ideal for identifying a modelâ€™s blind spots.",
          "url": "opencompass/opencompass_498.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_498",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "498",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/hendrycks/test",
            "paperLink": "https://arxiv.org/abs/2009.03300",
            "officialWebsiteLink": "https://github.com/hendrycks/test",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "4522",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": true,
            "updateDate": "2024-08-02 09:45:42",
            "createDate": "2024-01-11 14:09:29",
            "desc": {
              "cn": "MMLU (Massive Multitask Language Understanding) æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡åœ¨é›¶æ¬¡å­¦ä¹ å’Œå°‘æ¬¡å­¦ä¹ çš„ç¯å¢ƒä¸­è¯„ä¼°æ¨¡å‹æ¥æµ‹é‡é¢„è®­ç»ƒæœŸé—´è·å¾—çš„çŸ¥è¯†ã€‚è¿™ä½¿å¾—åŸºå‡†æµ‹è¯•æ›´å…·æŒ‘æˆ˜æ€§ï¼Œä¸”æ›´æ¥è¿‘æˆ‘ä»¬è¯„ä¼°äººç±»çš„æ–¹å¼ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†STEMã€äººæ–‡å­¦ç§‘ã€ç¤¾ä¼šç§‘å­¦ç­‰57ä¸ªä¸»é¢˜ã€‚å…¶éš¾åº¦èŒƒå›´ä»å°å­¦çº§åˆ«åˆ°ä¸“ä¸šçº§åˆ«ï¼Œæ—¨åœ¨æµ‹è¯•ä¸–ç•ŒçŸ¥è¯†å’Œè§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚æµ‹è¯•ä¸»é¢˜èŒƒå›´ä»ä¼ ç»Ÿé¢†åŸŸï¼Œå¦‚æ•°å­¦å’Œå†å²ï¼Œåˆ°æ›´ä¸“ä¸šçš„é¢†åŸŸï¼Œå¦‚æ³•å¾‹å’Œä¼¦ç†å­¦ã€‚é¢˜ç›®çš„ç²¾ç»†åº¦å’Œå¹¿åº¦ä½¿è¯¥åŸºå‡†æµ‹è¯•æˆä¸ºè¯†åˆ«æ¨¡å‹ç›²ç‚¹çš„ç†æƒ³é€‰æ‹©ã€‚",
              "en": "MMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics. The granularity and breadth of the subjects makes the benchmark ideal for identifying a modelâ€™s blind spots."
            }
          }
        },
        {
          "id": "opencompass_948",
          "name": "SecBench",
          "version": "1.0.0",
          "description": "Tencent Zhuque Lab and Tencent Security Keen Lab, together with Tencent Huyuan Team, Professor Jiang Yong/Professor Xia Shutao's team from Tsinghua University, Professor Luo Xiapu's research team from Hong Kong Polytechnic University, and OpenCompass team from Shanghai Artificial Intelligence Laboratory, have jointly built a safety benchmark, SecBench. We provide fair, impartial, objective, and comprehensive evaluation capabilities and promote the construction of large model in security dimension.",
          "url": "opencompass/opencompass_948.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_948",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "948",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ”",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [
              {
                "cn": "Safety",
                "en": "Safety"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 4,
            "githubLink": "",
            "paperLink": "",
            "officialWebsiteLink": "https://secbench.org/dataset",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "50163763",
              "name": "Tencent",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50163763-2065e33a-96ba-4182-9708-b1967a29e6a4.png",
              "nickname": "Tencent"
            },
            "lookNum": "4400",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:40:48",
            "supportOnlineEval": false,
            "updateDate": "2024-09-12 19:40:48",
            "createDate": "2024-09-12 19:35:12",
            "desc": {
              "cn": "è…¾è®¯æœ±é›€å®éªŒå®¤å’Œè…¾è®¯å®‰å…¨ç§‘æ©å®éªŒå®¤è”åˆè…¾è®¯æ··å…ƒå¤§æ¨¡å‹å›¢é˜Ÿã€æ¸…åå¤§å­¦æ±Ÿå‹‡æ•™æˆ/å¤æ ‘æ¶›æ•™æˆå›¢é˜Ÿã€é¦™æ¸¯ç†å·¥å¤§å­¦ç½—å¤æœ´æ•™æˆç ”ç©¶å›¢é˜Ÿä»¥åŠä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤OpenCompasså›¢é˜Ÿï¼Œé€šè¿‡å»ºè®¾å®‰å…¨å¤§æ¨¡å‹è¯„æµ‹åŸºå‡†SecBenchï¼Œä¸ºå®‰å…¨å¤§æ¨¡å‹ç ”å‘æä¾›å…¬å¹³ã€å…¬æ­£ã€å®¢è§‚ã€å…¨é¢çš„è¯„æµ‹èƒ½åŠ›ï¼Œæ¨åŠ¨å®‰å…¨å¤§æ¨¡å‹å»ºè®¾ã€‚",
              "en": "Tencent Zhuque Lab and Tencent Security Keen Lab, together with Tencent Huyuan Team, Professor Jiang Yong/Professor Xia Shutao's team from Tsinghua University, Professor Luo Xiapu's research team from Hong Kong Polytechnic University, and OpenCompass team from Shanghai Artificial Intelligence Laboratory, have jointly built a safety benchmark, SecBench. We provide fair, impartial, objective, and comprehensive evaluation capabilities and promote the construction of large model in security dimension."
            }
          }
        },
        {
          "id": "opencompass_537",
          "name": "HumanEval",
          "version": "1.0.0",
          "description": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases.",
          "url": "opencompass/opencompass_537.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_537",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "537",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [
              {
                "cn": "code",
                "en": "code"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/openai/human-eval",
            "paperLink": "https://arxiv.org/pdf/2107.03374.pdf",
            "officialWebsiteLink": "https://huggingface.co/datasets/openai_humaneval",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "3586",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:31:35",
            "supportOnlineEval": true,
            "updateDate": "2024-09-12 19:31:35",
            "createDate": "2024-09-12 19:27:36",
            "desc": {
              "cn": "è¿™æ˜¯ \"Evaluating Large Language Models Trained on Code\" è®ºæ–‡ä¸­æè¿°çš„ HumanEval é—®é¢˜è§£å†³æ•°æ®é›†çš„è¯„ä¼°å·¥å…·åŒ…ã€‚å®ƒç”¨äºæµ‹é‡ä»æ–‡æ¡£è„šæœ¬åˆæˆç¨‹åºçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚å®ƒç”± 164 ä¸ªåŸå§‹ç¼–ç¨‹é—®é¢˜ç»„æˆï¼Œè¯„ä¼°è¯­è¨€ç†è§£èƒ½åŠ›ã€ç®—æ³•å’Œç®€å•æ•°å­¦ï¼Œå…¶ä¸­ä¸€äº›é—®é¢˜ä¸ç®€å•çš„è½¯ä»¶é¢è¯•é¢˜ç±»ä¼¼ã€‚",
              "en": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases."
            }
          }
        },
        {
          "id": "opencompass_945",
          "name": "Flames",
          "version": "1.0.0",
          "description": "Flames is a highly adversarial benchmark in Chinese for LLM's value alignment evaluation developed by Shanghai AI Lab and Fudan NLP Group. Flames meticulously designs a dataset of 2,251 highly adversarial, manually crafted prompts, each tailored to probe a specific value dimension (i.e., Fairness, Safety, Morality, Legality, Data protection). Currently,  Flames releases 1,000 prompts for public use (Flames_1k_Chinese).",
          "url": "opencompass/opencompass_945.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_945",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "945",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ’¡",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [
              {
                "cn": "Safety",
                "en": "Safety"
              }
            ],
            "tags": [],
            "topicTags": [
              {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
              }
            ],
            "benchCertificateLevel": 4,
            "githubLink": "https://github.com/AIFlames/Flames",
            "paperLink": "https://arxiv.org/abs/2311.06899",
            "officialWebsiteLink": "https://flames.opencompass.org.cn/leaderboard",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "2982",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-16 11:02:53",
            "supportOnlineEval": false,
            "updateDate": "2024-10-16 11:02:53",
            "createDate": "2024-10-15 15:34:25",
            "desc": {
              "cn": "Flames æ˜¯ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤å’Œå¤æ—¦å¤§å­¦ NLPå›¢é˜Ÿå¼€å‘çš„ LLM ä»·å€¼å¯¹é½æ–¹å‘çš„ä¸­æ–‡é«˜åº¦å¯¹æŠ—æ€§åŸºå‡†ã€‚Flames ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªç”± 2,251 ä¸ªé«˜åº¦å¯¹æŠ—æ€§ã€äººå·¥åˆ›å»ºçš„æç¤ºè¯æˆçš„è¯„æµ‹é›†ï¼Œæ¯ä¸ªæç¤ºè¯éƒ½ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æ¢ç©¶ç‰¹å®šçš„ä»·å€¼ç»´åº¦ï¼ˆå³å…¬å¹³ã€å®‰å…¨ã€é“å¾·ã€åˆæ³•ã€æ•°æ®ä¿æŠ¤ï¼‰ã€‚ç›®å‰ï¼ŒFlames å‘å¸ƒäº† 1,000 ä¸ªæç¤ºè¯ä¾›å…¬ä¼—ä½¿ç”¨ï¼ˆFlames_1k_Chineseï¼‰ã€‚",
              "en": "Flames is a highly adversarial benchmark in Chinese for LLM's value alignment evaluation developed by Shanghai AI Lab and Fudan NLP Group. Flames meticulously designs a dataset of 2,251 highly adversarial, manually crafted prompts, each tailored to probe a specific value dimension (i.e., Fairness, Safety, Morality, Legality, Data protection). Currently,  Flames releases 1,000 prompts for public use (Flames_1k_Chinese)."
            }
          }
        },
        {
          "id": "opencompass_499",
          "name": "CMMLU",
          "version": "1.0.0",
          "description": "CMMLU is a comprehensive Chinese evaluation benchmark specifically designed to assess the knowledge and reasoning abilities of language models in the context of the Chinese language. CMMLU covers 67 topics ranging from basic subjects to advanced professional levels. It includes tasks that require calculations and reasoning in natural sciences, as well as tasks involving knowledge from humanities, social sciences, and practical aspects like Chinese driving rules. Moreover, many tasks within CMMLU have answers specific to China, which might not be universally applicable in other regions or languages. As a result, CMMLU serves as a fully localized Chinese evaluation benchmark.",
          "url": "opencompass/opencompass_499.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_499",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "499",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/haonan-li/CMMLU/tree/1b8ce5813be6a011b1042f2d8fc7f0806e2ac0bc",
            "paperLink": "https://arxiv.org/abs/2306.09212",
            "officialWebsiteLink": "https://github.com/haonan-li/CMMLU/tree/1b8ce5813be6a011b1042f2d8fc7f0806e2ac0bc",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "2540",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": true,
            "updateDate": "2024-08-02 09:46:14",
            "createDate": "2024-01-11 14:09:33",
            "desc": {
              "cn": "CMMLUæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„ä¸­æ–‡è¯„ä¼°åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡è¯­å¢ƒä¸‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚CMMLUæ¶µç›–äº†ä»åŸºç¡€å­¦ç§‘åˆ°é«˜çº§ä¸“ä¸šæ°´å¹³çš„67ä¸ªä¸»é¢˜ã€‚å®ƒåŒ…æ‹¬ï¼šéœ€è¦è®¡ç®—å’Œæ¨ç†çš„è‡ªç„¶ç§‘å­¦ï¼Œéœ€è¦çŸ¥è¯†çš„äººæ–‡ç§‘å­¦å’Œç¤¾ä¼šç§‘å­¦,ä»¥åŠéœ€è¦ç”Ÿæ´»å¸¸è¯†çš„ä¸­å›½é©¾é©¶è§„åˆ™ç­‰ã€‚æ­¤å¤–ï¼ŒCMMLUä¸­çš„è®¸å¤šä»»åŠ¡å…·æœ‰ä¸­å›½ç‰¹å®šçš„ç­”æ¡ˆï¼Œå¯èƒ½åœ¨å…¶ä»–åœ°åŒºæˆ–è¯­è¨€ä¸­å¹¶ä¸æ™®éé€‚ç”¨ã€‚å› æ­¤æ˜¯ä¸€ä¸ªå®Œå…¨ä¸­å›½åŒ–çš„ä¸­æ–‡æµ‹è¯•åŸºå‡†ã€‚",
              "en": "CMMLU is a comprehensive Chinese evaluation benchmark specifically designed to assess the knowledge and reasoning abilities of language models in the context of the Chinese language. CMMLU covers 67 topics ranging from basic subjects to advanced professional levels. It includes tasks that require calculations and reasoning in natural sciences, as well as tasks involving knowledge from humanities, social sciences, and practical aspects like Chinese driving rules. Moreover, many tasks within CMMLU have answers specific to China, which might not be universally applicable in other regions or languages. As a result, CMMLU serves as a fully localized Chinese evaluation benchmark."
            }
          }
        },
        {
          "id": "opencompass_1003",
          "name": "S-Eval",
          "version": "1.0.0",
          "description": "S-Eval is a new comprehensive, multi-dimensional and open-ended safety evaluation benchmark for LLMs consisting of 220,000 evaluation prompts (still in active expansion) across 102 risk subcategories and 10 advanced jailbreak attacks.",
          "url": "opencompass/opencompass_1003.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1003",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1003",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "âš–ï¸",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "LLM",
                "en": "LLM"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 4,
            "githubLink": "https://github.com/IS2Lab/S-Eval",
            "paperLink": "https://arxiv.org/abs/2405.14191",
            "officialWebsiteLink": "https://s-eval.github.io",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50151947",
              "name": "IS2Lab",
              "avatar": null,
              "nickname": "IS2Lab"
            },
            "lookNum": "2126",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-05-06 11:02:57",
            "supportOnlineEval": false,
            "updateDate": "2025-05-06 11:02:57",
            "createDate": "2025-04-22 16:28:30",
            "desc": {
              "cn": "S-Eval æ˜¯ä¸€ä¸ªé’ˆå¯¹ LLM çš„å…¨æ–°å…¨é¢ã€å¤šç»´ã€å¼€æ”¾å¼å®‰å…¨è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å« 102 ä¸ªé£é™©å­ç±»åˆ«çš„ 220,000 ä¸ªè¯„ä¼°æç¤ºï¼ˆä»åœ¨ç§¯ææ‰©å±•ä¸­ï¼‰å’Œ 10 ä¸ªé«˜çº§è¶Šç‹±æ”»å‡»ã€‚",
              "en": "S-Eval is a new comprehensive, multi-dimensional and open-ended safety evaluation benchmark for LLMs consisting of 220,000 evaluation prompts (still in active expansion) across 102 risk subcategories and 10 advanced jailbreak attacks."
            }
          }
        },
        {
          "id": "opencompass_531",
          "name": "HellaSwag",
          "version": "1.0.0",
          "description": "HellaSwag is a challenge dataset for evaluating commonsense natural language inference, which is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy). It consists of 70k multiple choice questions, each with a scenario and four possible endings, which requires to select the most reasonable ending. These questions come from two domains:activitynet and wikihow, involving video and text scenarios respectively. The correct answers of these questions are the real sentences for the next event, while the incorrect answers are adversarially generated and human verified, so as to fool machines but not humans.",
          "url": "opencompass/opencompass_531.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_531",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "531",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [
              {
                "cn": "reasoning",
                "en": "reasoning"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/rowanz/hellaswag",
            "paperLink": "https://arxiv.org/abs/1905.07830",
            "officialWebsiteLink": "https://allenai.org/data/hellaswag",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "2020",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:31:26",
            "supportOnlineEval": true,
            "updateDate": "2024-09-12 19:31:26",
            "createDate": "2024-09-12 19:28:41",
            "desc": {
              "cn": "HellaSwag æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¸¸è¯†æ€§è‡ªç„¶è¯­è¨€æ¨ç†çš„æ•°æ®é›†ï¼ŒHellaSwagçš„é—®é¢˜å¯¹äºæœ€å…ˆè¿›çš„æ¨¡å‹æ¥è¯´æ˜¯ç‰¹åˆ«å›°éš¾çš„ï¼Œå°½ç®¡å®ƒçš„é—®é¢˜å¯¹äºäººç±»æ¥è¯´éå¸¸è½»æ¾å°±èƒ½å›ç­”çš„ï¼ˆ> 95% çš„å‡†ç¡®ç‡ï¼‰ã€‚å®ƒç”±7ä¸‡å¤šé“å¤šé¡¹é€‰æ‹©é¢˜ç»„æˆï¼Œæ¯é“é¢˜éƒ½æœ‰ä¸€ä¸ªåœºæ™¯å’Œå››ç§å¯èƒ½çš„ç­”æ¡ˆï¼Œéœ€è¦é€‰æ‹©æœ€åˆç†çš„ç­”æ¡ˆã€‚è¿™äº›é—®é¢˜æ¥è‡ªä¸¤ä¸ªé¢†åŸŸï¼šactivitynetå’Œwikihowï¼Œåˆ†åˆ«æ¶‰åŠè§†é¢‘å’Œæ–‡æœ¬åœºæ™¯ã€‚è¿™äº›é—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆæ˜¯ä¸‹ä¸€ä¸ªäº‹ä»¶çš„çœŸå®å¥å­ï¼Œè€Œé”™è¯¯ç­”æ¡ˆæ˜¯é€šè¿‡å¯¹æŠ—æŠ€æœ¯ç”Ÿæˆçš„å¹¶ç»è¿‡äººç±»éªŒè¯ï¼Œè¿™äº›ç­”æ¡ˆå¯ä»¥æ¬ºéª—æœºå™¨ä½†ä¸èƒ½æ¬ºéª—äººç±»ã€‚",
              "en": "HellaSwag is a challenge dataset for evaluating commonsense natural language inference, which is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy). It consists of 70k multiple choice questions, each with a scenario and four possible endings, which requires to select the most reasonable ending. These questions come from two domains:activitynet and wikihow, involving video and text scenarios respectively. The correct answers of these questions are the real sentences for the next event, while the incorrect answers are adversarially generated and human verified, so as to fool machines but not humans."
            }
          }
        },
        {
          "id": "opencompass_692",
          "name": "ChemBench",
          "version": "1.0.0",
          "description": "ChemBench is a large-scale chemistry competency evaluation benchmark for language models, which includes nine chemistry core tasks and 4100 high-quality single-choice questions and answers.",
          "url": "opencompass/opencompass_692.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_692",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "692",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ§ª",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [
              {
                "cn": "Name_Conversion",
                "en": "Name_Conversion"
              },
              {
                "cn": "Property_Prediction",
                "en": "Property_Prediction"
              },
              {
                "cn": "Mol2caption",
                "en": "Mol2caption"
              },
              {
                "cn": "Caption2mol",
                "en": "Caption2mol"
              },
              {
                "cn": "Product_Prediction",
                "en": "Product_Prediction"
              },
              {
                "cn": "Retrosynthesis",
                "en": "Retrosynthesis"
              },
              {
                "cn": "Yield_Prediction",
                "en": "Yield_Prediction"
              },
              {
                "cn": "Temperature_Prediction",
                "en": "Temperature_Prediction"
              },
              {
                "cn": "Solvent_Prediction",
                "en": "Solvent_Prediction"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 4,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2402.06852",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "50029510",
              "name": "OpenScienceLab",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50029510-baa30340-1e57-432f-9b2b-6f656752c65e.png",
              "nickname": "OpenScienceLab"
            },
            "lookNum": "2003",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:51:38",
            "supportOnlineEval": false,
            "updateDate": "2024-09-12 19:51:38",
            "createDate": "2024-09-12 19:49:11",
            "desc": {
              "cn": "ChemBenchæ˜¯ä¸€ä¸ªåŒ…å«äº†ä¹é¡¹åŒ–å­¦æ ¸å¿ƒä»»åŠ¡ï¼Œ4100ä¸ªé«˜è´¨é‡å•é€‰é—®ç­”çš„å¤§è¯­è¨€æ¨¡å‹åŒ–å­¦èƒ½åŠ›è¯„æµ‹åŸºå‡†.",
              "en": "ChemBench is a large-scale chemistry competency evaluation benchmark for language models, which includes nine chemistry core tasks and 4100 high-quality single-choice questions and answers."
            }
          }
        },
        {
          "id": "opencompass_538",
          "name": "MBPP",
          "version": "1.0.0",
          "description": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases.",
          "url": "opencompass/opencompass_538.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_538",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "538",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [
              {
                "cn": "code",
                "en": "code"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/google-research/google-research/tree/master/mbpp",
            "paperLink": "https://arxiv.org/pdf/2108.07732v1",
            "officialWebsiteLink": "https://huggingface.co/datasets/mbpp",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1966",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:31:38",
            "supportOnlineEval": true,
            "updateDate": "2024-09-12 19:31:38",
            "createDate": "2024-09-12 19:27:17",
            "desc": {
              "cn": "è¯¥åŸºå‡†æµ‹è¯•ç”±å¤§çº¦1000ä¸ªå…¥é—¨çº§ç¨‹åºå‘˜å¯ä»¥è§£å†³çš„ä¼—åŒ…Pythonç¼–ç¨‹é—®é¢˜ç»„æˆï¼Œæ¶µç›–ç¼–ç¨‹åŸºç¡€çŸ¥è¯†ã€æ ‡å‡†åº“åŠŸèƒ½ç­‰ã€‚æ¯ä¸ªé—®é¢˜éƒ½ç”±ä»»åŠ¡æè¿°ã€ä»£ç è§£å†³æ–¹æ¡ˆå’Œ3ä¸ªè‡ªåŠ¨åŒ–æµ‹è¯•ç”¨ä¾‹ç»„æˆã€‚",
              "en": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases."
            }
          }
        },
        {
          "id": "opencompass_497",
          "name": "AGIEval",
          "version": "1.0.0",
          "description": "AGIEval is a human-centric benchmark specifically designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving. This benchmark is derived from 20 official, public, and high-standard admission and qualification exams intended for general human test-takers, such as general college admission tests (e.g., Chinese College Entrance Exam (Gaokao) and American SAT), law school admission tests, math competitions, lawyer qualification tests, and national civil service exams.",
          "url": "opencompass/opencompass_497.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_497",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "497",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [
              {
                "cn": "examination",
                "en": "examination"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/ruixiangcui/AGIEval",
            "paperLink": "https://arxiv.org/pdf/2304.06364",
            "officialWebsiteLink": "https://github.com/ruixiangcui/AGIEval",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1580",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:31:13",
            "supportOnlineEval": false,
            "updateDate": "2024-09-12 19:31:13",
            "createDate": "2024-09-12 19:30:02",
            "desc": {
              "cn": "AGIEvalæ˜¯ä¸€ä¸ªä»¥äººä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨æ¶‰åŠäººç±»è®¤çŸ¥å’Œé—®é¢˜è§£å†³çš„ä»»åŠ¡ä¸­çš„ä¸€èˆ¬èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•æºè‡ª20ä¸ªå®˜æ–¹ã€å…¬å¼€å’Œé«˜æ ‡å‡†çš„å…¥å­¦å’Œèµ„æ ¼è€ƒè¯•ï¼Œä¾‹å¦‚æ™®é€šå¤§å­¦å…¥å­¦è€ƒè¯•ï¼ˆä¾‹å¦‚ä¸­å›½é«˜è€ƒå’Œç¾å›½SATï¼‰ã€æ³•å­¦é™¢å…¥å­¦è€ƒè¯•ã€æ•°å­¦ç«èµ›ã€å¾‹å¸ˆèµ„æ ¼è€ƒè¯•ä»¥åŠå›½å®¶å…¬åŠ¡å‘˜è€ƒè¯•",
              "en": "AGIEval is a human-centric benchmark specifically designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving. This benchmark is derived from 20 official, public, and high-standard admission and qualification exams intended for general human test-takers, such as general college admission tests (e.g., Chinese College Entrance Exam (Gaokao) and American SAT), law school admission tests, math competitions, lawyer qualification tests, and national civil service exams."
            }
          }
        },
        {
          "id": "opencompass_505",
          "name": "CHID",
          "version": "1.0.0",
          "description": " CHID is a chinese idiom reading comprehension task, which requires to select the correct idiom to fill in the blank according to the context, with 10 candidate idioms.",
          "url": "opencompass/opencompass_505.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_505",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "505",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/chujiezheng/ChID-Dataset",
            "paperLink": "https://arxiv.org/abs/1906.01265",
            "officialWebsiteLink": "https://github.com/chujiezheng/ChID-Dataset",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1562",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:48:03",
            "createDate": "2024-01-11 14:10:02",
            "desc": {
              "cn": "CHIDæ˜¯ä¸€ä¸ªä¸­æ–‡æˆè¯­é˜…è¯»ç†è§£ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©æ­£ç¡®çš„æˆè¯­å¡«ç©ºï¼Œå…±æœ‰10ä¸ªå€™é€‰æˆè¯­ã€‚",
              "en": " CHID is a chinese idiom reading comprehension task, which requires to select the correct idiom to fill in the blank according to the context, with 10 candidate idioms."
            }
          }
        },
        {
          "id": "opencompass_529",
          "name": "COPA",
          "version": "1.0.0",
          "description": "COPA is a causal inference task, which requires to select the correct causal relation based on the given premise.",
          "url": "opencompass/opencompass_529.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_529",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "529",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://people.ict.usc.edu/~gordon/copa.html",
            "paperLink": "https://arxiv.org/abs/1905.00537",
            "officialWebsiteLink": "https://people.ict.usc.edu/~gordon/copa.html",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1504",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:51:45",
            "createDate": "2024-01-11 14:11:52",
            "desc": {
              "cn": "COPAæ˜¯ä¸€ä¸ªå› æœæ¨æ–­ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„å‰æï¼Œé€‰æ‹©æ­£ç¡®çš„å› æœå…³ç³»ã€‚\n",
              "en": "COPA is a causal inference task, which requires to select the correct causal relation based on the given premise."
            }
          }
        },
        {
          "id": "opencompass_502",
          "name": "ARC-c",
          "version": "1.0.0",
          "description": "The AI2â€™s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ",
          "url": "opencompass/opencompass_502.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_502",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "502",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://allenai.org/data/arc",
            "paperLink": "https://arxiv.org/pdf/1803.05457.pdf",
            "officialWebsiteLink": "https://allenai.org/data/arc",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1502",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:48:39",
            "createDate": "2024-01-11 14:09:48",
            "desc": {
              "cn": "AI2çš„æ¨ç†æŒ‘æˆ˜ï¼ˆARCï¼‰æ•°æ®é›†æ˜¯ä¸€ä¸ªå¤šé¡¹é€‰æ‹©é—®é¢˜å›ç­”æ•°æ®é›†ï¼ŒåŒ…å«äº†ä»ä¸‰å¹´çº§åˆ°ä¹å¹´çº§çš„ç§‘å­¦è€ƒè¯•ä¸­æå–çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šç®€å•å’ŒæŒ‘æˆ˜ï¼Œå…¶ä¸­åè€…åŒ…å«äº†éœ€è¦æ¨ç†èƒ½åŠ›çš„æ›´éš¾çš„é—®é¢˜ã€‚å¤§å¤šæ•°é—®é¢˜æœ‰4ä¸ªç­”æ¡ˆé€‰é¡¹ï¼Œä»…æœ‰ä¸åˆ°1ï¼…çš„é—®é¢˜æœ‰3ä¸ªæˆ–5ä¸ªç­”æ¡ˆé€‰é¡¹ã€‚",
              "en": "The AI2â€™s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. "
            }
          }
        },
        {
          "id": "opencompass_518",
          "name": "OpenbookQA",
          "version": "1.0.0",
          "description": "OpenBookQA contains questions that require multi-step reasoning, application of common-sense knowledge, and in-depth comprehension of text. It is a new type of question-answering dataset, modeled after open-book exams, designed to assess human understanding of a specific topic.",
          "url": "opencompass/opencompass_518.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_518",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "518",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/allenai/OpenBookQA",
            "paperLink": "https://arxiv.org/abs/1809.02789",
            "officialWebsiteLink": "https://allenai.org/data/open-book-qa",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1408",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:48:08",
            "createDate": "2024-01-11 14:11:07",
            "desc": {
              "cn": "OpenBookQAåŒ…å«éœ€è¦å¤šæ­¥æ¨ç†ã€è¿ç”¨å¸¸è¯†çŸ¥è¯†ã€æ·±å…¥ç†è§£æ–‡æœ¬ç­‰èƒ½åŠ›çš„é—®é¢˜ï¼Œæ˜¯ä¸€ç§æ–°å‹çš„é—®ç­”æ•°æ®é›†ï¼Œå…¶æ¨¡å¼å€Ÿé‰´äº†å¼€æ”¾å¼ä¹¦æœ¬è€ƒè¯•ï¼Œç”¨äºè¯„ä¼°äººç±»å¯¹æŸä¸€ä¸»é¢˜ç†è§£çš„ç¨‹åº¦ã€‚",
              "en": "OpenBookQA contains questions that require multi-step reasoning, application of common-sense knowledge, and in-depth comprehension of text. It is a new type of question-answering dataset, modeled after open-book exams, designed to assess human understanding of a specific topic."
            }
          }
        },
        {
          "id": "opencompass_631",
          "name": "OpenFinData",
          "version": "1.0.0",
          "description": "OpenFinData is an open-source financial evaluation dataset jointly released by EastMoney and Shanghai Artificial Intelligence Laboratory. This dataset represents the most realistic industrial scenario requirements and is currently the most comprehensive and professional financial evaluation dataset.",
          "url": "opencompass/opencompass_631.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_631",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "631",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ¦¾",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [
              {
                "cn": "Finance",
                "en": "Finance"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 4,
            "githubLink": "https://github.com/open-compass/OpenFinData",
            "paperLink": "",
            "officialWebsiteLink": "https://openfindata.org/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "50029256",
              "name": "eastmoney",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50029256-0729a0fd-9e97-4c38-b9f9-a580ad6f6a54.png",
              "nickname": "eastmoney"
            },
            "lookNum": "1405",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:40:43",
            "supportOnlineEval": false,
            "updateDate": "2024-09-12 19:40:43",
            "createDate": "2024-09-12 19:39:10",
            "desc": {
              "cn": "OpenFinDataæ˜¯ç”±ä¸œæ–¹è´¢å¯Œä¸ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤è”åˆå‘å¸ƒçš„å¼€æºé‡‘èè¯„æµ‹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ä»£è¡¨äº†æœ€çœŸå®çš„äº§ä¸šåœºæ™¯éœ€æ±‚ï¼Œæ˜¯ç›®å‰åœºæ™¯æœ€å…¨ã€ä¸“ä¸šæ€§æœ€æ·±çš„é‡‘èè¯„æµ‹æ•°æ®é›†ã€‚å®ƒåŸºäºä¸œæ–¹è´¢å¯Œå®é™…é‡‘èä¸šåŠ¡çš„å¤šæ ·åŒ–ä¸°å¯Œåœºæ™¯ï¼Œæ—¨åœ¨ä¸ºé‡‘èç§‘æŠ€é¢†åŸŸçš„ç ”ç©¶è€…å’Œå¼€å‘è€…æä¾›ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®èµ„æºã€‚",
              "en": "OpenFinData is an open-source financial evaluation dataset jointly released by EastMoney and Shanghai Artificial Intelligence Laboratory. This dataset represents the most realistic industrial scenario requirements and is currently the most comprehensive and professional financial evaluation dataset."
            }
          }
        },
        {
          "id": "opencompass_511",
          "name": "CommonSenseQA",
          "version": "1.0.0",
          "description": "\nCommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers. \n",
          "url": "opencompass/opencompass_511.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_511",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "511",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/jonathanherzig/commonsenseqa",
            "paperLink": "https://arxiv.org/abs/1811.00937",
            "officialWebsiteLink": "https://www.tau-nlp.sites.tau.ac.il/commonsenseqa",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1296",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:48:32",
            "createDate": "2024-01-11 14:10:39",
            "desc": {
              "cn": "CommonsenseQAæ˜¯ä¸€ä¸ªé€‰æ‹©é¢˜æ•°æ®é›†ï¼Œå®ƒéœ€è¦ä¸åŒç±»å‹çš„å¸¸è¯†çŸ¥è¯†æ¥é¢„æµ‹æ­£ç¡®ç­”æ¡ˆã€‚å®ƒåŒ…å«12,102ä¸ªé—®é¢˜ï¼Œæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆå’Œå››ä¸ªå¹²æ‰°ç­”æ¡ˆã€‚\n",
              "en": "\nCommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers. \n"
            }
          }
        },
        {
          "id": "opencompass_514",
          "name": "C3",
          "version": "1.0.0",
          "description": "A free-form multiple-Choice Chinese machine reading Comprehension dataset (C3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations",
          "url": "opencompass/opencompass_514.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_514",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "514",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [
              {
                "cn": "understanding",
                "en": "understanding"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/nlpdata/c3",
            "paperLink": "https://arxiv.org/abs/1904.09679",
            "officialWebsiteLink": "https://github.com/nlpdata/c3",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1247",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:31:23",
            "supportOnlineEval": false,
            "updateDate": "2024-09-12 19:31:23",
            "createDate": "2024-09-12 19:29:14",
            "desc": {
              "cn": "ä¸€ä¸ªè‡ªç”±å½¢å¼çš„å¤šé¡¹é€‰æ‹©ä¸­æ–‡æœºå™¨é˜…è¯»ç†è§£æ•°æ®é›†ï¼ˆC3ï¼‰ï¼ŒåŒ…å«13369ç¯‡æ–‡çŒ®ï¼ˆå¯¹è¯æˆ–æ›´æ­£å¼çš„æ··åˆä½“è£æ–‡æœ¬ï¼‰åŠå…¶ç›¸å…³çš„19577é“è‡ªç”±é€‰æ‹©é¢˜ï¼Œè¿™äº›é—®é¢˜éƒ½æ˜¯ä»æ±‰è¯­ä½œä¸ºç¬¬äºŒè¯­è¨€çš„è€ƒè¯•ä¸­æ”¶é›†åˆ°çš„",
              "en": "A free-form multiple-Choice Chinese machine reading Comprehension dataset (C3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations"
            }
          }
        },
        {
          "id": "opencompass_513",
          "name": "NQ",
          "version": "1.0.0",
          "description": "NQ (NaturalQuestion) corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. The inclusion of real user questions, and the requirement that solutions should read an entire page to find the answer, cause NQ to be a more realistic and challenging task than prior QA datasets.",
          "url": "opencompass/opencompass_513.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_513",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "513",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/google-research-datasets/natural-questions",
            "paperLink": "https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b8c26e4347adc3453c15d96a09e6f7f102293f71.pdf",
            "officialWebsiteLink": "https://ai.google.com/research/NaturalQuestions/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1236",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": true,
            "updateDate": "2024-08-02 09:48:17",
            "createDate": "2024-01-11 14:10:47",
            "desc": {
              "cn": "NQ æ•°æ®é›†æ¥è‡ªäºçœŸå®ç”¨æˆ·çš„é—®é¢˜ï¼Œå®ƒè¦æ±‚ QA ç³»ç»Ÿé˜…è¯»å’Œç†è§£æ•´ä¸ªç»´åŸºç™¾ç§‘æ–‡ç« ï¼Œè¿™äº›æ–‡ç« å¯èƒ½åŒ…å«ä¹Ÿå¯èƒ½ä¸åŒ…å«é—®é¢˜çš„ç­”æ¡ˆã€‚ç”±çœŸå®ç”¨æˆ·é—®é¢˜æ„æˆï¼Œä»¥åŠéœ€è¦é˜…è¯»æ•´ä¸ªé¡µé¢æ‰èƒ½æ‰¾åˆ°ç­”æ¡ˆçš„è¦æ±‚ï¼Œæ¯”ä»¥å¾€çš„ QA æ•°æ®é›†æ›´ç°å®å’Œæ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚",
              "en": "NQ (NaturalQuestion) corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. The inclusion of real user questions, and the requirement that solutions should read an entire page to find the answer, cause NQ to be a more realistic and challenging task than prior QA datasets."
            }
          }
        },
        {
          "id": "opencompass_564",
          "name": "LV-Eval",
          "version": "1.0.0",
          "description": "LV-Eval is a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. The average number of words is 102,380, and the Min/Max number of words is 11,896/387,406. It features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets.",
          "url": "opencompass/opencompass_564.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_564",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "564",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ—½",
            "dimensions": [
              {
                "cn": "é•¿æ–‡æœ¬",
                "en": "long-context"
              }
            ],
            "subDimensions": [
              {
                "cn": "Question Answering",
                "en": "Question Answering"
              },
              {
                "cn": "Synthetic",
                "en": "Synthetic"
              },
              {
                "cn": "Confusing Evidence",
                "en": "Confusing Evidence"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 4,
            "githubLink": "https://github.com/infinigence/LVEval",
            "paperLink": "https://arxiv.org/abs/2402.05136",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "066910",
              "name": null,
              "avatar": null,
              "nickname": "OpenXLab-KGJYN5OmL"
            },
            "lookNum": "1218",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 10:32:21",
            "createDate": "2024-02-18 11:35:12",
            "desc": {
              "cn": "LV-Evalæ˜¯ä¸€ä¸ªå…·å¤‡5ä¸ªé•¿åº¦ç­‰çº§ï¼ˆ16kã€32kã€64kã€128kå’Œ256kï¼‰ã€æœ€å¤§æ–‡æœ¬æµ‹è¯•é•¿åº¦è¾¾åˆ°256kçš„é•¿æ–‡æœ¬è¯„æµ‹åŸºå‡†ã€‚LV-Evalçš„å¹³å‡æ–‡æœ¬é•¿åº¦è¾¾åˆ°102,380å­—ï¼Œæœ€å°/æœ€å¤§æ–‡æœ¬é•¿åº¦ä¸º11,896/387,406å­—ã€‚LV-Evalä¸»è¦æœ‰ä¸¤ç±»è¯„æµ‹ä»»åŠ¡â€”â€”å•è·³QAå’Œå¤šè·³QAï¼Œå…±åŒ…å«11ä¸ªæ¶µç›–ä¸­è‹±æ–‡çš„è¯„æµ‹æ•°æ®å­é›†ã€‚",
              "en": "LV-Eval is a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. The average number of words is 102,380, and the Min/Max number of words is 11,896/387,406. It features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets."
            }
          }
        },
        {
          "id": "opencompass_510",
          "name": "BoolQ",
          "version": "1.0.0",
          "description": "BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring ---they are generated in unprompted and unconstrained settings. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context.",
          "url": "opencompass/opencompass_510.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_510",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "510",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [
              {
                "cn": "knowledge",
                "en": "knowledge"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/google-research-datasets/boolean-questions",
            "paperLink": "https://arxiv.org/abs/1905.10044",
            "officialWebsiteLink": "https://github.com/google-research-datasets/boolean-questions",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1156",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:31:18",
            "supportOnlineEval": false,
            "updateDate": "2024-09-12 19:31:18",
            "createDate": "2024-09-12 19:29:42",
            "desc": {
              "cn": "BoolQæ˜¯ä¸€ä¸ªåŒ…å«15942ä¸ªç¤ºä¾‹çš„æ˜¯/å¦é—®é¢˜çš„é—®ç­”æ•°æ®é›†ã€‚è¿™äº›é—®é¢˜æ˜¯è‡ªç„¶ç”Ÿæˆçš„â€”â€”å³åœ¨æ— promptå’Œæ— çº¦æŸçš„ç¯å¢ƒä¸­äº§ç”Ÿçš„ã€‚æ¯ä¸ªä¾‹å­éƒ½æ˜¯ä¸€ä¸ªä¸‰å…ƒç»„(é—®é¢˜ã€æ®µè½ã€ç­”æ¡ˆ)ï¼Œé¡µé¢æ ‡é¢˜æ˜¯å¯é€‰çš„é™„åŠ ä¸Šä¸‹æ–‡ã€‚",
              "en": "BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring ---they are generated in unprompted and unconstrained settings. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context."
            }
          }
        },
        {
          "id": "opencompass_1206",
          "name": "MMBench",
          "version": "1.0.0",
          "description": "MMBench is a collection of benchmarks to evaluate the multi-modal understanding capability of large vision language models (LVLMs). This benchmark contains 3,000 multiple-choice questions covering 20 fine-grained assessment dimensions.",
          "url": "opencompass/opencompass_1206.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1206",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1206",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 3,
            "githubLink": "https://github.com/open-compass/MMBench",
            "paperLink": "https://arxiv.org/abs/2307.06281",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1126",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-30 16:30:49",
            "supportOnlineEval": false,
            "updateDate": "2024-12-30 16:30:49",
            "createDate": "2024-12-30 16:16:18",
            "desc": {
              "cn": "MMBenchæ˜¯OpenCompass ç ”ç©¶å›¢é˜Ÿè‡ªå»ºçš„è§†è§‰è¯­è¨€æ¨¡å‹è¯„æµ‹æ•°æ®é›†ï¼Œå¯å®ç°ä»æ„ŸçŸ¥åˆ°è®¤çŸ¥èƒ½åŠ›é€çº§ç»†åˆ†è¯„ä¼°ã€‚æ­¤è¯„æµ‹åŸºå‡†åŒ…å«3000 é“å•é¡¹é€‰æ‹©é¢˜ ï¼Œè¦†ç›– 20ä¸ªç»†ç²’åº¦è¯„ä¼°ç»´åº¦ã€‚",
              "en": "MMBench is a collection of benchmarks to evaluate the multi-modal understanding capability of large vision language models (LVLMs). This benchmark contains 3,000 multiple-choice questions covering 20 fine-grained assessment dimensions."
            }
          }
        },
        {
          "id": "opencompass_500",
          "name": "GAOKAO-Bench",
          "version": "1.0.0",
          "description": "GAOKAO-bench is an evaluation framework that utilizes Chinese high school entrance examination (GAOKAO) questions as a dataset to evaluate the language understanding and logical reasoning abilities of large language models.",
          "url": "opencompass/opencompass_500.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_500",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "500",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/OpenLMLab/GAOKAO-Bench",
            "paperLink": "https://arxiv.org/abs/2305.12474",
            "officialWebsiteLink": "https://github.com/OpenLMLab/GAOKAO-Bench",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1100",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": true,
            "updateDate": "2024-08-02 09:49:06",
            "createDate": "2024-01-11 14:09:41",
            "desc": {
              "cn": "GAOKAO-benchæ˜¯ä¸€ä¸ªä»¥ä¸­å›½é«˜è€ƒé¢˜ç›®ä¸ºæ•°æ®é›†ï¼Œæ—¨åœ¨æä¾›å’Œäººç±»å¯¹é½çš„ï¼Œç›´è§‚ï¼Œé«˜æ•ˆåœ°æµ‹è¯„å¤§æ¨¡å‹è¯­è¨€ç†è§£èƒ½åŠ›ã€é€»è¾‘æ¨ç†èƒ½åŠ›çš„æµ‹è¯„æ¡†æ¶",
              "en": "GAOKAO-bench is an evaluation framework that utilizes Chinese high school entrance examination (GAOKAO) questions as a dataset to evaluate the language understanding and logical reasoning abilities of large language models."
            }
          }
        },
        {
          "id": "opencompass_543",
          "name": "HumanEval-X",
          "version": "1.0.0",
          "description": "HumanEval-X is a benchmark for evaluating the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks, such as code generation and translation.",
          "url": "opencompass/opencompass_543.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_543",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "543",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ—½",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [
              {
                "cn": "code",
                "en": "code"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/THUDM/CodeGeeX",
            "paperLink": "https://arxiv.org/abs/2303.17568",
            "officialWebsiteLink": "https://huggingface.co/datasets/THUDM/humaneval-x",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1021",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-12 19:32:11",
            "supportOnlineEval": false,
            "updateDate": "2024-09-12 19:32:11",
            "createDate": "2024-09-12 19:25:43",
            "desc": {
              "cn": "HumanEval-X æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ä»£ç ç”Ÿæˆæ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«äº†820ä¸ªé«˜è´¨é‡çš„äººå·¥åˆ¶ä½œçš„æ•°æ®æ ·æœ¬ï¼ˆæ¯ä¸ªéƒ½æœ‰æµ‹è¯•æ¡ˆä¾‹ï¼‰ï¼ŒåŒ…æ‹¬Pythonã€C++ã€Javaã€JavaScriptå’ŒGoè¯­è¨€ï¼Œå¯ç”¨äºå„ç§ä»»åŠ¡ï¼Œå¦‚ä»£ç ç”Ÿæˆå’Œç¿»è¯‘ã€‚",
              "en": "HumanEval-X is a benchmark for evaluating the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks, such as code generation and translation."
            }
          }
        },
        {
          "id": "opencompass_504",
          "name": "WiC",
          "version": "1.0.0",
          "description": "WiC is a benchmark for the evaluation of context-sensitive word embeddings. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. In fact, the dataset can also be viewed as an application of Word Sense Disambiguation in practise.",
          "url": "opencompass/opencompass_504.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_504",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "504",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://pilehvar.github.io/wic/",
            "paperLink": "https://arxiv.org/abs/1808.09121",
            "officialWebsiteLink": "https://pilehvar.github.io/wic/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "1009",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:48:51",
            "createDate": "2024-01-11 14:09:58",
            "desc": {
              "cn": "Word-in-Contextæ˜¯ä¸€ä¸ªè¯ä¹‰æ¶ˆæ­§ä»»åŠ¡ï¼Œè¢«è§†ä¸ºå¥å­å¯¹çš„äºŒå…ƒåˆ†ç±»ã€‚ç»™å®šä¸¤ä¸ªæ–‡æœ¬ç‰‡æ®µå’Œä¸€ä¸ªåœ¨ä¸¤ä¸ªå¥å­ä¸­éƒ½å‡ºç°çš„å¤šä¹‰è¯ï¼Œä»»åŠ¡æ˜¯ç¡®å®šè¯¥è¯åœ¨ä¸¤ä¸ªå¥å­ä¸­æ˜¯å¦å…·æœ‰ç›¸åŒçš„å«ä¹‰ã€‚",
              "en": "WiC is a benchmark for the evaluation of context-sensitive word embeddings. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. In fact, the dataset can also be viewed as an application of Word Sense Disambiguation in practise."
            }
          }
        },
        {
          "id": "opencompass_509",
          "name": "Flores",
          "version": "1.0.0",
          "description": "Flores is a benchmark dataset for machine translation between English and low-resource languages, which consists of sentences translated from Wikipedia, involving English and four low-resource languages, namely Nepali, Sinhala, Khmer and Pashto. Flores has two versions, we use the Flores-101 version here, which is the second version including 101 languages besides english. ",
          "url": "opencompass/opencompass_509.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_509",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "509",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/facebookresearch/flores/",
            "paperLink": "https://arxiv.org/abs/2106.03193",
            "officialWebsiteLink": "https://github.com/facebookresearch/flores/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "889",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:50:59",
            "createDate": "2024-01-11 14:10:32",
            "desc": {
              "cn": "Floresæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ä½èµ„æºè¯­è¨€æœºå™¨ç¿»è¯‘çš„åŸºå‡†æ•°æ®é›†ï¼Œå®ƒåŒ…å«äº†ä»ç»´åŸºç™¾ç§‘ç¿»è¯‘çš„å¥å­ï¼Œæ¶‰åŠè‹±è¯­å’Œå››ç§ä½èµ„æºè¯­è¨€ï¼Œåˆ†åˆ«æ˜¯å°¼æ³Šå°”è¯­ã€åƒ§ä¼½ç½—è¯­ã€é«˜æ£‰è¯­å’Œæ™®ä»€å›¾è¯­ã€‚Floresæœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œæˆ‘ä»¬è¿™é‡Œä½¿ç”¨çš„æ˜¯ç¬¬ä¸€ä¸ªç‰ˆæœ¬Flores-101ï¼Œå®ƒåŒ…å«æœ‰é™¤è‹±è¯­å¤–çš„101ç§è¯­è¨€ã€‚",
              "en": "Flores is a benchmark dataset for machine translation between English and low-resource languages, which consists of sentences translated from Wikipedia, involving English and four low-resource languages, namely Nepali, Sinhala, Khmer and Pashto. Flores has two versions, we use the Flores-101 version here, which is the second version including 101 languages besides english. "
            }
          }
        },
        {
          "id": "opencompass_524",
          "name": "CMNLI",
          "version": "1.0.0",
          "description": "CMNLI  is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral.",
          "url": "opencompass/opencompass_524.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_524",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "524",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/CLUEbenchmark/CLUE",
            "paperLink": "https://arxiv.org/abs/2004.05986",
            "officialWebsiteLink": "https://www.cluebenchmarks.com/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "883",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:49:37",
            "createDate": "2024-01-11 14:11:34",
            "desc": {
              "cn": "CMNLIæ˜¯ä¸€ä¸ªä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ä¸¤ä¸ªå¥å­åˆ¤æ–­å®ƒä»¬ä¹‹é—´çš„é€»è¾‘å…³ç³»ï¼Œæœ‰ä¸‰ç§å…³ç³»ï¼šè•´å«ã€çŸ›ç›¾å’Œä¸­ç«‹ã€‚",
              "en": "CMNLI  is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral."
            }
          }
        },
        {
          "id": "opencompass_520",
          "name": "LCSTS",
          "version": "1.0.0",
          "description": "LCSTS is a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text.",
          "url": "opencompass/opencompass_520.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_520",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "520",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "http://icrc.hitsz.edu.cn/Article/show/139.html",
            "paperLink": "https://arxiv.org/abs/1506.05865",
            "officialWebsiteLink": "http://icrc.hitsz.edu.cn/Article/show/139.html",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "882",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:49:18",
            "createDate": "2024-01-11 14:11:16",
            "desc": {
              "cn": "LCSTSæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ä¸­æ–‡çŸ­æ–‡æœ¬æ‘˜è¦æ•°æ®é›†ï¼Œä»ä¸­å›½å¾®åšç½‘ç«™æ–°æµªå¾®åšä¸­æ„å»ºè€Œæˆï¼Œå¹¶å·²å¼€æºã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡ 200 ä¸‡æ¡çœŸå®çš„ä¸­æ–‡çŸ­æ–‡æœ¬ï¼Œæ¯ä¸ªæ–‡æœ¬éƒ½æä¾›äº†ä¸€ä¸ªç®€çŸ­çš„æ‘˜è¦ã€‚",
              "en": "LCSTS is a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text."
            }
          }
        },
        {
          "id": "opencompass_1052",
          "name": "CaLM",
          "version": "1.0.0",
          "description": "CaLM is the first comprehensive benchmark for evaluating the causal reasoning capabilities of language models. The CaLM framework establishes a foundational taxonomy consisting of four modules: causal target, adaptation, metric, and error.\n",
          "url": "opencompass/opencompass_1052.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1052",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1052",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/OpenCausaLab/CaLM",
            "paperLink": "https://arxiv.org/abs/2405.00622",
            "officialWebsiteLink": "https://opencausalab.github.io/CaLM/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50204289",
              "name": "OpenCausaLab",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50204289-b2e0a1f4-5d24-4237-b62a-ed8731d8834d.png",
              "nickname": "OpenCausaLab"
            },
            "lookNum": "825",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-23 16:42:32",
            "supportOnlineEval": false,
            "updateDate": "2024-09-23 16:42:32",
            "createDate": "2024-09-13 13:55:09",
            "desc": {
              "cn": "CaLMæ˜¯ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤è”åˆåŒæµå¤§å­¦ã€ä¸Šæµ·äº¤é€šå¤§å­¦ã€åŒ—äº¬å¤§å­¦åŠå•†æ±¤ç§‘æŠ€å‘å¸ƒé¦–ä¸ªå¤§æ¨¡å‹å› æœæ¨ç†å¼€æ”¾è¯„æµ‹ä½“ç³»ã€‚é¦–æ¬¡ä»å› æœæ¨ç†è§’åº¦æå‡ºè¯„ä¼°æ¡†æ¶ï¼Œä¸ºAIç ”ç©¶è€…æ‰“é€ å¯é è¯„æµ‹å·¥å…·ï¼Œä»è€Œä¸ºæ¨è¿›å¤§æ¨¡å‹è®¤çŸ¥èƒ½åŠ›å‘äººç±»æ°´å¹³çœ‹é½æä¾›æŒ‡æ ‡å‚è€ƒã€‚",
              "en": "CaLM is the first comprehensive benchmark for evaluating the causal reasoning capabilities of language models. The CaLM framework establishes a foundational taxonomy consisting of four modules: causal target, adaptation, metric, and error.\n"
            }
          }
        },
        {
          "id": "opencompass_532",
          "name": "PIQA",
          "version": "1.0.0",
          "description": "PIQA is a physical interaction question answering task, which requires to select the most reasonable solution based on the given scenario and two possible solutions. This task is designed to test the model's knowledge in physical commonsense. This dataset consists of 16k training samples, 800 development samples and 2k test samples, all on English text.",
          "url": "opencompass/opencompass_532.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_532",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "532",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/francois-rozet/piqa",
            "paperLink": "https://arxiv.org/abs/1911.11641",
            "officialWebsiteLink": "https://yonatanbisk.com/piqa/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "811",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:49:25",
            "createDate": "2024-01-11 14:12:04",
            "desc": {
              "cn": "PIQAæ˜¯ä¸€ä¸ªç‰©ç†äº¤äº’é—®ç­”ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„åœºæ™¯å’Œä¸¤ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œé€‰æ‹©æœ€åˆç†çš„æ–¹æ¡ˆã€‚è¿™ä¸ªä»»åŠ¡æ˜¯ä¸ºäº†æµ‹è¯•æ¨¡å‹åœ¨ç‰©ç†å¸¸è¯†æ–¹é¢çš„çŸ¥è¯†ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…å«äº†16000ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œ800ä¸ªå¼€å‘æ ·æœ¬å’Œ2000ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œæ‰€æœ‰çš„æ–‡æœ¬éƒ½æ˜¯è‹±æ–‡æ–‡æœ¬ã€‚",
              "en": "PIQA is a physical interaction question answering task, which requires to select the most reasonable solution based on the given scenario and two possible solutions. This task is designed to test the model's knowledge in physical commonsense. This dataset consists of 16k training samples, 800 development samples and 2k test samples, all on English text."
            }
          }
        },
        {
          "id": "opencompass_533",
          "name": "SIQA",
          "version": "1.0.0",
          "description": "SIQA is a social interaction question answering task, which requires to select the most reasonable behavior based on the given scenario and three possible subsequent behaviors. This task is designed to test the model's knowledge in social commonsense. This dataset consists of 38,963 training samples, 1,951 development samples and 1,960 test samples, all on English text.",
          "url": "opencompass/opencompass_533.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_533",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "533",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "",
            "paperLink": "https://arxiv.org/pdf/1904.09728.pdf",
            "officialWebsiteLink": "https://leaderboard.allenai.org/socialiqa/submissions/public",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "811",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:49:13",
            "createDate": "2024-01-11 14:12:08",
            "desc": {
              "cn": "SIQA æ˜¯ä¸€ä¸ªç¤¾ä¼šäº¤äº’é—®ç­”ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„åœºæ™¯å’Œä¸‰ä¸ªå¯èƒ½çš„åç»­è¡Œä¸ºï¼Œé€‰æ‹©æœ€åˆç†çš„è¡Œä¸ºã€‚è¿™ä¸ªä»»åŠ¡æ˜¯ä¸ºäº†æµ‹è¯•æ¨¡å‹åœ¨ç¤¾ä¼šå¸¸è¯†æ–¹é¢çš„çŸ¥è¯†ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…å«äº† 38963 ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œ1951 ä¸ªå¼€å‘æ ·æœ¬å’Œ 1960 ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œæ‰€æœ‰çš„æ–‡æœ¬éƒ½æ˜¯è‹±æ–‡æ–‡æœ¬ã€‚",
              "en": "SIQA is a social interaction question answering task, which requires to select the most reasonable behavior based on the given scenario and three possible subsequent behaviors. This task is designed to test the model's knowledge in social commonsense. This dataset consists of 38,963 training samples, 1,951 development samples and 1,960 test samples, all on English text."
            }
          }
        },
        {
          "id": "opencompass_519",
          "name": "CSL",
          "version": "1.0.0",
          "description": "CSL is a large-scale Chinese Scientific Literature dataset, which contains the titles, abstracts, keywords and academic fields of 396k papers.",
          "url": "opencompass/opencompass_519.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_519",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "519",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/ydli-ai/CSL",
            "paperLink": "https://arxiv.org/abs/2209.05034",
            "officialWebsiteLink": "https://github.com/ydli-ai/CSL",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "804",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:50:33",
            "createDate": "2024-01-11 14:11:11",
            "desc": {
              "cn": "CSLæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ä¸­æ–‡ç§‘æŠ€æ–‡çŒ®æ•°æ®é›†ï¼ŒåŒ…å« 39.6 ä¸‡ç¯‡è®ºæ–‡çš„æ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯å’Œå­¦æœ¯é¢†åŸŸä¿¡æ¯ã€‚",
              "en": "CSL is a large-scale Chinese Scientific Literature dataset, which contains the titles, abstracts, keywords and academic fields of 396k papers."
            }
          }
        },
        {
          "id": "opencompass_536",
          "name": "DROP",
          "version": "1.0.0",
          "description": "DROP is a QA dataset which tests comprehensive understanding of paragraphs. In this crowdsourced, adversarially-created, 96k question-answering benchmark, a system must resolve multiple references in a question, map them onto a paragraph, and perform discrete operations over them (such as addition, counting, or sorting).",
          "url": "opencompass/opencompass_536.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_536",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "536",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/allenai/allennlp-reading-comprehension/blob/master/allennlp_rc/eval/drop_eval.py",
            "paperLink": "https://arxiv.org/abs/1903.00161",
            "officialWebsiteLink": "https://allennlp.org/drop",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "787",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:50:10",
            "createDate": "2024-01-11 14:12:19",
            "desc": {
              "cn": "DROP æ˜¯ä¸€ä¸ªæµ‹è¯•æ®µè½ç»¼åˆç†è§£èƒ½åŠ›çš„ QA æ•°æ®é›†ã€‚åœ¨è¿™ä¸ªä¼—åŒ…ã€å¯¹æŠ—æ€§åˆ›å»ºçš„ 96K é—®é¢˜è§£ç­”åŸºå‡†ä¸­ï¼Œç³»ç»Ÿå¿…é¡»è§£æé—®é¢˜ä¸­çš„å¤šä¸ªå¼•ç”¨ï¼Œå°†å®ƒä»¬æ˜ å°„åˆ°æ®µè½ä¸­ï¼Œå¹¶å¯¹å®ƒä»¬æ‰§è¡Œç¦»æ•£æ“ä½œï¼ˆå¦‚åŠ æ³•ã€è®¡æ•°æˆ–æ’åºï¼‰ã€‚",
              "en": "DROP is a QA dataset which tests comprehensive understanding of paragraphs. In this crowdsourced, adversarially-created, 96k question-answering benchmark, a system must resolve multiple references in a question, map them onto a paragraph, and perform discrete operations over them (such as addition, counting, or sorting)."
            }
          }
        },
        {
          "id": "opencompass_503",
          "name": "ARC-e",
          "version": "1.0.0",
          "description": "The AI2â€™s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ",
          "url": "opencompass/opencompass_503.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_503",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "503",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://allenai.org/data/arc",
            "paperLink": "https://arxiv.org/pdf/1803.05457.pdf",
            "officialWebsiteLink": "https://allenai.org/data/arc",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "785",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:49:30",
            "createDate": "2024-01-11 14:09:54",
            "desc": {
              "cn": "AI2çš„æ¨ç†æŒ‘æˆ˜ï¼ˆARCï¼‰æ•°æ®é›†æ˜¯ä¸€ä¸ªå¤šé¡¹é€‰æ‹©é—®é¢˜å›ç­”æ•°æ®é›†ï¼ŒåŒ…å«äº†ä»ä¸‰å¹´çº§åˆ°ä¹å¹´çº§çš„ç§‘å­¦è€ƒè¯•ä¸­æå–çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šç®€å•å’ŒæŒ‘æˆ˜ï¼Œå…¶ä¸­åè€…åŒ…å«äº†éœ€è¦æ¨ç†èƒ½åŠ›çš„æ›´éš¾çš„é—®é¢˜ã€‚å¤§å¤šæ•°é—®é¢˜æœ‰4ä¸ªç­”æ¡ˆé€‰é¡¹ï¼Œä»…æœ‰ä¸åˆ°1ï¼…çš„é—®é¢˜æœ‰3ä¸ªæˆ–5ä¸ªç­”æ¡ˆé€‰é¡¹ã€‚",
              "en": "The AI2â€™s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. "
            }
          }
        },
        {
          "id": "opencompass_924",
          "name": "CS-Bench",
          "version": "1.0.0",
          "description": "CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning.",
          "url": "opencompass/opencompass_924.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_924",
          "sample_count": 1000,
          "traits": [
            "Code",
            "Math",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "924",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ˜œ",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              },
              {
                "cn": "ä»£ç ",
                "en": "Code"
              },
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/csbench/csbench",
            "paperLink": "https://arxiv.org/pdf/2406.08587",
            "officialWebsiteLink": "https://csbench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "50103486",
              "name": null,
              "avatar": null,
              "nickname": "LeonDiao0427"
            },
            "lookNum": "784",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-07-11 09:52:59",
            "createDate": "2024-07-11 09:48:45",
            "desc": {
              "cn": "CS-Bench ç¬¬ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMsåœ¨è®¡ç®—æœºç§‘å­¦ä¸­è¡¨ç°çš„åŒè¯­ï¼ˆä¸­è‹±æ–‡ï¼‰åŸºå‡†ã€‚CS-BenchåŒ…æ‹¬çº¦5,000ä¸ªç²¾å¿ƒç­–åˆ’çš„æµ‹è¯•æ ·æœ¬ï¼Œæ¶µç›–äº†è®¡ç®—æœºç§‘å­¦4ä¸ªå…³é”®é¢†åŸŸä¸­çš„26ä¸ªå­é¢†åŸŸï¼Œå¹¶åŒ…æ‹¬å„ç§ä»»åŠ¡å½¢å¼å’ŒçŸ¥è¯†æ¨ç†çš„åˆ’åˆ†ã€‚",
              "en": "CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning."
            }
          }
        },
        {
          "id": "opencompass_1073",
          "name": "SafetyBench",
          "version": "1.0.0",
          "description": " SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data.",
          "url": "opencompass/opencompass_1073.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1073",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1073",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/thu-coai/SafetyBench",
            "paperLink": "https://arxiv.org/pdf/2309.07045",
            "officialWebsiteLink": "https://llmbench.ai/safety",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "5018933",
              "name": "thu-coai",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/5018933-17d059c5-d271-410a-82a9-ef8966927c24.png",
              "nickname": "thu-coai"
            },
            "lookNum": "735",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-27 17:50:17",
            "supportOnlineEval": false,
            "updateDate": "2024-09-27 17:50:17",
            "createDate": "2024-09-27 12:52:37",
            "desc": {
              "cn": "SafetyBench æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨æ€§ï¼ŒåŒ…å« 11,435 é“å¤šæ ·åŒ–çš„é€‰æ‹©é¢˜ï¼Œæ¶µç›– 7 ä¸ªä¸åŒçš„å®‰å…¨å…³æ³¨ç±»åˆ«ã€‚SafetyBench è¿˜åŒ…å«ä¸­æ–‡å’Œè‹±æ–‡çš„æ•°æ®ï¼Œæ–¹ä¾¿åŒè¯­è¯„ä¼°ã€‚",
              "en": " SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data."
            }
          }
        },
        {
          "id": "opencompass_521",
          "name": "XSum",
          "version": "1.0.0",
          "description": "XSum is a single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question â€œWhat is the article about?â€. The dataset collects real-world, large scale data by harvesting online articles from the British Broadcasting Corporation (BBC).",
          "url": "opencompass/opencompass_521.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_521",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "521",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/EdinburghNLP/XSum",
            "paperLink": "https://arxiv.org/abs/1808.08745",
            "officialWebsiteLink": "https://github.com/EdinburghNLP/XSum",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "726",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:50:47",
            "createDate": "2024-01-11 14:11:21",
            "desc": {
              "cn": "XSumæ˜¯ä¸€ä¸ªå•æ–‡æ¡£æ‘˜è¦ä»»åŠ¡ï¼Œä¸æ”¯æŒæŠ½å–å¼ç­–ç•¥ï¼Œéœ€è¦é‡‡ç”¨æŠ½è±¡å»ºæ¨¡æ–¹æ³•ã€‚å…¶æ€æƒ³æ˜¯åˆ›å»ºä¸€ä¸ªç®€çŸ­çš„ä¸€å¥è¯æ–°é—»æ‘˜è¦ï¼Œå›ç­”â€œè¿™ç¯‡æ–‡ç« æ˜¯å…³äºä»€ä¹ˆçš„ï¼Ÿâ€çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä»è‹±å›½å¹¿æ’­å…¬å¸ï¼ˆBBCï¼‰æ”¶é›†åœ¨çº¿æ–‡ç« ï¼Œå¾—åˆ°äº†å¤§é‡çš„ç°å®æ•°æ®ã€‚",
              "en": "XSum is a single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question â€œWhat is the article about?â€. The dataset collects real-world, large scale data by harvesting online articles from the British Broadcasting Corporation (BBC)."
            }
          }
        },
        {
          "id": "opencompass_1097",
          "name": "HelloBench",
          "version": "1.0.0",
          "description": "HelloBench is a hierarchical long text generation benchmark to evaluate LLMs' performance in generating long text.  Based on Bloom's Taxonomy, HelloBench categorizes long text generation tasks into five subtasks: open-ended QA, summarization, chat, text completion, and heuristic text generation.",
          "url": "opencompass/opencompass_1097.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1097",
          "sample_count": 1000,
          "traits": [
            "Long-Context"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1097",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "é•¿æ–‡æœ¬",
                "en": "Long-Context"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 3,
            "githubLink": "https://github.com/Quehry/HelloBench",
            "paperLink": "https://arxiv.org/pdf/2409.16191",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "711",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-30 15:32:45",
            "supportOnlineEval": false,
            "updateDate": "2024-09-30 15:32:45",
            "createDate": "2024-09-30 15:06:26",
            "desc": {
              "cn": "HelloBenchä¸ºé•¿æ–‡æœ¬ç”ŸæˆåŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ã€å¼€æ”¾å¼çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°LLMåœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ–¹é¢çš„æ€§èƒ½ã€‚åŸºäºBloomçš„åˆ†ç±»æ³•ï¼ŒHelloBenchå°†é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡åˆ†ä¸ºäº”ä¸ªå­ä»»åŠ¡ï¼šå¼€æ”¾å¼QAã€æ‘˜è¦ã€èŠå¤©ã€æ–‡æœ¬å®Œæˆå’Œå¯å‘å¼æ–‡æœ¬ç”Ÿæˆã€‚",
              "en": "HelloBench is a hierarchical long text generation benchmark to evaluate LLMs' performance in generating long text.  Based on Bloom's Taxonomy, HelloBench categorizes long text generation tasks into five subtasks: open-ended QA, summarization, chat, text completion, and heuristic text generation."
            }
          }
        },
        {
          "id": "opencompass_506",
          "name": "AFQMC",
          "version": "1.0.0",
          "description": " AFQMC is an Ant Financial chinese semantic similarity task, which requires to judge whether two sentences have the same meaning or not.",
          "url": "opencompass/opencompass_506.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_506",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "506",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/IDEA-CCNL/Fengshenbang-LM/",
            "paperLink": "https://arxiv.org/abs/2209.02970",
            "officialWebsiteLink": "https://tianchi.aliyun.com/dataset/106411",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "708",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:50:04",
            "createDate": "2024-01-11 14:10:07",
            "desc": {
              "cn": "AFQMCä¸€ä¸ªèš‚èšé‡‘æœä¸­æ–‡è¯­ä¹‰ç›¸ä¼¼åº¦ä»»åŠ¡ï¼Œè¦æ±‚åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦å…·æœ‰ç›¸åŒçš„è¯­ä¹‰ã€‚",
              "en": " AFQMC is an Ant Financial chinese semantic similarity task, which requires to judge whether two sentences have the same meaning or not."
            }
          }
        },
        {
          "id": "opencompass_516",
          "name": "RACE(High)",
          "version": "1.0.0",
          "description": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. ",
          "url": "opencompass/opencompass_516.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_516",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "516",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/qizhex/RACE_AR_baselines",
            "paperLink": "https://arxiv.org/abs/1704.04683",
            "officialWebsiteLink": "https://www.cs.cmu.edu/~glai1/data/race/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "694",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:49:43",
            "createDate": "2024-01-11 14:10:58",
            "desc": {
              "cn": "RACE æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„é˜…è¯»ç†è§£æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡ 28,000 ä¸ªæ®µè½å’Œè¿‘ 100,000 ä¸ªé—®é¢˜ã€‚è¯¥æ•°æ®é›†æ˜¯ä»ä¸­å›½çš„è‹±è¯­è€ƒè¯•ä¸­æ”¶é›†è€Œæ¥ï¼Œè¿™äº›è€ƒè¯•æ˜¯ä¸ºä¸­å­¦å’Œé«˜ä¸­å­¦ç”Ÿè®¾è®¡çš„ã€‚\n",
              "en": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. "
            }
          }
        },
        {
          "id": "opencompass_507",
          "name": "WSC",
          "version": "1.0.0",
          "description": "WSC is a pronoun disambiguation task, which requires to determine which noun the pronoun refers to according to the context.",
          "url": "opencompass/opencompass_507.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_507",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "507",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html",
            "paperLink": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.729.9814&rep=rep1&type=pdf",
            "officialWebsiteLink": "https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "684",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:51:25",
            "createDate": "2024-01-11 14:10:14",
            "desc": {
              "cn": "WSCæ˜¯ä¸€ä¸ªä»£è¯æ¶ˆæ­§ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­ä»£è¯æŒ‡ä»£çš„æ˜¯å“ªä¸ªåè¯ã€‚",
              "en": "WSC is a pronoun disambiguation task, which requires to determine which noun the pronoun refers to according to the context."
            }
          }
        },
        {
          "id": "opencompass_544",
          "name": "DS-1000",
          "version": "1.0.0",
          "description": "DS-1000 is a code generation benchmark with a thousand data science questions spanning seven Python libraries that (1) reflects diverse, realistic, and practical use cases, (2) has a reliable metric, (3) defends against memorization by perturbing questions.",
          "url": "opencompass/opencompass_544.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_544",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "544",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ—½",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/xlang-ai/DS-1000",
            "paperLink": "https://arxiv.org/pdf/2211.11501.pdf",
            "officialWebsiteLink": "https://ds1000-code-gen.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "673",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-01-26 15:43:03",
            "createDate": "2024-01-26 15:43:03",
            "desc": {
              "cn": "DS-1000 æ˜¯ä¸€ä¸ªä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸€åƒä¸ªæ•°æ®ç§‘å­¦é—®é¢˜ï¼Œæ¶µç›–ä¸ƒä¸ªPythonåº“ï¼Œå…¶ç‰¹ç‚¹æ˜¯ï¼ˆ1ï¼‰åæ˜ å¤šæ ·åŒ–ã€ç°å®ä¸”å®ç”¨çš„ç”¨ä¾‹ï¼Œï¼ˆ2ï¼‰å…·æœ‰å¯é çš„åº¦é‡æ ‡å‡†ï¼Œï¼ˆ3ï¼‰é€šè¿‡æ‰°ä¹±é—®é¢˜æ¥é˜²æ­¢è®°å¿†åŒ–ã€‚",
              "en": "DS-1000 is a code generation benchmark with a thousand data science questions spanning seven Python libraries that (1) reflects diverse, realistic, and practical use cases, (2) has a reliable metric, (3) defends against memorization by perturbing questions."
            }
          }
        },
        {
          "id": "opencompass_525",
          "name": "OCNLI",
          "version": "1.0.0",
          "description": "OCNLI is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral.",
          "url": "opencompass/opencompass_525.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_525",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "525",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/cluebenchmark/OCNLI",
            "paperLink": "https://arxiv.org/abs/2010.05444",
            "officialWebsiteLink": "https://github.com/cluebenchmark/OCNLI",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "671",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:50:54",
            "createDate": "2024-01-11 14:11:37",
            "desc": {
              "cn": "OCNLIæ˜¯ä¸€ä¸ªä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ä¸¤ä¸ªå¥å­åˆ¤æ–­å®ƒä»¬ä¹‹é—´çš„é€»è¾‘å…³ç³»ï¼Œæœ‰ä¸‰ç§å…³ç³»ï¼šè•´å«ã€çŸ›ç›¾å’Œä¸­ç«‹ã€‚",
              "en": "OCNLI is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral."
            }
          }
        },
        {
          "id": "opencompass_517",
          "name": "RACE(Middle)",
          "version": "1.0.0",
          "description": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. ",
          "url": "opencompass/opencompass_517.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_517",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "517",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/qizhex/RACE_AR_baselines",
            "paperLink": "https://arxiv.org/abs/1704.04683",
            "officialWebsiteLink": "https://www.cs.cmu.edu/~glai1/data/race/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "663",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:52:17",
            "createDate": "2024-01-11 14:11:03",
            "desc": {
              "cn": "RACE æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„é˜…è¯»ç†è§£æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡ 28,000 ä¸ªæ®µè½å’Œè¿‘ 100,000 ä¸ªé—®é¢˜ã€‚è¯¥æ•°æ®é›†æ˜¯ä»ä¸­å›½çš„è‹±è¯­è€ƒè¯•ä¸­æ”¶é›†è€Œæ¥ï¼Œè¿™äº›è€ƒè¯•æ˜¯ä¸ºä¸­å­¦å’Œé«˜ä¸­å­¦ç”Ÿè®¾è®¡çš„ã€‚\n",
              "en": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. "
            }
          }
        },
        {
          "id": "opencompass_508",
          "name": "TyDiQA",
          "version": "1.0.0",
          "description": "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language expresses.",
          "url": "opencompass/opencompass_508.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_508",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "508",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/google-research-datasets/tydiqa",
            "paperLink": "https://arxiv.org/abs/2003.05002",
            "officialWebsiteLink": "https://ai.google.com/research/tydiqa",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "647",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-01-11 14:10:23",
            "createDate": "2024-01-11 14:10:23",
            "desc": {
              "cn": "TyDi QA æ˜¯ä¸€ä¸ªæ¶µç›– 11 ç§ä¸åŒè¯­è¨€çš„é—®é¢˜å›ç­”æ•°æ®é›†ï¼ŒåŒ…å« 20.4 ä¸‡ä¸ªé—®é¢˜-ç­”æ¡ˆå¯¹ã€‚TyDi QA çš„è¯­è¨€ç§ç±»å¤šæ ·ï¼Œæ¶µç›–äº†è¯­è¨€å­¦ç‰¹å¾çš„å„ç§ç±»å‹ã€‚",
              "en": "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language expresses."
            }
          }
        },
        {
          "id": "opencompass_523",
          "name": "LAMBADA",
          "version": "1.0.0",
          "description": "The LAMBADA evaluates the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse.\nThe LAMBADA dataset is extracted from BookCorpus and consists of 10'022 passages, divided into 4'869 development and 5'153 test passages, comprising 203 million words.",
          "url": "opencompass/opencompass_523.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_523",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "523",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://zenodo.org/record/2630551",
            "paperLink": "https://arxiv.org/abs/1606.06031",
            "officialWebsiteLink": "https://zenodo.org/record/2630551",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "636",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:51:18",
            "createDate": "2024-01-11 14:11:30",
            "desc": {
              "cn": "LAMBADA é€šè¿‡ä¸€ä¸ªå•è¯é¢„æµ‹ä»»åŠ¡æ¥è¯„ä¼°è®¡ç®—æ¨¡å‹å¯¹æ–‡æœ¬ç†è§£çš„èƒ½åŠ›ã€‚LAMBADA æ˜¯æœ‰å¦‚ä¸‹ç‰¹ç‚¹çš„ä¸€ç»„å™è¿°æ€§æ–‡ç« ï¼šå¦‚æœé¢å¯¹æ•´ç¯‡æ–‡ç« ï¼Œäººä»¬å¯ä»¥çŒœæµ‹å®ƒä»¬çš„æœ€åä¸€ä¸ªå•è¯ï¼Œä½†å¦‚æœä»–ä»¬åªçœ‹åˆ°ç›®æ ‡å•è¯å‰é¢çš„æœ€åä¸€å¥è¯ï¼Œå°±æ— æ³•çŒœæµ‹ã€‚ä¸ºäº†åœ¨ LAMBADA ä¸Šç”±å¥½çš„æ•ˆæœï¼Œæ¨¡å‹ä¸èƒ½ä»…ä»…ä¾èµ–äºå±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œè€Œå¿…é¡»èƒ½å¤Ÿè·Ÿè¸ªæ›´å¹¿æ³›çš„è¯è¯­ä¿¡æ¯ã€‚\nLAMBADA æ•°æ®é›†æ˜¯ä» BookCorpus ä¸­æå–çš„ï¼ŒåŒ…æ‹¬ 10,022 æ®µè½ï¼Œåˆ†ä¸º 4,869 ä¸ªå¼€å‘æ®µè½å’Œ 5,153 ä¸ªæµ‹è¯•æ®µè½ï¼Œå…±è®¡ 2.03 äº¿ä¸ªå•è¯ã€‚",
              "en": "The LAMBADA evaluates the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse.\nThe LAMBADA dataset is extracted from BookCorpus and consists of 10'022 passages, divided into 4'869 development and 5'153 test passages, comprising 203 million words."
            }
          }
        },
        {
          "id": "opencompass_557",
          "name": "OCRBench",
          "version": "1.0.0",
          "description": "OCRBench provides a comprehensive evaluation of Large Multimodal Models, such as GPT4V and Gemini, in various text-related visual tasks including Text Recognition, Scene Text-Centric Visual Question Answering (VQA), Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER). ",
          "url": "opencompass/opencompass_557.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_557",
          "sample_count": 1000,
          "traits": [
            "Language",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "557",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ‘¹",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [
              {
                "cn": "vision-language",
                "en": "vision-language"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Yuliang-Liu/MultimodalOCR",
            "paperLink": "https://arxiv.org/abs/2305.07895",
            "officialWebsiteLink": "https://huggingface.co/spaces/echo840/ocrbench-leaderboard",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "40041912",
              "name": "echo840",
              "avatar": null,
              "nickname": "echo840"
            },
            "lookNum": "636",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-01-30 10:43:27",
            "createDate": "2024-01-30 10:43:27",
            "desc": {
              "cn": "OCRBenchå¯¹ GPT4V å’Œ Gemini ç­‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å„ç§æ–‡æœ¬ç›¸å…³çš„è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬æ–‡æœ¬è¯†åˆ«ã€åœºæ™¯æ–‡æœ¬ä¸ºä¸­å¿ƒçš„è§†è§‰é—®ç­” (VQA)ã€é¢å‘æ–‡æ¡£çš„ VQAã€å…³é”®ä¿¡æ¯æå– (KIE) å’Œæ‰‹å†™æ•°å­¦è¡¨è¾¾å¼è¯†åˆ« (HMER)ã€‚",
              "en": "OCRBench provides a comprehensive evaluation of Large Multimodal Models, such as GPT4V and Gemini, in various text-related visual tasks including Text Recognition, Scene Text-Centric Visual Question Answering (VQA), Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER). "
            }
          }
        },
        {
          "id": "opencompass_1172",
          "name": "MT-Bench-101",
          "version": "1.0.0",
          "description": "MT-Bench-101 is specifically designed to evaluate the finegrained abilities of LLMs in multi-turn dialogues. ",
          "url": "opencompass/opencompass_1172.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1172",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1172",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/mtbench101/mt-bench-101",
            "paperLink": "https://arxiv.org/pdf/2402.14762",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "606",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-27 17:23:10",
            "supportOnlineEval": false,
            "updateDate": "2024-10-27 17:23:10",
            "createDate": "2024-10-21 14:33:55",
            "desc": {
              "cn": "MT-Bench-101 ä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼° LLMs åœ¨å¤šè½®å¯¹è¯ä¸­çš„ç»†ç²’åº¦èƒ½åŠ›ã€‚é€šè¿‡å¯¹çœŸå®å¤šè½®å¯¹è¯æ•°æ®çš„è¯¦ç»†åˆ†æï¼Œæ„å»ºäº†ä¸€ä¸ªä¸‰å±‚çº§çš„èƒ½åŠ›åˆ†ç±»æ³•ï¼Œæ¶µç›– 1388 ä¸ªå¤šè½®å¯¹è¯ä¸­çš„ 4208 ä¸ªè½®æ¬¡ï¼Œæ¶‰åŠ 13 ç§ä¸åŒçš„ä»»åŠ¡ã€‚",
              "en": "MT-Bench-101 is specifically designed to evaluate the finegrained abilities of LLMs in multi-turn dialogues. "
            }
          }
        },
        {
          "id": "opencompass_1136",
          "name": "IFEval",
          "version": "1.0.0",
          "description": "IFEval is a straightforward and easy-to reproduce evaluation benchmark. It focuses on a set of â€œverifiable instructionsâ€ such as â€œwrite in more than 400 wordsâ€ and â€œmention the keyword of AI at least 3 timesâ€.",
          "url": "opencompass/opencompass_1136.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1136",
          "sample_count": 1000,
          "traits": [
            "Instruct"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1136",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æŒ‡ä»¤è·Ÿéš",
                "en": "Instruct"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/google-research/google-research/tree/master/instruction_following_eval",
            "paperLink": "https://arxiv.org/pdf/2311.07911",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "581",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-14 20:15:14",
            "supportOnlineEval": false,
            "updateDate": "2024-10-14 20:15:14",
            "createDate": "2024-10-12 16:05:28",
            "desc": {
              "cn": "IFEval æ˜¯ä¸€ä¸ªç®€å•ä¸”æ˜“äºå¤ç°çš„è¯„ä¼°åŸºå‡†ã€‚å®ƒå…³æ³¨ä¸€ç»„â€œå¯éªŒè¯çš„æŒ‡ä»¤â€ï¼Œä¾‹å¦‚â€œå†™è¶…è¿‡ 400 ä¸ªå•è¯â€å’Œâ€œè‡³å°‘æåˆ°å…³é”®è¯ AI 3 æ¬¡â€ã€‚",
              "en": "IFEval is a straightforward and easy-to reproduce evaluation benchmark. It focuses on a set of â€œverifiable instructionsâ€ such as â€œwrite in more than 400 wordsâ€ and â€œmention the keyword of AI at least 3 timesâ€."
            }
          }
        },
        {
          "id": "opencompass_530",
          "name": "ReCoRD",
          "version": "1.0.0",
          "description": "ReCoRD is a reading comprehension task, which requires to extract the answer from the article based on the given news article and question.",
          "url": "opencompass/opencompass_530.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_530",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "530",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://sheng-z.github.io/ReCoRD-explorer/",
            "paperLink": "https://arxiv.org/abs/1905.00537",
            "officialWebsiteLink": "https://sheng-z.github.io/ReCoRD-explorer/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "563",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 11:35:25",
            "createDate": "2024-01-11 14:11:57",
            "desc": {
              "cn": "ReCoRDæ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„æ–°é—»æ–‡ç« å’Œé—®é¢˜ï¼Œä»æ–‡ç« ä¸­æŠ½å–å‡ºç­”æ¡ˆã€‚\n",
              "en": "ReCoRD is a reading comprehension task, which requires to extract the answer from the article based on the given news article and question."
            }
          }
        },
        {
          "id": "opencompass_1085",
          "name": "InfoBench",
          "version": "1.0.0",
          "description": "InfoBench is a benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories.\n",
          "url": "opencompass/opencompass_1085.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1085",
          "sample_count": 1000,
          "traits": [
            "Instruct"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1085",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æŒ‡ä»¤è·Ÿéš",
                "en": "Instruct"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/qinyiwei/InfoBench",
            "paperLink": "https://aclanthology.org/2024.findings-acl.772.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50186103",
              "name": "Tencent-AI-Lab",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50186103-0a763557-5cd0-4d44-b53a-aa07caf31eb7.png",
              "nickname": "Tencent-AI-Lab"
            },
            "lookNum": "532",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-29 16:24:48",
            "supportOnlineEval": false,
            "updateDate": "2024-09-29 16:24:48",
            "createDate": "2024-09-29 13:22:54",
            "desc": {
              "cn": "InfoBench æ˜¯ä¸€ä¸ªæŒ‡ä»¤è¿½éšè¯„æµ‹åŸºå‡†ï¼ŒåŒ…å« 500 æ¡å¤šæ ·åŒ–çš„æŒ‡ä»¤å’Œ 2,250 ä¸ªåˆ†è§£é—®é¢˜ï¼Œæ¶µç›–å¤šä¸ªçº¦æŸç±»åˆ«ã€‚",
              "en": "InfoBench is a benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories.\n"
            }
          }
        },
        {
          "id": "opencompass_1155",
          "name": "Ada-LEval",
          "version": "1.0.0",
          "description": "Ada-LEval is a length-adaptable benchmark for evaluating the long-context understanding\nof LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable\na more reliable evaluation of LLMsâ€™ long context capabilities.",
          "url": "opencompass/opencompass_1155.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1155",
          "sample_count": 1000,
          "traits": [
            "Long-Context"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1155",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "é•¿æ–‡æœ¬",
                "en": "Long-Context"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
              }
            ],
            "benchCertificateLevel": 3,
            "githubLink": "https://github.com/open-compass/Ada-LEval",
            "paperLink": "https://aclanthology.org/2024.naacl-long.205.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "531",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-27 17:23:13",
            "supportOnlineEval": false,
            "updateDate": "2024-10-27 17:23:13",
            "createDate": "2024-10-15 16:58:09",
            "desc": {
              "cn": "Ada-LEval ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹é•¿ä¸Šä¸‹æ–‡çš„ç†è§£èƒ½åŠ›ã€‚Ada-LEval åŒ…å«ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å­é›†ï¼ŒTSort å’Œ BestAnswerï¼Œèƒ½å¤Ÿæ›´å¯é åœ°è¯„ä¼° LLMs çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ã€‚",
              "en": "Ada-LEval is a length-adaptable benchmark for evaluating the long-context understanding\nof LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable\na more reliable evaluation of LLMsâ€™ long context capabilities."
            }
          }
        },
        {
          "id": "opencompass_522",
          "name": "EPRSTMT",
          "version": "1.0.0",
          "description": "(E-commerce Product Review Dataset for Sentiment Analysis), also known as EPRSTMT, is a binary sentiment analysis dataset based on product reviews on e-commerce platform. Each sample is labelled as Positive or Negative. It collect by ICIP Lab of Beijing Normal University.",
          "url": "opencompass/opencompass_522.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_522",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "522",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/CLUEbenchmark/FewCLUE",
            "paperLink": "https://arxiv.org/abs/2107.07498",
            "officialWebsiteLink": "https://github.com/CLUEbenchmark/FewCLUE",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "503",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:52:03",
            "createDate": "2024-01-11 14:11:26",
            "desc": {
              "cn": "EPRSTMTï¼Œä¹Ÿç§°ä½œç”µå­å•†åŠ¡äº§å“è¯„è®ºæƒ…æ„Ÿåˆ†ææ•°æ®é›†ï¼Œæ˜¯ä¸€ä¸ªåŸºäºç”µå­å•†åŠ¡å¹³å°ä¸Šçš„äº§å“è¯„è®ºçš„äºŒå…ƒæƒ…æ„Ÿåˆ†ææ•°æ®é›†ã€‚æ¯ä¸ªæ ·æœ¬éƒ½è¢«æ ‡è®°ä¸ºç§¯ææˆ–æ¶ˆæã€‚è¯¥æ•°æ®é›†ç”±åŒ—äº¬å¸ˆèŒƒå¤§å­¦ ICIP å®éªŒå®¤æ”¶é›†ã€‚",
              "en": "(E-commerce Product Review Dataset for Sentiment Analysis), also known as EPRSTMT, is a binary sentiment analysis dataset based on product reviews on e-commerce platform. Each sample is labelled as Positive or Negative. It collect by ICIP Lab of Beijing Normal University."
            }
          }
        },
        {
          "id": "opencompass_527",
          "name": "AX-g",
          "version": "1.0.0",
          "description": "AX-g is a Winogender diagnostic task, which requires to determine which noun the pronoun refers to according to the given sentence and pronoun. This task is selected from a subset of the Winogender dataset, mainly used to test the model's ability in dealing with gender bias and discrimination.",
          "url": "opencompass/opencompass_527.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_527",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "527",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/rudinger/winogender-schemas",
            "paperLink": "https://arxiv.org/abs/1905.00537",
            "officialWebsiteLink": "https://github.com/rudinger/winogender-schemas",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "482",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 11:37:55",
            "createDate": "2024-01-11 14:11:44",
            "desc": {
              "cn": "AX-gæ˜¯ä¸€ä¸ªWinogenderè¯Šæ–­ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„å¥å­å’Œä»£è¯ï¼Œåˆ¤æ–­ä»£è¯æŒ‡ä»£çš„æ˜¯å“ªä¸ªåè¯ã€‚è¿™ä¸ªä»»åŠ¡æ˜¯ä»Winogenderæ•°æ®é›†ä¸­é€‰å–äº†ä¸€éƒ¨åˆ†æ•°æ®ï¼Œä¸»è¦ç”¨æ¥æµ‹è¯•æ¨¡å‹åœ¨å¤„ç†æ€§åˆ«åè§å’Œæ€§åˆ«æ­§è§†æ–¹é¢çš„èƒ½åŠ›ã€‚",
              "en": "AX-g is a Winogender diagnostic task, which requires to determine which noun the pronoun refers to according to the given sentence and pronoun. This task is selected from a subset of the Winogender dataset, mainly used to test the model's ability in dealing with gender bias and discrimination."
            }
          }
        },
        {
          "id": "opencompass_528",
          "name": "RTE",
          "version": "1.0.0",
          "description": "RTE is a natural language inference task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral.",
          "url": "opencompass/opencompass_528.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_528",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "528",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://tac.nist.gov//2011/RTE/index.html",
            "paperLink": "https://arxiv.org/abs/1905.00537",
            "officialWebsiteLink": "https://tac.nist.gov//2011/RTE/index.html",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "460",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 09:52:48",
            "createDate": "2024-01-11 14:11:48",
            "desc": {
              "cn": "RTEæ˜¯ä¸€ä¸ªè‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„å¥å­å¯¹ï¼Œåˆ¤æ–­å®ƒä»¬ä¹‹é—´çš„é€»è¾‘å…³ç³»ï¼Œæœ‰ä¸‰ç§å…³ç³»ï¼šè•´å«ã€çŸ›ç›¾å’Œä¸­ç«‹ã€‚",
              "en": "RTE is a natural language inference task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral."
            }
          }
        },
        {
          "id": "opencompass_1096",
          "name": "TruthfulQA",
          "version": "1.0.0",
          "description": "TruthfulQA is  a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics.",
          "url": "opencompass/opencompass_1096.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1096",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1096",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/sylinrl/TruthfulQA",
            "paperLink": "https://arxiv.org/pdf/2109.07958",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "460",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-30 15:32:49",
            "supportOnlineEval": false,
            "updateDate": "2024-09-30 15:32:49",
            "createDate": "2024-09-29 18:03:55",
            "desc": {
              "cn": "TruthfulQA ç”¨äºæµ‹é‡è¯­è¨€æ¨¡å‹åœ¨å›ç­”é—®é¢˜æ—¶çš„çœŸå®åº¦ã€‚è¯¥åŸºå‡†åŒ…å« 817 ä¸ªé—®é¢˜ï¼Œæ¶µç›– 38 ä¸ªç±»åˆ«ï¼ŒåŒ…æ‹¬å¥åº·ã€æ³•å¾‹ã€é‡‘èå’Œæ”¿æ²»ã€‚",
              "en": "TruthfulQA is  a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics."
            }
          }
        },
        {
          "id": "opencompass_526",
          "name": "AX-b",
          "version": "1.0.0",
          "description": "AX-b is a broad-coverage diagnostic task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral. This task is selected from a subset of the GLUE broad-coverage diagnostic dataset, mainly used to test the model's understanding ability in grammar, semantics, world knowledge and so on.",
          "url": "opencompass/opencompass_526.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_526",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "526",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://gluebenchmark.com/diagnostics",
            "paperLink": "https://arxiv.org/abs/1905.00537",
            "officialWebsiteLink": "https://gluebenchmark.com/diagnostics",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "423",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-02 11:38:03",
            "createDate": "2024-01-11 14:11:41",
            "desc": {
              "cn": "AX-bæ˜¯ä¸€ä¸ªå¹¿è¦†ç›–è¯Šæ–­ä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„å¥å­å¯¹ï¼Œåˆ¤æ–­å®ƒä»¬ä¹‹é—´çš„é€»è¾‘å…³ç³»ï¼Œæœ‰ä¸‰ç§å…³ç³»ï¼šè•´å«ã€çŸ›ç›¾å’Œä¸­ç«‹ã€‚è¿™ä¸ªä»»åŠ¡æ˜¯ä»GLUEçš„å¹¿è¦†ç›–è¯Šæ–­æ•°æ®é›†ä¸­é€‰å–äº†ä¸€éƒ¨åˆ†æ•°æ®ï¼Œä¸»è¦ç”¨æ¥æµ‹è¯•æ¨¡å‹åœ¨è¯­æ³•ã€è¯­ä¹‰ã€ä¸–ç•ŒçŸ¥è¯†ç­‰æ–¹é¢çš„ç†è§£èƒ½åŠ›ã€‚",
              "en": "AX-b is a broad-coverage diagnostic task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral. This task is selected from a subset of the GLUE broad-coverage diagnostic dataset, mainly used to test the model's understanding ability in grammar, semantics, world knowledge and so on."
            }
          }
        },
        {
          "id": "opencompass_1074",
          "name": "NewsBench",
          "version": "1.0.0",
          "description": "NewsBench is a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. ",
          "url": "opencompass/opencompass_1074.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1074",
          "sample_count": 1000,
          "traits": [
            "Creation"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1074",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "åˆ›ä½œ",
                "en": "Creation"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/IAAR-Shanghai/NewsBench",
            "paperLink": "https://arxiv.org/pdf/2403.00862",
            "officialWebsiteLink": "https://iaar-shanghai.github.io/NewsBench/#/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50187574",
              "name": "IAAR-Shanghai",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50187574-4cf5b8da-55c3-4c75-aa9b-9c2d053b5b6d.png",
              "nickname": "IAAR-Shanghai"
            },
            "lookNum": "408",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-27 17:50:27",
            "supportOnlineEval": false,
            "updateDate": "2024-09-27 17:50:27",
            "createDate": "2024-09-27 13:08:42",
            "desc": {
              "cn": "NewsBench æ˜¯ä¸€ä¸ªæ–°é¢–çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡æ–°é—»ç¼–è¾‘èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚æ„å»ºçš„åŸºå‡†æ•°æ®é›†èšç„¦äºå†™ä½œèƒ½åŠ›çš„å››ä¸ªæ–¹é¢å’Œå®‰å…¨éµå¾ªçš„å…­ä¸ªæ–¹é¢ï¼ŒåŒ…å« 1,267 ä¸ªæ‰‹åŠ¨ç²¾å¿ƒè®¾è®¡çš„æµ‹è¯•æ ·æœ¬ï¼Œç±»å‹åŒ…æ‹¬é€‰æ‹©é¢˜å’Œç®€ç­”é¢˜ï¼Œæ¶µç›– 24 ä¸ªæ–°é—»é¢†åŸŸçš„äº”é¡¹ç¼–è¾‘ä»»åŠ¡ã€‚",
              "en": "NewsBench is a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. "
            }
          }
        },
        {
          "id": "opencompass_1108",
          "name": "HotpotQA",
          "version": "1.0.0",
          "description": "HOTPOTQA is a dataset with 113k Wikipedia-based question-answer pairs.",
          "url": "opencompass/opencompass_1108.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1108",
          "sample_count": 1000,
          "traits": [
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1108",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/hotpotqa/hotpot",
            "paperLink": "https://arxiv.org/pdf/1809.09600",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "402",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-10 18:36:24",
            "supportOnlineEval": false,
            "updateDate": "2025-01-10 18:36:24",
            "createDate": "2025-01-10 18:28:24",
            "desc": {
              "cn": "HotpotQA ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å« 113,000 ä¸ªåŸºäºç»´åŸºç™¾ç§‘çš„é—®é¢˜å’Œç­”æ¡ˆã€‚",
              "en": "HOTPOTQA is a dataset with 113k Wikipedia-based question-answer pairs."
            }
          }
        },
        {
          "id": "opencompass_1121",
          "name": "HaluEval",
          "version": "1.0.0",
          "description": "HaluEval evaluates the performance of LLMs in recognizing hallucination. It includes 5,000 general user queries with ChatGPT responses and 30,000 task-specific examples from three tasks, i.e., question answering, knowledge-grounded dialogue, and text summarization.",
          "url": "opencompass/opencompass_1121.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1121",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1121",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/RUCAIBox/HaluEval",
            "paperLink": "https://arxiv.org/pdf/2305.11747",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "395",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-13 15:31:34",
            "supportOnlineEval": false,
            "updateDate": "2025-03-13 15:31:34",
            "createDate": "2025-02-13 18:15:01",
            "desc": {
              "cn": "HaluEvalç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹è¯†åˆ«å¹»è§‰çš„èƒ½åŠ›ï¼ŒåŒ…å« 5,000 æ¡æ™®é€šç”¨æˆ·æŸ¥è¯¢åŠ ChatGPT çš„å›ç­”ï¼Œä»¥åŠæ¥è‡ªä¸‰ä¸ªä»»åŠ¡çš„ 30,000 ä¸ªç‰¹å®šä»»åŠ¡ç¤ºä¾‹ï¼Œå³é—®ç­”ã€åŸºäºçŸ¥è¯†çš„å¯¹è¯å’Œæ–‡æœ¬æ‘˜è¦ã€‚",
              "en": "HaluEval evaluates the performance of LLMs in recognizing hallucination. It includes 5,000 general user queries with ChatGPT responses and 30,000 task-specific examples from three tasks, i.e., question answering, knowledge-grounded dialogue, and text summarization."
            }
          }
        },
        {
          "id": "opencompass_568",
          "name": "CriticBench",
          "version": "1.0.0",
          "description": "CriticBench, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. CriticBench encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity.",
          "url": "opencompass/opencompass_568.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_568",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "568",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ˜‚",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [
              {
                "cn": "critique",
                "en": "critique"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/open-compass/CriticBench",
            "paperLink": "https://arxiv.org/abs/2402.13764",
            "officialWebsiteLink": "https://open-compass.github.io/CriticBench",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "40025311",
              "name": "Tian-Lan",
              "avatar": null,
              "nickname": "Tian-Lan"
            },
            "lookNum": "376",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-02-23 19:41:43",
            "createDate": "2024-02-23 18:01:59",
            "desc": {
              "cn": "CriticBenchæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨å…¨é¢å¯é åœ°è¯„ä¼°LLMçš„å››ä¸ªå…³é”®æ‰¹åˆ¤èƒ½åŠ›ç»´åº¦ã€‚CriticBenchåŒ…æ‹¬ä¹é¡¹ä¸åŒçš„ä»»åŠ¡ï¼Œæ¯é¡¹ä»»åŠ¡éƒ½è¯„ä¼°LLMåœ¨ä¸åŒè´¨é‡ç²’åº¦æ°´å¹³ä¸Šå¯¹å“åº”è¿›è¡Œæ‰¹è¯„çš„èƒ½åŠ›ã€‚",
              "en": "CriticBench, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. CriticBench encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity."
            }
          }
        },
        {
          "id": "opencompass_930",
          "name": "MR-Ben-Meta-Reasoning-Benchmark",
          "version": "1.0.0",
          "description": "Welcome to the dataset page for the Meta-Reasoning Benchmark associated with our recent publication `Mr-Ben: A Comprehensive Meta-Reasoning Benchmark for Large Language Models`. We have provided a demo evaluate script for you to try out benchmark in mere two steps. We encourage everyone to try out our benchmark in the SOTA models and return its results to us. We would be happy to include it in the eval_results and update the evaluation tables below for you.",
          "url": "opencompass/opencompass_930.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_930",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "930",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ—½",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [
              {
                "cn": "long-context",
                "en": "long-context"
              },
              {
                "cn": "understanding",
                "en": "understanding"
              },
              {
                "cn": "knowledge",
                "en": "knowledge"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/dvlab-research/Mr-Ben",
            "paperLink": "https://arxiv.org/abs/2406.13975",
            "officialWebsiteLink": "https://randolph-zeng.github.io/Mr-Ben.github.io/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50104209",
              "name": null,
              "avatar": null,
              "nickname": "OpenXLab-lFJ83HfdP"
            },
            "lookNum": "368",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-07-12 13:38:55",
            "createDate": "2024-07-12 11:20:05",
            "desc": {
              "cn": "æœ¬å·¥ä½œè”åˆMIT,æ¸…å,å‰‘æ¡¥ç­‰çŸ¥åé™¢æ ¡, æå‡ºäº†ä¸€ä¸ªè¯„æµ‹å¤§è¯­è¨€æ¨¡å‹å¯¹å¤æ‚é—®é¢˜çš„æ¨ç†è¿‡ç¨‹çš„â€œé˜…å·â€æ‰¹æ”¹èƒ½åŠ›çš„è¯„æµ‹æ•°æ®é›†ï¼Œæœ‰åˆ«äºä»¥å‰çš„ä»¥ç»“æœåŒ¹é…ä¸ºè¯„æµ‹æ¨¡å¼çš„æ•°æ®é›†MR-Benï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åŸºäºGSM8K[1], MMLU[2], LogiQA[3], MHPP[4]ç­‰æ•°æ®é›†ç»ç”±ç»†è‡´çš„é«˜æ°´å¹³äººå·¥æ ‡æ³¨æ„å»ºè€Œæˆï¼Œæ˜¾è‘—åœ°å¢åŠ äº†éš¾åº¦åŠåŒºåˆ†åº¦ã€‚æˆ‘ä»¬ç»†è‡´åœ°åˆ†æäº†åŒ…æ‹¬claude3.5, GPT4-Turbo, Kimi, Zhipu, Yi-Large, Qwen2, DeepseekCoderv2 ç­‰å›½å†…å¤–ä¸€çº¿çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å¼€æºçš„æ¨¡å‹åœ¨å¤æ‚æ¨ç†çš„åœºæ™¯ä¸‹æœ‰æœ›è¿½ä¸Šé¡¶å°–çš„é—­æºæ¨¡å‹ã€‚è¯¥è¯„æµ‹æ•°æ®é›†çš„æ‰€æœ‰æ•°æ®å‡å·²å¼€æºï¼Œå¹¶ä¸”æ”¯æŒä¸€é”®è¯„æµ‹ã€‚æ¬¢è¿æ‰€æœ‰åšå¤§æ¨¡å‹è®­ç»ƒçš„å°ä¼™ä¼´å‘æˆ‘ä»¬åˆ†äº«ä½ çš„è¯„æµ‹ç»“æœï¼Œæˆ‘ä»¬ä¼šåŠæ—¶æ›´æ–°æ¦œå•ã€‚",
              "en": "Welcome to the dataset page for the Meta-Reasoning Benchmark associated with our recent publication `Mr-Ben: A Comprehensive Meta-Reasoning Benchmark for Large Language Models`. We have provided a demo evaluate script for you to try out benchmark in mere two steps. We encourage everyone to try out our benchmark in the SOTA models and return its results to us. We would be happy to include it in the eval_results and update the evaluation tables below for you."
            }
          }
        },
        {
          "id": "opencompass_1075",
          "name": "AlignBench",
          "version": "1.0.0",
          "description": "ALIGNBENCH is a comprehensive multidimensional benchmark for evaluating LLMsâ€™ alignment in Chinese. We tailor a humanin-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references.",
          "url": "opencompass/opencompass_1075.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1075",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1075",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/THUDM/AlignBench",
            "paperLink": "https://aclanthology.org/2024.acl-long.624.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "043910",
              "name": "THUDM",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/043910-ad5eb16e-0c3f-4e1b-b1d6-e9dc8ab355a7.png",
              "nickname": "æ™ºè°±.AI"
            },
            "lookNum": "355",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-11-01 13:39:39",
            "supportOnlineEval": false,
            "updateDate": "2024-11-01 13:39:39",
            "createDate": "2024-10-29 17:11:05",
            "desc": {
              "cn": "AlignBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹å¯¹é½æ€§èƒ½çš„å…¨é¢ã€å¤šç»´åº¦çš„è¯„æµ‹åŸºå‡†ã€‚AlignBench æ„å»ºäº†äººç±»å‚ä¸çš„æ•°æ®æ„å»ºæµç¨‹ï¼Œæ¥ä¿è¯è¯„æµ‹æ•°æ®çš„åŠ¨æ€æ›´æ–°ã€‚AlignBench é‡‡ç”¨å¤šç»´åº¦ã€è§„åˆ™æ ¡å‡†çš„æ¨¡å‹è¯„ä»·æ–¹æ³•ï¼ˆLLM-as-Judgeï¼‰ï¼Œå¹¶ä¸”ç»“åˆæ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ç”Ÿæˆå¯¹æ¨¡å‹å›å¤çš„å¤šç»´åº¦åˆ†æå’Œæœ€ç»ˆçš„ç»¼åˆè¯„åˆ†ï¼Œå¢å¼ºäº†è¯„æµ‹çš„é«˜å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚",
              "en": "ALIGNBENCH is a comprehensive multidimensional benchmark for evaluating LLMsâ€™ alignment in Chinese. We tailor a humanin-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references."
            }
          }
        },
        {
          "id": "opencompass_1089",
          "name": "MathBench",
          "version": "1.0.0",
          "description": "MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large\nlanguage models. MathBench spans a wide range of mathematical disciplines, offering a\ndetailed evaluation of both theoretical understanding and practical problem-solving skills.",
          "url": "opencompass/opencompass_1089.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1089",
          "sample_count": 1000,
          "traits": [
            "Math"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1089",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 3,
            "githubLink": "https://github.com/open-compass/MathBench",
            "paperLink": "https://aclanthology.org/2024.findings-acl.411.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "355",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-30 15:32:54",
            "supportOnlineEval": false,
            "updateDate": "2024-09-30 15:32:54",
            "createDate": "2024-09-29 15:21:55",
            "desc": {
              "cn": "MathBench ä¸¥æ ¼è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦èƒ½åŠ›ã€‚MathBench æ¶‰åŠå¹¿æ³›çš„æ•°å­¦å­¦ç§‘ï¼Œæä¾›å¯¹ç†è®ºç†è§£å’Œå®é™…é—®é¢˜è§£å†³æŠ€èƒ½çš„è¯¦ç»†è¯„ä¼°ã€‚",
              "en": "MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large\nlanguage models. MathBench spans a wide range of mathematical disciplines, offering a\ndetailed evaluation of both theoretical understanding and practical problem-solving skills."
            }
          }
        },
        {
          "id": "opencompass_1077",
          "name": "SALAD-Bench",
          "version": "1.0.0",
          "description": "SALAD-Bench, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy\nspanning three levels, and versatile functionalities.",
          "url": "opencompass/opencompass_1077.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1077",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1077",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/OpenSafetyLab/SALAD-BENCH",
            "paperLink": "https://aclanthology.org/2024.findings-acl.235.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50186740",
              "name": "OpenSafetyLab",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50186740-5e27e10c-3894-4bfd-b0c9-2079f251882d.png",
              "nickname": "OpenSafetyLab"
            },
            "lookNum": "331",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-29 16:25:08",
            "supportOnlineEval": false,
            "updateDate": "2024-09-29 16:25:08",
            "createDate": "2024-09-27 16:14:48",
            "desc": {
              "cn": "SALAD-Bench æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€æ”»å‡»å’Œé˜²å¾¡æ–¹æ³•çš„å®‰å…¨åŸºå‡†ã€‚SALAD-Bench çš„ç‰¹ç‚¹åœ¨äºå…¶å¹¿æ³›æ€§ï¼Œè¶…è¶Šäº†ä¼ ç»ŸåŸºå‡†ï¼Œå…·æœ‰å¤§è§„æ¨¡ã€ä¸°å¯Œçš„å¤šæ ·æ€§ã€å¤æ‚çš„ä¸‰å±‚åˆ†ç±»æ³•ä»¥åŠå¤šåŠŸèƒ½æ€§ã€‚",
              "en": "SALAD-Bench, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy\nspanning three levels, and versatile functionalities."
            }
          }
        },
        {
          "id": "opencompass_1135",
          "name": "GPQA",
          "version": "1.0.0",
          "description": "GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry.",
          "url": "opencompass/opencompass_1135.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1135",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1135",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/idavidrein/gpqa",
            "paperLink": "https://arxiv.org/pdf/2311.12022",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "330",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-14 20:26:40",
            "supportOnlineEval": false,
            "updateDate": "2024-10-14 20:26:40",
            "createDate": "2024-10-12 16:00:16",
            "desc": {
              "cn": "GPQA åŒ…å« 448 é“ç”±ç”Ÿç‰©å­¦ã€ç‰©ç†å­¦å’ŒåŒ–å­¦é¢†åŸŸä¸“å®¶æ’°å†™çš„å¤šé¡¹é€‰æ‹©é¢˜ã€‚",
              "en": "GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry."
            }
          }
        },
        {
          "id": "opencompass_1080",
          "name": "E-EVAL",
          "version": "1.0.0",
          "description": "E-EVAL is the first comprehensive evaluation benchmark specifically tailored for Chinese K-12 education. E-EVAL comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels, covering a diverse array of subjects.\n",
          "url": "opencompass/opencompass_1080.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1080",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1080",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/AI-EDU-LAB/E-EVAL",
            "paperLink": "https://aclanthology.org/2024.findings-acl.462.pdf",
            "officialWebsiteLink": "https://eevalbenchmark.com/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50186339",
              "name": "AI-EDU-LAB",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50186339-e3a36811-6e48-440b-a243-b7d59c6d4bd7.png",
              "nickname": "AI-EDU-LAB"
            },
            "lookNum": "314",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-29 16:25:01",
            "supportOnlineEval": false,
            "updateDate": "2024-09-29 16:25:01",
            "createDate": "2024-09-27 18:25:46",
            "desc": {
              "cn": "E-EVAL æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹ä¸­å›½ K-12 æ•™è‚²çš„ç»¼åˆè¯„ä¼°åŸºå‡†ã€‚E-EVAL åŒ…å« 4,351 é“é€‰æ‹©é¢˜ï¼Œæ¶µç›–å°å­¦ã€åˆä¸­å’Œé«˜ä¸­å„ä¸ªå¹´çº§ï¼Œæ¶‰åŠå¤šç§å­¦ç§‘ã€‚",
              "en": "E-EVAL is the first comprehensive evaluation benchmark specifically tailored for Chinese K-12 education. E-EVAL comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels, covering a diverse array of subjects.\n"
            }
          }
        },
        {
          "id": "opencompass_1276",
          "name": "MMLU-Pro",
          "version": "1.0.0",
          "description": "MMLU-Pro is the extension of MMLU, integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options.",
          "url": "opencompass/opencompass_1276.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1276",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1276",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/TIGER-AI-Lab/MMLU-Pro",
            "paperLink": "https://arxiv.org/abs/2406.01574",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "312",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-25 14:44:23",
            "supportOnlineEval": false,
            "updateDate": "2024-12-25 14:44:23",
            "createDate": "2024-12-24 19:48:37",
            "desc": {
              "cn": "MMLU-Proæ˜¯MMLUçš„æ‰©å±•ç‰ˆæœ¬ï¼Œæ¶µç›–äº†æ›´å…·æŒ‘æˆ˜æ€§ã€ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„é—®é¢˜ï¼Œå¹¶å°†é€‰æ‹©é›†ä»4ä¸ªé€‰é¡¹æ‰©å±•åˆ°10ä¸ªé€‰é¡¹ã€‚",
              "en": "MMLU-Pro is the extension of MMLU, integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options."
            }
          }
        },
        {
          "id": "opencompass_1128",
          "name": "Gorilla",
          "version": "1.0.0",
          "description": "Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. ",
          "url": "opencompass/opencompass_1128.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1128",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1128",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/ShishirPatil/gorilla",
            "paperLink": "https://arxiv.org/pdf/2305.15334",
            "officialWebsiteLink": "https://gorilla.cs.berkeley.edu/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "306",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-16 11:03:02",
            "supportOnlineEval": false,
            "updateDate": "2024-10-16 11:03:02",
            "createDate": "2024-10-11 15:12:19",
            "desc": {
              "cn": "Gorilla ä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è°ƒç”¨ API ä½¿ç”¨å·¥å…·ã€‚é’ˆå¯¹è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ŒGorilla èƒ½å¤Ÿç”Ÿæˆè¯­ä¹‰å’Œè¯­æ³•ä¸Šæ­£ç¡®çš„ API è°ƒç”¨ã€‚",
              "en": "Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. "
            }
          }
        },
        {
          "id": "opencompass_1141",
          "name": "CHARM",
          "version": "1.0.0",
          "description": "CHARM is the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense.",
          "url": "opencompass/opencompass_1141.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1141",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1141",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/opendatalab/CHARM",
            "paperLink": "https://aclanthology.org/2024.acl-long.604.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "300",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-19 18:04:11",
            "supportOnlineEval": false,
            "updateDate": "2024-10-19 18:04:11",
            "createDate": "2024-10-14 14:20:06",
            "desc": {
              "cn": "CHARM æ˜¯é¦–ä¸ªå…¨é¢æ·±å…¥è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸­æ–‡ä¸­çš„å¸¸è¯†æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œæ¶µç›–äº†å…¨çƒé€šç”¨çš„å¸¸è¯†å’Œç‰¹å®šäºä¸­å›½çš„å¸¸è¯†ã€‚",
              "en": "CHARM is the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense."
            }
          }
        },
        {
          "id": "opencompass_1219",
          "name": "CS-Eval",
          "version": "1.0.0",
          "description": "CS-Eval is a large language model cybersecurity capability evaluation suite jointly established by Alibaba Security, Fudan University, and the University of Chinese Academy of Sciences. The dataset encompasses 11 major cybersecurity categories and 42 subcategories, offering comprehensive assessment ",
          "url": "opencompass/opencompass_1219.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1219",
          "sample_count": 1000,
          "traits": [
            "Examination",
            "Knowledge",
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1219",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              },
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              },
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/CS-EVAL/CS-Eval",
            "paperLink": "https://arxiv.org/pdf/2411.16239",
            "officialWebsiteLink": "https://cs-eval.com",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "41601314",
              "name": null,
              "avatar": null,
              "nickname": "OpenXLab-JdyYs9Oa3"
            },
            "lookNum": "278",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-11-27 16:53:44",
            "supportOnlineEval": false,
            "updateDate": "2024-11-27 16:53:44",
            "createDate": "2024-11-26 10:42:49",
            "desc": {
              "cn": "CS-Eval æ˜¯ç”±é˜¿é‡Œå®‰å…¨ã€å¤æ—¦å¤§å­¦å’Œä¸­å›½ç§‘å­¦é™¢å¤§å­¦è”åˆå»ºç«‹çš„å¤§æ¨¡å‹ç½‘ç»œå®‰å…¨èƒ½åŠ›è¯„æµ‹é›†ã€‚æ•°æ®é›†è¦†ç›–11ä¸ªç½‘ç»œå®‰å…¨å¤§ç±»é¢†åŸŸã€42ä¸ªå­ç±»é¢†åŸŸï¼Œæä¾›çŸ¥è¯†å‹å’Œå®æˆ˜å‹çš„ç»¼åˆè¯„ä¼°ä»»åŠ¡ï¼Œæ”¯æŒç”¨æˆ·è‡ªä¸»è¯„æµ‹ï¼ŒåŒæ—¶ä¸ºå¤§æ¨¡å‹è½åœ°ç½‘ç»œå®‰å…¨æä¾›å‚è€ƒå’Œå¯å‘ã€‚\n",
              "en": "CS-Eval is a large language model cybersecurity capability evaluation suite jointly established by Alibaba Security, Fudan University, and the University of Chinese Academy of Sciences. The dataset encompasses 11 major cybersecurity categories and 42 subcategories, offering comprehensive assessment "
            }
          }
        },
        {
          "id": "opencompass_1070",
          "name": "OlympiadBench",
          "version": "1.0.0",
          "description": "OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring\n8,476 problems from Olympiad-level mathematics and physics competitions, including the\nChinese college entrance exam. Each problem is detailed with expert-level annotations\nfor step-by-step reasoning. ",
          "url": "opencompass/opencompass_1070.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1070",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1070",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/OpenBMB/OlympiadBench",
            "paperLink": "https://arxiv.org/pdf/2402.14008",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "044167",
              "name": "OpenBMB",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/044167-87da8649-04d0-47ea-90e1-a77a1d2e9585.png",
              "nickname": "OpenBMB"
            },
            "lookNum": "272",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-27 16:53:44",
            "supportOnlineEval": false,
            "updateDate": "2024-09-27 16:53:44",
            "createDate": "2024-09-26 17:51:57",
            "desc": {
              "cn": "OlympiadBench æ˜¯ä¸€ä¸ªå¥¥æ—åŒ¹å…‹çº§åˆ«çš„åŒè¯­å¤šæ¨¡æ€ç§‘å­¦åŸºå‡†ï¼ŒåŒ…å«æ¥è‡ªå¥¥æ—åŒ¹å…‹çº§æ•°å­¦å’Œç‰©ç†ç«èµ›çš„8,476é“é¢˜ç›®ï¼ŒåŒ…æ‹¬ä¸­å›½é«˜è€ƒã€‚æ¯é“é¢˜ç›®éƒ½é…æœ‰ä¸“å®¶çº§åˆ«çš„æ³¨é‡Šï¼Œæä¾›é€æ­¥æ¨ç†çš„è¯¦ç»†è¯´æ˜ã€‚",
              "en": "OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring\n8,476 problems from Olympiad-level mathematics and physics competitions, including the\nChinese college entrance exam. Each problem is detailed with expert-level annotations\nfor step-by-step reasoning. "
            }
          }
        },
        {
          "id": "opencompass_1351",
          "name": "AgentHarm",
          "version": "1.0.0",
          "description": "AgentHarm tests the robustness of LLMs to jailbreak attacks. It includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment.",
          "url": "opencompass/opencompass_1351.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1351",
          "sample_count": 1000,
          "traits": [
            "Agent",
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1351",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              },
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/UKGovernmentBEIS/inspect_evals/tree/main/src/inspect_evals/agentharm",
            "paperLink": "https://arxiv.org/abs/2410.09024",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "262",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:23:28",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:23:28",
            "createDate": "2025-01-09 16:22:26",
            "desc": {
              "cn": "AgentHarmç”¨äºè¯„ä¼°LLMæ™ºèƒ½ä½“å¯¹è¶Šç‹±æ”»å‡»çš„é²æ£’æ€§ï¼ŒåŒ…æ‹¬110å¥—æ¶æ„æ™ºèƒ½ä½“ä»»åŠ¡ï¼ˆå…¶ä¸­æœ‰440ä¸ªå¼ºåŒ–ä»»åŠ¡ï¼‰ï¼Œæ¶µç›–æ¬ºè¯ˆã€ç½‘ç»œçŠ¯ç½ªå’Œéªšæ‰°ç­‰11ä¸ªå±å®³ç±»åˆ«ã€‚",
              "en": "AgentHarm tests the robustness of LLMs to jailbreak attacks. It includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment."
            }
          }
        },
        {
          "id": "opencompass_1538",
          "name": "SuperGPQA",
          "version": "1.0.0",
          "description": "SuperGPQA, a comprehensive benchmark designed to evaluate the knowledge and reasoning abilities of Large Language Models (LLMs) across 285 graduate-level disciplines. SuperGPQA features at least 50 questions per discipline, covering a broad spectrum of graduate-level topics.",
          "url": "opencompass/opencompass_1538.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1538",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1538",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/SuperGPQA/SuperGPQA/",
            "paperLink": "https://arxiv.org/abs/2502.14739",
            "officialWebsiteLink": "https://supergpqa.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "255",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-02-26 18:52:28",
            "supportOnlineEval": false,
            "updateDate": "2025-02-26 18:52:28",
            "createDate": "2025-02-26 09:51:42",
            "desc": {
              "cn": "SuperGPQAï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ 285 ä¸ªç ”ç©¶ç”Ÿå­¦ç§‘é¢†åŸŸçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›çš„å…¨é¢åŸºå‡†ã€‚SuperGPQA æ¯ä¸ªå­¦ç§‘è‡³å°‘åŒ…å« 50 ä¸ªé—®é¢˜ï¼Œæ¶µç›–å¹¿æ³›çš„ç¡•å£«ç ”ç©¶ç”Ÿå­¦ç§‘ä¸»é¢˜ã€‚",
              "en": "SuperGPQA, a comprehensive benchmark designed to evaluate the knowledge and reasoning abilities of Large Language Models (LLMs) across 285 graduate-level disciplines. SuperGPQA features at least 50 questions per discipline, covering a broad spectrum of graduate-level topics."
            }
          }
        },
        {
          "id": "opencompass_1091",
          "name": "RoleLLM",
          "version": "1.0.0",
          "description": "RoleLLM is a role-playing framework of data construction and evaluation (RoleBench), as well as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMA, RoleGLM). We also propose Context-Instruct for long-text knowledge extraction and role-specific knowledge injection.",
          "url": "opencompass/opencompass_1091.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1091",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1091",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/InteractiveNLP-Team/RoleLLM-public",
            "paperLink": "https://aclanthology.org/2024.findings-acl.878.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "50187516",
              "name": "InteractiveNLP-Team",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50187516-87c7afab-16ed-48c4-8147-1298485c7231.png",
              "nickname": "InteractiveNLP-Team"
            },
            "lookNum": "255",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-30 15:54:49",
            "supportOnlineEval": false,
            "updateDate": "2024-09-30 15:54:49",
            "createDate": "2024-09-29 16:37:55",
            "desc": {
              "cn": "RoleLLM æ˜¯ä¸€ä¸ªè§’è‰²æ‰®æ¼”çš„æ•°æ®æ„å»ºå’Œè¯„ä¼°æ¡†æ¶ï¼ŒåŒæ—¶æä¾›é—­æºå’Œå¼€æºæ¨¡å‹çš„è§£å†³æ–¹æ¡ˆï¼ˆRoleGPTã€RoleLLaMAã€RoleGLMï¼‰ã€‚",
              "en": "RoleLLM is a role-playing framework of data construction and evaluation (RoleBench), as well as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMA, RoleGLM). We also propose Context-Instruct for long-text knowledge extraction and role-specific knowledge injection."
            }
          }
        },
        {
          "id": "opencompass_1137",
          "name": "CMB",
          "version": "1.0.0",
          "description": "CMB is Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework.",
          "url": "opencompass/opencompass_1137.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1137",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1137",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/FreedomIntelligence/CMB",
            "paperLink": "https://arxiv.org/pdf/2308.08833",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "251",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-16 11:02:56",
            "supportOnlineEval": false,
            "updateDate": "2024-10-16 11:02:56",
            "createDate": "2024-10-15 16:31:07",
            "desc": {
              "cn": "CMB æ˜¯ä¸€ä¸ªç»¼åˆåŒ»å­¦åŸºå‡†ï¼Œä¸“ä¸ºä¸­æ–‡è€Œè®¾è®¡ï¼Œå¹¶å®Œå…¨ä¾èµ–äºæœ¬åœŸçš„ä¸­æ–‡è¯­è¨€å’Œæ–‡åŒ–æ¡†æ¶ä¸­ã€‚",
              "en": "CMB is Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework."
            }
          }
        },
        {
          "id": "opencompass_1082",
          "name": "StudentEval",
          "version": "1.0.0",
          "description": "STUDENTEVAL contains 1,749 prompts written by 80 students who have only completed one introductory Python course. STUDENTEVAL contains numerous non-expert prompts describing the same problem, enabling exploration of key factors in prompt success. ",
          "url": "opencompass/opencompass_1082.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1082",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1082",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Wellesley-EASEL-lab/StudentEval",
            "paperLink": "https://aclanthology.org/2024.findings-acl.501.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50186257",
              "name": "Wellesley-EASEL-lab",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50186257-118d14f5-a2d6-40c7-b3c5-712031ca526e.png",
              "nickname": "Wellesley-EASEL-lab"
            },
            "lookNum": "249",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-29 16:24:55",
            "supportOnlineEval": false,
            "updateDate": "2024-09-29 16:24:55",
            "createDate": "2024-09-29 11:16:00",
            "desc": {
              "cn": "StudentEval åŒ…å« 1,749 ä¸ªç”± 80 åä»…å®Œæˆä¸€é—¨å…¥é—¨ Python è¯¾ç¨‹çš„å­¦ç”Ÿæ’°å†™çš„æç¤ºã€‚StudentEval ä¸­åŒ…å«è®¸å¤šéä¸“å®¶æç¤ºï¼Œæè¿°ç›¸åŒçš„é—®é¢˜ï¼Œä½¿å¾—æ¢ç´¢æç¤ºæˆåŠŸçš„å…³é”®å› ç´ æˆä¸ºå¯èƒ½ã€‚",
              "en": "STUDENTEVAL contains 1,749 prompts written by 80 students who have only completed one introductory Python course. STUDENTEVAL contains numerous non-expert prompts describing the same problem, enabling exploration of key factors in prompt success. "
            }
          }
        },
        {
          "id": "opencompass_1244",
          "name": "Omni-MATH",
          "version": "1.0.0",
          "description": "Omni-MATH focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels",
          "url": "opencompass/opencompass_1244.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1244",
          "sample_count": 1000,
          "traits": [
            "Math",
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1244",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              },
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/KbsdJames/Omni-MATH",
            "paperLink": "https://arxiv.org/abs/2410.07985",
            "officialWebsiteLink": "https://omni-math.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "248",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-30 16:29:59",
            "supportOnlineEval": false,
            "updateDate": "2024-12-30 16:29:59",
            "createDate": "2024-12-30 16:20:58",
            "desc": {
              "cn": "Omni-MATHç”¨äºè¯„ä¼°LLMåœ¨å¥¥æ—åŒ¹å…‹æ°´å¹³ä¸Šçš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬4428é“ç«èµ›çº§é—®é¢˜ï¼Œå¹¶å¸¦æœ‰ä¸¥æ ¼çš„äººå·¥æ³¨é‡Šã€‚è¿™äº›é—®é¢˜è¢«ç²¾å¿ƒåˆ†ç±»ä¸ºè¶…è¿‡33ä¸ªå­é¢†åŸŸï¼Œæ¶µç›–10å¤šä¸ªä¸åŒçš„éš¾åº¦çº§åˆ«ã€‚",
              "en": "Omni-MATH focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels"
            }
          }
        },
        {
          "id": "opencompass_1081",
          "name": "NaturalCodeBench",
          "version": "1.0.0",
          "description": "NaturalCodeBench is challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks. NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains.",
          "url": "opencompass/opencompass_1081.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1081",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1081",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/THUDM/NaturalCodeBench",
            "paperLink": "https://aclanthology.org/2024.findings-acl.471.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "043910",
              "name": "THUDM",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/043910-ad5eb16e-0c3f-4e1b-b1d6-e9dc8ab355a7.png",
              "nickname": "æ™ºè°±.AI"
            },
            "lookNum": "244",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-29 16:24:58",
            "supportOnlineEval": false,
            "updateDate": "2024-09-29 16:24:58",
            "createDate": "2024-09-29 10:05:45",
            "desc": {
              "cn": "NaturalCodeBench æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»£ç åŸºå‡†ï¼Œæ—¨åœ¨åæ˜ çœŸå®ç¼–ç ä»»åŠ¡ä¸­çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚NaturalCodeBench åŒ…å« 402 ä¸ªé«˜è´¨é‡çš„ Python å’Œ Java é—®é¢˜ï¼Œè¿™äº›é—®é¢˜æ˜¯ä»åœ¨çº¿ç¼–ç æœåŠ¡çš„è‡ªç„¶ç”¨æˆ·æŸ¥è¯¢ä¸­ç²¾å¿ƒæŒ‘é€‰çš„ï¼Œæ¶µç›–äº† 6 ä¸ªä¸åŒçš„é¢†åŸŸã€‚",
              "en": "NaturalCodeBench is challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks. NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains."
            }
          }
        },
        {
          "id": "opencompass_1083",
          "name": "GAOKAO-MM",
          "version": "1.0.0",
          "description": "GAOKAO-MM is a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. ",
          "url": "opencompass/opencompass_1083.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1083",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1083",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/OpenMOSS/GAOKAO-MM",
            "paperLink": "https://aclanthology.org/2024.findings-acl.521.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50175431",
              "name": "OpenMOSS",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50175431-71e98faf-7e15-46ce-8c84-56eb373cf63a.png",
              "nickname": "OpenMOSS"
            },
            "lookNum": "240",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-29 16:24:53",
            "supportOnlineEval": false,
            "updateDate": "2024-09-29 16:24:53",
            "createDate": "2024-09-29 11:48:20",
            "desc": {
              "cn": "GAOKAO-MM æ˜¯ä¸€ä¸ªåŸºäºä¸­å›½é«˜è€ƒçš„å¤šæ¨¡æ€åŸºå‡†ï¼ŒåŒ…å« 8 ä¸ªç§‘ç›®å’Œ 12 ç§å›¾åƒç±»å‹ï¼Œä¾‹å¦‚å›¾è¡¨ã€å‡½æ•°å›¾ã€åœ°å›¾å’Œç…§ç‰‡ã€‚GAOKAO-MM æºè‡ªæœ¬åœŸä¸­æ–‡è¯­å¢ƒï¼Œå¹¶å¯¹æ¨¡å‹çš„èƒ½åŠ›è®¾ç½®äº†äººç±»æ°´å¹³çš„è¦æ±‚ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€ç†è§£ã€çŸ¥è¯†å’Œæ¨ç†ã€‚",
              "en": "GAOKAO-MM is a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. "
            }
          }
        },
        {
          "id": "opencompass_1115",
          "name": "MathQA",
          "version": "1.0.0",
          "description": "MathQA is a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs\ncorresponding to word problems in the AQuA dataset.",
          "url": "opencompass/opencompass_1115.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1115",
          "sample_count": 1000,
          "traits": [
            "Math"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1115",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://math-qa.github.io/math-QA/",
            "paperLink": "https://arxiv.org/pdf/1905.13319v1",
            "officialWebsiteLink": "https://math-qa.github.io/math-QA/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "239",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-11 19:59:27",
            "supportOnlineEval": false,
            "updateDate": "2024-10-11 19:59:27",
            "createDate": "2024-10-10 14:14:12",
            "desc": {
              "cn": "MathQA æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼ŒåŒ…å« 37,000 é“è‹±è¯­å¤šé¡¹é€‰æ‹©æ•°å­¦æ–‡å­—é—®é¢˜ï¼Œæ¶µç›–å¤šä¸ªæ•°å­¦é¢†åŸŸç±»åˆ«ã€‚",
              "en": "MathQA is a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs\ncorresponding to word problems in the AQuA dataset."
            }
          }
        },
        {
          "id": "opencompass_1079",
          "name": "CFLUE",
          "version": "1.0.0",
          "description": "CFLUE is the Chinese Financial Language Understanding Evaluation benchmark, designed to assess the capability of LLMs across various dimensions. ",
          "url": "opencompass/opencompass_1079.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1079",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1079",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/aliyun/cflue",
            "paperLink": "https://aclanthology.org/2024.findings-acl.337.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50186956",
              "name": "Alibaba_Cloud",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50186956-08003a88-f404-4b02-a282-c0c377547acd.png",
              "nickname": "Alibaba_Cloud"
            },
            "lookNum": "237",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-29 16:25:03",
            "supportOnlineEval": false,
            "updateDate": "2024-09-29 16:25:03",
            "createDate": "2024-09-27 17:54:49",
            "desc": {
              "cn": "CFLUE æ˜¯ä¸­å›½é‡‘èè¯­è¨€ç†è§£è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªç»´åº¦ä¸Šçš„èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼ŒCFLUE æä¾›äº†é’ˆå¯¹çŸ¥è¯†è¯„ä¼°å’Œåº”ç”¨è¯„ä¼°é‡èº«å®šåˆ¶çš„æ•°æ®é›†ã€‚åœ¨çŸ¥è¯†è¯„ä¼°æ–¹é¢ï¼Œå®ƒåŒ…å«è¶…è¿‡ 38,000 é“é€‰æ‹©é¢˜åŠç›¸å…³çš„è§£å†³æ–¹æ¡ˆè§£é‡Šã€‚",
              "en": "CFLUE is the Chinese Financial Language Understanding Evaluation benchmark, designed to assess the capability of LLMs across various dimensions. "
            }
          }
        },
        {
          "id": "opencompass_1093",
          "name": "APPS",
          "version": "1.0.0",
          "description": "APPS is a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.",
          "url": "opencompass/opencompass_1093.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1093",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1093",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/hendrycks/apps",
            "paperLink": "https://arxiv.org/pdf/2105.09938",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "237",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-30 15:32:52",
            "supportOnlineEval": false,
            "updateDate": "2024-09-30 15:32:52",
            "createDate": "2024-09-29 17:01:36",
            "desc": {
              "cn": "APPS æ˜¯ä¸€ä¸ªä»£ç ç”Ÿæˆè¯„æµ‹åŸºå‡†ï¼Œè¯¥è¯„æµ‹åŸºå‡†æµ‹é‡æ¨¡å‹æ ¹æ®ä»»æ„è‡ªç„¶è¯­è¨€è§„èŒƒç”Ÿæˆä»¤äººæ»¡æ„çš„ Python ä»£ç çš„èƒ½åŠ›ã€‚",
              "en": "APPS is a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code."
            }
          }
        },
        {
          "id": "opencompass_1134",
          "name": "TheoremQA",
          "version": "1.0.0",
          "description": "TheoremQA is the first theorem-driven question-answering dataset designed to evaluate AI modelsâ€™ capabilities to apply theorems to solve challenging science problems. It is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance.",
          "url": "opencompass/opencompass_1134.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1134",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1134",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/TIGER-AI-Lab/TheoremQA",
            "paperLink": "https://arxiv.org/pdf/2305.12524",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "237",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-14 20:15:11",
            "supportOnlineEval": false,
            "updateDate": "2024-10-14 20:15:11",
            "createDate": "2024-10-12 13:48:08",
            "desc": {
              "cn": "TheoremQA æ˜¯ç¬¬ä¸€ä¸ªåŸºäºå®šç†çš„é—®é¢˜å›ç­”æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼° AI æ¨¡å‹åº”ç”¨å®šç†è§£å†³å¤æ‚ç§‘å­¦é—®é¢˜çš„èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†ç”±é¢†åŸŸä¸“å®¶ç²¾å¿ƒç­–åˆ’ï¼ŒåŒ…å« 800 ä¸ªé«˜è´¨é‡é—®é¢˜ï¼Œæ¶µç›–æ¥è‡ªæ•°å­¦ã€ç‰©ç†ã€ç”µæ°”ä¸è®¡ç®—æœºç§‘å­¦ä»¥åŠé‡‘èçš„ 350 ä¸ªå®šç†ã€‚",
              "en": "TheoremQA is the first theorem-driven question-answering dataset designed to evaluate AI modelsâ€™ capabilities to apply theorems to solve challenging science problems. It is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance."
            }
          }
        },
        {
          "id": "opencompass_1207",
          "name": "MMBench-Video",
          "version": "1.0.0",
          "description": "MMBench-Video is a comprehensive video understanding evaluation benchmark that covers long videos, multiple shots, and evaluates the temporal understanding ability of MLLMs. Contains over 600 videos, 16 categories, and manually annotated Q&A pairs.",
          "url": "opencompass/opencompass_1207.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1207",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1207",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 3,
            "githubLink": "https://github.com/open-compass/VLMEvalKit",
            "paperLink": "https://arxiv.org/abs/2406.14515",
            "officialWebsiteLink": "https://mmbench-video.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "237",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 17:04:03",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 17:04:03",
            "createDate": "2025-01-21 14:14:14",
            "desc": {
              "cn": "MMBench-Videoæ˜¯å…¨é¢è§†é¢‘ç†è§£è¯„æµ‹åŸºå‡†ï¼Œè¦†ç›–é•¿è§†é¢‘ã€å¤šé•œå¤´ï¼Œè¯„ä¼°MLLMsæ—¶åºç†è§£èƒ½åŠ›ã€‚åŒ…å«16ç±»å…±600+è§†é¢‘ä»¥åŠäººå·¥æ ‡æ³¨é—®ç­”å¯¹ã€‚",
              "en": "MMBench-Video is a comprehensive video understanding evaluation benchmark that covers long videos, multiple shots, and evaluates the temporal understanding ability of MLLMs. Contains over 600 videos, 16 categories, and manually annotated Q&A pairs."
            }
          }
        },
        {
          "id": "opencompass_1109",
          "name": "WinoGrande",
          "version": "1.0.0",
          "description": "WINOGRANDE is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset.",
          "url": "opencompass/opencompass_1109.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1109",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1109",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/allenai/winogrande",
            "paperLink": "https://arxiv.org/pdf/1907.10641",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "236",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-09 20:03:23",
            "supportOnlineEval": true,
            "updateDate": "2024-10-09 20:03:23",
            "createDate": "2024-10-09 18:41:18",
            "desc": {
              "cn": "WINOGRANDE åŒ…å« 44,000 ä¸ªé—®é¢˜ï¼Œå—åˆ° WSC è®¾è®¡çš„å¯å‘ï¼Œä½†è¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥æé«˜æ•°æ®é›†çš„è§„æ¨¡å’Œéš¾åº¦ã€‚",
              "en": "WINOGRANDE is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset."
            }
          }
        },
        {
          "id": "opencompass_1124",
          "name": "RealToxicityPrompts",
          "version": "1.0.0",
          "description": "RealToxicityPrompts is a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely used toxicity classiï¬er. ",
          "url": "opencompass/opencompass_1124.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1124",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1124",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/allenai/real-toxicity-prompts",
            "paperLink": "https://aclanthology.org/2020.findings-emnlp.301.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "236",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-11 20:24:44",
            "supportOnlineEval": false,
            "updateDate": "2024-10-11 20:24:44",
            "createDate": "2024-10-11 13:38:08",
            "desc": {
              "cn": "RealToxicityPrompts æ˜¯ä¸€ä¸ªåŒ…å« 100,000 ä¸ªè‡ªç„¶å‡ºç°çš„ã€å¥å­çº§æç¤ºçš„æ•°æ®é›†ï¼Œè¿™äº›æç¤ºæ¥è‡ªäºå¤§é‡çš„è‹±è¯­ç½‘ç»œæ–‡æœ¬ï¼Œå¹¶é…æœ‰æ¥è‡ªå¹¿æ³›ä½¿ç”¨çš„æ¯’æ€§åˆ†ç±»å™¨çš„æ¯’æ€§è¯„åˆ†ã€‚",
              "en": "RealToxicityPrompts is a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely used toxicity classiï¬er. "
            }
          }
        },
        {
          "id": "opencompass_1287",
          "name": "MedBench",
          "version": "1.0.0",
          "description": "MedBench is committed to building a scientific, fair and rigorous Chinese medical model evaluation system and open platform. Based on authoritative standards, we constantly update and maintain high-quality datasets, and comprehensively quantify capabilities of models in various medical dimensions.",
          "url": "opencompass/opencompass_1287.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1287",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1287",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Medical",
                "en": "Medical"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/open-compass/opencompass/tree/main/opencompass/datasets/medbench/",
            "paperLink": "https://www.sciopen.com/article/10.26599/BDMA.2024.9020044",
            "officialWebsiteLink": "https://medbench.opencompass.org.cn/home",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "235",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-27 14:45:32",
            "supportOnlineEval": false,
            "updateDate": "2024-12-27 14:45:32",
            "createDate": "2024-12-27 14:01:19",
            "desc": {
              "cn": "MedBenchè‡´åŠ›äºæ‰“é€ ä¸€ä¸ªç§‘å­¦ã€å…¬å¹³ä¸”ä¸¥è°¨çš„ä¸­æ–‡åŒ»ç–—å¤§æ¨¡å‹è¯„æµ‹ä½“ç³»åŠå¼€æ”¾å¹³å°ã€‚æˆ‘ä»¬åŸºäºåŒ»å­¦æƒå¨æ ‡å‡†ï¼Œä¸æ–­æ›´æ–°ç»´æŠ¤é«˜è´¨é‡çš„åŒ»å­¦æ•°æ®é›†ï¼Œå…¨æ–¹ä½å¤šç»´åº¦é‡åŒ–æ¨¡å‹åœ¨å„ä¸ªåŒ»å­¦ç»´åº¦çš„èƒ½åŠ›ã€‚",
              "en": "MedBench is committed to building a scientific, fair and rigorous Chinese medical model evaluation system and open platform. Based on authoritative standards, we constantly update and maintain high-quality datasets, and comprehensively quantify capabilities of models in various medical dimensions."
            }
          }
        },
        {
          "id": "opencompass_1245",
          "name": "KOR-Bench",
          "version": "1.0.0",
          "description": "Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench) encompasses five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual. KOR-Bench emphasizes the effectiveness of models in applying new rule descriptions to solve novel rule-driven questions. ",
          "url": "opencompass/opencompass_1245.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1245",
          "sample_count": 1000,
          "traits": [
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1245",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/KOR-Bench/KOR-Bench",
            "paperLink": "https://arxiv.org/abs/2410.06526",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "234",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 11:12:43",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 11:12:43",
            "createDate": "2025-01-10 19:42:40",
            "desc": {
              "cn": "KOR-Benchç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬äº”ä¸ªä»»åŠ¡ç±»åˆ«ï¼šæ“ä½œã€é€»è¾‘ã€å¯†ç ã€æ‹¼å›¾å’Œåäº‹å®ã€‚",
              "en": "Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench) encompasses five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual. KOR-Bench emphasizes the effectiveness of models in applying new rule descriptions to solve novel rule-driven questions. "
            }
          }
        },
        {
          "id": "opencompass_1088",
          "name": "UHGEval",
          "version": "1.0.0",
          "description": "UHGEval(Unconstrained Hallucination Generation Evaluation) benchmark contains hallucinations generated by LLMs with minimal restrictions.",
          "url": "opencompass/opencompass_1088.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1088",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1088",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/IAAR-Shanghai/UHGEval",
            "paperLink": "https://arxiv.org/pdf/2311.15296",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "230",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-10 10:34:42",
            "supportOnlineEval": false,
            "updateDate": "2024-10-10 10:34:42",
            "createDate": "2024-10-09 20:11:08",
            "desc": {
              "cn": "UHGEval åŸºå‡†ï¼ŒåŒ…å«ç”±é™åˆ¶æ¡ä»¶æœ€å°çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„å¹»è§‰ã€‚",
              "en": "UHGEval(Unconstrained Hallucination Generation Evaluation) benchmark contains hallucinations generated by LLMs with minimal restrictions."
            }
          }
        },
        {
          "id": "opencompass_1328",
          "name": "GTA",
          "version": "1.0.0",
          "description": "GTA is meant for LLM's tool-use evaluations under real-world scenarios, including 229 real-world tasks and executable tool chains.",
          "url": "opencompass/opencompass_1328.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1328",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1328",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/open-compass/GTA",
            "paperLink": "https://arxiv.org/abs/2407.08713",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "223",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-06 16:32:41",
            "supportOnlineEval": false,
            "updateDate": "2025-01-06 16:32:41",
            "createDate": "2025-01-03 11:35:50",
            "desc": {
              "cn": "GTAç”¨äºè¯„ä¼°LLMè°ƒç”¨å·¥å…·è§£å†³å®é™…é—®é¢˜çš„èƒ½åŠ›ï¼Œç”±229ä¸ªçœŸå®ä»»åŠ¡å’Œå¯æ‰§è¡Œå·¥å…·é“¾ç»„æˆã€‚",
              "en": "GTA is meant for LLM's tool-use evaluations under real-world scenarios, including 229 real-world tasks and executable tool chains."
            }
          }
        },
        {
          "id": "opencompass_1078",
          "name": "DebugBench",
          "version": "1.0.0",
          "description": "DebugBench is an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. ",
          "url": "opencompass/opencompass_1078.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1078",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1078",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/thunlp/DebugBench",
            "paperLink": "https://aclanthology.org/2024.findings-acl.247.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50188779",
              "name": "THUNLP",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50188779-1d9e7f11-f6dd-4c37-a464-f75de27aa50a.png",
              "nickname": "THUNLP"
            },
            "lookNum": "222",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-29 16:25:05",
            "supportOnlineEval": false,
            "updateDate": "2024-09-29 16:25:05",
            "createDate": "2024-09-27 17:44:25",
            "desc": {
              "cn": "DebugBench æ˜¯ä¸€ä¸ªåŒ…å« 4,253 ä¸ªå®ä¾‹çš„ LLM è°ƒè¯•åŸºå‡†ã€‚å®ƒæ¶µç›–äº† C++ã€Java å’Œ Python ä¸­çš„å››ä¸ªä¸»è¦é”™è¯¯ç±»åˆ«å’Œ 18 ç§æ¬¡è¦ç±»å‹ã€‚",
              "en": "DebugBench is an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. "
            }
          }
        },
        {
          "id": "opencompass_1133",
          "name": "Spider",
          "version": "1.0.0",
          "description": "Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. ",
          "url": "opencompass/opencompass_1133.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1133",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1133",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/taoyds/spider/tree/master/evaluation_examples",
            "paperLink": "https://arxiv.org/pdf/1809.08887v5",
            "officialWebsiteLink": "https://yale-lily.github.io/spider",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "221",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-16 11:03:12",
            "supportOnlineEval": false,
            "updateDate": "2024-10-16 11:03:12",
            "createDate": "2024-10-12 13:37:46",
            "desc": {
              "cn": "Spider æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤æ‚ä¸”è·¨é¢†åŸŸçš„è¯­ä¹‰è§£æå’Œæ–‡æœ¬åˆ° SQL æ•°æ®é›†ã€‚Spider æŒ‘æˆ˜çš„ç›®æ ‡æ˜¯å¼€å‘è‡ªç„¶è¯­è¨€æ¥å£ä»¥è®¿é—®è·¨é¢†åŸŸæ•°æ®åº“ã€‚è¯¥æ•°æ®é›†åŒ…å« 10,181 ä¸ªé—®é¢˜å’Œ 5,693 ä¸ªç‹¬ç‰¹çš„å¤æ‚ SQL æŸ¥è¯¢ï¼Œæ¶µç›– 200 ä¸ªåŒ…å«å¤šä¸ªè¡¨çš„æ•°æ®åº“ï¼Œæ¶‰åŠ 138 ä¸ªä¸åŒçš„é¢†åŸŸã€‚",
              "en": "Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. "
            }
          }
        },
        {
          "id": "opencompass_1100",
          "name": "ScienceQA",
          "version": "1.0.0",
          "description": "SCIENCEQA is a new benchmark that consists of âˆ¼21k multimodal multiple choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations.",
          "url": "opencompass/opencompass_1100.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1100",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1100",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://scienceqa.github.io/",
            "paperLink": "https://lupantech.github.io/papers/neurips22_scienceqa.pdf",
            "officialWebsiteLink": "https://scienceqa.github.io/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "219",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-09 20:03:37",
            "supportOnlineEval": false,
            "updateDate": "2024-10-09 20:03:37",
            "createDate": "2024-10-09 14:29:43",
            "desc": {
              "cn": "SCIENCEQA åŒ…å«çº¦ 21,000 é“å¤šæ¨¡æ€é€‰æ‹©é¢˜ï¼Œæ¶µç›–å¤šç§ç§‘å­¦ä¸»é¢˜ï¼Œå¹¶é™„æœ‰ç›¸åº”çš„è®²åº§å’Œè§£é‡Šçš„ç­”æ¡ˆæ³¨é‡Šã€‚",
              "en": "SCIENCEQA is a new benchmark that consists of âˆ¼21k multimodal multiple choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations."
            }
          }
        },
        {
          "id": "opencompass_1086",
          "name": "Belebele",
          "version": "1.0.0",
          "description": "BELEBELE, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in\nhigh-, medium-, and low-resource languages.",
          "url": "opencompass/opencompass_1086.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1086",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1086",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/facebookresearch/belebele",
            "paperLink": "https://arxiv.org/pdf/2308.16884",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "211",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-30 15:33:02",
            "supportOnlineEval": false,
            "updateDate": "2024-09-30 15:33:02",
            "createDate": "2024-09-29 13:42:49",
            "desc": {
              "cn": "BELEBELE æ˜¯ä¸€ä¸ªå¤šé¡¹é€‰æ‹©æœºå™¨é˜…è¯»ç†è§£ï¼ˆMRCï¼‰æ•°æ®é›†ï¼Œæ¶µç›– 122 ç§è¯­è¨€å˜ä½“ã€‚è¯¥æ•°æ®é›†æ˜¾è‘—æ‰©å±•äº†è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰åŸºå‡†çš„è¯­è¨€è¦†ç›–èŒƒå›´ï¼Œä½¿å¾—å¯ä»¥åœ¨é«˜ã€ä¸­ã€ä½èµ„æºè¯­è¨€ä¸­è¯„ä¼°æ–‡æœ¬æ¨¡å‹ã€‚",
              "en": "BELEBELE, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in\nhigh-, medium-, and low-resource languages."
            }
          }
        },
        {
          "id": "opencompass_1123",
          "name": "Crows-Pairs",
          "version": "1.0.0",
          "description": "CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping.",
          "url": "opencompass/opencompass_1123.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1123",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1123",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/nyu-mll/crows-pairs",
            "paperLink": "https://arxiv.org/pdf/2010.00133v1",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "210",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-11 20:24:47",
            "supportOnlineEval": false,
            "updateDate": "2024-10-11 20:24:47",
            "createDate": "2024-10-11 13:33:41",
            "desc": {
              "cn": "CrowS-Pairs åŒ…å« 1508 ä¸ªç¤ºä¾‹ï¼Œæ¶µç›–ä¸ä¹ç§åè§ç±»å‹ç›¸å…³çš„åˆ»æ¿å°è±¡ï¼Œä¾‹å¦‚ç§æ—ã€å®—æ•™å’Œå¹´é¾„ã€‚åœ¨ CrowS-Pairs ä¸­ï¼Œæ¨¡å‹ä¼šæ¥æ”¶åˆ°ä¸¤å¥è¯ï¼šä¸€å¥æ˜¯æ›´å…·åˆ»æ¿å°è±¡çš„ï¼Œå¦ä¸€å¥åˆ™æ˜¯è¾ƒå°‘åˆ»æ¿å°è±¡çš„ã€‚",
              "en": "CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping."
            }
          }
        },
        {
          "id": "opencompass_1087",
          "name": "Reveal",
          "version": "1.0.0",
          "description": "REVEAL(ReasoningVerification Evaluation) is a new dataset to benchmark automatic verifiers of complex Chain-ofThought reasoning in open-domain question answering settings. ",
          "url": "opencompass/opencompass_1087.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1087",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1087",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://reveal-dataset.github.io/",
            "paperLink": "https://arxiv.org/pdf/2402.00559",
            "officialWebsiteLink": "https://reveal-dataset.github.io/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "203",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-30 15:32:59",
            "supportOnlineEval": false,
            "updateDate": "2024-09-30 15:32:59",
            "createDate": "2024-09-29 14:26:40",
            "desc": {
              "cn": "Reveal æ˜¯ä¸€ä¸ªç”¨äºåŸºå‡†æµ‹è¯•å¼€æ”¾åŸŸé—®ç­”ç¯å¢ƒä¸­å¤æ‚é“¾å¼æ¨ç†è‡ªåŠ¨éªŒè¯å™¨çš„æ–°æ•°æ®é›†ã€‚Reveal åŒ…å«å…³äºè¯­è¨€æ¨¡å‹ç­”æ¡ˆä¸­æ¯ä¸ªæ¨ç†æ­¥éª¤çš„ç›¸å…³æ€§ã€è¯æ®æ®µè½çš„å½’å› å’Œé€»è¾‘æ­£ç¡®æ€§çš„å…¨é¢æ ‡ç­¾ï¼Œæ¶µç›–å¤šç§æ•°æ®é›†å’Œæœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ã€‚",
              "en": "REVEAL(ReasoningVerification Evaluation) is a new dataset to benchmark automatic verifiers of complex Chain-ofThought reasoning in open-domain question answering settings. "
            }
          }
        },
        {
          "id": "opencompass_1069",
          "name": "AIR-Bench",
          "version": "1.0.0",
          "description": "AIR-Bench is the first benchmark designed to evaluate the\nability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format.",
          "url": "opencompass/opencompass_1069.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1069",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1069",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/OFA-Sys/AIR-Bench",
            "paperLink": "https://arxiv.org/pdf/2402.07729",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50186884",
              "name": "OFA-Sys",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50186884-c56d3538-0c00-4d32-86ed-c28d0817e429.jfif",
              "nickname": "OFA-Sys"
            },
            "lookNum": "199",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-27 16:53:47",
            "supportOnlineEval": false,
            "updateDate": "2024-09-27 16:53:47",
            "createDate": "2024-09-26 16:07:49",
            "desc": {
              "cn": "AIR-Bench æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼° LALMs ç†è§£å„ç§éŸ³é¢‘ä¿¡å·ï¼ˆåŒ…æ‹¬äººç±»è¯­è¨€ã€è‡ªç„¶å£°éŸ³å’ŒéŸ³ä¹ï¼‰èƒ½åŠ›çš„åŸºå‡†ï¼Œå¹¶è¿›ä¸€æ­¥è¯„ä¼°å…¶ä»¥æ–‡æœ¬å½¢å¼ä¸äººç±»äº’åŠ¨çš„èƒ½åŠ›ã€‚AIR-Bench åŒ…æ‹¬ä¸¤ä¸ªç»´åº¦ï¼šåŸºç¡€åŸºå‡†å’ŒèŠå¤©åŸºå‡†ã€‚å‰è€…ç”±19ä¸ªä»»åŠ¡ç»„æˆï¼ŒåŒ…å«çº¦19,000ä¸ªå•é€‰é¢˜ï¼Œæ—¨åœ¨æ£€æŸ¥LALMsçš„åŸºæœ¬å•ä»»åŠ¡èƒ½åŠ›ã€‚åè€…åŒ…å«2,000ä¸ªå¼€æ”¾å¼é—®ç­”æ•°æ®å®ä¾‹ï¼Œç›´æ¥è¯„ä¼°æ¨¡å‹å¯¹å¤æ‚éŸ³é¢‘çš„ç†è§£åŠå…¶éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚",
              "en": "AIR-Bench is the first benchmark designed to evaluate the\nability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format."
            }
          }
        },
        {
          "id": "opencompass_1148",
          "name": "AbsPyramid",
          "version": "1.0.0",
          "description": "ABSPYRAMID is a unified entailment graph of 221K textual descriptions of abstraction knowledge. ABSPYRAMID collects abstract knowledge for three components\nof diverse events to comprehensively evaluate the abstraction ability of language\nmodels in the open domain.",
          "url": "opencompass/opencompass_1148.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1148",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1148",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/HKUST-KnowComp/AbsPyramid",
            "paperLink": "https://aclanthology.org/2024.findings-naacl.252.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50109576",
              "name": "HKUST-KnowComp",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50109576-bd7795e6-da4b-40f0-a4e6-979ddb3230c2.png",
              "nickname": "OpenXLab-dJloo3kUv"
            },
            "lookNum": "199",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-27 17:22:48",
            "supportOnlineEval": false,
            "updateDate": "2024-10-27 17:22:48",
            "createDate": "2024-10-15 14:04:22",
            "desc": {
              "cn": "ABSPYRAMID æ˜¯åŒ…å« 221,000 æ¡æ–‡æœ¬æè¿°çš„æŠ½è±¡çŸ¥è¯†,æ”¶é›†äº†å¤šç§äº‹ä»¶çš„ä¸‰ä¸ªç»„æˆéƒ¨åˆ†çš„æŠ½è±¡çŸ¥è¯†ï¼Œä»¥å…¨é¢è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å¼€æ”¾åŸŸä¸­çš„æŠ½è±¡èƒ½åŠ›ã€‚",
              "en": "ABSPYRAMID is a unified entailment graph of 221K textual descriptions of abstraction knowledge. ABSPYRAMID collects abstract knowledge for three components\nof diverse events to comprehensively evaluate the abstraction ability of language\nmodels in the open domain."
            }
          }
        },
        {
          "id": "opencompass_1164",
          "name": "TaskBench",
          "version": "1.0.0",
          "description": "TaskBench aims to evaluate the capability of LLMs in task automation, containing 28,271 samples spanning 3 critical stages: task decomposition, tool invocation, and parameter prediction.",
          "url": "opencompass/opencompass_1164.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1164",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1164",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/microsoft/JARVIS/tree/main/taskbench",
            "paperLink": "https://arxiv.org/pdf/2311.18760",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "197",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-06 16:32:37",
            "supportOnlineEval": false,
            "updateDate": "2025-01-06 16:32:37",
            "createDate": "2024-10-18 15:47:56",
            "desc": {
              "cn": "TaskBenchæ—¨åœ¨è¯„ä¼°LLMåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„èƒ½åŠ›ï¼ŒåŒ…å«é¢å‘ä»»åŠ¡åˆ†è§£ã€å·¥å…·è°ƒç”¨å’Œå‚æ•°é¢„æµ‹ä¸‰ä¸ªå…³é”®é˜¶æ®µçš„28271ä¸ªæ ·æœ¬ã€‚",
              "en": "TaskBench aims to evaluate the capability of LLMs in task automation, containing 28,271 samples spanning 3 critical stages: task decomposition, tool invocation, and parameter prediction."
            }
          }
        },
        {
          "id": "opencompass_1266",
          "name": "BABILong",
          "version": "1.0.0",
          "description": "BABILong is designed to test language models' ability to reason across facts distributed in extremely long documents. It contains a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. ",
          "url": "opencompass/opencompass_1266.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1266",
          "sample_count": 1000,
          "traits": [
            "Long-Context"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1266",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "é•¿æ–‡æœ¬",
                "en": "Long-Context"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/booydar/babilong",
            "paperLink": "https://arxiv.org/abs/2406.10149",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "193",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-25 14:14:54",
            "supportOnlineEval": false,
            "updateDate": "2024-12-25 14:14:54",
            "createDate": "2024-12-25 14:14:32",
            "desc": {
              "cn": "BABILongæ—¨åœ¨æµ‹è¯•è¯­è¨€æ¨¡å‹å¯¹åˆ†å¸ƒåœ¨æé•¿æ–‡æ¡£ä¸­çš„äº‹å®è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œæ¶µç›–äº‹å®é“¾æ¥ã€ç®€å•å½’çº³ã€æ¨å¯¼ã€è®¡æ•°å’Œå¤„ç†åˆ—è¡¨/é›†åˆç­‰20ç§å„ç±»æ¨ç†ä»»åŠ¡ã€‚",
              "en": "BABILong is designed to test language models' ability to reason across facts distributed in extremely long documents. It contains a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. "
            }
          }
        },
        {
          "id": "opencompass_1248",
          "name": "MMMU",
          "version": "1.0.0",
          "description": "MMMU is a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks.",
          "url": "opencompass/opencompass_1248.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1248",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1248",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Multimodal",
                "en": "Multimodal"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/MMMU-Benchmark/MMMU",
            "paperLink": "https://arxiv.org/abs/2311.16502",
            "officialWebsiteLink": "https://mmmu-benchmark.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "189",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-22 18:09:32",
            "supportOnlineEval": false,
            "updateDate": "2024-12-22 18:09:32",
            "createDate": "2024-12-20 19:45:07",
            "desc": {
              "cn": "MMMUç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨å¤æ‚å¤šå­¦ç§‘ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ä»å¤§å­¦è€ƒè¯•å’Œæ•™ç§‘ä¹¦ä¸­ç²¾å¿ƒæ”¶é›†çš„11.5Kå¤šæ¨¡æ€é—®é¢˜ï¼Œæ¶µç›–å…­ä¸ªæ ¸å¿ƒå­¦ç§‘ï¼šè‰ºæœ¯ä¸è®¾è®¡ã€å•†ä¸šã€ç§‘å­¦ã€å¥åº·ä¸åŒ»å­¦ã€äººæ–‡ä¸ç¤¾ä¼šç§‘å­¦ä»¥åŠæŠ€æœ¯ä¸å·¥ç¨‹ã€‚",
              "en": "MMMU is a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks."
            }
          }
        },
        {
          "id": "opencompass_1129",
          "name": "WikiSQL",
          "version": "1.0.0",
          "description": "WikiSQL is a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia that is an order of magnitude larger than comparable datasets.",
          "url": "opencompass/opencompass_1129.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1129",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1129",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/salesforce/WikiSQL",
            "paperLink": "https://arxiv.org/pdf/1709.00103v7",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "187",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-16 11:03:06",
            "supportOnlineEval": false,
            "updateDate": "2024-10-16 11:03:06",
            "createDate": "2024-10-11 15:27:39",
            "desc": {
              "cn": "WikiSQL æ˜¯ä¸€ä¸ªåŒ…å« 80,654 ä¸ªæ‰‹åŠ¨æ ‡æ³¨ç¤ºä¾‹çš„é—®é¢˜å’Œ SQL æŸ¥è¯¢çš„æ•°æ®é›†ï¼Œåˆ†å¸ƒåœ¨æ¥è‡ªç»´åŸºç™¾ç§‘çš„ 24,241 ä¸ªè¡¨æ ¼ä¸­ã€‚",
              "en": "WikiSQL is a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia that is an order of magnitude larger than comparable datasets."
            }
          }
        },
        {
          "id": "opencompass_1076",
          "name": "PCA-Bench",
          "version": "1.0.0",
          "description": "PCA-Bench is a multimodal decisionmaking benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous\nbenchmarks focusing on simplistic tasks and individual model capability.",
          "url": "opencompass/opencompass_1076.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1076",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1076",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/pkunlp-icler/PCA-EVAL",
            "paperLink": "https://aclanthology.org/2024.findings-acl.64.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50188713",
              "name": "PKUNLP-ICLER",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50188713-85c05c22-aa65-44f2-b06f-aa859ed819eb.png",
              "nickname": "PKUNLP-ICLER"
            },
            "lookNum": "186",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-27 17:50:33",
            "supportOnlineEval": false,
            "updateDate": "2024-09-27 17:50:33",
            "createDate": "2024-09-27 15:11:18",
            "desc": {
              "cn": "PCA-Bench æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å†³ç­–åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç»¼åˆèƒ½åŠ›ã€‚ä¸ä¹‹å‰ä¸“æ³¨äºç®€å•ä»»åŠ¡å’Œå•ä¸ªæ¨¡å‹èƒ½åŠ›çš„åŸºå‡†ä¸åŒï¼ŒPCA-Bench å¼•å…¥äº†ä¸‰ä¸ªå¤æ‚åœºæ™¯ï¼šè‡ªåŠ¨é©¾é©¶ã€å®¶åº­æœºå™¨äººå’Œå¼€æ”¾ä¸–ç•Œæ¸¸æˆã€‚",
              "en": "PCA-Bench is a multimodal decisionmaking benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous\nbenchmarks focusing on simplistic tasks and individual model capability."
            }
          }
        },
        {
          "id": "opencompass_1599",
          "name": "GAIA",
          "version": "1.0.0",
          "description": "GAIA is a benchmark which aims at evaluating next-generation LLMs (LLMs with augmented capabilities due to added tooling, efficient prompting, access to search, etc), mading of more than 450 non-trivial question with an unambiguous answer, requiring different levels of tooling and autonomy to solve.",
          "url": "opencompass/opencompass_1599.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1599",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1599",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2311.12983",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "185",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 11:17:15",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 11:17:15",
            "createDate": "2025-03-06 11:15:47",
            "desc": {
              "cn": "GAIA æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ä¸‹ä¸€ä»£LLMsï¼ˆç”±äºå¢åŠ äº†å·¥å…·ã€é«˜æ•ˆçš„æç¤ºã€è®¿é—®æœç´¢ç­‰åŠŸèƒ½è€Œå…·æœ‰å¢å¼ºèƒ½åŠ›çš„LLMsï¼‰çš„åŸºå‡†ï¼Œç”±è¶…è¿‡ 450 ä¸ªéå¹³å‡¡é—®é¢˜ç»„æˆï¼Œè¿™äº›é—®é¢˜æœ‰æ˜ç¡®çš„ç­”æ¡ˆï¼Œéœ€è¦ä¸åŒå±‚æ¬¡çš„å·¥å…·å’Œè‡ªä¸»æ€§æ¥è§£å†³ã€‚",
              "en": "GAIA is a benchmark which aims at evaluating next-generation LLMs (LLMs with augmented capabilities due to added tooling, efficient prompting, access to search, etc), mading of more than 450 non-trivial question with an unambiguous answer, requiring different levels of tooling and autonomy to solve."
            }
          }
        },
        {
          "id": "opencompass_1542",
          "name": "StructFlowBench",
          "version": "1.0.0",
          "description": "StructFlowBench is a structurally annotated multi-turn benchmark that leverages a structure-driven generation paradigm to enhance the simulation of complex dialogue scenarios.",
          "url": "opencompass/opencompass_1542.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1542",
          "sample_count": 1000,
          "traits": [
            "Instruct"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1542",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æŒ‡ä»¤è·Ÿéš",
                "en": "Instruct"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/MLGroupJLU/StructFlowBench",
            "paperLink": "https://arxiv.org/abs/2502.14494",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "182",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-02-25 19:09:02",
            "supportOnlineEval": false,
            "updateDate": "2025-02-25 19:09:02",
            "createDate": "2025-02-25 12:46:25",
            "desc": {
              "cn": "StructFlowBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«155æ¡æ•°æ®çš„ç»“æ„åŒ–æ ‡æ³¨å¤šè½®åŸºå‡†ï¼Œå®ƒåˆ©ç”¨ç»“æ„é©±åŠ¨ç”ŸæˆèŒƒå¼æ¥å¢å¼ºå¤æ‚å¯¹è¯åœºæ™¯çš„æ¨¡æ‹Ÿã€‚",
              "en": "StructFlowBench is a structurally annotated multi-turn benchmark that leverages a structure-driven generation paradigm to enhance the simulation of complex dialogue scenarios."
            }
          }
        },
        {
          "id": "opencompass_1359",
          "name": "SEED-Bench",
          "version": "1.0.0",
          "description": "SEED-Bench aims at the evaluation of generative comprehension in MLLMs, consisting of 19K multiple choice questions, which spans 12 evaluation dimensions including the comprehension of both the image and video modality. ",
          "url": "opencompass/opencompass_1359.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1359",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1359",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
            "paperLink": "https://arxiv.org/abs/2307.16125",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "178",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:23:55",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:23:55",
            "createDate": "2025-01-09 21:34:54",
            "desc": {
              "cn": "SEED-Benchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„ç†è§£èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¯¹å›¾åƒå’Œè§†é¢‘çš„ç†è§£ï¼Œç”±è·¨è¶Š12ä¸ªè¯„ä¼°ç»´åº¦çš„19Ké“å¤šé¡¹é€‰æ‹©é¢˜ç»„æˆã€‚",
              "en": "SEED-Bench aims at the evaluation of generative comprehension in MLLMs, consisting of 19K multiple choice questions, which spans 12 evaluation dimensions including the comprehension of both the image and video modality. "
            }
          }
        },
        {
          "id": "opencompass_1132",
          "name": "TabFact",
          "version": "1.0.0",
          "description": "TabFac consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED. ",
          "url": "opencompass/opencompass_1132.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1132",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1132",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/wenhuchen/Table-Fact-Checking",
            "paperLink": "https://arxiv.org/pdf/1909.02164v5",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "175",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-16 11:03:09",
            "supportOnlineEval": false,
            "updateDate": "2024-10-16 11:03:09",
            "createDate": "2024-10-12 11:35:24",
            "desc": {
              "cn": "TabFac åŒ…å« 117,854 æ¡æ‰‹åŠ¨æ ‡æ³¨çš„è¯­å¥ï¼Œæ¶‰åŠ 16,573 ä¸ªç»´åŸºç™¾ç§‘è¡¨æ ¼ï¼Œæ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°ç»“æ„åŒ–æ•°æ®ä¸Šè¯­è¨€æ¨ç†çš„æ•°æ®é›†ï¼Œæ¶‰åŠåœ¨ç¬¦å·å’Œè¯­è¨€ä¸¤ä¸ªæ–¹é¢çš„æ··åˆæ¨ç†èƒ½åŠ›ã€‚",
              "en": "TabFac consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED. "
            }
          }
        },
        {
          "id": "opencompass_1175",
          "name": "MMStar",
          "version": "1.0.0",
          "description": "MMStar is an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMsâ€™ multi-modal capacities with carefully balanced and purified samples.",
          "url": "opencompass/opencompass_1175.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1175",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1175",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/MMStar-Benchmark/MMStar",
            "paperLink": "https://arxiv.org/pdf/2403.20330",
            "officialWebsiteLink": "https://mmstar-benchmark.github.io/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "173",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 11:27:36",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 11:27:36",
            "createDate": "2024-10-21 16:21:15",
            "desc": {
              "cn": "MMStar æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†ï¼ŒåŒ…å« 1,500 ä¸ªç»è¿‡äººå·¥ç²¾å¿ƒæŒ‘é€‰çš„æ ·æœ¬ã€‚MMStar è¯„ä¼° 6 é¡¹æ ¸å¿ƒèƒ½åŠ›å’Œ 18 ä¸ªè¯¦ç»†ç»´åº¦ï¼Œæ—¨åœ¨é€šè¿‡ç²¾å¿ƒå¹³è¡¡å’Œå‡€åŒ–çš„æ ·æœ¬ï¼Œè¯„ä¼° LVLM çš„å¤šæ¨¡æ€èƒ½åŠ›ã€‚",
              "en": "MMStar is an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMsâ€™ multi-modal capacities with carefully balanced and purified samples."
            }
          }
        },
        {
          "id": "opencompass_1084",
          "name": "StableToolBench",
          "version": "1.0.0",
          "description": "StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status.",
          "url": "opencompass/opencompass_1084.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1084",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1084",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/THUNLP-MT/StableToolBench",
            "paperLink": "https://aclanthology.org/2024.findings-acl.664.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "50186303",
              "name": "THUNLP-MT",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50186303-a2701a3a-208f-4cc9-88b7-e09515c70865.png",
              "nickname": "THUNLP-MT"
            },
            "lookNum": "172",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-29 16:24:50",
            "supportOnlineEval": false,
            "updateDate": "2024-09-29 16:24:50",
            "createDate": "2024-09-29 13:09:10",
            "desc": {
              "cn": "StableToolBench æ˜¯ä¸€ä¸ªä» ToolBench å‘å±•è€Œæ¥çš„åŸºå‡†ï¼Œæå‡ºäº†ä¸€ä¸ªè™šæ‹Ÿ API æœåŠ¡å™¨å’Œç¨³å®šçš„è¯„ä¼°ç³»ç»Ÿã€‚è™šæ‹Ÿ API æœåŠ¡å™¨åŒ…å«ä¸€ä¸ªç¼“å­˜ç³»ç»Ÿå’Œ API æ¨¡æ‹Ÿå™¨ï¼Œè¿™äº›ç»„ä»¶ç›¸è¾…ç›¸æˆï¼Œä»¥ç¼“è§£ API çŠ¶æ€å˜åŒ–å¸¦æ¥çš„å½±å“ã€‚",
              "en": "StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status."
            }
          }
        },
        {
          "id": "opencompass_1152",
          "name": "SportQA",
          "version": "1.0.0",
          "description": "SportQA aims to evaluate LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenariobased reasoning tasks.",
          "url": "opencompass/opencompass_1152.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1152",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1152",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/haotianxia/SportQA",
            "paperLink": "https://aclanthology.org/2024.naacl-long.283.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "172",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-27 17:23:16",
            "supportOnlineEval": false,
            "updateDate": "2024-10-27 17:23:16",
            "createDate": "2024-10-15 15:51:00",
            "desc": {
              "cn": "SportQA ä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½“è‚²ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚SportQA åŒ…å«è¶…è¿‡ 70,000 é“å¤šé¡¹é€‰æ‹©é¢˜ï¼Œåˆ†ä¸ºä¸‰ä¸ªä¸åŒçš„éš¾åº¦çº§åˆ«ï¼Œé’ˆå¯¹ä»åŸºæœ¬å†å²äº‹å®åˆ°å¤æ‚æƒ…å¢ƒæ¨ç†ä»»åŠ¡çš„å„ç§ä½“è‚²çŸ¥è¯†ã€‚",
              "en": "SportQA aims to evaluate LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenariobased reasoning tasks."
            }
          }
        },
        {
          "id": "opencompass_1098",
          "name": "GrailQA",
          "version": "1.0.0",
          "description": "GrailQA is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). ",
          "url": "opencompass/opencompass_1098.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1098",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1098",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/dki-lab/GrailQA",
            "paperLink": "https://arxiv.org/pdf/2011.07743",
            "officialWebsiteLink": "https://dki-lab.github.io/GrailQA/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "171",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-09 20:03:43",
            "supportOnlineEval": false,
            "updateDate": "2024-10-09 20:03:43",
            "createDate": "2024-10-09 11:14:58",
            "desc": {
              "cn": "GrailQA æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼Œç”¨äºçŸ¥è¯†åº“é—®ç­”ï¼ŒåŒ…å« 64,331 ä¸ªé—®é¢˜ï¼Œå¹¶é™„æœ‰ç­”æ¡ˆå’Œä¸åŒè¯­æ³•çš„ç›¸åº”é€»è¾‘å½¢å¼ã€‚",
              "en": "GrailQA is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). "
            }
          }
        },
        {
          "id": "opencompass_1125",
          "name": "Mind2Web",
          "version": "1.0.0",
          "description": "MIND2WEB is the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website.",
          "url": "opencompass/opencompass_1125.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1125",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1125",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/OSU-NLP-Group/Mind2Web",
            "paperLink": "https://arxiv.org/pdf/2306.06070",
            "officialWebsiteLink": "https://osu-nlp-group.github.io/Mind2Web/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "170",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-11 20:24:41",
            "supportOnlineEval": false,
            "updateDate": "2024-10-11 20:24:41",
            "createDate": "2024-10-11 13:51:20",
            "desc": {
              "cn": "MIND2WEB æ˜¯é¦–ä¸ªç”¨äºå¼€å‘å’Œè¯„ä¼°é€šç”¨ç½‘é¡µä»£ç†çš„æ•°æ®é›†ï¼Œèƒ½å¤Ÿæ ¹æ®è¯­è¨€æŒ‡ä»¤åœ¨ä»»ä½•ç½‘ç«™ä¸Šå®Œæˆå¤æ‚ä»»åŠ¡ã€‚",
              "en": "MIND2WEB is the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website."
            }
          }
        },
        {
          "id": "opencompass_1072",
          "name": "xCodeEval",
          "version": "1.0.0",
          "description": "xCodeEval is the largest executable multilingual multitask benchmark to date consisting of 25M document-level coding examples (16.5B tokens) from about 7.5K unique problems covering up to 11 programming languages with execution-level parallelism.",
          "url": "opencompass/opencompass_1072.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1072",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1072",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/ntunlp/xCodeEval",
            "paperLink": "https://arxiv.org/pdf/2303.03004",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "50187454",
              "name": "NTU-NLP",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50187454-16b303e1-3675-4b47-b9ec-83f20eef9b46.png",
              "nickname": "NTU-NLP"
            },
            "lookNum": "167",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-09-27 16:53:40",
            "supportOnlineEval": false,
            "updateDate": "2024-09-27 16:53:40",
            "createDate": "2024-09-27 12:40:47",
            "desc": {
              "cn": "xCodeEval æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¯æ‰§è¡Œå¤šè¯­è¨€å¤šä»»åŠ¡åŸºå‡†ï¼ŒåŒ…å« 2500 ä¸‡ä¸ªæ–‡æ¡£çº§ç¼–ç ç¤ºä¾‹ï¼ˆ165 äº¿ä¸ªæ ‡è®°ï¼‰ï¼Œæ¥è‡ªçº¦ 7500 ä¸ªç‹¬ç‰¹é—®é¢˜ï¼Œæ¶µç›–å¤šè¾¾ 11 ç§ç¼–ç¨‹è¯­è¨€ã€‚å®ƒåŒ…æ‹¬ 7 ä¸ªä»»åŠ¡ï¼Œæ¶‰åŠä»£ç ç†è§£ã€ç”Ÿæˆã€ç¿»è¯‘å’Œæ£€ç´¢ã€‚",
              "en": "xCodeEval is the largest executable multilingual multitask benchmark to date consisting of 25M document-level coding examples (16.5B tokens) from about 7.5K unique problems covering up to 11 programming languages with execution-level parallelism."
            }
          }
        },
        {
          "id": "opencompass_1116",
          "name": "AQUA-RAT",
          "version": "1.0.0",
          "description": "AQUA-RAT contains the algebraic word problems. The dataset consists of about 100,000 algebraic word problems with natural language rationales.",
          "url": "opencompass/opencompass_1116.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1116",
          "sample_count": 1000,
          "traits": [
            "Math"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1116",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/google-deepmind/AQuA",
            "paperLink": "https://arxiv.org/pdf/1705.04146",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "160",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-11 19:59:29",
            "supportOnlineEval": false,
            "updateDate": "2024-10-11 19:59:29",
            "createDate": "2024-10-10 14:20:23",
            "desc": {
              "cn": "AQUA-RAT åŒ…å«ä»£æ•°æ–‡å­—é—®é¢˜ã€‚è¯¥æ•°æ®é›†ç”±çº¦ 100,000 é“å¸¦æœ‰è‡ªç„¶è¯­è¨€æ¨ç†çš„ä»£æ•°æ–‡å­—é—®é¢˜ç»„æˆã€‚",
              "en": "AQUA-RAT contains the algebraic word problems. The dataset consists of about 100,000 algebraic word problems with natural language rationales."
            }
          }
        },
        {
          "id": "opencompass_1017",
          "name": "Yue_Benchmark",
          "version": "1.0.0",
          "description": "The benchmarks introduced for evaluating large language models (LLMs) on Cantonese include Yue-TruthfulQA, Yue-GSM8K, Yue-ARC-C, Yue-MMLU, and Yue-TRANS. Each of these benchmarks focuses on different aspects of language understanding and generation in Cantonese, offering a comprehensive means of assessing the capabilities of LLMs in handling this language.",
          "url": "opencompass/opencompass_1017.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1017",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1017",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ²",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [
              {
                "cn": "factual generation",
                "en": "factual generation"
              },
              {
                "cn": "complex reasoning",
                "en": "complex reasoning"
              },
              {
                "cn": "general knowledge",
                "en": "general knowledge"
              },
              {
                "cn": "mathematical logic",
                "en": "mathematical logic"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/jiangjyjy/Yue-Benchmark",
            "paperLink": "https://arxiv.org/abs/2408.16756",
            "officialWebsiteLink": "https://huggingface.co/datasets/BillBao/Yue-Benchmark",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "50157836",
              "name": null,
              "avatar": null,
              "nickname": "enemy"
            },
            "lookNum": "157",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-30 11:30:05",
            "supportOnlineEval": false,
            "updateDate": "2024-12-30 11:30:05",
            "createDate": "2024-12-28 23:41:20",
            "desc": {
              "cn": "Yue_Benchmarkç”¨äºè¯„ä¼°ç²¤è¯­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚è¯¥è¯„æµ‹é›†åŒ…å«ï¼šYue TruthtyQAã€Yue-GSM8Kã€Yue-ARC-Cã€Yue MMLUå’ŒYue TRANSï¼Œä¾§é‡äºç²¤è¯­è¯­è¨€ç†è§£å’Œç”Ÿæˆçš„ä¸åŒæ–¹é¢ï¼Œä¸ºè¯„ä¼°LLMç²¤è¯­èƒ½åŠ›æä¾›äº†ä¸€ç§å…¨é¢çš„æ–¹æ³•ã€‚",
              "en": "The benchmarks introduced for evaluating large language models (LLMs) on Cantonese include Yue-TruthfulQA, Yue-GSM8K, Yue-ARC-C, Yue-MMLU, and Yue-TRANS. Each of these benchmarks focuses on different aspects of language understanding and generation in Cantonese, offering a comprehensive means of assessing the capabilities of LLMs in handling this language."
            }
          }
        },
        {
          "id": "opencompass_1142",
          "name": "MIRAGE",
          "version": "1.0.0",
          "description": "MIRAGE is a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. ",
          "url": "opencompass/opencompass_1142.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1142",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1142",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "ACL 2024",
                "en": "ACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://teddy-xionggz.github.io/benchmark-medical-rag/",
            "paperLink": "https://aclanthology.org/2024.findings-acl.372.pdf",
            "officialWebsiteLink": "https://teddy-xionggz.github.io/benchmark-medical-rag/",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "153",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-19 18:04:07",
            "supportOnlineEval": false,
            "updateDate": "2024-10-19 18:04:07",
            "createDate": "2024-10-14 18:05:57",
            "desc": {
              "cn": "MIRAGE æ˜¯é¦–ä¸ªæ­¤ç±»åŸºå‡†ï¼ŒåŒ…å«æ¥è‡ªäº”ä¸ªåŒ»å­¦é—®ç­”æ•°æ®é›†çš„ 7,663 ä¸ªé—®é¢˜ã€‚",
              "en": "MIRAGE is a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. "
            }
          }
        },
        {
          "id": "opencompass_1251",
          "name": "PlanBench",
          "version": "1.0.0",
          "description": "PlanBench is an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. ",
          "url": "opencompass/opencompass_1251.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1251",
          "sample_count": 1000,
          "traits": [
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1251",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/karthikv792/LLMs-Planning",
            "paperLink": "https://arxiv.org/abs/2206.10498",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "147",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-30 16:29:45",
            "supportOnlineEval": false,
            "updateDate": "2024-12-30 16:29:45",
            "createDate": "2024-12-30 16:23:47",
            "desc": {
              "cn": "PlanBenchç”¨äºè¯„ä¼°LLMçš„è§„åˆ’èƒ½åŠ›ï¼ŒåŸºäºè‡ªåŠ¨åŒ–è§„åˆ’ç¤¾åŒºï¼ˆå°¤å…¶æ˜¯åœ¨å›½é™…è§„åˆ’ç«èµ›ï¼‰ä¸­æ¶‰åŠçš„å„ç§é¢†åŸŸæ¥æµ‹è¯•å¤§æ¨¡å‹åœ¨è§„åˆ’æˆ–æ¨ç†è¡ŒåŠ¨å’Œå˜æ›´æ–¹é¢çš„èƒ½åŠ›ã€‚",
              "en": "PlanBench is an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. "
            }
          }
        },
        {
          "id": "opencompass_1413",
          "name": "LiveCodeBench",
          "version": "1.0.0",
          "description": "LiveCodeBench evaluates LLMs' coding abilities. It continuously collects new problems over time from contests across LeetCode, AtCoder, and CodeForces. Notably, it also focuses on a broader range of code related capabilities besides code generation.",
          "url": "opencompass/opencompass_1413.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1413",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1413",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/LiveCodeBench/LiveCodeBench",
            "paperLink": "https://arxiv.org/abs/2403.07974",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "146",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:47:04",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:47:04",
            "createDate": "2025-01-17 20:40:50",
            "desc": {
              "cn": "LiveCodeBenchç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„ä»£ç èƒ½åŠ›ï¼ŒåŒ…å«æ¥è‡ªLeetCodeã€AtCoderå’ŒCodeForcesçš„åŠ¨æ€æ›´æ–°çš„é—®é¢˜ï¼Œå¹¶åœ¨ä»£ç ç”Ÿæˆèƒ½åŠ›çš„åŸºç¡€ä¸Šå°†æ›´å¹¿æ³›çš„ç›¸å…³èƒ½åŠ›çº³å…¥è€ƒé‡ã€‚",
              "en": "LiveCodeBench evaluates LLMs' coding abilities. It continuously collects new problems over time from contests across LeetCode, AtCoder, and CodeForces. Notably, it also focuses on a broader range of code related capabilities besides code generation."
            }
          }
        },
        {
          "id": "opencompass_1335",
          "name": "LTMbenchmark",
          "version": "1.0.0",
          "description": "LTMbenchmark assess the long-term memory, continual learning, and information integration capabilities of the agents via dynamic conversational tasks. ",
          "url": "opencompass/opencompass_1335.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1335",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1335",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/GoodAI/goodai-ltm-benchmark",
            "paperLink": "https://arxiv.org/abs/2409.20222",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "144",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 14:05:39",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 14:05:39",
            "createDate": "2025-01-03 16:10:04",
            "desc": {
              "cn": "LTMbenchmarké€šè¿‡åŠ¨æ€å¯¹è¯ä»»åŠ¡è¯„ä¼°æ™ºèƒ½ä½“çš„é•¿æœŸè®°å¿†ã€æŒç»­å­¦ä¹ å’Œä¿¡æ¯é›†æˆèƒ½åŠ›ã€‚",
              "en": "LTMbenchmark assess the long-term memory, continual learning, and information integration capabilities of the agents via dynamic conversational tasks. "
            }
          }
        },
        {
          "id": "opencompass_1252",
          "name": "RE-Bench",
          "version": "1.0.0",
          "description": "RE-Bench (Research Engineering Benchmark, v1) consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts.",
          "url": "opencompass/opencompass_1252.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1252",
          "sample_count": 1000,
          "traits": [
            "Agent",
            "Code",
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1252",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              },
              {
                "cn": "ä»£ç ",
                "en": "Code"
              },
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/METR/ai-rd-tasks/tree/main",
            "paperLink": "https://arxiv.org/abs/2411.15114",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "143",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-30 16:30:41",
            "supportOnlineEval": false,
            "updateDate": "2024-12-30 16:30:41",
            "createDate": "2024-12-30 16:24:14",
            "desc": {
              "cn": "RE-Benchç”¨äºè¯„ä¼°AIæ™ºèƒ½ä½“ç ”å‘çš„è‡ªåŠ¨åŒ–èƒ½åŠ›ï¼Œå®ƒç”±61ä½äººç±»ä¸“å®¶71æ¬¡åœ¨7ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾å¼MLç ”ç©¶å·¥ç¨‹ç¯å¢ƒä¸­çš„8å°æ—¶å°è¯•çš„æ•°æ®ç»„æˆã€‚",
              "en": "RE-Bench (Research Engineering Benchmark, v1) consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts."
            }
          }
        },
        {
          "id": "opencompass_1576",
          "name": "PhysReason",
          "version": "1.0.0",
          "description": "PhysReason is a comprehensive physics-based reasoning benchmark consisting of 1,200 physics problems spanning multiple domains, with a focus on both knowledge-based (25%) and reasoning-based (75%) questions.",
          "url": "opencompass/opencompass_1576.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1576",
          "sample_count": 1000,
          "traits": [
            "Examination",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1576",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2502.12054",
            "officialWebsiteLink": "https://dxzxy12138.github.io/PhysReason/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "139",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-04 16:59:45",
            "supportOnlineEval": false,
            "updateDate": "2025-03-04 16:59:45",
            "createDate": "2025-03-03 16:02:34",
            "desc": {
              "cn": "PhysReason æ˜¯ä¸€ä¸ªåŒ…å« 1,200 ä¸ªç‰©ç†é—®é¢˜çš„ç»¼åˆç‰©ç†æ¨ç†åŸºå‡†ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸï¼Œé‡ç‚¹å…³æ³¨åŸºäºçŸ¥è¯†ï¼ˆ25%ï¼‰å’Œæ¨ç†ï¼ˆ75%ï¼‰çš„é—®é¢˜ã€‚",
              "en": "PhysReason is a comprehensive physics-based reasoning benchmark consisting of 1,200 physics problems spanning multiple domains, with a focus on both knowledge-based (25%) and reasoning-based (75%) questions."
            }
          }
        },
        {
          "id": "opencompass_1641",
          "name": "MedAgents-Bench",
          "version": "1.0.0",
          "description": "MedAgentsBench features 862 challenging medical questions from seven datasets, focusing on cases where models struggle. It emphasizes multi-step clinical reasoning and addresses limitations in existing benchmarks by eliminating simple questions and standardizing evaluation protocols.",
          "url": "opencompass/opencompass_1641.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1641",
          "sample_count": 1000,
          "traits": [
            "Knowledge",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1641",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              },
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Medical",
                "en": "Medical"
              },
              {
                "cn": "Agent",
                "en": "Agent"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/gersteinlab/medagents-benchmark",
            "paperLink": "https://arxiv.org/abs/2503.07459",
            "officialWebsiteLink": "https://github.com/gersteinlab/medagents-benchmark",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "26204315",
              "name": null,
              "avatar": null,
              "nickname": "super-dainiu"
            },
            "lookNum": "136",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-17 13:59:33",
            "supportOnlineEval": false,
            "updateDate": "2025-03-17 13:59:33",
            "createDate": "2025-03-17 12:44:07",
            "desc": {
              "cn": "MedAgentsBenchæ˜¯ä¸€ä¸ªä¸“æ³¨äºå¤æ‚åŒ»å­¦æ¨ç†çš„åŸºå‡†æµ‹è¯•ï¼Œä»ä¸ƒä¸ªåŒ»å­¦æ•°æ®é›†ä¸­ç²¾é€‰äº†862ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚è¿™äº›æ•°æ®é›†åŒ…æ‹¬MedQAã€PubMedQAã€MedMCQAã€MedBulletsã€MedExQAã€MedXpertQAå’ŒMMLU/MMLU-Proï¼Œæ¶µç›–äº†ä»åŒ»å­¦æ‰§ç…§è€ƒè¯•åˆ°ç ”ç©¶æ–‡çŒ®çš„å¤šç§åŒ»å­¦é—®é¢˜ã€‚\nè¯¥åŸºå‡†é€‰æ‹©å°‘äº50%æ¨¡å‹èƒ½æ­£ç¡®å›ç­”çš„é—®é¢˜ï¼Œç¡®ä¿åŒ»å­¦çŸ¥è¯†é¢†åŸŸå…¨é¢è¦†ç›–ï¼Œå¹¶ä¼˜å…ˆé€‰æ‹©éœ€è¦å¤šæ­¥ä¸´åºŠæ¨ç†çš„é—®é¢˜ã€‚è¿™è§£å†³äº†ç°æœ‰è¯„ä¼°ä¸­ç®€å•é—®é¢˜æ™®éå­˜åœ¨ã€è¯„ä¼°åè®®ä¸ä¸€è‡´ï¼Œä»¥åŠç¼ºä¹æ€§èƒ½-æˆæœ¬-æ—¶é—´åˆ†æçš„å±€é™ã€‚",
              "en": "MedAgentsBench features 862 challenging medical questions from seven datasets, focusing on cases where models struggle. It emphasizes multi-step clinical reasoning and addresses limitations in existing benchmarks by eliminating simple questions and standardizing evaluation protocols."
            }
          }
        },
        {
          "id": "opencompass_1322",
          "name": "InfiBench",
          "version": "1.0.0",
          "description": "InfiBench is a large-scale freeform question-answering (QA) benchmark for code to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages.",
          "url": "opencompass/opencompass_1322.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1322",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1322",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/infi-coder/infibench-evaluation-harness/",
            "paperLink": "https://arxiv.org/abs/2404.07940",
            "officialWebsiteLink": "https://infi-coder.github.io/infibench/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "135",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 14:05:42",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 14:05:42",
            "createDate": "2024-12-31 15:59:24",
            "desc": {
              "cn": "InfiBenchç”¨äºè¯„æµ‹LLMå›ç­”ä»£ç ç›¸å…³é—®é¢˜çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¶µç›–15ç§ç¼–ç¨‹è¯­è¨€çš„234ä¸ªç²¾å¿ƒæŒ‘é€‰çš„é«˜è´¨é‡Stack Overflowé—®é¢˜ã€‚",
              "en": "InfiBench is a large-scale freeform question-answering (QA) benchmark for code to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages."
            }
          }
        },
        {
          "id": "opencompass_1562",
          "name": "BenchMAX",
          "version": "1.0.0",
          "description": "BenchMAX is a comprehensive, high-quality, and multiway parallel multilingual benchmark comprising 10 tasks designed to assess crucial capabilities across 17 diverse language.",
          "url": "opencompass/opencompass_1562.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1562",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1562",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/CONE-MT/BenchMAX",
            "paperLink": "https://arxiv.org/abs/2502.07346",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "134",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-03 10:56:52",
            "supportOnlineEval": false,
            "updateDate": "2025-03-03 10:56:52",
            "createDate": "2025-02-27 11:05:13",
            "desc": {
              "cn": "BenchMAX æ˜¯ä¸€ä¸ªå…¨é¢ã€é«˜è´¨é‡çš„å¤šå‘å¹¶è¡Œå¤šè¯­è¨€åŸºå‡†ï¼ŒåŒ…å« 10 ä¸ªä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼° 17 ç§ä¸åŒè¯­è¨€çš„å…³é”®èƒ½åŠ›ã€‚",
              "en": "BenchMAX is a comprehensive, high-quality, and multiway parallel multilingual benchmark comprising 10 tasks designed to assess crucial capabilities across 17 diverse language."
            }
          }
        },
        {
          "id": "opencompass_1283",
          "name": "P-MMEval",
          "version": "1.0.0",
          "description": "P-MMEval is a comprehensive multilingual multitask benchmark, covering effective fundamental and capability-specialized datasets.",
          "url": "opencompass/opencompass_1283.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1283",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1283",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2411.09116",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "43209637",
              "name": "Qwen",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/43209637-2eaaac15-94a9-4382-9047-16ae007f33f6.png",
              "nickname": "Qwen"
            },
            "lookNum": "132",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-25 19:59:39",
            "supportOnlineEval": false,
            "updateDate": "2024-12-25 19:59:39",
            "createDate": "2024-12-25 19:57:03",
            "desc": {
              "cn": "P-MMEvalæ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šè¯­è¨€å¤šä»»åŠ¡åŸºå‡†ï¼Œæ¶µç›–äº†é«˜æ•ˆçš„åŸºç¡€å’Œä¸“é¡¹èƒ½åŠ›æ•°æ®é›†ã€‚",
              "en": "P-MMEval is a comprehensive multilingual multitask benchmark, covering effective fundamental and capability-specialized datasets."
            }
          }
        },
        {
          "id": "opencompass_1334",
          "name": "MMDU",
          "version": "1.0.0",
          "description": "MMDU is intended for evaluating the multi-image multi-turn dialogue capabilities. It comprises 110 high-quality multi-image multi-turn dialogues with more than 1600 questions, each accompanied by detailed long-form answers. ",
          "url": "opencompass/opencompass_1334.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1334",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1334",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Liuziyu77/MMDU",
            "paperLink": "https://arxiv.org/abs/2406.11833",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "132",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 16:19:55",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 16:19:55",
            "createDate": "2025-01-03 14:51:38",
            "desc": {
              "cn": "MMDUç”¨äºè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šå›¾åƒå¤šè½®å¯¹è¯èƒ½åŠ›ï¼ŒåŒ…å«110ä¸ªé«˜è´¨é‡çš„å¤šå›¾åƒå¤šè½®å¯¹è¯ï¼Œç”±1600å¤šä¸ªé™„æœ‰è¯¦ç»†çš„é•¿ç¯‡ç­”æ¡ˆçš„é—®é¢˜ç»„æˆã€‚",
              "en": "MMDU is intended for evaluating the multi-image multi-turn dialogue capabilities. It comprises 110 high-quality multi-image multi-turn dialogues with more than 1600 questions, each accompanied by detailed long-form answers. "
            }
          }
        },
        {
          "id": "opencompass_1117",
          "name": "NaturalProofs",
          "version": "1.0.0",
          "description": "NATURALPROOFS is a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language. NATURALPROOFS unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization.",
          "url": "opencompass/opencompass_1117.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1117",
          "sample_count": 1000,
          "traits": [
            "Math"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1117",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/wellecks/naturalproofs#naturalproofs-dataset",
            "paperLink": "https://arxiv.org/pdf/2104.01112v2",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "132",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-11 19:59:32",
            "supportOnlineEval": false,
            "updateDate": "2024-10-11 19:59:32",
            "createDate": "2024-10-10 14:39:51",
            "desc": {
              "cn": "NATURALPROOFS æ˜¯ä¸€ä¸ªå¤šé¢†åŸŸçš„æ•°å­¦è¯­å¥åŠå…¶è¯æ˜çš„è¯­æ–™åº“ï¼Œé‡‡ç”¨è‡ªç„¶æ•°å­¦è¯­è¨€ç¼–å†™ã€‚æ•´åˆäº†å¹¿æ³›è¦†ç›–ã€æ·±å…¥è¦†ç›–å’Œä½èµ„æºæ•°å­¦æ¥æºï¼Œä¾¿äºè¯„ä¼°åˆ†å¸ƒå†…å’Œé›¶æ ·æœ¬æ³›åŒ–çš„èƒ½åŠ›ã€‚",
              "en": "NATURALPROOFS is a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language. NATURALPROOFS unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization."
            }
          }
        },
        {
          "id": "opencompass_1238",
          "name": "LINGOLY",
          "version": "1.0.0",
          "description": "Aiming at evaluating LLM's advanced reasoning abilities, LingOly is composed of  olympiad-level linguistic reasoning puzzles in low-resource and extinct languages. It covers more than 90 mostly low-resource languages, and contains 1,133 problems across 6 formats and 5 levels of human difficulty.",
          "url": "opencompass/opencompass_1238.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1238",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1238",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/am-bean/lingOly",
            "paperLink": "https://arxiv.org/abs/2406.06196",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "131",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-25 11:46:22",
            "supportOnlineEval": false,
            "updateDate": "2024-12-25 11:46:22",
            "createDate": "2024-12-20 14:03:59",
            "desc": {
              "cn": "LingOlyç”±ä½èµ„æºå’Œå·²ç­ç»è¯­è¨€çš„å¥¥æ—åŒ¹å…‹çº§åˆ«è¯­è¨€æ¨ç†è°œé¢˜ç»„æˆï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„é«˜çº§æ¨ç†èƒ½åŠ›ã€‚å…¶ä¸­æ¶µç›–äº†90å¤šç§è¯­è¨€ï¼Œå…±æœ‰1133ä¸ªæ¶‰åŠ6ç§æ ¼å¼å’Œ5ä¸ªäººå·¥éš¾åº¦çº§åˆ«çš„é—®é¢˜ã€‚",
              "en": "Aiming at evaluating LLM's advanced reasoning abilities, LingOly is composed of  olympiad-level linguistic reasoning puzzles in low-resource and extinct languages. It covers more than 90 mostly low-resource languages, and contains 1,133 problems across 6 formats and 5 levels of human difficulty."
            }
          }
        },
        {
          "id": "opencompass_1246",
          "name": "LiveBench",
          "version": "1.0.0",
          "description": "LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. ",
          "url": "opencompass/opencompass_1246.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1246",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1246",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/livebench/livebench",
            "paperLink": "https://arxiv.org/abs/2406.19314",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "128",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-22 18:09:35",
            "supportOnlineEval": false,
            "updateDate": "2024-12-22 18:09:35",
            "createDate": "2024-12-20 19:35:51",
            "desc": {
              "cn": "LiveBenchæ˜¯ä¸€ä¸ªLLMåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–æ•°å­¦ã€ç¼–ç ã€æ¨ç†ã€è¯­è¨€ã€æŒ‡ä»¤éµå¾ªå’Œæ•°æ®åˆ†æï¼ŒåŒ…å«åŸºäºæœ€è¿‘å‘å¸ƒçš„æ•°å­¦ç«èµ›ã€arXiv è®ºæ–‡ã€æ–°é—»æ–‡ç« å’Œæ•°æ®é›†çš„é—®é¢˜ï¼ŒåŠç»å…¸åŸºå‡†æµ‹è¯•ï¼ˆå¦‚ Big-Bench Hardã€AMPS å’Œ IFEvalï¼‰çš„æ›´éš¾ã€æ— æ±¡æŸ“çš„ä»»åŠ¡ã€‚",
              "en": "LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. "
            }
          }
        },
        {
          "id": "opencompass_1107",
          "name": "StrategyQA",
          "version": "1.0.0",
          "description": "STRATEGYQA is a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy",
          "url": "opencompass/opencompass_1107.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1107",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1107",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/eladsegal/strategyqa",
            "paperLink": "https://arxiv.org/pdf/2101.02235",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "125",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-09 20:03:28",
            "supportOnlineEval": false,
            "updateDate": "2024-10-09 20:03:28",
            "createDate": "2024-10-09 17:54:13",
            "desc": {
              "cn": "STRATEGYQA æ˜¯ä¸€ä¸ªé—®ç­”åŸºå‡†ï¼Œå…¶ä¸­æ‰€éœ€çš„æ¨ç†æ­¥éª¤åœ¨é—®é¢˜ä¸­æ˜¯éšå«çš„ï¼Œå¯ä»¥é€šè¿‡ç­–ç•¥è¿›è¡Œæ¨æ–­ã€‚",
              "en": "STRATEGYQA is a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy"
            }
          }
        },
        {
          "id": "opencompass_1239",
          "name": "CVQA",
          "version": "1.0.0",
          "description": "CVQA is a new culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures. It includes culturally-driven images and 10k questions from across 30 countries on 4 continents.",
          "url": "opencompass/opencompass_1239.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1239",
          "sample_count": 1000,
          "traits": [
            "Knowledge",
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1239",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2406.05967",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "124",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-30 16:30:03",
            "supportOnlineEval": false,
            "updateDate": "2024-12-30 16:30:03",
            "createDate": "2024-12-30 16:18:03",
            "desc": {
              "cn": "CVQAæ˜¯ä¸€ç§æ–°çš„æ–‡åŒ–å¤šå…ƒåŒ–å¤šè¯­è¨€è§†è§‰é—®ç­”åŸºå‡†ï¼Œæ—¨åœ¨æ¶µç›–ä¸°å¯Œçš„è¯­è¨€å’Œæ–‡åŒ–ï¼ŒåŒ…æ‹¬æ¥è‡ªå››å¤§æ´²30ä¸ªå›½å®¶/åœ°åŒºçš„æ–‡åŒ–å‘å›¾åƒå’Œ10000ä¸ªé—®é¢˜ã€‚",
              "en": "CVQA is a new culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures. It includes culturally-driven images and 10k questions from across 30 countries on 4 continents."
            }
          }
        },
        {
          "id": "opencompass_1146",
          "name": "BUST",
          "version": "1.0.0",
          "description": "BUST is a comprehensive benchmark for evaluating synthetic text detectors, focusing on their effectiveness against outputs from various Large Language Models (LLMs). ",
          "url": "opencompass/opencompass_1146.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1146",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1146",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/IDSIA-NLP/BUST",
            "paperLink": "https://aclanthology.org/2024.naacl-long.444.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "50126080",
              "name": "IDSIA-NLP",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50126080-9bda56f6-aaad-43e3-86d5-1278eb268d20.png",
              "nickname": "OpenXLab-rR7CGPrkS"
            },
            "lookNum": "123",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-27 17:23:25",
            "supportOnlineEval": false,
            "updateDate": "2024-10-27 17:23:25",
            "createDate": "2024-10-15 13:46:56",
            "desc": {
              "cn": "BUST æ˜¯ä¸€ä¸ªç»¼åˆåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°åˆæˆæ–‡æœ¬æ£€æµ‹å™¨ï¼ŒBUST ä½¿ç”¨å¤šç§æŒ‡æ ‡æ¥è¯„ä¼°æ£€æµ‹å™¨ï¼ŒåŒ…æ‹¬è¯­è¨€ç‰¹å¾ã€å¯è¯»æ€§å’Œä½œè€…æ€åº¦ã€‚",
              "en": "BUST is a comprehensive benchmark for evaluating synthetic text detectors, focusing on their effectiveness against outputs from various Large Language Models (LLMs). "
            }
          }
        },
        {
          "id": "opencompass_1355",
          "name": "HallusionBench",
          "version": "1.0.0",
          "description": "HallusionBench is a comprehensive benchmark designed for the evaluation of image-context reasoning, comprising 346 images paired with 1129 questions.",
          "url": "opencompass/opencompass_1355.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1355",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1355",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/tianyi-lab/HallusionBench",
            "paperLink": "https://arxiv.org/abs/2310.14566",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "122",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 11:27:39",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 11:27:39",
            "createDate": "2025-01-09 20:08:24",
            "desc": {
              "cn": "HallusionBenchæ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°å›¾åƒä¸Šä¸‹æ–‡æ¨ç†è€Œè®¾è®¡çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬346å¼ å›¾åƒå’Œ1129ä¸ªé—®é¢˜ã€‚",
              "en": "HallusionBench is a comprehensive benchmark designed for the evaluation of image-context reasoning, comprising 346 images paired with 1129 questions."
            }
          }
        },
        {
          "id": "opencompass_1113",
          "name": "SVAMP",
          "version": "1.0.0",
          "description": "SVAMP includes one-unknown arithmetic word problems with grade level up to 4 by applying simple variations over word problems in an existing dataset. SVAMP further highlights the brittle nature of existing models when trained on these benchmark datasets.",
          "url": "opencompass/opencompass_1113.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1113",
          "sample_count": 1000,
          "traits": [
            "Math"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1113",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 1,
            "githubLink": "https://github.com/arkilpatel/SVAMP",
            "paperLink": "https://arxiv.org/pdf/2103.07191v2",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "121",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-11 19:59:21",
            "supportOnlineEval": false,
            "updateDate": "2024-10-11 19:59:21",
            "createDate": "2024-10-10 13:41:55",
            "desc": {
              "cn": "SVAMP æ˜¯ä¸€ä¸ªåŒ…å«ç®—æœ¯æ–‡å­—é—®é¢˜çš„æ•°æ®é›†ï¼Œæœ€é«˜é€‚ç”¨äºå››å¹´çº§çš„å­¦ç”Ÿï¼Œæ˜¯é€šè¿‡å¯¹ç°æœ‰æ•°æ®é›†ä¸­çš„æ–‡å­—é—®é¢˜åº”ç”¨ç®€å•å˜ä½“è€Œç”Ÿæˆã€‚",
              "en": "SVAMP includes one-unknown arithmetic word problems with grade level up to 4 by applying simple variations over word problems in an existing dataset. SVAMP further highlights the brittle nature of existing models when trained on these benchmark datasets."
            }
          }
        },
        {
          "id": "opencompass_1572",
          "name": "LOKI",
          "version": "1.0.0",
          "description": "LOKI, a multimodal synthetic data detection benchmark, designed specifically to comprehensively assess the capabilities of LMMs in detecting synthetic data.",
          "url": "opencompass/opencompass_1572.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1572",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1572",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/opendatalab/LOKI",
            "paperLink": "https://arxiv.org/abs/2410.09732",
            "officialWebsiteLink": "https://opendatalab.github.io/LOKI/#fingding",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "118",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-18 15:43:49",
            "supportOnlineEval": false,
            "updateDate": "2025-03-18 15:43:49",
            "createDate": "2025-03-18 15:43:15",
            "desc": {
              "cn": "LOKIæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åˆæˆæ•°æ®æ£€æµ‹åŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå…¨é¢è¯„ä¼° LMMs åœ¨æ£€æµ‹åˆæˆæ•°æ®æ–¹é¢çš„èƒ½åŠ›ã€‚",
              "en": "LOKI, a multimodal synthetic data detection benchmark, designed specifically to comprehensively assess the capabilities of LMMs in detecting synthetic data."
            }
          }
        },
        {
          "id": "opencompass_1541",
          "name": "VLM2-Bench",
          "version": "1.0.0",
          "description": "VLMÂ²-Bench is the first comprehensive benchmark that evaluates vision-language models' (VLMs) ability to visually link matching cues across multi-image sequences and videos. The benchmark consists of 9 subtasks with over 3,000 test cases.",
          "url": "opencompass/opencompass_1541.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1541",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1541",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/vlm2-bench/VLM2-Bench",
            "paperLink": "https://arxiv.org/abs/2502.12084",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "117",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-02-26 18:53:47",
            "supportOnlineEval": false,
            "updateDate": "2025-02-26 18:53:47",
            "createDate": "2025-02-25 20:23:33",
            "desc": {
              "cn": "VLMÂ²-Bench æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šå›¾åƒåºåˆ—å’Œè§†é¢‘ä¸­è§†è§‰é“¾æ¥åŒ¹é…çº¿ç´¢èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…æ‹¬ 9 ä¸ªå­ä»»åŠ¡ï¼Œè¶…è¿‡ 3000 ä¸ªæµ‹è¯•æ¡ˆä¾‹ï¼Œæ—¨åœ¨è¯„ä¼°äººç±»æ—¥å¸¸ä½¿ç”¨çš„æ ¹æœ¬è§†è§‰é“¾æ¥èƒ½åŠ›ã€‚",
              "en": "VLMÂ²-Bench is the first comprehensive benchmark that evaluates vision-language models' (VLMs) ability to visually link matching cues across multi-image sequences and videos. The benchmark consists of 9 subtasks with over 3,000 test cases."
            }
          }
        },
        {
          "id": "opencompass_1349",
          "name": "CyberSecEval",
          "version": "1.0.0",
          "description": "CyberSecEval is a comprehensive benchmark developed to help bolster the cybersecurity of LLMs. It provides a thorough evaluation in two crucial security domains: the propensity to generate insecure code and the level of compliance when asked to assist in cyberattacks.",
          "url": "opencompass/opencompass_1349.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1349",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1349",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks",
            "paperLink": "https://arxiv.org/abs/2312.04724",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "116",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:23:31",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:23:31",
            "createDate": "2025-01-09 12:17:41",
            "desc": {
              "cn": "CyberSecEvalæ—¨åœ¨è¯„ä¼°LLMçš„å®‰å…¨æ€§ï¼Œèšç„¦äºå¤§æ¨¡å‹ç”Ÿæˆä¸å®‰å…¨ä»£ç çš„å€¾å‘ä»¥åŠå½“è¢«è¦æ±‚ååŠ©ç½‘ç»œæ”»å‡»æ—¶çš„åˆè§„æ€§æ°´å¹³ã€‚",
              "en": "CyberSecEval is a comprehensive benchmark developed to help bolster the cybersecurity of LLMs. It provides a thorough evaluation in two crucial security domains: the propensity to generate insecure code and the level of compliance when asked to assist in cyberattacks."
            }
          }
        },
        {
          "id": "opencompass_1643",
          "name": "Creation-MMBench",
          "version": "1.0.0",
          "description": "A multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs. It features three main aspects: 1. Comprehensive Creation Benchmark for MLLM and LLM. 2. Robust Evaluation Methodology. 3. Attractive Experiment Insight. ",
          "url": "opencompass/opencompass_1643.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1643",
          "sample_count": 1000,
          "traits": [
            "Creation",
            "Long-Context",
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "medium",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1643",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "é•¿æ–‡æœ¬",
                "en": "Long-Context"
              },
              {
                "cn": "åˆ›ä½œ",
                "en": "Creation"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 3,
            "githubLink": "https://github.com/open-compass/Creation-MMBench",
            "paperLink": "",
            "officialWebsiteLink": "https://open-compass.github.io/Creation-MMBench/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "40047277",
              "name": "fangxy-09",
              "avatar": "https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKFASuxOhldeP1BsgbQewWr71yNBNAehdVp5Qqrxq0D6hQ0libbrYZs9n5GVtoicy4uOvtNrmebicSKQ/132",
              "nickname": "FangXinyu-0913"
            },
            "lookNum": "112",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-05-06 11:00:38",
            "supportOnlineEval": false,
            "updateDate": "2025-05-06 11:00:38",
            "createDate": "2025-04-30 12:29:15",
            "desc": {
              "cn": "ä¸“ä¸ºè¯„ä¼° å¤šæ¨¡æ€å¤§æ¨¡å‹ çš„åˆ›ä½œèƒ½åŠ›è€Œè®¾è®¡çš„å¤šæ¨¡æ€åŸºå‡†ã€‚é‡‡ç”¨ä¸¤ä¸ªä¸åŒæŒ‡æ ‡å¯¹æ¨¡å‹çš„åŸºç¡€æ„ŸçŸ¥èƒ½åŠ›å’Œæ·±å±‚æ¬¡è§†è§‰åˆ›ä½œèƒ½åŠ›è¿›è¡Œè¯„ä¼°ï¼Œé‡‡ç”¨GPT-4oä½œä¸ºè¯„åˆ¤æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚",
              "en": "A multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs. It features three main aspects: 1. Comprehensive Creation Benchmark for MLLM and LLM. 2. Robust Evaluation Methodology. 3. Attractive Experiment Insight. "
            }
          }
        },
        {
          "id": "opencompass_1543",
          "name": "KITAB-Bench",
          "version": "1.0.0",
          "description": "KITAB-Bench is a Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding,  and spans 36 sub-domains with over 8,809 samples, carefully curated to rigorously evaluate essential skills required for Arabic OCR and document analysis.",
          "url": "opencompass/opencompass_1543.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1543",
          "sample_count": 1000,
          "traits": [
            "Language",
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1543",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/mbzuai-oryx/KITAB-Bench",
            "paperLink": "https://arxiv.org/abs/2502.14949",
            "officialWebsiteLink": "https://mbzuai-oryx.github.io/KITAB-Bench/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "110",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-05 13:36:30",
            "supportOnlineEval": false,
            "updateDate": "2025-03-05 13:36:30",
            "createDate": "2025-03-05 13:36:03",
            "desc": {
              "cn": "KITAB-Benchæ˜¯ä¸€ä¸ªå…¨é¢å¤šé¢†åŸŸé˜¿æ‹‰ä¼¯æ–‡ OCR å’Œæ–‡æ¡£ç†è§£åŸºå‡†ï¼ŒåŒ…å« 36 ä¸ªå­é¢†åŸŸï¼Œè¶…è¿‡ 8,809 ä¸ªæ ·æœ¬ï¼Œç»è¿‡ç²¾å¿ƒæŒ‘é€‰ï¼Œä»¥ä¸¥æ ¼è¯„ä¼°é˜¿æ‹‰ä¼¯ OCR å’Œæ–‡æ¡£åˆ†ææ‰€éœ€çš„åŸºæœ¬æŠ€èƒ½ã€‚",
              "en": "KITAB-Bench is a Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding,  and spans 36 sub-domains with over 8,809 samples, carefully curated to rigorously evaluate essential skills required for Arabic OCR and document analysis."
            }
          }
        },
        {
          "id": "opencompass_1103",
          "name": "QASC",
          "version": "1.0.0",
          "description": "QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.",
          "url": "opencompass/opencompass_1103.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1103",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1103",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/allenai/qasc",
            "paperLink": "https://arxiv.org/pdf/1910.11473",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "110",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-09 20:03:32",
            "supportOnlineEval": false,
            "updateDate": "2024-10-09 20:03:32",
            "createDate": "2024-10-09 16:45:32",
            "desc": {
              "cn": "QASC æ˜¯ä¸€ä¸ªä¸“æ³¨äºå¥å­ç»„åˆçš„é—®ç­”æ•°æ®é›†ã€‚å®ƒåŒ…å« 9,980 é“å°å­¦ç§‘å­¦çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆ8,134 é“ç”¨äºè®­ç»ƒï¼Œ926 é“ç”¨äºå¼€å‘ï¼Œ920 é“ç”¨äºæµ‹è¯•ï¼‰ï¼Œå¹¶é…æœ‰ä¸€ä¸ªåŒ…å« 1,700 ä¸‡ä¸ªå¥å­çš„è¯­æ–™åº“ã€‚",
              "en": "QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences."
            }
          }
        },
        {
          "id": "opencompass_1242",
          "name": "AgentBoard",
          "version": "1.0.0",
          "description": "AgentBoard is tailored to analytical evaluation of LLM agents. It offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization.",
          "url": "opencompass/opencompass_1242.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1242",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1242",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/hkust-nlp/AgentBoard/tree/main",
            "paperLink": "https://arxiv.org/abs/2401.13178",
            "officialWebsiteLink": "https://hkust-nlp.github.io/agentboard/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "109",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-25 11:46:29",
            "supportOnlineEval": false,
            "updateDate": "2024-12-25 11:46:29",
            "createDate": "2024-12-20 17:19:44",
            "desc": {
              "cn": "AgentBoardä¸“ç”¨äºLLM Agentçš„åˆ†æè¯„ä¼°ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªç²¾ç»†æŒ‡æ ‡ç”¨äºæ•è·å¢é‡è¿›æ­¥ï¼Œä»¥åŠä¸€ä¸ªå…¨é¢çš„è¯„ä¼°å·¥å…·åŒ…ï¼Œèƒ½åŸºäºäº¤äº’å¼å¯è§†åŒ–è¯„ä¼°è¿›è¡Œå¤šæ–¹é¢åˆ†æã€‚",
              "en": "AgentBoard is tailored to analytical evaluation of LLM agents. It offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization."
            }
          }
        },
        {
          "id": "opencompass_1357",
          "name": "MME",
          "version": "1.0.0",
          "description": "MME is a comprehensive MLLM evaluation benchmark. It measures both perception and cognition abilities on a total of 14 subtasks.",
          "url": "opencompass/opencompass_1357.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1357",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1357",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation",
            "paperLink": "https://arxiv.org/abs/2306.13394",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "108",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 16:03:56",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 16:03:56",
            "createDate": "2025-01-09 20:56:27",
            "desc": {
              "cn": "MMEæ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€å¤§æ¨¡å‹è¯„ä¼°åŸºå‡†ï¼Œæ¶µç›–14 ä¸ªè€ƒå¯Ÿæ„ŸçŸ¥å’Œè®¤çŸ¥èƒ½åŠ›çš„å­ä»»åŠ¡ã€‚",
              "en": "MME is a comprehensive MLLM evaluation benchmark. It measures both perception and cognition abilities on a total of 14 subtasks."
            }
          }
        },
        {
          "id": "opencompass_1000",
          "name": "Q-Bench",
          "version": "1.0.0",
          "description": "Q-Bench/Q-Bench+ is a benchmark for general-purpose foundation models on low-level vision.",
          "url": "opencompass/opencompass_1000.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1000",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1000",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "ğŸ²",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [
              {
                "cn": "yes-or-no",
                "en": "yes-or-no"
              },
              {
                "cn": "what",
                "en": "what"
              },
              {
                "cn": "how",
                "en": "how"
              },
              {
                "cn": "distortion",
                "en": "distortion"
              },
              {
                "cn": "others",
                "en": "others"
              },
              {
                "cn": "in-context distortion",
                "en": "in-context distortion"
              },
              {
                "cn": "in-context others",
                "en": "in-context others"
              },
              {
                "cn": "overall",
                "en": "overall"
              }
            ],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Q-Future/Q-Bench",
            "paperLink": "https://arxiv.org/abs/2309.14181",
            "officialWebsiteLink": "https://q-future.github.io/Q-Bench",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "50151145",
              "name": null,
              "avatar": null,
              "nickname": "Orange"
            },
            "lookNum": "108",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": null,
            "supportOnlineEval": false,
            "updateDate": "2024-08-16 20:27:46",
            "createDate": "2024-08-13 09:33:41",
            "desc": {
              "cn": "Q-Bench/Q-Bench+æ˜¯ä¸€ä¸ªé¢å‘å¤šæ¨¡æ€å¤§æ¨¡å‹åº•å±‚è§†è§‰ç†è§£çš„æ•°æ®é›†ã€‚æ­¤æ•°æ®é›†ä»åº•å±‚è§†è§‰çš„æ„ŸçŸ¥ã€æè¿°ã€è¯„ä»·èƒ½åŠ›å‡ºå‘æ¥å¯¹å¤šæ¨¡æ€å¤§æ¨¡å‹è¿›è¡Œå®Œæ•´çš„æµ‹è¯•ï¼Œæµ‹è¯•çš„å¯¹è±¡æ—¢åŒ…æ‹¬å•å¼ å›¾åƒä¹ŸåŒ…æ‹¬å›¾åƒå¯¹ã€‚",
              "en": "Q-Bench/Q-Bench+ is a benchmark for general-purpose foundation models on low-level vision."
            }
          }
        },
        {
          "id": "opencompass_1564",
          "name": "EmbodiedBench",
          "version": "1.0.0",
          "description": "EmbodiedBench is a comprehensive benchmark designed to evaluate Multi-modal Large Language Models (MLLMs) as embodied agents.",
          "url": "opencompass/opencompass_1564.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1564",
          "sample_count": 1000,
          "traits": [
            "Agent",
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1564",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/EmbodiedBench/EmbodiedBench",
            "paperLink": "https://arxiv.org/abs/2502.09560",
            "officialWebsiteLink": "https://embodiedbench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "106",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-03 11:00:23",
            "supportOnlineEval": false,
            "updateDate": "2025-03-03 11:00:23",
            "createDate": "2025-02-27 11:25:40",
            "desc": {
              "cn": "EmbodiedBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºå…·èº«æ™ºèƒ½ä½“çš„å…¨é¢åŸºå‡†ã€‚",
              "en": "EmbodiedBench is a comprehensive benchmark designed to evaluate Multi-modal Large Language Models (MLLMs) as embodied agents."
            }
          }
        },
        {
          "id": "opencompass_1617",
          "name": "IFIR",
          "version": "1.0.0",
          "description": "IFIR is the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature.",
          "url": "opencompass/opencompass_1617.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1617",
          "sample_count": 1000,
          "traits": [
            "Instruct"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1617",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æŒ‡ä»¤è·Ÿéš",
                "en": "Instruct"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/SighingSnow/IFIR",
            "paperLink": "https://arxiv.org/abs/2503.04644",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "106",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-10 17:11:56",
            "supportOnlineEval": false,
            "updateDate": "2025-03-10 17:11:56",
            "createDate": "2025-03-10 11:01:15",
            "desc": {
              "cn": " IFIRï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ä¸“å®¶é¢†åŸŸæŒ‡ä»¤è·Ÿéšä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰çš„ç»¼åˆåŸºå‡†ã€‚IFIR åŒ…å« 2,426 ä¸ªé«˜è´¨é‡ç¤ºä¾‹ï¼Œæ¶µç›–å››ä¸ªä¸“ä¸šé¢†åŸŸï¼ˆé‡‘èã€æ³•å¾‹ã€åŒ»ç–—ä¿å¥å’Œç§‘å­¦æ–‡çŒ®ï¼‰çš„å…«ä¸ªå­é›†ã€‚",
              "en": "IFIR is the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature."
            }
          }
        },
        {
          "id": "opencompass_1367",
          "name": "A-OKVQA",
          "version": "1.0.0",
          "description": "A-OKVQA assesses commonsense reasoning abilities. It is a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer.",
          "url": "opencompass/opencompass_1367.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1367",
          "sample_count": 1000,
          "traits": [
            "Knowledge",
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1367",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              },
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "VQA",
                "en": "VQA"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/allenai/aokvqa",
            "paperLink": "https://arxiv.org/abs/2206.01718",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "106",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:23:39",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:23:39",
            "createDate": "2025-01-10 12:28:01",
            "desc": {
              "cn": "A-OKVQAç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å¸¸è¯†åŠæ¨ç†èƒ½åŠ›ï¼Œç”±25Kä¸ªä¸åŒçš„é—®é¢˜ç»„æˆï¼Œéœ€è¦å¯¹å›¾åƒä¸­æè¿°çš„åœºæ™¯è¿›è¡ŒæŸç§å½¢å¼çš„å¸¸è¯†æ€§æ¨ç†æ¥å›ç­”ã€‚",
              "en": "A-OKVQA assesses commonsense reasoning abilities. It is a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer."
            }
          }
        },
        {
          "id": "opencompass_1532",
          "name": "ZeroBench",
          "version": "1.0.0",
          "description": "ZeroBench is a challenging visual reasoning benchmark for LMMs. It consists of a main set of 100 high-quality, manually curated questions covering numerous domains, reasoning types and image type. Questions have been designed and calibrated to be beyond the capabilities of current frontier models.\n",
          "url": "opencompass/opencompass_1532.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1532",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1532",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/jonathan-roberts1/zerobench",
            "paperLink": "https://arxiv.org/abs/2502.09696",
            "officialWebsiteLink": "https://zerobench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "106",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-02-25 19:54:07",
            "supportOnlineEval": false,
            "updateDate": "2025-02-25 19:54:07",
            "createDate": "2025-02-24 13:40:00",
            "desc": {
              "cn": "ZeroBench æ˜¯é’ˆå¯¹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æ¨ç†åŸºå‡†ã€‚å®ƒç”±ä¸€ç»„ä¸»è¦çš„ 100 ä¸ªé«˜è´¨é‡äººå·¥é—®é¢˜ç»„æˆï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸã€æ¨ç†ç±»å‹å’Œå›¾åƒç±»å‹ã€‚ZeroBench ä¸­çš„é—®é¢˜ç»è¿‡è®¾è®¡å’Œæ ¡å‡†ï¼Œå·²ç»è¶…å‡ºäº†å½“å‰å‰æ²¿æ¨¡å‹çš„èƒ½åŠ›èŒƒå›´ã€‚",
              "en": "ZeroBench is a challenging visual reasoning benchmark for LMMs. It consists of a main set of 100 high-quality, manually curated questions covering numerous domains, reasoning types and image type. Questions have been designed and calibrated to be beyond the capabilities of current frontier models.\n"
            }
          }
        },
        {
          "id": "opencompass_1546",
          "name": "CodeCriticBench",
          "version": "1.0.0",
          "description": "CodeCriticBench assesses LLMs' critiquing ability in code generation and QA tasks. Covering 10 criteria, it features a 4.3k-samples dataset with three difficulty levels and balanced distribution.",
          "url": "opencompass/opencompass_1546.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1546",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1546",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/multimodal-art-projection/CodeCriticBench",
            "paperLink": "https://arxiv.org/abs/2502.16614",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "105",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-02-26 18:41:24",
            "supportOnlineEval": false,
            "updateDate": "2025-02-26 18:41:24",
            "createDate": "2025-02-26 11:00:09",
            "desc": {
              "cn": "CodeCriticBench ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°LLMsåœ¨ä»£ç ç”Ÿæˆå’Œä»£ç é—®ç­”ä»»åŠ¡ä¸­çš„æ‰¹è¯„èƒ½åŠ›ã€‚å…¶æ¶µç›– 10 ä¸ªä¸åŒçš„æ ‡å‡†ï¼Œæ•°æ®é›†æ ¹æ®éš¾åº¦åˆ†ä¸ºä¸‰ä¸ªç­‰çº§ï¼Œå…±åŒ…å«4.3kä¸ªæ ·æœ¬ï¼Œç¡®ä¿äº†éš¾åº¦çº§åˆ«çš„å¹³è¡¡åˆ†å¸ƒã€‚",
              "en": "CodeCriticBench assesses LLMs' critiquing ability in code generation and QA tasks. Covering 10 criteria, it features a 4.3k-samples dataset with three difficulty levels and balanced distribution."
            }
          }
        },
        {
          "id": "opencompass_1333",
          "name": "RedCode",
          "version": "1.0.0",
          "description": "RedCode provides comprehensive and practical evaluations on the safety of code agents, including 4,050 risky test cases covering 25 types of critical vulnerabilities spanning 8 domains and 160 prompts aiming to generate harmful code or software.",
          "url": "opencompass/opencompass_1333.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1333",
          "sample_count": 1000,
          "traits": [
            "Agent",
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1333",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              },
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/AI-secure/RedCode",
            "paperLink": "https://arxiv.org/abs/2411.07781",
            "officialWebsiteLink": "https://redcode-agent.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "104",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 11:12:36",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 11:12:36",
            "createDate": "2025-01-10 19:41:07",
            "desc": {
              "cn": "RedCodeæ—¨åœ¨ä¸ºLLMä»£ç æ™ºèƒ½ä½“çš„å®‰å…¨æ€§æä¾›å…¨é¢å®ç”¨çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬æ¥è‡ª8ä¸ªé¢†åŸŸ25ç§å…³é”®æ¼æ´çš„4050ä¸ªé£é™©æµ‹è¯•ç”¨ä¾‹ï¼Œä»¥åŠ160ä¸ªç”Ÿæˆæœ‰å®³ä»£ç çš„æç¤ºã€‚",
              "en": "RedCode provides comprehensive and practical evaluations on the safety of code agents, including 4,050 risky test cases covering 25 types of critical vulnerabilities spanning 8 domains and 160 prompts aiming to generate harmful code or software."
            }
          }
        },
        {
          "id": "opencompass_1250",
          "name": "Collie",
          "version": "1.0.0",
          "description": "COLLIE is a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning).",
          "url": "opencompass/opencompass_1250.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1250",
          "sample_count": 1000,
          "traits": [
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1250",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/princeton-nlp/Collie",
            "paperLink": "https://arxiv.org/abs/2307.08689",
            "officialWebsiteLink": "https://collie-benchmark.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "103",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-30 16:29:49",
            "supportOnlineEval": false,
            "updateDate": "2024-12-30 16:29:49",
            "createDate": "2024-12-30 16:23:20",
            "desc": {
              "cn": "COLLIEç”¨äºè¯„ä¼°å¤§æ¨¡å‹åœ¨çº¦æŸæ€§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¯æŒ‡å®šå…·æœ‰ä¸åŒç”Ÿæˆçº§åˆ«ï¼ˆå•è¯ã€å¥å­ã€æ®µè½ã€æ®µè½ï¼‰å’Œå»ºæ¨¡æŒ‘æˆ˜ï¼ˆä¾‹å¦‚ï¼Œè¯­è¨€ç†è§£ã€é€»è¾‘æ¨ç†ã€è®¡æ•°ã€è¯­ä¹‰è§„åˆ’ï¼‰çš„ä¸°å¯Œç»„åˆã€‚",
              "en": "COLLIE is a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning)."
            }
          }
        },
        {
          "id": "opencompass_1149",
          "name": "InstruSum",
          "version": "1.0.0",
          "description": "InstruSum evaluates the task of instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics.\n",
          "url": "opencompass/opencompass_1149.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1149",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1149",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/yale-nlp/InstruSum",
            "paperLink": "https://aclanthology.org/2024.findings-naacl.280.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "50107925",
              "name": "yale-nlp",
              "avatar": null,
              "nickname": "OpenXLab-kJ37KHAvs"
            },
            "lookNum": "101",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-27 17:22:45",
            "supportOnlineEval": false,
            "updateDate": "2024-10-27 17:22:45",
            "createDate": "2024-10-15 14:35:05",
            "desc": {
              "cn": "InstruSum æ˜¯è¯„æµ‹æŒ‡ä»¤å¯æ§çš„æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ï¼Œæ¨¡å‹è¾“å…¥åŒ…æ‹¬æºæ–‡ç« å’Œå¯¹æ‰€éœ€æ‘˜è¦ç‰¹å¾çš„è‡ªç„¶è¯­è¨€è¦æ±‚ã€‚",
              "en": "InstruSum evaluates the task of instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics.\n"
            }
          }
        },
        {
          "id": "opencompass_1332",
          "name": "UniBench",
          "version": "1.0.0",
          "description": "UniBench is meant for evaluating VLMs' reasoning abilities. It is a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. ",
          "url": "opencompass/opencompass_1332.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1332",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1332",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/facebookresearch/unibench",
            "paperLink": "https://arxiv.org/abs/2408.04810",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "100",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-08 11:56:31",
            "supportOnlineEval": false,
            "updateDate": "2025-01-08 11:56:31",
            "createDate": "2025-01-03 14:22:05",
            "desc": {
              "cn": "UniBenchæ—¨åœ¨è¯„ä¼°VLMçš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬50ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¯¹è±¡è¯†åˆ«ã€ç©ºé—´æ„ŸçŸ¥ã€è®¡æ•°ç­‰ä»»åŠ¡ã€‚",
              "en": "UniBench is meant for evaluating VLMs' reasoning abilities. It is a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. "
            }
          }
        },
        {
          "id": "opencompass_1241",
          "name": "MedCalc-Bench",
          "version": "1.0.0",
          "description": " MedCalc-Bench focuses on evaluating the medical calculation capability of LLMs. It contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks.",
          "url": "opencompass/opencompass_1241.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1241",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1241",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/ncbi-nlp/MedCalc-Bench",
            "paperLink": "https://arxiv.org/abs/2406.12036",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "99",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-25 11:46:27",
            "supportOnlineEval": false,
            "updateDate": "2024-12-25 11:46:27",
            "createDate": "2024-12-20 17:03:18",
            "desc": {
              "cn": "MedCalc-Benchä¸“æ³¨äºè¯„ä¼°LLMçš„åŒ»å­¦è®¡ç®—èƒ½åŠ›ï¼ŒåŒ…å«æ¥è‡ª55ä¸ªä¸åŒåŒ»å­¦è®¡ç®—ä»»åŠ¡çš„1000ä¸ªç»è¿‡äººå·¥å®¡æŸ¥çš„å®ä¾‹ã€‚",
              "en": " MedCalc-Bench focuses on evaluating the medical calculation capability of LLMs. It contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks."
            }
          }
        },
        {
          "id": "opencompass_1748",
          "name": "VisualPuzzles",
          "version": "1.0.0",
          "description": "VisualPuzzles is a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of 1168 diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. ",
          "url": "opencompass/opencompass_1748.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1748",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning",
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1748",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              },
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/neulab/VisualPuzzles",
            "paperLink": "https://arxiv.org/abs/2504.10342",
            "officialWebsiteLink": "https://neulab.github.io/VisualPuzzles/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "53009543",
              "name": null,
              "avatar": null,
              "nickname": "OpenXLab-oErCdNaBY"
            },
            "lookNum": "98",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-18 17:11:40",
            "supportOnlineEval": false,
            "updateDate": "2025-04-18 17:11:40",
            "createDate": "2025-04-17 21:40:00",
            "desc": {
              "cn": "LLM èƒ½è€ƒå…¬åŠ¡å‘˜å—ï¼Ÿæˆ‘ä»¬åšäº†ä¸ªæµ‹è¯•â€¦\n\nè¿‘å¹´æ¥ï¼Œå¤§æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›çªé£çŒ›è¿›ï¼Œä¼¼ä¹â€œè¶Šæ¥è¶Šèªæ˜â€äº†ã€‚ä½†æœ‰ä¸€ä¸ªå…³é”®é—®é¢˜ä»ç„¶æ‘†åœ¨çœ¼å‰ï¼š\nğŸ¤” å®ƒä»¬çœŸçš„ä¼šâ€œæ¨ç†â€å—ï¼Ÿ\n\nğŸš€ æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåä¸º VisualPuzzles ğŸ§© çš„å…¨æ–°æ•°æ®é›†ï¼Œä¸“é—¨ç”¨æ¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼š\nè„±ç¦»ä¸“ä¸šçŸ¥è¯†çš„æ”¯æŒï¼Œå¤§æ¨¡å‹èƒ½é é€»è¾‘æœ¬èº«è§£é¢˜å—ï¼Ÿ\næˆ‘ä»¬ä»å¤šä¸ªæ¥æºç²¾å¿ƒæŒ‘é€‰æˆ–æ”¹ç¼–äº† 1168 é“å›¾æ–‡é€»è¾‘é¢˜ï¼Œå…¶ä¸­ä¸€ä¸ªé‡è¦æ¥æºä¾¿æ˜¯ä¸­å›½å›½å®¶å…¬åŠ¡å‘˜è€ƒè¯•è¡Œæµ‹ä¸­çš„é€»è¾‘æ¨ç†é¢˜ï¼ˆæ²¡é”™ï¼ŒçœŸÂ·è€ƒå…¬éš¾åº¦ï¼‰ğŸ¯",
              "en": "VisualPuzzles is a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of 1168 diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. "
            }
          }
        },
        {
          "id": "opencompass_1275",
          "name": "MMLongBench-Doc",
          "version": "1.0.0",
          "description": "MMLongBench-Doc is a long-context multi-modal benchmark comprising 1,062 expert-annotated questions.",
          "url": "opencompass/opencompass_1275.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1275",
          "sample_count": 1000,
          "traits": [
            "Long-Context"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1275",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "é•¿æ–‡æœ¬",
                "en": "Long-Context"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/mayubo2333/MMLongBench-Doc",
            "paperLink": "https://arxiv.org/abs/2407.01523",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "97",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 14:14:42",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 14:14:42",
            "createDate": "2025-01-07 14:12:34",
            "desc": {
              "cn": "MMLONGBENCH-DOCæ˜¯ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œç”±1062ä¸ªä¸“å®¶æ³¨é‡Šçš„é—®é¢˜ç»„æˆã€‚",
              "en": "MMLongBench-Doc is a long-context multi-modal benchmark comprising 1,062 expert-annotated questions."
            }
          }
        },
        {
          "id": "opencompass_1548",
          "name": "MedHallu",
          "version": "1.0.0",
          "description": "MedHallu is a comprehensive benchmark dataset designed to evaluate the ability of large language models to detect hallucinations in medical question-answering tasks. ",
          "url": "opencompass/opencompass_1548.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1548",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1548",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Medical",
                "en": "Medical"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/MedHallu/MedHalu",
            "paperLink": "https://arxiv.org/abs/2502.14302",
            "officialWebsiteLink": "https://medhallu.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "96",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-02-26 19:00:05",
            "supportOnlineEval": false,
            "updateDate": "2025-02-26 19:00:05",
            "createDate": "2025-02-26 13:33:02",
            "desc": {
              "cn": "MedHallu æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦é—®é¢˜è§£ç­”ä»»åŠ¡ä¸­æ£€æµ‹å¹»è§‰çš„èƒ½åŠ›ã€‚",
              "en": "MedHallu is a comprehensive benchmark dataset designed to evaluate the ability of large language models to detect hallucinations in medical question-answering tasks. "
            }
          }
        },
        {
          "id": "opencompass_1330",
          "name": "SG-Bench",
          "version": "1.0.0",
          "description": "SG-Bench assess LLM safety across various tasks and prompt types. It integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak. ",
          "url": "opencompass/opencompass_1330.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1330",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1330",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/MurrayTom/SG-Bench",
            "paperLink": "https://arxiv.org/abs/2410.21965",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "96",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-08 11:56:40",
            "supportOnlineEval": false,
            "updateDate": "2025-01-08 11:56:40",
            "createDate": "2025-01-03 14:07:00",
            "desc": {
              "cn": "SG-Benchç”¨äºè¯„ä¼°LLMåœ¨ä¸åŒä»»åŠ¡å’Œæç¤ºä¸‹çš„å®‰å…¨æ€§ï¼Œæ•´åˆäº†ç”Ÿæˆæ€§å’Œåˆ¤åˆ«æ€§è¯„ä¼°ä»»åŠ¡ï¼Œå¹¶åŒ…å«æ‰©å±•æ•°æ®ä»¥åº¦é‡æç¤ºå·¥ç¨‹å’Œè¶Šç‹±å¯¹å®‰å…¨æ€§çš„å½±å“ã€‚",
              "en": "SG-Bench assess LLM safety across various tasks and prompt types. It integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak. "
            }
          }
        },
        {
          "id": "opencompass_1268",
          "name": "CTIBench",
          "version": "1.0.0",
          "description": "CTIBench is a benchmark designed to assess LLMs' performance in CTI (Cyber threat intelligence) applications. It includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. ",
          "url": "opencompass/opencompass_1268.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1268",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1268",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/xashru/cti-bench",
            "paperLink": "https://arxiv.org/abs/2406.07599",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "96",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-25 14:15:02",
            "supportOnlineEval": false,
            "updateDate": "2024-12-25 14:15:02",
            "createDate": "2024-12-24 15:01:43",
            "desc": {
              "cn": "CTIBenchæ—¨åœ¨è¯„ä¼°LLMåœ¨CTIï¼ˆç½‘ç»œå®‰å…¨æƒ…æŠ¥ï¼‰åœºæ™¯ä¸‹çš„èƒ½åŠ›ï¼ŒåŒ…å«å¤šä¸ªæ•°æ®é›†ï¼Œä¸“æ³¨äºè¯„æµ‹LLMåœ¨ç½‘ç»œå¨èƒç¯å¢ƒä¸­è·å¾—çš„çŸ¥è¯†ã€‚",
              "en": "CTIBench is a benchmark designed to assess LLMs' performance in CTI (Cyber threat intelligence) applications. It includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. "
            }
          }
        },
        {
          "id": "opencompass_1323",
          "name": "SciFIBench",
          "version": "1.0.0",
          "description": "SciFIBench is a scientific figure interpretation benchmark for LMMs, consisting of 2000 questions split between two tasks across 8 categories. ",
          "url": "opencompass/opencompass_1323.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1323",
          "sample_count": 1000,
          "traits": [
            "Knowledge",
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1323",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/jonathan-roberts1/SciFIBench",
            "paperLink": "https://arxiv.org/abs/2405.08807",
            "officialWebsiteLink": "https://scifibench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "95",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 11:12:46",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 11:12:46",
            "createDate": "2025-01-13 11:57:33",
            "desc": {
              "cn": " SciFIBenchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„ç§‘å­¦å›¾è¡¨è§£é‡Šèƒ½åŠ›ï¼Œç”±2000ä¸ªé—®é¢˜ç»„æˆï¼Œæ¶µç›–8ä¸ªç±»åˆ«çš„2ç§ä»»åŠ¡ã€‚",
              "en": "SciFIBench is a scientific figure interpretation benchmark for LMMs, consisting of 2000 questions split between two tasks across 8 categories. "
            }
          }
        },
        {
          "id": "opencompass_1102",
          "name": "MS_MARCO",
          "version": "1.0.0",
          "description": "MS MARCO comprises of 1,010,916 anonymized questionsâ€”sampled from Bingâ€™s search query logsâ€”each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages.",
          "url": "opencompass/opencompass_1102.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1102",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1102",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/microsoft/MSMARCO-Question-Answering",
            "paperLink": "https://arxiv.org/pdf/1611.09268",
            "officialWebsiteLink": "https://microsoft.github.io/msmarco/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "95",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-09 20:03:35",
            "supportOnlineEval": false,
            "updateDate": "2024-10-09 20:03:35",
            "createDate": "2024-10-09 16:38:58",
            "desc": {
              "cn": "MS MARCO æ•°æ®é›†åŒ…å« 1,010,916 ä¸ªæ¥è‡ª Bing çš„æœç´¢æŸ¥è¯¢æ—¥å¿—çš„åŒ¿åé—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½æœ‰ä¸€ä¸ªäººå·¥ç”Ÿæˆçš„ç­”æ¡ˆå’Œ 182,669 ä¸ªå®Œå…¨ç”±äººé‡å†™çš„ç”Ÿæˆç­”æ¡ˆã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜åŒ…å«ä» 3,563,535 ä¸ªç”± Bing æ£€ç´¢çš„ç½‘é¡µæ–‡æ¡£ä¸­æå–çš„ 8,841,823 ä¸ªæ®µè½ï¼Œè¿™äº›æ®µè½æä¾›äº†ç­–åˆ’è‡ªç„¶è¯­è¨€ç­”æ¡ˆæ‰€éœ€çš„ä¿¡æ¯ã€‚",
              "en": "MS MARCO comprises of 1,010,916 anonymized questionsâ€”sampled from Bingâ€™s search query logsâ€”each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages."
            }
          }
        },
        {
          "id": "opencompass_1523",
          "name": "MMIE",
          "version": "1.0.0",
          "description": " MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs).",
          "url": "opencompass/opencompass_1523.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1523",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1523",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Multimodal",
                "en": "Multimodal"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Lillianwei-h/MMIE",
            "paperLink": "https://arxiv.org/abs/2410.10139",
            "officialWebsiteLink": "https://mmie-bench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "95",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:58:15",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:58:15",
            "createDate": "2025-02-21 11:23:50",
            "desc": {
              "cn": " MMIEï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çŸ¥è¯†å¯†é›†å‹åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­çš„äº¤é”™å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚",
              "en": " MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs)."
            }
          }
        },
        {
          "id": "opencompass_1273",
          "name": "MolPuzzle",
          "version": "1.0.0",
          "description": "MolPuzzle is a benchmark for evaluating MLM's reasoning ability, comprising 234 instances of structure elucidation, which feature over 18,000 QA samples.",
          "url": "opencompass/opencompass_1273.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1273",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1273",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/KehanGuo2/MolPuzzle",
            "paperLink": "https://openreview.net/pdf?id=t1mAXb4Cop",
            "officialWebsiteLink": "https://kehanguo2.github.io/Molpuzzle.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "95",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 14:14:38",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 14:14:38",
            "createDate": "2025-01-07 14:07:05",
            "desc": {
              "cn": "MolPuzzleç”¨äºè€ƒå¯ŸMLMçš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«234ä¸ªç»“æ„è§£æå®ä¾‹ä»¥åŠè¶…è¿‡18000ä¸ªQAæ ·æœ¬ã€‚",
              "en": "MolPuzzle is a benchmark for evaluating MLM's reasoning ability, comprising 234 instances of structure elucidation, which feature over 18,000 QA samples."
            }
          }
        },
        {
          "id": "opencompass_1324",
          "name": "FLUB",
          "version": "1.0.0",
          "description": "FLUB evaluates the reasoning and understanding abilities of LLMs. It includes three tasks with increasing difficulty, consisting of the tricky, humorous, and misleading texts collected from the real internet environment.",
          "url": "opencompass/opencompass_1324.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1324",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1324",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/THUKElab/FLUB",
            "paperLink": "https://arxiv.org/abs/2402.11100",
            "officialWebsiteLink": "https://thukelab.github.io/FLUB/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "94",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 14:05:50",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 14:05:50",
            "createDate": "2025-01-02 19:42:41",
            "desc": {
              "cn": "FLUBç”¨äºè¯„ä¼°LLMçš„æ¨ç†å’Œç†è§£èƒ½åŠ›ï¼Œå…¶ä¸­åŒ…å«3ä¸ªéš¾åº¦é€’è¿›çš„ä»»åŠ¡ï¼Œç”±ä»çœŸå®äº’è”ç½‘ç¯å¢ƒä¸­æ”¶é›†çš„ç‹¡çŒ¾ã€å¹½é»˜å’Œè¯¯å¯¼æ€§çš„æ–‡æœ¬æ„æˆã€‚",
              "en": "FLUB evaluates the reasoning and understanding abilities of LLMs. It includes three tasks with increasing difficulty, consisting of the tricky, humorous, and misleading texts collected from the real internet environment."
            }
          }
        },
        {
          "id": "opencompass_1336",
          "name": "CompBench",
          "version": "1.0.0",
          "description": "CompBench is designed to evaluate the comparative reasoning capability of multimodal large language models, including a collection of around 40K image pairs and visually oriented questions covering 8 dimensions",
          "url": "opencompass/opencompass_1336.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1336",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1336",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/RaptorMai/CompBench",
            "paperLink": "https://arxiv.org/abs/2407.16837",
            "officialWebsiteLink": "https://compbench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "94",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 14:05:36",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 14:05:36",
            "createDate": "2025-01-03 16:21:25",
            "desc": {
              "cn": "CompBenchæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ¯”è¾ƒæ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«çº¦4ä¸‡ä¸ªå›¾åƒå¯¹åŠ8ä¸ªç»´åº¦çš„è§†è§‰é…å¯¹é—®é¢˜ã€‚",
              "en": "CompBench is designed to evaluate the comparative reasoning capability of multimodal large language models, including a collection of around 40K image pairs and visually oriented questions covering 8 dimensions"
            }
          }
        },
        {
          "id": "opencompass_1337",
          "name": "JailTrickBench",
          "version": "1.0.0",
          "description": "JailTrickBench can evaluate the impact of various attack settings on LLM performance, including 8 key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives.",
          "url": "opencompass/opencompass_1337.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1337",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1337",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/usail-hkust/JailTrickBench",
            "paperLink": "https://arxiv.org/abs/2406.09324",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "94",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 14:05:47",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 14:05:47",
            "createDate": "2025-01-03 16:33:01",
            "desc": {
              "cn": "JailTrickBenchç”¨äºè¯„ä¼°LLMåº”å¯¹å„ç§è¶Šç‹±æ”»å‡»çš„èƒ½åŠ›ï¼Œæ¶µç›–ä»ç›®æ ‡çº§å’Œæ”»å‡»çº§2ä¸ªè§’åº¦å®æ–½è¶Šç‹±æ”»å‡»çš„8ä¸ªå…³é”®å› ç´ ã€‚",
              "en": "JailTrickBench can evaluate the impact of various attack settings on LLM performance, including 8 key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives."
            }
          }
        },
        {
          "id": "opencompass_1331",
          "name": "MedSafetyBench",
          "version": "1.0.0",
          "description": "MedSafetyBench is designed to measure the medical safety of LLMs. It includes 1,800 medical safety demonstrations, where each safety demonstration consists of a harmful medical request and a corresponding safe response. ",
          "url": "opencompass/opencompass_1331.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1331",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1331",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/AI4LIFE-GROUP/med-safety-bench",
            "paperLink": "https://arxiv.org/abs/2403.03744",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "93",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-06 16:32:34",
            "supportOnlineEval": false,
            "updateDate": "2025-01-06 16:32:34",
            "createDate": "2025-01-03 14:14:58",
            "desc": {
              "cn": "MedSafetyBenchç”¨äºè¯„ä¼°LLMåœ¨åŒ»ç–—å®‰å…¨ä¸Šçš„è¡¨ç°ï¼ŒåŒ…å«1800ä¸ªç”±æœ‰å®³è¯·æ±‚å’Œå®‰å…¨å“åº”ç»„æˆçš„åŒ»ç–—å®‰å…¨åœºæ™¯ã€‚",
              "en": "MedSafetyBench is designed to measure the medical safety of LLMs. It includes 1,800 medical safety demonstrations, where each safety demonstration consists of a harmful medical request and a corresponding safe response. "
            }
          }
        },
        {
          "id": "opencompass_1269",
          "name": "ConvBench",
          "version": "1.0.0",
          "description": "ConvBench is a novel multi-turn conversation evaluation benchmark tailored for Large Vision-Language Models (LVLMs). It comprises 577 meticulously curated multi-turn conversations encompassing 215 tasks reflective of real-world demands.",
          "url": "opencompass/opencompass_1269.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1269",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1269",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/shirlyliu64/ConvBench",
            "paperLink": "https://arxiv.org/abs/2403.20194",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "93",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 14:14:55",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 14:14:55",
            "createDate": "2025-01-07 14:14:09",
            "desc": {
              "cn": "ConvBenchæ˜¯ä¸“ç”¨äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ ï¼ˆLVLMï¼‰çš„æ–°å‹å¤šè½®å¯¹è¯è¯„ä¼°åŸºå‡†ï¼Œç”±åŸºäº215 ä¸ªåæ˜ å®é™…éœ€æ±‚çš„ä»»åŠ¡çš„577ä¸ªå¤šè½®æ¬¡å¯¹è¯ç»„æˆã€‚ ",
              "en": "ConvBench is a novel multi-turn conversation evaluation benchmark tailored for Large Vision-Language Models (LVLMs). It comprises 577 meticulously curated multi-turn conversations encompassing 215 tasks reflective of real-world demands."
            }
          }
        },
        {
          "id": "opencompass_1451",
          "name": "MME-RealWorld",
          "version": "1.0.0",
          "description": "MME-RealWorld evaluates MLLMs' real-world recognition, featuring 13,366 high-resolution images averaging 2,000 Ã— 1,500 pixels. ",
          "url": "opencompass/opencompass_1451.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1451",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1451",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/yfzhang114/MME-RealWorld",
            "paperLink": "https://arxiv.org/abs/2408.13257",
            "officialWebsiteLink": "https://mme-realworld.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "91",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-05 15:26:47",
            "supportOnlineEval": false,
            "updateDate": "2025-03-05 15:26:47",
            "createDate": "2025-01-27 12:05:30",
            "desc": {
              "cn": "MME-RealWorldç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹å¯¹çœŸå®åœºæ™¯çš„ç†è§£èƒ½åŠ›ï¼ŒåŒ…å«13366ä¸ªå¹³å‡2000*1500åƒç´ çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚",
              "en": "MME-RealWorld evaluates MLLMs' real-world recognition, featuring 13,366 high-resolution images averaging 2,000 Ã— 1,500 pixels. "
            }
          }
        },
        {
          "id": "opencompass_1565",
          "name": "MME-CoT",
          "version": "1.0.0",
          "description": "MME-CoT is a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes.",
          "url": "opencompass/opencompass_1565.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1565",
          "sample_count": 1000,
          "traits": [
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1565",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/CaraJ7/MME-CoT",
            "paperLink": "https://arxiv.org/abs/2502.09621",
            "officialWebsiteLink": "https://mmecot.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "90",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-03 11:01:40",
            "supportOnlineEval": false,
            "updateDate": "2025-03-03 11:01:40",
            "createDate": "2025-02-27 11:38:00",
            "desc": {
              "cn": "MME-CoTï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼° LMMs CoT æ¨ç†æ€§èƒ½çš„åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªé¢†åŸŸï¼šæ•°å­¦ã€ç§‘å­¦ã€OCRã€é€»è¾‘ã€æ—¶ç©ºå’Œä¸€èˆ¬åœºæ™¯ã€‚",
              "en": "MME-CoT is a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes."
            }
          }
        },
        {
          "id": "opencompass_1270",
          "name": "Spider2-V",
          "version": "1.0.0",
          "description": "Spider2-V is the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications.",
          "url": "opencompass/opencompass_1270.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1270",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1270",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/xlang-ai/Spider2-V",
            "paperLink": "https://arxiv.org/abs/2407.10956",
            "officialWebsiteLink": "https://spider2-v.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "90",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 14:14:51",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 14:14:51",
            "createDate": "2025-01-07 14:07:32",
            "desc": {
              "cn": "Spider2-Væ˜¯ç¬¬ä¸€ä¸ªä¸“æ³¨äºä¸“ä¸šæ•°æ®ç§‘å­¦å’Œå·¥ç¨‹å·¥ä½œæµç¨‹çš„å¤šæ¨¡æ€ä»£ç†åŸºå‡†æµ‹è¯•ï¼Œæ•´åˆäº†20 ä¸ªä¼ä¸šçº§ä¸“ä¸šåº”ç”¨ç¨‹åºï¼ŒåŒ…å«æ¥è‡ªçœŸå®è®¡ç®—æœºç¯å¢ƒçš„494 ä¸ªçœŸå®ä»»åŠ¡ã€‚",
              "en": "Spider2-V is the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications."
            }
          }
        },
        {
          "id": "opencompass_1582",
          "name": "CodeMMLU",
          "version": "1.0.0",
          "description": "CodeMMLU is a comprehensive benchmark designed to evaluate the capabilities of large language models (LLMs) in coding and software knowledge. It builds upon the structure of multiple-choice question answering (MCQA) to cover a wide range of programming tasks and domains.",
          "url": "opencompass/opencompass_1582.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1582",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1582",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/FSoft-AI4Code/CodeMMLU",
            "paperLink": "https://arxiv.org/abs/2406.15877",
            "officialWebsiteLink": "https://fsoft-ai4code.github.io/codemmlu/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "89",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:46:54",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:46:54",
            "createDate": "2025-03-04 11:00:59",
            "desc": {
              "cn": "CodeMMLU æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¼–ç å’Œè½¯ä»¶çŸ¥è¯†æ–¹é¢èƒ½åŠ›çš„å…¨é¢åŸºå‡†ã€‚å®ƒåŸºäºå¤šé¡¹é€‰æ‹©é¢˜å›ç­”ï¼ˆMCQAï¼‰çš„ç»“æ„ï¼Œæ¶µç›–äº†å¹¿æ³›çš„ç¼–ç¨‹ä»»åŠ¡å’Œé¢†åŸŸï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€ç¼ºé™·æ£€æµ‹ã€è½¯ä»¶å·¥ç¨‹åŸåˆ™ç­‰ã€‚",
              "en": "CodeMMLU is a comprehensive benchmark designed to evaluate the capabilities of large language models (LLMs) in coding and software knowledge. It builds upon the structure of multiple-choice question answering (MCQA) to cover a wide range of programming tasks and domains."
            }
          }
        },
        {
          "id": "opencompass_1566",
          "name": "MM-IQ",
          "version": "1.0.0",
          "description": "MM-IQ is a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.",
          "url": "opencompass/opencompass_1566.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1566",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1566",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/AceCHQ/MMIQ",
            "paperLink": "https://arxiv.org/abs/2502.00698",
            "officialWebsiteLink": "https://acechq.github.io/MMIQ-benchmark/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "88",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-03 11:03:22",
            "supportOnlineEval": false,
            "updateDate": "2025-03-03 11:03:22",
            "createDate": "2025-02-27 13:43:06",
            "desc": {
              "cn": "MM-IQï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 2,710 ä¸ªç²¾å¿ƒæŒ‘é€‰çš„æµ‹è¯•é¡¹ç›®çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–äº† 8 ç§ä¸åŒçš„æ¨ç†èŒƒå¼ã€‚",
              "en": "MM-IQ is a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms."
            }
          }
        },
        {
          "id": "opencompass_1701",
          "name": "WritingBench",
          "version": "1.0.0",
          "description": "WritingBench: A Comprehensive Benchmark for Generative Writing",
          "url": "opencompass/opencompass_1701.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1701",
          "sample_count": 1000,
          "traits": [
            "Creation",
            "Long-Context"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1701",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "é•¿æ–‡æœ¬",
                "en": "Long-Context"
              },
              {
                "cn": "åˆ›ä½œ",
                "en": "Creation"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/X-PLUG/WritingBench",
            "paperLink": "https://arxiv.org/pdf/2503.05244",
            "officialWebsiteLink": "https://modelscope.cn/studios/iic/DeepWriting",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "52302232",
              "name": null,
              "avatar": null,
              "nickname": "OpenXLab-qSFSXYARf"
            },
            "lookNum": "88",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-03 19:45:43",
            "supportOnlineEval": false,
            "updateDate": "2025-04-03 19:45:43",
            "createDate": "2025-04-02 15:22:21",
            "desc": {
              "cn": "WritingBench: A Comprehensive Benchmark for Generative Writing",
              "en": "WritingBench: A Comprehensive Benchmark for Generative Writing"
            }
          }
        },
        {
          "id": "opencompass_1272",
          "name": "GSM1k",
          "version": "1.0.0",
          "description": "GSM1k is meant for evaluating LLM's math reasoning ability. It mirrors the style and complexity of the established GSM8k benchmark while consider the problem of data-leaking and overfitting",
          "url": "opencompass/opencompass_1272.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1272",
          "sample_count": 1000,
          "traits": [
            "Math"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1272",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/scaleapi/gsm1k_eval",
            "paperLink": "https://arxiv.org/abs/2405.00332",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "88",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-25 14:36:23",
            "supportOnlineEval": false,
            "updateDate": "2024-12-25 14:36:23",
            "createDate": "2024-12-24 19:16:04",
            "desc": {
              "cn": "GSM1kå¯ç”¨äºè¯„ä¼°LLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼›å®ƒä¸GSM8kä¿æŒäº†é£æ ¼å’Œå¤æ‚æ€§çš„ä¸€è‡´ï¼ŒåŒæ—¶è€ƒè™‘äº†æ•°æ®æ³„éœ²å’Œè¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚",
              "en": "GSM1k is meant for evaluating LLM's math reasoning ability. It mirrors the style and complexity of the established GSM8k benchmark while consider the problem of data-leaking and overfitting"
            }
          }
        },
        {
          "id": "opencompass_1114",
          "name": "ASDiv",
          "version": "1.0.0",
          "description": "ASDiv is a new MWP corpus that contains diverse lexicon patterns with wide problem type coverage. Each problem provides consistent equations and answers. It is further annotated with the corresponding problem type and grade level.",
          "url": "opencompass/opencompass_1114.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1114",
          "sample_count": 1000,
          "traits": [
            "Math"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1114",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/chaochun/nlu-asdiv-dataset/tree/master",
            "paperLink": "https://aclanthology.org/2020.acl-main.92.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "87",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-11 19:59:24",
            "supportOnlineEval": false,
            "updateDate": "2024-10-11 19:59:24",
            "createDate": "2024-10-10 14:06:57",
            "desc": {
              "cn": "ASDiv æ˜¯ä¸€ä¸ªæ–°çš„æ•°å­¦æ–‡å­—é—®é¢˜ï¼ˆMWPï¼‰è¯­æ–™åº“ï¼ŒåŒ…å«å¤šæ ·çš„è¯æ±‡æ¨¡å¼ï¼Œè¦†ç›–å¹¿æ³›çš„é—®é¢˜ç±»å‹ã€‚æ¯ä¸ªé—®é¢˜æä¾›å¯¹åº”çš„æ–¹ç¨‹å’Œç­”æ¡ˆã€‚å®ƒè¿›ä¸€æ­¥æ ‡æ³¨äº†ç›¸åº”çš„é—®é¢˜ç±»å‹å’Œå¹´çº§æ°´å¹³ï¼Œå¯ç”¨äºæµ‹è¯•ç³»ç»Ÿçš„èƒ½åŠ›ï¼Œå¹¶æŒ‡æ˜é—®é¢˜çš„éš¾åº¦ç­‰çº§ã€‚",
              "en": "ASDiv is a new MWP corpus that contains diverse lexicon patterns with wide problem type coverage. Each problem provides consistent equations and answers. It is further annotated with the corresponding problem type and grade level."
            }
          }
        },
        {
          "id": "opencompass_1361",
          "name": "RealworldQA",
          "version": "1.0.0",
          "description": "RealWorldQA is a benchmark designed for real-world understanding, including 765 images, each accompanied by a question and a verifiable answer.",
          "url": "opencompass/opencompass_1361.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1361",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1361",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "86",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 17:25:35",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 17:25:35",
            "createDate": "2025-01-09 22:10:13",
            "desc": {
              "cn": "RealWorldQAç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼ŒåŒ…å«765å¼ å›¾åƒï¼Œæ¯å¼ å›¾åƒéƒ½é…æœ‰ä¸€ä¸ªé—®é¢˜å’Œæ˜“äºéªŒè¯çš„ç­”æ¡ˆã€‚",
              "en": "RealWorldQA is a benchmark designed for real-world understanding, including 765 images, each accompanied by a question and a verifiable answer."
            }
          }
        },
        {
          "id": "opencompass_1365",
          "name": "BLINK",
          "version": "1.0.0",
          "description": "BLINK focuses on MLLMs' core visual perception abilities. It contains 3,807 multiple-choice questions spanning 14 classic computer vision tasks. ",
          "url": "opencompass/opencompass_1365.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1365",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1365",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/zeyofu/BLINK_Benchmark",
            "paperLink": "https://arxiv.org/abs/2404.12390",
            "officialWebsiteLink": "https://zeyofu.github.io/blink/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "86",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:23:42",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:23:42",
            "createDate": "2025-01-10 12:06:09",
            "desc": {
              "cn": "BLINKç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼ŒåŒ…å«æ¥è‡ª14ä¸ªç»å…¸è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„3807é“å¤šé¡¹é€‰æ‹©é¢˜ã€‚",
              "en": "BLINK focuses on MLLMs' core visual perception abilities. It contains 3,807 multiple-choice questions spanning 14 classic computer vision tasks. "
            }
          }
        },
        {
          "id": "opencompass_1648",
          "name": "DME",
          "version": "1.0.0",
          "description": "VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. Based on LlavaBench and MMvet, we have curated two more challenging versions of the datasets: LlavaBench_hard and MMvet_hard. These are the hardest multimodal combinations.",
          "url": "opencompass/opencompass_1648.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1648",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1648",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/yangyue5114/DME",
            "paperLink": "https://arxiv.org/abs/2410.08695",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "86",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-18 17:39:57",
            "supportOnlineEval": false,
            "updateDate": "2025-03-18 17:39:57",
            "createDate": "2025-03-18 17:37:18",
            "desc": {
              "cn": "VLB ä¸º LVLMs æä¾›äº†ä¸€ç§ç¨³å¥ä¸”å…¨é¢çš„è¯„ä¼°ï¼Œé™ä½äº†æ•°æ®æ±¡æŸ“å¹¶å…·æœ‰çµæ´»çš„å¤æ‚æ€§ã€‚åŸºäº LlavaBench å’Œ MMvetï¼Œæˆ‘ä»¬ç²¾å¿ƒåˆ¶ä½œäº†ä¸¤ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ç‰ˆæœ¬ï¼šLlavaBench_hard å’Œ MMvet_hardã€‚è¿™äº›æ˜¯æˆ‘ä»¬åŠ¨æ€ç­–ç•¥ä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€ç»„åˆï¼ˆV1+L4ï¼‰ã€‚",
              "en": "VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. Based on LlavaBench and MMvet, we have curated two more challenging versions of the datasets: LlavaBench_hard and MMvet_hard. These are the hardest multimodal combinations."
            }
          }
        },
        {
          "id": "opencompass_1319",
          "name": "ActionAtlas",
          "version": "1.0.0",
          "description": "ActionAtlas is a multiple-choice video question answering benchmark, including 934 videos showcasing 580 unique actions across 56 sports, with a total of 1896 actions within choices.",
          "url": "opencompass/opencompass_1319.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1319",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1319",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/mrsalehi/action-atlas",
            "paperLink": "https://arxiv.org/abs/2410.05774",
            "officialWebsiteLink": "https://mrsalehi.github.io/action-atlas/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "85",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 15:07:55",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 15:07:55",
            "createDate": "2024-12-31 15:19:23",
            "desc": {
              "cn": "ActionAtlasæ˜¯ä¸€ä¸ªå¤šé¡¹é€‰æ‹©è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬934ä¸ªè§†é¢‘ï¼Œå±•ç¤ºäº†56é¡¹è¿åŠ¨ä¸­çš„580ä¸ªç‹¬ç‰¹åŠ¨ä½œï¼Œé€‰é¡¹å…±åŒ…å«1896ä¸ªåŠ¨ä½œã€‚",
              "en": "ActionAtlas is a multiple-choice video question answering benchmark, including 934 videos showcasing 580 unique actions across 56 sports, with a total of 1896 actions within choices."
            }
          }
        },
        {
          "id": "opencompass_1705",
          "name": "OlymMATH",
          "version": "1.0.0",
          "description": "A benchmark of 200 Olympiad math problems across algebra, geometry, number theory, and combinatorics. Available in English and Chinese, it features two difficulty levels: EASY (AIME-level) to test standard reasoning, and HARD to challenge advanced models.",
          "url": "opencompass/opencompass_1705.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1705",
          "sample_count": 1000,
          "traits": [
            "Math",
            "Reasoning",
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1705",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              },
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/RUCAIBox/OlymMATH",
            "paperLink": "https://arxiv.org/abs/2503.21380",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "52300476",
              "name": "CoderBak",
              "avatar": null,
              "nickname": "CoderBak"
            },
            "lookNum": "85",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-03 19:46:03",
            "supportOnlineEval": false,
            "updateDate": "2025-04-03 19:46:03",
            "createDate": "2025-04-02 23:55:49",
            "desc": {
              "cn": "ä¸€ä¸ªåŒ…å« 200 é“å¥¥æ—åŒ¹å…‹æ•°å­¦é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ä»£æ•°ã€å‡ ä½•ã€æ•°è®ºå’Œç»„åˆã€‚æˆ‘ä»¬æä¾›è‹±æ–‡å’Œä¸­æ–‡ç‰ˆæœ¬ï¼Œå¹¶æä¾›ä¸¤ä¸ªéš¾åº¦ç­‰çº§ï¼šEASYï¼ˆAIMEæ°´å¹³ï¼‰ç”¨äºæµ‹è¯•æ ‡å‡†æ¨ç†èƒ½åŠ›ï¼Œä»¥åŠ HARD ç”¨äºæŒ‘æˆ˜é«˜çº§æ¨¡å‹ã€‚",
              "en": "A benchmark of 200 Olympiad math problems across algebra, geometry, number theory, and combinatorics. Available in English and Chinese, it features two difficulty levels: EASY (AIME-level) to test standard reasoning, and HARD to challenge advanced models."
            }
          }
        },
        {
          "id": "opencompass_1326",
          "name": "MedJourney",
          "version": "1.0.0",
          "description": "MedJourney offers a comprehensive assessment of LLMs' effectiveness in real-world clinical settings. It includes multiple tasks from 4 stages of a typical patient's hospital visit journey and comprises 12 datasets.",
          "url": "opencompass/opencompass_1326.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1326",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1326",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Medical-AI-Learning/MedJourney",
            "paperLink": "https://openreview.net/pdf?id=XXaIoJyYs7",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "84",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-06 16:32:46",
            "supportOnlineEval": false,
            "updateDate": "2025-01-06 16:32:46",
            "createDate": "2025-01-02 20:04:00",
            "desc": {
              "cn": "MedJourneyç”¨äºè¯„ä¼° LLM åœ¨çœŸå®ä¸´åºŠç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ï¼Œå…¶ä¸­åŒ…å«å¤šä¸ªä»»åŠ¡ï¼Œæ¶µç›–æ¥è‡ªæ‚£è€…å°±è¯Šå…¸å‹æµç¨‹çš„4ä¸ªé˜¶æ®µçš„12ä¸ªæ•°æ®é›†ã€‚",
              "en": "MedJourney offers a comprehensive assessment of LLMs' effectiveness in real-world clinical settings. It includes multiple tasks from 4 stages of a typical patient's hospital visit journey and comprises 12 datasets."
            }
          }
        },
        {
          "id": "opencompass_1099",
          "name": "MKQA",
          "version": "1.0.0",
          "description": "MKQA is an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). ",
          "url": "opencompass/opencompass_1099.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1099",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1099",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/apple/ml-mkqa",
            "paperLink": "https://arxiv.org/pdf/2007.15207",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "84",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-09 20:03:40",
            "supportOnlineEval": false,
            "updateDate": "2024-10-09 20:03:40",
            "createDate": "2024-10-09 14:12:42",
            "desc": {
              "cn": "MKQA æ˜¯ä¸€ä¸ªå¼€æ”¾åŸŸé—®ç­”è¯„ä¼°é›†ï¼ŒåŒ…å« 10,000 å¯¹é—®é¢˜å’Œç­”æ¡ˆï¼Œæ¶µç›– 26 ç§ç±»å‹å¤šæ ·çš„è¯­è¨€ï¼ˆæ€»è®¡ 260,000 å¯¹é—®é¢˜å’Œç­”æ¡ˆï¼‰ã€‚",
              "en": "MKQA is an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). "
            }
          }
        },
        {
          "id": "opencompass_1325",
          "name": "LLM-Uncertainty-Bench",
          "version": "1.0.0",
          "description": "LLM-Uncertainty-Bench is a new benchmarking approach for LLMs that integrates uncertainty quantification. It spans 5 representative natural language processing tasks, each has a dataset with 10,000 instances.",
          "url": "opencompass/opencompass_1325.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1325",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1325",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/smartyfh/LLM-Uncertainty-Bench",
            "paperLink": "https://arxiv.org/abs/2401.12794",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "83",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-06 14:21:33",
            "supportOnlineEval": false,
            "updateDate": "2025-01-06 14:21:33",
            "createDate": "2025-01-02 19:52:18",
            "desc": {
              "cn": "LLM-Uncertainty-Benchå°†ä¸ç¡®å®šæ€§çº³å…¥LLMè¯„ä¼°ï¼ŒåŒ…å«5ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æœ‰åŒ…å«10000ä¸ªå®ä¾‹çš„æ•°æ®é›†æ”¯æ’‘ã€‚",
              "en": "LLM-Uncertainty-Bench is a new benchmarking approach for LLMs that integrates uncertainty quantification. It spans 5 representative natural language processing tasks, each has a dataset with 10,000 instances."
            }
          }
        },
        {
          "id": "opencompass_1606",
          "name": "MCiteBench",
          "version": "1.0.0",
          "description": "MCiteBench is a benchmark to evaluate multimodal citation text generation in MLLMs. It consists of 3,000 samples from 1,749 academic papers, featuring 2,000 Explanation tasks and 1,000 Locating tasks, with balanced evidence across text, figures, tables, and mixed modalities.",
          "url": "opencompass/opencompass_1606.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1606",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1606",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/caiyuhu/MCiteBench",
            "paperLink": "https://arxiv.org/abs/2503.02589",
            "officialWebsiteLink": "https://caiyuhu.github.io/MCiteBench/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "83",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-20 14:11:57",
            "supportOnlineEval": false,
            "updateDate": "2025-03-20 14:11:57",
            "createDate": "2025-03-20 14:11:46",
            "desc": {
              "cn": "MCiteBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­å¤šæ¨¡æ€å¼•ç”¨æ–‡æœ¬ç”Ÿæˆçš„åŸºå‡†ï¼Œç”± 1,749 ç¯‡å­¦æœ¯è®ºæ–‡ä¸­çš„ 3,000 ä¸ªæ ·æœ¬ç»„æˆï¼ŒåŒ…æ‹¬ 2,000 ä¸ªè§£é‡Šä»»åŠ¡å’Œ 1,000 ä¸ªå®šä½ä»»åŠ¡ï¼Œåœ¨æ–‡æœ¬ã€å›¾è¡¨ã€è¡¨æ ¼å’Œæ··åˆæ¨¡æ€ä¸­å¹³è¡¡è¯æ®ã€‚",
              "en": "MCiteBench is a benchmark to evaluate multimodal citation text generation in MLLMs. It consists of 3,000 samples from 1,749 academic papers, featuring 2,000 Explanation tasks and 1,000 Locating tasks, with balanced evidence across text, figures, tables, and mixed modalities."
            }
          }
        },
        {
          "id": "opencompass_1120",
          "name": "ProofNet",
          "version": "1.0.0",
          "description": "ProofNet is a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. ",
          "url": "opencompass/opencompass_1120.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1120",
          "sample_count": 1000,
          "traits": [
            "Math"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1120",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/zhangir-azerbayev/ProofNet",
            "paperLink": "https://arxiv.org/pdf/2302.12433",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "81",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-10 20:21:42",
            "supportOnlineEval": false,
            "updateDate": "2024-10-10 20:21:42",
            "createDate": "2024-10-10 14:54:13",
            "desc": {
              "cn": "ProofNet æ˜¯ä¸€ä¸ªç”¨äºæœ¬ç§‘æ•°å­¦çš„è‡ªåŠ¨å½¢å¼åŒ–å’Œå½¢å¼è¯æ˜çš„åŸºå‡†ã€‚åŒ…å« 371 ä¸ªç¤ºä¾‹ï¼Œæ¯ä¸ªç¤ºä¾‹åŒ…æ‹¬ä¸€ä¸ª Lean 3 ä¸­çš„å½¢å¼å®šç†é™ˆè¿°ã€ä¸€ä¸ªè‡ªç„¶è¯­è¨€å®šç†é™ˆè¿°å’Œä¸€ä¸ªè‡ªç„¶è¯­è¨€è¯æ˜ã€‚è¿™äº›é—®é¢˜ä¸»è¦æ¥è‡ªæµè¡Œçš„æœ¬ç§‘çº¯æ•°å­¦æ•™æï¼Œæ¶µç›–å®åˆ†æã€å¤åˆ†æã€çº¿æ€§ä»£æ•°ã€æŠ½è±¡ä»£æ•°å’Œæ‹“æ‰‘ç­‰ä¸»é¢˜ã€‚",
              "en": "ProofNet is a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. "
            }
          }
        },
        {
          "id": "opencompass_1376",
          "name": "GenAI-Bench",
          "version": "1.0.0",
          "description": "GenAI-Bench is a benchmark designed to benchmark MLLMsâ€™s ability in judging the quality of AI generative contents, containing over 40,000 human ratings to evaluate the performance of MLLMs on aligning with human preferences.",
          "url": "opencompass/opencompass_1376.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1376",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1376",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/TIGER-AI-Lab/GenAI-Bench",
            "paperLink": "https://arxiv.org/abs/2406.13743",
            "officialWebsiteLink": "https://linzhiqiu.github.io/papers/genai_bench/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "81",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:23:33",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:23:33",
            "createDate": "2025-01-10 17:02:53",
            "desc": {
              "cn": "GenAI-Benchç”¨äºè¡¡é‡MLLMåˆ¤æ–­AIç”Ÿæˆå†…å®¹è´¨é‡çš„èƒ½åŠ›ï¼ŒåŒ…å«è¶…è¿‡40000ä¸ªäººå·¥è¯„åˆ†ç”¨ä»¥è¯„ä¼°å¤§æ¨¡å‹ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚",
              "en": "GenAI-Bench is a benchmark designed to benchmark MLLMsâ€™s ability in judging the quality of AI generative contents, containing over 40,000 human ratings to evaluate the performance of MLLMs on aligning with human preferences."
            }
          }
        },
        {
          "id": "opencompass_1579",
          "name": "PosterSum",
          "version": "1.0.0",
          "description": "The PosterSum dataset is a multimodal benchmark designed for the summarization of scientific posters into research paper abstracts. The dataset consists of 16,305 research posters collected from major machine learning conferences, including ICLR, ICML, and NeurIPS, spanning the years 2022-2024. ",
          "url": "opencompass/opencompass_1579.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1579",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1579",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/saxenarohit/postersum",
            "paperLink": "https://arxiv.org/abs/2502.17540",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "79",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:46:01",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:46:01",
            "createDate": "2025-03-05 18:56:15",
            "desc": {
              "cn": "PosterSum æ•°æ®é›†æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨å°†ç§‘å­¦æµ·æŠ¥æ€»ç»“æˆç ”ç©¶è®ºæ–‡æ‘˜è¦ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»2022-2024å¹´çš„ä¸»è¦æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆåŒ…æ‹¬ ICLRã€ICML å’Œ NeurIPSï¼‰æ”¶é›†çš„ 16,305 ç¯‡ç ”ç©¶æµ·æŠ¥ã€‚",
              "en": "The PosterSum dataset is a multimodal benchmark designed for the summarization of scientific posters into research paper abstracts. The dataset consists of 16,305 research posters collected from major machine learning conferences, including ICLR, ICML, and NeurIPS, spanning the years 2022-2024. "
            }
          }
        },
        {
          "id": "opencompass_1607",
          "name": "ToolRet",
          "version": "1.0.0",
          "description": "ToolRet is a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets.",
          "url": "opencompass/opencompass_1607.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1607",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1607",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/mangopy/tool-retrieval-benchmark",
            "paperLink": "https://arxiv.org/abs/2503.01763",
            "officialWebsiteLink": "https://mangopy.github.io/tool-retrieval-benchmark/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "78",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:43:53",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:43:53",
            "createDate": "2025-03-06 14:40:00",
            "desc": {
              "cn": "ToolRetæ˜¯ä¸€ä¸ªåŒ…å« 7.6k ä¸ªä¸åŒæ£€ç´¢ä»»åŠ¡çš„å¼‚æ„å·¥å…·æ£€ç´¢åŸºå‡†ï¼Œä»¥åŠä»ç°æœ‰æ•°æ®é›†ä¸­æ”¶é›†çš„ 43k ä¸ªå·¥å…·è¯­æ–™åº“ã€‚",
              "en": "ToolRet is a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets."
            }
          }
        },
        {
          "id": "opencompass_1453",
          "name": "MMIU",
          "version": "1.0.0",
          "description": "MMIU test MLLMs' multi-image understanding capabilities, encompassesing 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions.",
          "url": "opencompass/opencompass_1453.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1453",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1453",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/OpenGVLab/MMIU",
            "paperLink": "https://arxiv.org/abs/2408.02718",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "78",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-02-26 18:55:33",
            "supportOnlineEval": false,
            "updateDate": "2025-02-26 18:55:33",
            "createDate": "2025-01-27 14:55:19",
            "desc": {
              "cn": "MMIUç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å¤šå›¾ç†è§£èƒ½åŠ›ï¼ŒåŒ…å«7ç§ç±»å‹çš„å¤šå›¾åƒå…³ç³»ã€52ä¸ªä»»åŠ¡ã€77Kå›¾åƒå’Œ11Kç²¾å¿ƒç­–åˆ’çš„å¤šé€‰é¢˜ã€‚",
              "en": "MMIU test MLLMs' multi-image understanding capabilities, encompassesing 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions."
            }
          }
        },
        {
          "id": "opencompass_1554",
          "name": "JL1-CD",
          "version": "1.0.0",
          "description": "JL1-CD is a large-scale, sub-meter, all-inclusive open-source dataset for remote sensing image change detection (CD). It contains 5,000 pairs of 512Ã—512 pixel satellite images with a resolution of 0.5 to 0.75 meters, covering various types of surface changes in multiple regions of China. ",
          "url": "opencompass/opencompass_1554.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1554",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1554",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/circleLZY/MTKD-CD",
            "paperLink": "https://arxiv.org/abs/2502.13407",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "77",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:46:12",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:46:12",
            "createDate": "2025-03-05 18:54:32",
            "desc": {
              "cn": "JL1-CD æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€äºšç±³çº§ã€å…¨åŒ…å«çš„å¼€æºé¥æ„Ÿå½±åƒå˜åŒ–æ£€æµ‹ï¼ˆCDï¼‰æ•°æ®é›†ã€‚å®ƒåŒ…å« 5000 å¯¹ 512Ã—512 åƒç´ çš„å«æ˜Ÿå½±åƒï¼Œåˆ†è¾¨ç‡ä¸º 0.5 è‡³ 0.75 ç±³ï¼Œè¦†ç›–ä¸­å›½å¤šä¸ªåœ°åŒºçš„å„ç§åœ°è¡¨å˜åŒ–ã€‚",
              "en": "JL1-CD is a large-scale, sub-meter, all-inclusive open-source dataset for remote sensing image change detection (CD). It contains 5,000 pairs of 512Ã—512 pixel satellite images with a resolution of 0.5 to 0.75 meters, covering various types of surface changes in multiple regions of China. "
            }
          }
        },
        {
          "id": "opencompass_1145",
          "name": "FREB-TQA",
          "version": "1.0.0",
          "description": "FREB-TQA is a Fine-grained Robustness Evaluation Benchmark for Table Question Answering. ",
          "url": "opencompass/opencompass_1145.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1145",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1145",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/boschresearch/FREB-TQA",
            "paperLink": "https://aclanthology.org/2024.naacl-long.137.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "50109778",
              "name": "Bosch_Research",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/50109778-dd360e6a-5366-4ab9-8d82-260b4e376600.png",
              "nickname": "OpenXLab-XxvqMQwcb"
            },
            "lookNum": "77",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-27 17:23:19",
            "supportOnlineEval": false,
            "updateDate": "2024-10-27 17:23:19",
            "createDate": "2024-10-15 13:24:34",
            "desc": {
              "cn": "FREB-TQA æ˜¯ä¸€ä¸ªç»†ç²’åº¦çš„ç¨³å¥æ€§è¯„ä¼°åŸºå‡†ï¼Œä¸“æ³¨äºè¡¨æ ¼é—®ç­”ï¼ˆTQAï¼‰ã€‚",
              "en": "FREB-TQA is a Fine-grained Robustness Evaluation Benchmark for Table Question Answering. "
            }
          }
        },
        {
          "id": "opencompass_1243",
          "name": "EmbodiedAgentInterface",
          "version": "1.0.0",
          "description": "Embodied Agent Interface supports the formalization of various types of tasks and input-output specifications of LLM-based modules, offering a comprehensive assessment of LLMs' performance for different subtasks and pinpointing the strengths and weaknesses in LLM-powered embodied AI systems.",
          "url": "opencompass/opencompass_1243.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1243",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1243",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Embodied Decision Making",
                "en": "Embodied Decision Making"
              }
            ],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/embodied-agent-interface/embodied-agent-interface",
            "paperLink": "https://arxiv.org/abs/2410.07166",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "77",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-25 11:46:31",
            "supportOnlineEval": false,
            "updateDate": "2024-12-25 11:46:31",
            "createDate": "2024-12-20 17:40:13",
            "desc": {
              "cn": "Embodied Agent Interfaceæ”¯æŒå„ç§ç±»å‹çš„ä»»åŠ¡å’ŒåŸºäºLLMæ¨¡å—çš„è¾“å…¥è¾“å‡ºè§„èŒƒçš„å½¢å¼åŒ–ï¼Œå¯¹LLMåœ¨ä¸åŒå­ä»»åŠ¡ä¸­çš„æ€§èƒ½è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒæŒ‡å‡ºäº†åŸºäºLLMçš„å…·èº«AIç³»ç»Ÿçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚",
              "en": "Embodied Agent Interface supports the formalization of various types of tasks and input-output specifications of LLM-based modules, offering a comprehensive assessment of LLMs' performance for different subtasks and pinpointing the strengths and weaknesses in LLM-powered embodied AI systems."
            }
          }
        },
        {
          "id": "opencompass_1605",
          "name": "Deepfake-Eval-2024",
          "version": "1.0.0",
          "description": "Deepfake-Eval-2024 is an in-the-wild deepfake dataset. Deepfake-Eval-2024 contains 44 hours of videos, 56.5 hours of audio, and 1,975 images, encompassing contemporary manipulation technologies, diverse media content, 88 different website sources, and 52 different languages. ",
          "url": "opencompass/opencompass_1605.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1605",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1605",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/nuriachandra/Deepfake-Eval-2024",
            "paperLink": "https://arxiv.org/abs/2503.02857",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "76",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:45:17",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:45:17",
            "createDate": "2025-03-06 14:04:04",
            "desc": {
              "cn": "Deepfake-Eval-2024 æ˜¯ä¸€ä¸ªçœŸå®åœºæ™¯çš„æ·±åº¦ä¼ªé€ æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«44å°æ—¶çš„è§†é¢‘ã€56.5å°æ—¶çš„éŸ³é¢‘å’Œ1,975å¼ å›¾åƒï¼Œæ¶µç›–å½“ä»£ç¯¡æ”¹æŠ€æœ¯ã€å¤šæ ·åŒ–çš„åª’ä½“å†…å®¹ã€æ¥è‡ª88ä¸ªä¸åŒç½‘ç«™æ¥æºçš„ç´ æä»¥åŠ52ç§ä¸åŒè¯­è¨€ã€‚",
              "en": "Deepfake-Eval-2024 is an in-the-wild deepfake dataset. Deepfake-Eval-2024 contains 44 hours of videos, 56.5 hours of audio, and 1,975 images, encompassing contemporary manipulation technologies, diverse media content, 88 different website sources, and 52 different languages. "
            }
          }
        },
        {
          "id": "opencompass_1618",
          "name": "FedMABench",
          "version": "1.0.0",
          "description": "FedMABench is an open-source benchmark for federated training and evaluation of mobile agents, specifically designed for heterogeneous scenarios.",
          "url": "opencompass/opencompass_1618.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1618",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1618",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/wwh0411/FedMABench",
            "paperLink": "https://arxiv.org/abs/2503.05143",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "75",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-10 17:12:07",
            "supportOnlineEval": false,
            "updateDate": "2025-03-10 17:12:07",
            "createDate": "2025-03-10 17:06:15",
            "desc": {
              "cn": "FedMABench æ˜¯ä¸€ä¸ªå¼€æºçš„è”é‚¦è®­ç»ƒå’Œè¯„ä¼°ç§»åŠ¨ä»£ç†çš„åŸºå‡†ï¼Œç‰¹åˆ«ä¸ºå¼‚æ„åœºæ™¯è®¾è®¡ã€‚",
              "en": "FedMABench is an open-source benchmark for federated training and evaluation of mobile agents, specifically designed for heterogeneous scenarios."
            }
          }
        },
        {
          "id": "opencompass_1736",
          "name": "MMTB",
          "version": "1.0.0",
          "description": "Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly acce",
          "url": "opencompass/opencompass_1736.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1736",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1736",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/yupeijei1997/MMTB",
            "paperLink": "https://arxiv.org/abs/2504.02623",
            "officialWebsiteLink": "https://harrywgcn.github.io/mmtb-leaderboard/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "90306731",
              "name": null,
              "avatar": null,
              "nickname": "yupeijei1997"
            },
            "lookNum": "75",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-11 17:08:39",
            "supportOnlineEval": false,
            "updateDate": "2025-04-11 17:08:39",
            "createDate": "2025-04-11 15:30:25",
            "desc": {
              "cn": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡­å€Ÿå…¶å…ˆè¿›çš„ç†è§£èƒ½åŠ›å’Œè§„åˆ’èƒ½åŠ›ï¼Œåœ¨ä½œä¸ºå·¥å…·è°ƒç”¨çš„æ™ºèƒ½ä½“æ–¹é¢å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç”¨æˆ·è¶Šæ¥è¶Šä¾èµ–åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡è¿­ä»£äº¤äº’æ¥è§£å†³å¤æ‚ä»»åŠ¡ã€‚ ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦æ˜¯åœ¨å•ä»»åŠ¡åœºæ™¯ä¸­è¯„ä¼°æ™ºèƒ½ä½“ï¼Œæ— æ³•ä½“ç°ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº† Multi-Mission Tool Bench æµ‹è¯•ã€‚åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¯ä¸ªæµ‹è¯•ç”¨ä¾‹éƒ½åŒ…å«å¤šä¸ªç›¸äº’å…³è”çš„ä»»åŠ¡ã€‚è¿™ç§è®¾è®¡è¦æ±‚æ™ºèƒ½ä½“èƒ½å¤ŸåŠ¨æ€é€‚åº”ä¸æ–­å˜åŒ–çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„åŸºå‡†æµ‹è¯•æ¢ç´¢äº†åœ¨å›ºå®šä»»åŠ¡æ•°é‡ä¸‹æ‰€æœ‰å¯èƒ½çš„ä»»åŠ¡åˆ‡æ¢æ¨¡å¼ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ•°æ®ç”Ÿæˆæ¡†æ¶æ¥æ„å»ºè¿™ä¸ªåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ©ç”¨åŠ¨æ€å†³ç­–æ ‘æ¥",
              "en": "Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly acce"
            }
          }
        },
        {
          "id": "opencompass_1581",
          "name": "HoloBench",
          "version": "1.0.0",
          "description": "HoloBench is a benchmark designed to evaluate the ability of long-context language models (LCLMs) to perform holistic reasoning over extended text contexts. ",
          "url": "opencompass/opencompass_1581.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1581",
          "sample_count": 1000,
          "traits": [
            "Long-Context",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1581",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              },
              {
                "cn": "é•¿æ–‡æœ¬",
                "en": "Long-Context"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/megagonlabs/holobench",
            "paperLink": "https://arxiv.org/abs/2410.11996",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "74",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:46:42",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:46:42",
            "createDate": "2025-03-04 10:53:51",
            "desc": {
              "cn": "HoloBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ï¼ˆLCLMsï¼‰åœ¨æ‰©å±•æ–‡æœ¬ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ•´ä½“æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚",
              "en": "HoloBench is a benchmark designed to evaluate the ability of long-context language models (LCLMs) to perform holistic reasoning over extended text contexts. "
            }
          }
        },
        {
          "id": "opencompass_1364",
          "name": "MMT-Bench",
          "version": "1.0.0",
          "description": "MMT-Bench is designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. It comprises 31,325 multi-choice visual questions, covering 32 core meta-tasks and 162 subtasks in multimodal understanding.",
          "url": "opencompass/opencompass_1364.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1364",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1364",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/OpenGVLab/MMT-Bench",
            "paperLink": "https://arxiv.org/abs/2404.16006",
            "officialWebsiteLink": "https://mmt-bench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "74",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:23:44",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:23:44",
            "createDate": "2025-01-10 11:56:21",
            "desc": {
              "cn": "MMT-Benchè€ƒå¯Ÿå¤šæ¨¡æ€å¤§æ¨¡å‹çš„è§†è§‰è¯†åˆ«ã€å®šä½ã€æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼ŒåŒ…æ‹¬31325ä¸ªå¤šé€‰è§†è§‰é—®é¢˜ï¼Œæ¶µç›–äº†32ä¸ªæ ¸å¿ƒå…ƒä»»åŠ¡å’Œ162ä¸ªå¤šæ¨¡æ€ç†è§£å­ä»»åŠ¡ã€‚",
              "en": "MMT-Bench is designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. It comprises 31,325 multi-choice visual questions, covering 32 core meta-tasks and 162 subtasks in multimodal understanding."
            }
          }
        },
        {
          "id": "opencompass_1279",
          "name": "WhodunitBench",
          "version": "1.0.0",
          "description": "WhodunitBench is used to evaluate large multimodal agent under complex tasks and dynamic scenarios.",
          "url": "opencompass/opencompass_1279.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1279",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1279",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/jun0wanan/WhodunitBench-Murder_Mystery_Games",
            "paperLink": "https://openreview.net/pdf?id=qmvtDIfbmS",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "74",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-25 18:10:25",
            "supportOnlineEval": false,
            "updateDate": "2024-12-25 18:10:25",
            "createDate": "2024-12-24 20:24:49",
            "desc": {
              "cn": "WhodunitBenchç”¨äºè¯„ä¼°å¤§å‹å¤šæ¨¡å¼ä»£ç†åœ¨å¤æ‚ä»»åŠ¡åœºæ™¯ä¸‹çš„åŠ¨æ€è¯„ä¼°ã€‚",
              "en": "WhodunitBench is used to evaluate large multimodal agent under complex tasks and dynamic scenarios."
            }
          }
        },
        {
          "id": "opencompass_1329",
          "name": "OlympicArena",
          "version": "1.0.0",
          "description": "OlympicArena evaluates cognitive reasoning abilities. It includes 11,163 bilingual problems spanning seven fields and 62 international Olympic competitions. ",
          "url": "opencompass/opencompass_1329.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1329",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1329",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/GAIR-NLP/OlympicArena",
            "paperLink": "https://arxiv.org/abs/2406.12753",
            "officialWebsiteLink": "https://gair-nlp.github.io/OlympicArena/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "73",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-06 16:32:39",
            "supportOnlineEval": false,
            "updateDate": "2025-01-06 16:32:39",
            "createDate": "2025-01-03 11:50:50",
            "desc": {
              "cn": "OlympicArenaç”¨äºè¯„ä¼°å¤§æ¨¡å‹çš„è®¤çŸ¥æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«æ¥è‡ª7ä¸ªé¢†åŸŸã€62é¡¹å›½é™…å¥¥æ—åŒ¹å…‹æ¯”èµ›çš„11163ä¸ªåŒè¯­é—®é¢˜ã€‚",
              "en": "OlympicArena evaluates cognitive reasoning abilities. It includes 11,163 bilingual problems spanning seven fields and 62 international Olympic competitions. "
            }
          }
        },
        {
          "id": "opencompass_1772",
          "name": "S1-Bench",
          "version": "1.0.0",
          "description": "S1-Bench is a novel benchmark designed to evaluate the performance of LRMs on simple tasks that are more aligned with intuitive System 1 thinking, rather than deliberate System 2 reasoning. S1-Bench offers a set of simple, diverse, and naturally clear questions across multiple domains and languages.",
          "url": "opencompass/opencompass_1772.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1772",
          "sample_count": 1000,
          "traits": [
            "Instruct",
            "Strong Reasoning",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1772",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              },
              {
                "cn": "æŒ‡ä»¤è·Ÿéš",
                "en": "Instruct"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "ç³»ç»Ÿ1",
                "en": "ç³»ç»Ÿ1"
              },
              {
                "cn": "å¿«æ€è€ƒ",
                "en": "å¿«æ€è€ƒ"
              },
              {
                "cn": "LRM",
                "en": "LRM"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/WYRipple/S1_Bench",
            "paperLink": "https://arxiv.org/abs/2504.10368",
            "officialWebsiteLink": "",
            "leaderboardLink": true,
            "creatorInfo": {
              "uid": "86305501",
              "name": "WYRipple",
              "avatar": "https://thirdwx.qlogo.cn/mmopen/vi_32/skbupBhJ87rXicbT3SlhqHwQCLK486tTT6yJribPhbj5sMzbyU6F17SBErBljjNXtRqGcCd8ZBt0MzIPsEFz0hdWNg7JRf5EL1X5FDkvqC3xU/132",
              "nickname": "WYRipple"
            },
            "lookNum": "72",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-25 17:00:35",
            "supportOnlineEval": false,
            "updateDate": "2025-04-25 17:00:35",
            "createDate": "2025-04-24 17:48:23",
            "desc": {
              "cn": "S1-Benchæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§æ¨¡å‹åœ¨ç®€å•ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿™äº›ä»»åŠ¡æ›´å€¾å‘äºç›´è§‚çš„ç³»ç»Ÿ1æ€ç»´ï¼Œè€Œéæ·±æ€ç†Ÿè™‘çš„ç³»ç»Ÿ2æ¨ç†ã€‚å°½ç®¡å¤§æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­é€šè¿‡æ˜ç¡®çš„æ€ç»´é“¾å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†å®ƒä»¬å¯¹æ·±åº¦åˆ†ææ€ç»´çš„ä¾èµ–å¯èƒ½é™åˆ¶äº†å…¶ç³»ç»Ÿ1æ€ç»´èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç›®å‰ç¼ºä¹è¯„ä¼°å¤§æ¨¡å‹åœ¨éœ€è¦æ­¤ç±»èƒ½åŠ›çš„ä»»åŠ¡ä¸­è¡¨ç°çš„åŸºå‡†ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼ŒS1-Benchæä¾›äº†ä¸€ç»„ç®€å•ã€å¤šæ ·ä¸”è‡ªç„¶æ¸…æ™°çš„é—®é¢˜ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸå’Œè¯­è¨€ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤§æ¨¡å‹åœ¨æ­¤ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚",
              "en": "S1-Bench is a novel benchmark designed to evaluate the performance of LRMs on simple tasks that are more aligned with intuitive System 1 thinking, rather than deliberate System 2 reasoning. S1-Bench offers a set of simple, diverse, and naturally clear questions across multiple domains and languages."
            }
          }
        },
        {
          "id": "opencompass_1609",
          "name": "MASK",
          "version": "1.0.0",
          "description": "The MASK evaluation provides a rigorous benchmark for evaluating honesty in large language models by measuring whether models remain truthful when incentivized to lie. The public set contains 1,028 high-quality human-labeled examples across six distinct archetypes.",
          "url": "opencompass/opencompass_1609.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1609",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1609",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/centerforaisafety/mask",
            "paperLink": "https://arxiv.org/abs/2503.03750",
            "officialWebsiteLink": "https://www.mask-benchmark.ai/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "71",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:43:36",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:43:36",
            "createDate": "2025-03-06 16:39:16",
            "desc": {
              "cn": "MASK è¯„ä¼°æä¾›äº†ä¸€ä¸ªä¸¥æ ¼çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è¯šå®åº¦ï¼Œé€šè¿‡æµ‹é‡æ¨¡å‹åœ¨å—åˆ°è¯±ä½¿è¯´è°çš„æ¿€åŠ±æ—¶æ˜¯å¦ä¿æŒçœŸå®æ€§ã€‚å…¬å…±é›†åŒ…å« 1,028 ä¸ªé«˜è´¨é‡çš„äººæ ‡æ³¨ç¤ºä¾‹ï¼Œæ¶µç›–å…­ä¸ªä¸åŒçš„åŸå‹ã€‚",
              "en": "The MASK evaluation provides a rigorous benchmark for evaluating honesty in large language models by measuring whether models remain truthful when incentivized to lie. The public set contains 1,028 high-quality human-labeled examples across six distinct archetypes."
            }
          }
        },
        {
          "id": "opencompass_1278",
          "name": "ChronoMagic-Bench",
          "version": "1.0.0",
          "description": "ChronoMagic-Bench can evaluate the temporal and metamorphic capabilities of the T2V (text-to-video) models in time-lapse video generation, introducing 1,649 prompts and real-world videos as references.",
          "url": "opencompass/opencompass_1278.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1278",
          "sample_count": 1000,
          "traits": [
            "Creation"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1278",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "åˆ›ä½œ",
                "en": "Creation"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/PKU-YuanGroup/ChronoMagic-Bench",
            "paperLink": "https://arxiv.org/abs/2406.18522",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "71",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 14:14:44",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 14:14:44",
            "createDate": "2025-01-07 14:13:34",
            "desc": {
              "cn": "ChronoMagic-Benchç”¨æ¥è¯„ä¼° T2V ï¼ˆæ–‡æœ¬åˆ°è§†é¢‘ ï¼‰æ¨¡å‹åœ¨å»¶æ—¶è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶é—´å’Œå˜å½¢èƒ½åŠ›ï¼Œå¼•å…¥äº†1649ä¸ªæç¤ºå’ŒçœŸå®ä¸–ç•Œçš„è§†é¢‘ä½œä¸ºå‚è€ƒã€‚",
              "en": "ChronoMagic-Bench can evaluate the temporal and metamorphic capabilities of the T2V (text-to-video) models in time-lapse video generation, introducing 1,649 prompts and real-world videos as references."
            }
          }
        },
        {
          "id": "opencompass_1280",
          "name": "AMBROSIA",
          "version": "1.0.0",
          "description": "AMBROSIA is a new benchmark for recognizing and interpreting ambiguous requests in text-to-SQL. It contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries.",
          "url": "opencompass/opencompass_1280.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1280",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1280",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/saparina/ambrosia",
            "paperLink": "https://arxiv.org/abs/2406.19073",
            "officialWebsiteLink": "https://ambrosia-benchmark.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "70",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-25 14:31:36",
            "supportOnlineEval": false,
            "updateDate": "2024-12-25 14:31:36",
            "createDate": "2024-12-24 20:31:47",
            "desc": {
              "cn": "AMBROSIAæ˜¯è¯†åˆ«å’Œè§£é‡Štext-to-SQLä¸­æ­§ä¹‰è¯·æ±‚çš„æ–°åŸºå‡†ï¼Œå…¶ä¸­åŒ…å«ä¸‰ç§ä¸åŒç±»å‹çš„æ­§ä¹‰ï¼ˆèŒƒå›´æ­§ä¹‰ã€é™„ä»¶æ­§ä¹‰å’Œæ¨¡ç³Šæ€§ï¼‰é—®é¢˜ã€å®ƒä»¬çš„è§£é‡Šå’Œç›¸åº”çš„SQLæŸ¥è¯¢ã€‚",
              "en": "AMBROSIA is a new benchmark for recognizing and interpreting ambiguous requests in text-to-SQL. It contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries."
            }
          }
        },
        {
          "id": "opencompass_1537",
          "name": "MVL-SIB",
          "version": "1.0.0",
          "description": "MVL-SIB is a multilingual dataset that provides image-sentence pairs spanning 205 languages and 7 topical categories (entertainment, geography, health, politics, science, sports, travel). It was constructed by extending the SIB-200 benchmark.",
          "url": "opencompass/opencompass_1537.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1537",
          "sample_count": 1000,
          "traits": [
            "Language",
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1537",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              },
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2502.12852",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "70",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-03 11:48:04",
            "supportOnlineEval": false,
            "updateDate": "2025-03-03 11:48:04",
            "createDate": "2025-02-24 18:19:14",
            "desc": {
              "cn": "MVL-SIB æ˜¯ä¸€ä¸ªå¤šè¯­è¨€æ•°æ®é›†ï¼Œæä¾›äº†æ¶µç›– 205 ç§è¯­è¨€å’Œ 7 ä¸ªä¸»é¢˜ç±»åˆ«çš„å›¾åƒ-å¥å­å¯¹ï¼ˆ entertainment ï¼Œ geography ï¼Œ health ï¼Œ politics ï¼Œ science ï¼Œ sports ï¼Œ travel ï¼‰ã€‚å®ƒé€šè¿‡æ‰©å±• SIB-200 åŸºå‡†æ„å»ºè€Œæˆã€‚",
              "en": "MVL-SIB is a multilingual dataset that provides image-sentence pairs spanning 205 languages and 7 topical categories (entertainment, geography, health, politics, science, sports, travel). It was constructed by extending the SIB-200 benchmark."
            }
          }
        },
        {
          "id": "opencompass_1524",
          "name": "RM-Bench",
          "version": "1.0.0",
          "description": "RM-Bench, a benchmark dataset for evaluating reward models of language modeling.",
          "url": "opencompass/opencompass_1524.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1524",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1524",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "VQA",
                "en": "VQA"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/THU-KEG/RM-Bench",
            "paperLink": "https://arxiv.org/abs/2410.16184",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "70",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 17:00:31",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 17:00:31",
            "createDate": "2025-02-21 11:34:57",
            "desc": {
              "cn": "RM-Benchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹å¥–åŠ±æ¨¡å‹çš„åŸºå‡†æ•°æ®é›†",
              "en": "RM-Bench, a benchmark dataset for evaluating reward models of language modeling."
            }
          }
        },
        {
          "id": "opencompass_1358",
          "name": "Video-MME",
          "version": "1.0.0",
          "description": "Video-MME is an evaluation benchmark of multi-modal LLMs in video analysis, including 900 videos in various duration with a total of 254 hours which spans 6 primary visual domains with 30 subfields.",
          "url": "opencompass/opencompass_1358.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1358",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1358",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/BradyFU/Video-MME",
            "paperLink": "https://github.com/BradyFU/Video-MME",
            "officialWebsiteLink": "https://video-mme.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "69",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:23:57",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:23:57",
            "createDate": "2025-01-09 21:09:31",
            "desc": {
              "cn": "Video-MMEç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è§†é¢‘åˆ†æèƒ½åŠ›ï¼ŒåŒ…å«900ä¸ªä¸åŒé•¿åº¦çš„è§†é¢‘ï¼Œæ¥è‡ª6ä¸ªä¸»è¦è§†è§‰é¢†åŸŸå’Œ30ä¸ªå­é¢†åŸŸï¼Œæ€»æ—¶é•¿è¾¾254å°æ—¶ã€‚",
              "en": "Video-MME is an evaluation benchmark of multi-modal LLMs in video analysis, including 900 videos in various duration with a total of 254 hours which spans 6 primary visual domains with 30 subfields."
            }
          }
        },
        {
          "id": "opencompass_1500",
          "name": "CRPE",
          "version": "1.0.0",
          "description": "CRPE is a benchmark designed to quantitatively evaluate the object recognition and relation comprehension ability of models. It consists of four splits, and the evaluation is formulated as single-choice questions. ",
          "url": "opencompass/opencompass_1500.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1500",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1500",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/OpenGVLab/all-seeing",
            "paperLink": "https://arxiv.org/abs/2402.19474",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "69",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:46:25",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:46:25",
            "createDate": "2025-02-13 19:32:36",
            "desc": {
              "cn": "CRPEç”¨äºå®šé‡è¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å¯¹è±¡è¯†åˆ«å’Œå…³ç³»ç†è§£èƒ½åŠ›ï¼Œåˆ†ä¸ºå››éƒ¨åˆ†ï¼Œä»¥å•é€‰é¢˜å½¢å¼å‘ˆç°ã€‚",
              "en": "CRPE is a benchmark designed to quantitatively evaluate the object recognition and relation comprehension ability of models. It consists of four splits, and the evaluation is formulated as single-choice questions. "
            }
          }
        },
        {
          "id": "opencompass_1518",
          "name": "miniCTX",
          "version": "1.0.0",
          "description": "A context-rich benchmark for evaluating neural theorem proving in realistic scenarios, providing premises, full context, multi-source benchmark, and temporal splits, enabling evaluation of a model's ability to work with context that evolves over time.",
          "url": "opencompass/opencompass_1518.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1518",
          "sample_count": 1000,
          "traits": [
            "Math",
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1518",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              },
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Strong Reasoning",
                "en": "Strong Reasoning"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/cmu-l3/minictx-eval",
            "paperLink": "https://www.arxiv.org/pdf/2408.03350",
            "officialWebsiteLink": "https://cmu-l3.github.io/minictx/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "69",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:53:10",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:53:10",
            "createDate": "2025-02-20 15:38:03",
            "desc": {
              "cn": "ä¸€ä¸ªç”¨äºè¯„ä¼°ç°å®åœºæ™¯ä¸­ç¥ç»å®šç†è¯æ˜çš„ä¸°å¯Œä¸Šä¸‹æ–‡åŸºå‡†ã€‚é€šè¿‡æä¾›å‰æã€å®Œæ•´ä¸Šä¸‹æ–‡ã€å¤šæºåŸºå‡†å’Œæ—¶åºåˆ†å‰²ï¼Œçªå‡ºå…¶è¯„ä¼°æ¨¡å‹å¤„ç†éšæ—¶é—´æ¼”å˜çš„ä¸Šä¸‹æ–‡èƒ½åŠ›ã€‚",
              "en": "A context-rich benchmark for evaluating neural theorem proving in realistic scenarios, providing premises, full context, multi-source benchmark, and temporal splits, enabling evaluation of a model's ability to work with context that evolves over time."
            }
          }
        },
        {
          "id": "opencompass_1318",
          "name": "WikiContradict",
          "version": "1.0.0",
          "description": "WikiContradict is a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts.",
          "url": "opencompass/opencompass_1318.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1318",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1318",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2406.13805",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "68",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-06 14:21:41",
            "supportOnlineEval": false,
            "updateDate": "2025-01-06 14:21:41",
            "createDate": "2024-12-31 14:05:23",
            "desc": {
              "cn": "WikiContradictæ—¨åœ¨è¯„ä¼°LLMé‡åˆ°åŒ…å«çœŸå®ä¸–ç•ŒçŸ¥è¯†å†²çªçš„æ®µè½æ£€ç´¢å¢å¼ºæ—¶çš„æ€§èƒ½ï¼Œç”±253ä¸ªé«˜è´¨é‡çš„äººå·¥æ³¨é‡Šå®ä¾‹ç»„æˆã€‚",
              "en": "WikiContradict is a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts."
            }
          }
        },
        {
          "id": "opencompass_1580",
          "name": "A-Bench",
          "version": "1.0.0",
          "description": " A-Bench is a benchmark designed to diagnose whether LMMs are masters at evaluating AIGIs. 2,864 AIGIs from 16 text-to-image models are sampled, each paired with question-answers annotated by human experts, and tested across 18 leading LMMs.",
          "url": "opencompass/opencompass_1580.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1580",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1580",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Q-Future/A-Bench",
            "paperLink": "https://arxiv.org/abs/2406.03070",
            "officialWebsiteLink": "https://a-bench-sjtu.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "68",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:46:33",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:46:33",
            "createDate": "2025-03-04 10:43:10",
            "desc": {
              "cn": "A-Benchæ˜¯ä¸€ä¸ªæ—¨åœ¨è¯Šæ–­ LMMs æ˜¯å¦æ“…é•¿è¯„ä¼° AIGIs çš„åŸºå‡†ï¼Œä» 16 ä¸ªæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­é‡‡æ ·äº† 2,864 ä¸ª AIGIsï¼Œæ¯ä¸ªéƒ½ä¸ç”±äººç±»ä¸“å®¶æ ‡æ³¨çš„é—®é¢˜-ç­”æ¡ˆé…å¯¹ã€‚",
              "en": " A-Bench is a benchmark designed to diagnose whether LMMs are masters at evaluating AIGIs. 2,864 AIGIs from 16 text-to-image models are sampled, each paired with question-answers annotated by human experts, and tested across 18 leading LMMs."
            }
          }
        },
        {
          "id": "opencompass_1147",
          "name": "M3T",
          "version": "1.0.0",
          "description": "M3T is a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents. This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications.",
          "url": "opencompass/opencompass_1147.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1147",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1147",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/amazon-science/m3t-multi-modal-translation-bench",
            "paperLink": "https://aclanthology.org/2024.naacl-short.41.pdf",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "5010775",
              "name": "amazon-science",
              "avatar": null,
              "nickname": "OpenXLab-hzYw4sVeC"
            },
            "lookNum": "68",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-10-27 17:23:28",
            "supportOnlineEval": false,
            "updateDate": "2024-10-27 17:23:28",
            "createDate": "2024-10-15 13:57:09",
            "desc": {
              "cn": "M3T æ˜¯æ—¨åœ¨è¯„ä¼°ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNMTï¼‰ç³»ç»Ÿåœ¨ç¿»è¯‘åŠç»“æ„åŒ–æ–‡æ¡£çš„ç»¼åˆä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚",
              "en": "M3T is a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents. This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications."
            }
          }
        },
        {
          "id": "opencompass_1534",
          "name": "CHASE-Code",
          "version": "1.0.0",
          "description": "CHASE is a unified framework to synthetically generate challenging problems using LLMs without human involvement",
          "url": "opencompass/opencompass_1534.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1534",
          "sample_count": 1000,
          "traits": [
            "Code",
            "Math"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1534",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              },
              {
                "cn": "æ•°å­¦",
                "en": "Math"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/McGill-NLP/CHASE",
            "paperLink": "https://arxiv.org/pdf/2502.14678",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "68",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 17:02:59",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 17:02:59",
            "createDate": "2025-02-24 14:12:19",
            "desc": {
              "cn": "CHASEæ˜¯ä¸€ä¸ªæ— éœ€äººå·¥å‚ä¸çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºåˆæˆç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜",
              "en": "CHASE is a unified framework to synthetically generate challenging problems using LLMs without human involvement"
            }
          }
        },
        {
          "id": "opencompass_1574",
          "name": "Text2World",
          "version": "1.0.0",
          "description": "Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. ",
          "url": "opencompass/opencompass_1574.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1574",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1574",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Aaron617/text2world",
            "paperLink": "https://arxiv.org/abs/2502.13092",
            "officialWebsiteLink": "https://text-to-world.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "67",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-04 16:58:03",
            "supportOnlineEval": false,
            "updateDate": "2025-03-04 16:58:03",
            "createDate": "2025-03-03 14:38:08",
            "desc": {
              "cn": "Text2World åŸºäºè§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€ (PDDL)ï¼Œæ‹¥æœ‰æ•°ç™¾ä¸ªä¸åŒçš„é¢†åŸŸï¼Œå¹¶é‡‡ç”¨å¤šæ ‡å‡†ã€åŸºäºæ‰§è¡Œçš„è¡¡é‡æ ‡å‡†æ¥è¿›è¡Œæ›´ç¨³å¥çš„è¯„ä¼°ã€‚",
              "en": "Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. "
            }
          }
        },
        {
          "id": "opencompass_1327",
          "name": "EHRNoteQA",
          "version": "1.0.0",
          "description": "EHRNoteQA evaluates LLM's ability to assist clinical decision-making based on electronic health records, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. ",
          "url": "opencompass/opencompass_1327.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1327",
          "sample_count": 1000,
          "traits": [
            "Knowledge"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1327",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "çŸ¥è¯†",
                "en": "Knowledge"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/ji-youn-kim/EHRNoteQA",
            "paperLink": "https://arxiv.org/abs/2402.16040",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "67",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-06 16:32:43",
            "supportOnlineEval": false,
            "updateDate": "2025-01-06 16:32:43",
            "createDate": "2025-01-02 20:15:09",
            "desc": {
              "cn": "EHRNoteQAç”¨äºè¯„ä¼°LLMåŸºäºç”µå­å¥åº·è®°å½•è¾…åŠ©ä¸´åºŠå†³ç­–çš„èƒ½åŠ›ï¼Œç”±962ä¸ªé—®ç­”å¯¹ç»„æˆï¼Œæ¯ä¸ªé—®ç­”å¯¹éƒ½ä¸ä¸åŒæ‚£è€…çš„å‡ºé™¢æ€»ç»“ç›¸å…³è”ã€‚",
              "en": "EHRNoteQA evaluates LLM's ability to assist clinical decision-making based on electronic health records, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. "
            }
          }
        },
        {
          "id": "opencompass_1608",
          "name": "SwiLTra-Bench",
          "version": "1.0.0",
          "description": "SwiLTra-Bench is a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems.",
          "url": "opencompass/opencompass_1608.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1608",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1608",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2503.01372",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "67",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:43:44",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:43:44",
            "createDate": "2025-03-06 15:15:16",
            "desc": {
              "cn": "SwiLTra-Benchæ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡ 18 ä¸‡å¯¹å¯¹é½çš„ç‘å£«æ³•å¾‹ç¿»è¯‘è¯­æ–™åº“çš„å…¨é¢å¤šè¯­è¨€åŸºå‡†ï¼ŒåŒ…æ‹¬æ‰€æœ‰ç‘å£«è¯­è¨€ä»¥åŠè‹±è¯­çš„æ³•å¾‹ã€æ‘˜è¦å’Œæ–°é—»ç¨¿ï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºLLMçš„ç¿»è¯‘ç³»ç»Ÿã€‚",
              "en": "SwiLTra-Bench is a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems."
            }
          }
        },
        {
          "id": "opencompass_1321",
          "name": "ShoppingMMLU",
          "version": "1.0.0",
          "description": "Shopping MMLU is a diverse multi-task online shopping benchmark derived from real-world Amazon data. It consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality.",
          "url": "opencompass/opencompass_1321.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1321",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1321",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/KL4805/ShoppingMMLU",
            "paperLink": "https://arxiv.org/abs/2410.20745",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "65",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-06 14:21:36",
            "supportOnlineEval": false,
            "updateDate": "2025-01-06 14:21:36",
            "createDate": "2024-12-31 15:44:55",
            "desc": {
              "cn": "Shopping MMLUæ˜¯ä¸€ä¸ªåŸºäºçœŸå®äºšé©¬é€Šæ•°æ®çš„å¤šæ ·åŒ–å¤šä»»åŠ¡åœ¨çº¿è´­ç‰©åŸºå‡†æµ‹è¯•ï¼Œç”±57é¡¹ä»»åŠ¡ç»„æˆï¼Œæ¶µç›–æ¦‚å¿µç†è§£ã€çŸ¥è¯†æ¨ç†ã€ç”¨æˆ·è¡Œä¸ºå¯¹é½å’Œå¤šè¯­è¨€4å¤§è´­ç‰©åœºæ™¯æŠ€èƒ½ã€‚",
              "en": "Shopping MMLU is a diverse multi-task online shopping benchmark derived from real-world Amazon data. It consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality."
            }
          }
        },
        {
          "id": "opencompass_1360",
          "name": "LLaVA-Bench",
          "version": "1.0.0",
          "description": "LLaVA-Bench evaluates the model's ability in more challenging tasks, including a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc.",
          "url": "opencompass/opencompass_1360.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1360",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1360",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/haotian-liu/LLaVA/blob/main/docs/LLaVA_Bench.md",
            "paperLink": "",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "65",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:23:49",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:23:49",
            "createDate": "2025-01-09 21:47:19",
            "desc": {
              "cn": "LLaVA-Benchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹åº”å¯¹å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå†…å«24 å¼ å›¾åƒåŠ60ä¸ªé—®é¢˜ï¼ŒåŒ…æ‹¬å®¤å†…å’Œå®¤å¤–åœºæ™¯ã€æ¨¡å› ã€ç»˜ç”»ã€ç´ æç­‰ã€‚",
              "en": "LLaVA-Bench evaluates the model's ability in more challenging tasks, including a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc."
            }
          }
        },
        {
          "id": "opencompass_1539",
          "name": "MM-RLHF",
          "version": "1.0.0",
          "description": " MM-RLHF, a comprehensive project for aligning Multimodal Large Language Models (MLLMs) with human preferences. The dataset and algorithms enable consistent performance improvements across 10 dimensions and 27 benchmarks for open-source MLLMs.",
          "url": "opencompass/opencompass_1539.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1539",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1539",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "VQA",
                "en": "VQA"
              },
              {
                "cn": "Multimodal",
                "en": "Multimodal"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Kwai-YuanQi/MM-RLHF",
            "paperLink": "https://arxiv.org/abs/2406.08487",
            "officialWebsiteLink": "https://mm-rlhf.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "64",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:50:29",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:50:29",
            "createDate": "2025-02-24 19:28:08",
            "desc": {
              "cn": "MM-RLHFï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸äººç±»åå¥½å¯¹é½çš„å…¨é¢é¡¹ç›®ï¼Œä½¿å¼€æºå¤šè¯­è¨€æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ 10 ä¸ªç»´åº¦å’Œ 27 ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°æŒç»­çš„æ€§èƒ½æå‡ã€‚",
              "en": " MM-RLHF, a comprehensive project for aligning Multimodal Large Language Models (MLLMs) with human preferences. The dataset and algorithms enable consistent performance improvements across 10 dimensions and 27 benchmarks for open-source MLLMs."
            }
          }
        },
        {
          "id": "opencompass_1356",
          "name": "MM-Vet",
          "version": "1.0.0",
          "description": "MM-Vet evaluates the capabilities to deal with complicated multimodal tasks, defining 6 core VL capabilities and examining the 16 integrations of interest derived from the capability combination.",
          "url": "opencompass/opencompass_1356.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1356",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1356",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/yuweihao/MM-Vet",
            "paperLink": "https://arxiv.org/abs/2308.02490",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "64",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:24:00",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:24:00",
            "createDate": "2025-01-09 20:41:34",
            "desc": {
              "cn": "MM-Vetç”¨äºè¯„ä¼°å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡èƒ½åŠ›ï¼Œæ¶µç›–äº†6ä¸ªæ ¸å¿ƒè§†è§‰è¯­è¨€åŠŸèƒ½çš„16ç§åŠŸèƒ½ç»„åˆã€‚",
              "en": "MM-Vet evaluates the capabilities to deal with complicated multimodal tasks, defining 6 core VL capabilities and examining the 16 integrations of interest derived from the capability combination."
            }
          }
        },
        {
          "id": "opencompass_1557",
          "name": "AIRBench-2024",
          "version": "1.0.0",
          "description": "AIR-Bench 2024, the first policy-aligned AI safety benchmark, structures 8 government regulations and 16 corporate policies into four security tiers, with 5,694 diverse prompts spanning these categories.",
          "url": "opencompass/opencompass_1557.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1557",
          "sample_count": 1000,
          "traits": [
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1557",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/stanford-crfm/air-bench-2024",
            "paperLink": "https://arxiv.org/abs/2407.17436",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "63",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:50:05",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:50:05",
            "createDate": "2025-02-26 17:30:51",
            "desc": {
              "cn": "AIR-Bench 2024æ˜¯é¦–ä¸ªä¸æ–°å…´æ”¿åºœæ³•è§„å’Œä¼ä¸šæ”¿ç­–ç›¸ä¸€è‡´çš„ AI å®‰å…¨åŸºå‡†ï¼Œ å°† 8 é¡¹æ”¿åºœæ³•è§„å’Œ 16 é¡¹ä¼ä¸šæ”¿ç­–åˆ†è§£ä¸ºå››çº§å®‰å…¨åˆ†ç±»ï¼Œæ¶µç›–äº†è¿™äº›ç±»åˆ«çš„ 5,694 ä¸ªå¤šæ ·åŒ–çš„æç¤ºã€‚",
              "en": "AIR-Bench 2024, the first policy-aligned AI safety benchmark, structures 8 government regulations and 16 corporate policies into four security tiers, with 5,694 diverse prompts spanning these categories."
            }
          }
        },
        {
          "id": "opencompass_1587",
          "name": "MR-GSM8K",
          "version": "1.0.0",
          "description": "MR-GSM8K is a challenging benchmark designed to evaluate the meta-reasoning capabilities of state-of-the-art Large Language Models (LLMs). It goes beyond traditional evaluation metrics by focusing on the reasoning process rather than just the final answer.",
          "url": "opencompass/opencompass_1587.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1587",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1587",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/dvlab-research/MR-GSM8K",
            "paperLink": "https://arxiv.org/abs/2312.17080",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "63",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:47:12",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:47:12",
            "createDate": "2025-03-04 14:13:29",
            "desc": {
              "cn": "MR-GSM8K æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°æœ€å…ˆè¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…ƒæ¨ç†èƒ½åŠ›çš„æŒ‘æˆ˜æ€§åŸºå‡†ã€‚å®ƒè¶…è¶Šäº†ä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¸“æ³¨äºæ¨ç†è¿‡ç¨‹è€Œéä»…ä»…å…³æ³¨æœ€ç»ˆç­”æ¡ˆï¼Œä»è€Œå¯¹æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›è¿›è¡Œæ›´ç»†è‡´çš„è¯„ä¼°ã€‚",
              "en": "MR-GSM8K is a challenging benchmark designed to evaluate the meta-reasoning capabilities of state-of-the-art Large Language Models (LLMs). It goes beyond traditional evaluation metrics by focusing on the reasoning process rather than just the final answer."
            }
          }
        },
        {
          "id": "opencompass_1267",
          "name": "SpreadsheetBench",
          "version": "1.0.0",
          "description": "SpreadsheetBench is a challenging spreadsheet manipulation benchmark built from 912 real questions gathered from online Excel forums.",
          "url": "opencompass/opencompass_1267.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1267",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1267",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/RUCKBReasoning/SpreadsheetBench",
            "paperLink": "https://arxiv.org/abs/2406.14991",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "63",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-25 14:11:06",
            "supportOnlineEval": false,
            "updateDate": "2024-12-25 14:11:06",
            "createDate": "2024-12-24 14:50:13",
            "desc": {
              "cn": "SpreadsheetBenchæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç”µå­è¡¨æ ¼æ“ä½œåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«912ä¸ªæ¥è‡ªåœ¨çº¿Excelè®ºå›çš„çœŸå®é—®é¢˜",
              "en": "SpreadsheetBench is a challenging spreadsheet manipulation benchmark built from 912 real questions gathered from online Excel forums."
            }
          }
        },
        {
          "id": "opencompass_1274",
          "name": "IMDL-BenCo",
          "version": "1.0.0",
          "description": "IMDL-BenCo offers a comprehensive IMDL benchmark and modular codebase.",
          "url": "opencompass/opencompass_1274.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1274",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1274",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/scu-zjz/IMDLBenCo",
            "paperLink": "https://arxiv.org/abs/2406.10580",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "63",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2024-12-30 16:30:17",
            "supportOnlineEval": false,
            "updateDate": "2024-12-30 16:30:17",
            "createDate": "2024-12-30 16:26:51",
            "desc": {
              "cn": "IMDL-BenCoæä¾›äº†å…¨é¢çš„IMDLåŸºå‡†æµ‹è¯•å’Œæ¨¡å—åŒ–ä»£ç åº“ã€‚",
              "en": "IMDL-BenCo offers a comprehensive IMDL benchmark and modular codebase."
            }
          }
        },
        {
          "id": "opencompass_1362",
          "name": "POPE",
          "version": "1.0.0",
          "description": "POPE is an improved evaluation method for LVLMs' object hallucination by proposing a polling-based query method. It offers a more stable and flexible solution.",
          "url": "opencompass/opencompass_1362.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1362",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1362",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/AoiDragon/POPE",
            "paperLink": "https://arxiv.org/abs/2305.10355",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "61",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:23:47",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:23:47",
            "createDate": "2025-01-10 10:53:21",
            "desc": {
              "cn": "POPEç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰©ä½“å¹»è§‰ï¼ŒåŸºäºè½®è¯¢çš„æŸ¥è¯¢æ–¹æ³•è®¾è®¡ï¼Œæä¾›äº†ä¸€ç§æ›´ç¨³å®šã€æ›´çµæ´»çš„è¯„ä¼°æ–¹æ¡ˆã€‚",
              "en": "POPE is an improved evaluation method for LVLMs' object hallucination by proposing a polling-based query method. It offers a more stable and flexible solution."
            }
          }
        },
        {
          "id": "opencompass_1680",
          "name": "BigOBench",
          "version": "1.0.0",
          "description": "BigO(Bench)æ˜¯ä¸€ä¸ªåŒ…å«çº¦ 300 ä¸ªéœ€è¦ç”¨ Python è§£å†³çš„ä»£ç é—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠ 3,105 ä¸ªç¼–ç é—®é¢˜å’Œ 1,190,250 ä¸ªè§£å†³æ–¹æ¡ˆç”¨äºè®­ç»ƒï¼Œä»¥è¯„ä¼°LLMsèƒ½å¦æ‰¾åˆ°ä»£ç è§£å†³æ–¹æ¡ˆçš„æ—¶é—´-ç©ºé—´å¤æ‚åº¦ï¼Œæˆ–è€…ç”Ÿæˆç¬¦åˆæ—¶é—´-ç©ºé—´å¤æ‚åº¦è¦æ±‚çš„ä»£ç è§£å†³æ–¹æ¡ˆã€‚",
          "url": "opencompass/opencompass_1680.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1680",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1680",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/facebookresearch/bigobench",
            "paperLink": "https://arxiv.org/abs/2503.15242",
            "officialWebsiteLink": "https://facebookresearch.github.io/BigOBench/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "61",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-26 15:56:21",
            "supportOnlineEval": false,
            "updateDate": "2025-03-26 15:56:21",
            "createDate": "2025-03-26 14:44:38",
            "desc": {
              "cn": "BigO(Bench)æ˜¯ä¸€ä¸ªåŒ…å«çº¦ 300 ä¸ªéœ€è¦ç”¨ Python è§£å†³çš„ä»£ç é—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠ 3,105 ä¸ªç¼–ç é—®é¢˜å’Œ 1,190,250 ä¸ªè§£å†³æ–¹æ¡ˆç”¨äºè®­ç»ƒï¼Œä»¥è¯„ä¼°LLMsèƒ½å¦æ‰¾åˆ°ä»£ç è§£å†³æ–¹æ¡ˆçš„æ—¶é—´-ç©ºé—´å¤æ‚åº¦ï¼Œæˆ–è€…ç”Ÿæˆç¬¦åˆæ—¶é—´-ç©ºé—´å¤æ‚åº¦è¦æ±‚çš„ä»£ç è§£å†³æ–¹æ¡ˆã€‚",
              "en": "BigO(Bench)æ˜¯ä¸€ä¸ªåŒ…å«çº¦ 300 ä¸ªéœ€è¦ç”¨ Python è§£å†³çš„ä»£ç é—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠ 3,105 ä¸ªç¼–ç é—®é¢˜å’Œ 1,190,250 ä¸ªè§£å†³æ–¹æ¡ˆç”¨äºè®­ç»ƒï¼Œä»¥è¯„ä¼°LLMsèƒ½å¦æ‰¾åˆ°ä»£ç è§£å†³æ–¹æ¡ˆçš„æ—¶é—´-ç©ºé—´å¤æ‚åº¦ï¼Œæˆ–è€…ç”Ÿæˆç¬¦åˆæ—¶é—´-ç©ºé—´å¤æ‚åº¦è¦æ±‚çš„ä»£ç è§£å†³æ–¹æ¡ˆã€‚"
            }
          }
        },
        {
          "id": "opencompass_1320",
          "name": "IaC-Eval",
          "version": "1.0.0",
          "description": "IaC-Eval is meant for quantitatively evaluating the capabilities of LLMs in cloud IaC code generation, containing 458 questions ranging from simple to difficult across various cloud services.",
          "url": "opencompass/opencompass_1320.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1320",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1320",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/autoiac-project/iac-eval",
            "paperLink": "https://openreview.net/pdf?id=7TCK0aBL1C",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "60",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-06 14:21:38",
            "supportOnlineEval": false,
            "updateDate": "2025-01-06 14:21:38",
            "createDate": "2024-12-31 15:35:20",
            "desc": {
              "cn": "IaC-Evalç”¨äºå®šé‡è¯„ä¼°LLMåœ¨äº‘IaCä»£ç ç”Ÿæˆä¸­çš„åŠŸèƒ½ï¼Œå…¶ä¸­åŒ…å«458ä¸ªä»æ˜“åˆ°éš¾çš„é—®é¢˜ï¼Œæ¶µç›–äº†å„ç§äº‘æœåŠ¡ã€‚",
              "en": "IaC-Eval is meant for quantitatively evaluating the capabilities of LLMs in cloud IaC code generation, containing 458 questions ranging from simple to difficult across various cloud services."
            }
          }
        },
        {
          "id": "opencompass_1396",
          "name": "AV-Odyssey-Bench",
          "version": "1.0.0",
          "description": "AV-Odyssey Bench. This benchmark encompasses 26 different tasks and 4,555 carefully crafted problems, each incorporating text, visual, and audio components. All data are newly collected and annotated by humans, not from any existing audio-visual dataset.",
          "url": "opencompass/opencompass_1396.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1396",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1396",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "audio-visual",
                "en": "audio-visual"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/AV-Odyssey/AV-Odyssey",
            "paperLink": "https://arxiv.org/pdf/2412.02611",
            "officialWebsiteLink": "https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "40068663",
              "name": null,
              "avatar": null,
              "nickname": "è±ªà¼™à¾‡"
            },
            "lookNum": "60",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-15 18:34:14",
            "supportOnlineEval": false,
            "updateDate": "2025-01-15 18:34:14",
            "createDate": "2025-01-15 17:26:48",
            "desc": {
              "cn": "AV-Odyssey Bench. This benchmark encompasses 26 different tasks and 4,555 carefully crafted problems, each incorporating text, visual, and audio components. All data are newly collected and annotated by humans, not from any existing audio-visual dataset.",
              "en": "AV-Odyssey Bench. This benchmark encompasses 26 different tasks and 4,555 carefully crafted problems, each incorporating text, visual, and audio components. All data are newly collected and annotated by humans, not from any existing audio-visual dataset."
            }
          }
        },
        {
          "id": "opencompass_1547",
          "name": "MMIR",
          "version": "1.0.0",
          "description": "A benchmark for evaluating Multimodal Large Language Models (MLLMs) on detecting and reasoning about inconsistencies in layout-rich multimodal content. MMIR features 534 challenging samples across five reasoning-heavy inconsistency categories.",
          "url": "opencompass/opencompass_1547.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1547",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1547",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/eric-ai-lab/MMIR",
            "paperLink": "https://arxiv.org/abs/2502.16033",
            "officialWebsiteLink": "https://jackie-2000.github.io/mmir.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "59",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:48:36",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:48:36",
            "createDate": "2025-02-26 11:11:06",
            "desc": {
              "cn": "ç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ï¼ˆMLLMï¼‰æ£€æµ‹å’Œæ¨ç†å¸ƒå±€ä¸°å¯Œçš„å¤šæ¨¡æ€å†…å®¹ä¸­çš„ä¸ä¸€è‡´æ€§çš„åŸºå‡†ã€‚MMIR åŒ…å« 534 ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œæ¶‰åŠäº”ä¸ªæ¨ç†èƒ½åŠ›è¾ƒå¼ºçš„ä¸ä¸€è‡´ç±»åˆ«ã€‚",
              "en": "A benchmark for evaluating Multimodal Large Language Models (MLLMs) on detecting and reasoning about inconsistencies in layout-rich multimodal content. MMIR features 534 challenging samples across five reasoning-heavy inconsistency categories."
            }
          }
        },
        {
          "id": "opencompass_1317",
          "name": "RepLiQA",
          "version": "1.0.0",
          "description": "RepLiQA is suited for question-answering and topic retrieval tasks, including collection of five splits of test sets. Accurate answers can only be generated if a model can find relevant content within the provided document.",
          "url": "opencompass/opencompass_1317.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1317",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1317",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/ServiceNow/repliqa",
            "paperLink": "https://arxiv.org/abs/2406.11811",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "59",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-06 14:21:43",
            "supportOnlineEval": false,
            "updateDate": "2025-01-06 14:21:43",
            "createDate": "2024-12-31 13:56:35",
            "desc": {
              "cn": "RepLiQAé€‚ç”¨äºé—®ç­”å’Œä¸»é¢˜æ£€ç´¢ä»»åŠ¡ï¼Œé›†åˆäº†æ˜¯5ä¸ªæµ‹è¯•é›†ï¼›åªæœ‰å½“æ¨¡å‹å¯ä»¥åœ¨æä¾›çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹æ—¶ï¼Œæ‰èƒ½ç”Ÿæˆå‡†ç¡®çš„ç­”æ¡ˆã€‚",
              "en": "RepLiQA is suited for question-answering and topic retrieval tasks, including collection of five splits of test sets. Accurate answers can only be generated if a model can find relevant content within the provided document."
            }
          }
        },
        {
          "id": "opencompass_1654",
          "name": "V-STaR",
          "version": "1.0.0",
          "description": "V-STaR is a spatio-temporal reasoning benchmark for Video-LLMs, evaluating Video-LLMâ€™s spatio-temporal reasoning ability in answering questions explicitly in the context of â€œwhenâ€, â€œwhereâ€, and â€œwhatâ€.",
          "url": "opencompass/opencompass_1654.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1654",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1654",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/V-STaR-Bench/V-STaR",
            "paperLink": "https://arxiv.org/abs/2503.11495",
            "officialWebsiteLink": "https://v-star-bench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "59",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-20 14:27:09",
            "supportOnlineEval": false,
            "updateDate": "2025-03-20 14:27:09",
            "createDate": "2025-03-20 13:44:36",
            "desc": {
              "cn": "V-STaR æ˜¯ä¸€ä¸ªé’ˆå¯¹ Video-LLMsçš„ç©ºé—´æ—¶é—´æ¨ç†åŸºå‡†ï¼Œè¯„ä¼° Video-LLMåœ¨â€œä½•æ—¶â€ã€â€œä½•åœ°â€å’Œâ€œä½•ç‰©â€çš„ä¸Šä¸‹æ–‡ä¸­æ˜ç¡®å›ç­”é—®é¢˜çš„ç©ºé—´æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚",
              "en": "V-STaR is a spatio-temporal reasoning benchmark for Video-LLMs, evaluating Video-LLMâ€™s spatio-temporal reasoning ability in answering questions explicitly in the context of â€œwhenâ€, â€œwhereâ€, and â€œwhatâ€."
            }
          }
        },
        {
          "id": "opencompass_1584",
          "name": "MMSearch",
          "version": "1.0.0",
          "description": "Logo MMSearch is a multimodal search benchmark crafted to evaluate the potential of LMMs to function as a multimodal AI search engine. This benchmark encompasses a meticulously collected dataset of 300 queries spanning 14 subfields. ",
          "url": "opencompass/opencompass_1584.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1584",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1584",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/CaraJ7/MMSearch",
            "paperLink": "https://arxiv.org/abs/2409.12959",
            "officialWebsiteLink": "https://mmsearch.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "58",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:47:36",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:47:36",
            "createDate": "2025-03-04 11:25:58",
            "desc": {
              "cn": "MMSearch æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æœç´¢åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMMsï¼‰ä½œä¸ºå¤šæ¨¡æ€ AI æœç´¢å¼•æ“çš„æ½œåŠ›ã€‚è¯¥åŸºå‡†åŒ…å«äº†ä¸€ä¸ªç²¾å¿ƒæ”¶é›†çš„åŒ…å« 300 ä¸ªæŸ¥è¯¢çš„æ•°æ®é›†ï¼Œæ¶µç›– 14 ä¸ªå­é¢†åŸŸã€‚",
              "en": "Logo MMSearch is a multimodal search benchmark crafted to evaluate the potential of LMMs to function as a multimodal AI search engine. This benchmark encompasses a meticulously collected dataset of 300 queries spanning 14 subfields. "
            }
          }
        },
        {
          "id": "opencompass_1625",
          "name": "ubuntu_osworld",
          "version": "1.0.0",
          "description": "OSWorld is a first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across operating systems. ",
          "url": "opencompass/opencompass_1625.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1625",
          "sample_count": 1000,
          "traits": [
            "Agent",
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1625",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/xlang-ai/OSWorld",
            "paperLink": "https://arxiv.org/abs/2404.07972",
            "officialWebsiteLink": "https://os-world.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "58",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-12 11:07:38",
            "supportOnlineEval": false,
            "updateDate": "2025-03-12 11:07:38",
            "createDate": "2025-03-11 16:09:49",
            "desc": {
              "cn": "OSWorld æ˜¯ä¸€ä¸ªé¦–åˆ›çš„ã€å¯æ‰©å±•çš„ã€çœŸå®è®¡ç®—æœºç¯å¢ƒï¼Œç”¨äºå¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼Œæ”¯æŒæ“ä½œç³»ç»Ÿè·¨å¹³å°çš„ä»»åŠ¡è®¾ç½®ã€åŸºäºæ‰§è¡Œçš„è¯„ä¼°å’Œäº¤äº’å¼å­¦ä¹ ã€‚",
              "en": "OSWorld is a first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across operating systems. "
            }
          }
        },
        {
          "id": "opencompass_1277",
          "name": "VideoGUI",
          "version": "1.0.0",
          "description": "VideoGUI is designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, it focuses on tasks involving professional and novel software and complex activities (e.g., video editing). ",
          "url": "opencompass/opencompass_1277.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1277",
          "sample_count": 1000,
          "traits": [
            "Creation"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1277",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "åˆ›ä½œ",
                "en": "Creation"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [
              {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
              }
            ],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/showlab/videogui",
            "paperLink": "https://arxiv.org/abs/2406.10227",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "58",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-07 14:14:48",
            "supportOnlineEval": false,
            "updateDate": "2025-01-07 14:14:48",
            "createDate": "2025-01-07 14:13:51",
            "desc": {
              "cn": "VideoGUIæ—¨åœ¨è¯„ä¼°ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„GUIä»»åŠ¡ä¸Šçš„GUIåŠ©æ‰‹ï¼Œæ¥è‡ªé«˜è´¨é‡çš„ç½‘ç»œæ•™å­¦è§†é¢‘ï¼Œä¾§é‡äºæ¶‰åŠä¸“ä¸šå’Œæ–°é¢–è½¯ä»¶å’Œå¤æ‚æ´»åŠ¨ï¼ˆä¾‹å¦‚è§†é¢‘ç¼–è¾‘ï¼‰çš„ä»»åŠ¡ã€‚",
              "en": "VideoGUI is designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, it focuses on tasks involving professional and novel software and complex activities (e.g., video editing). "
            }
          }
        },
        {
          "id": "opencompass_1553",
          "name": "OmniAlign-V",
          "version": "1.0.0",
          "description": "OmniAlign-V datasets mainly focus on improving the alignment of Multi-modal Large Language Models(MLLMs) with human preference. It contains 205k high-quality Image-Quetion-Answer pairs with open-ended, creative quetions and long, knowledge-rich, comprehensive answers.",
          "url": "opencompass/opencompass_1553.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1553",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1553",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/PhoenixZ810/OmniAlign-V",
            "paperLink": "https://arxiv.org/abs/2502.18411",
            "officialWebsiteLink": "https://phoenixz810.github.io/OmniAlign-V",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "57",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:49:29",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:49:29",
            "createDate": "2025-02-26 14:41:09",
            "desc": {
              "cn": "OmniAlign-V æ•°æ®é›†ä¸»è¦å…³æ³¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸äººç±»åå¥½çš„å¯¹é½ã€‚å®ƒåŒ…å« 205k ä¸ªé«˜è´¨é‡çš„å›¾åƒ-é—®ç­”å¯¹ï¼ŒåŒ…å«å¼€æ”¾å¼ã€åˆ›æ„æ€§é—®é¢˜ä»¥åŠé•¿ç¯‡ã€çŸ¥è¯†ä¸°å¯Œã€å†…å®¹å…¨é¢çš„ç­”æ¡ˆã€‚",
              "en": "OmniAlign-V datasets mainly focus on improving the alignment of Multi-modal Large Language Models(MLLMs) with human preference. It contains 205k high-quality Image-Quetion-Answer pairs with open-ended, creative quetions and long, knowledge-rich, comprehensive answers."
            }
          }
        },
        {
          "id": "opencompass_1586",
          "name": "MRAG-Bench",
          "version": "1.0.0",
          "description": "MRAG-Bench consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct scenarios, providing a robust and systematic evaluation of Large Vision Language Model (LVLM)â€™s vision-centric multimodal retrieval-augmented generation (RAG) abilities.",
          "url": "opencompass/opencompass_1586.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1586",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1586",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "VQA",
                "en": "VQA"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/mragbench/MRAG-Bench",
            "paperLink": "https://arxiv.org/abs/2410.08182",
            "officialWebsiteLink": "https://mragbench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "57",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:47:20",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:47:20",
            "createDate": "2025-03-04 13:56:04",
            "desc": {
              "cn": "MRAG-Bench åŒ…å« 16,130 å¼ å›¾ç‰‡å’Œ 1,353 ä¸ªè·¨è¶Š 9 ä¸ªä¸åŒåœºæ™¯çš„äººæ ‡æ³¨å¤šé€‰é¢˜ï¼Œä¸ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„è§†è§‰ä¸­å¿ƒå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èƒ½åŠ›æä¾›äº†ç¨³å¥å’Œç³»ç»Ÿçš„è¯„ä¼°ã€‚",
              "en": "MRAG-Bench consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct scenarios, providing a robust and systematic evaluation of Large Vision Language Model (LVLM)â€™s vision-centric multimodal retrieval-augmented generation (RAG) abilities."
            }
          }
        },
        {
          "id": "opencompass_1578",
          "name": "MMKE-Bench",
          "version": "1.0.0",
          "description": " MMKE-Bench is a benchmark designed to evaluate the ability of LMMs to edit visual knowledge in real-world scenarios. It includes 2,940 pieces of knowledge and 8,363 images across 33 broad categories, with automatically generated, human-verified evaluation questions.",
          "url": "opencompass/opencompass_1578.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1578",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1578",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/MMKE-Bench-ICLR/MMKE-Bench",
            "paperLink": "https://arxiv.org/abs/2502.19870",
            "officialWebsiteLink": "https://mmke-bench-iclr.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "55",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-04 17:02:34",
            "supportOnlineEval": false,
            "updateDate": "2025-03-04 17:02:34",
            "createDate": "2025-03-03 16:27:20",
            "desc": {
              "cn": "MMKE-Benchæ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼° LMM åœ¨ç°å®åœºæ™¯ä¸­ç¼–è¾‘è§†è§‰çŸ¥è¯†èƒ½åŠ›çš„åŸºå‡†ï¼ŒåŒ…æ‹¬ 33 ä¸ªå¹¿æ³›ç±»åˆ«ä¸­çš„ 2,940 æ¡çŸ¥è¯†å’Œ 8,363 å¼ å›¾åƒï¼Œä»¥åŠè‡ªåŠ¨ç”Ÿæˆå¹¶ç”±äººå·¥éªŒè¯çš„è¯„ä¼°é—®é¢˜ã€‚",
              "en": " MMKE-Bench is a benchmark designed to evaluate the ability of LMMs to edit visual knowledge in real-world scenarios. It includes 2,940 pieces of knowledge and 8,363 images across 33 broad categories, with automatically generated, human-verified evaluation questions."
            }
          }
        },
        {
          "id": "opencompass_1583",
          "name": "JudgeBench",
          "version": "1.0.0",
          "description": "JudgeBench is a benchmark aimed at evaluating LLM-based judges for objective correctness on challenging response pairs.",
          "url": "opencompass/opencompass_1583.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1583",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1583",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/ScalerLab/JudgeBench",
            "paperLink": "https://arxiv.org/abs/2410.12784",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "55",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:48:25",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:48:25",
            "createDate": "2025-03-04 11:06:42",
            "desc": {
              "cn": "JudgeBench æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°åŸºäºLLMçš„è£åˆ¤åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å“åº”å¯¹ä¸Šçš„å®¢è§‚æ­£ç¡®æ€§çš„åŸºå‡†ã€‚",
              "en": "JudgeBench is a benchmark aimed at evaluating LLM-based judges for objective correctness on challenging response pairs."
            }
          }
        },
        {
          "id": "opencompass_1363",
          "name": "SEED-Bench-2",
          "version": "1.0.0",
          "description": "SEED-Bench-2 assesses both text and image generation of MLLMs. It spans 27 evaluation dimensions, featuring 24K multiple-choice questions with precise human annotations.",
          "url": "opencompass/opencompass_1363.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1363",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1363",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
            "paperLink": "https://arxiv.org/abs/2311.17092",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "55",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-14 15:23:52",
            "supportOnlineEval": false,
            "updateDate": "2025-01-14 15:23:52",
            "createDate": "2025-01-10 11:31:59",
            "desc": {
              "cn": "SEED-Bench-2ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆèƒ½åŠ›ï¼ŒåŒ…æ‹¬è·¨è¶Š27ä¸ªç»´åº¦çš„24Ké“å¤šé€‰é¢˜åŠå‡†ç¡®çš„äººå·¥æ³¨é‡Šã€‚",
              "en": "SEED-Bench-2 assesses both text and image generation of MLLMs. It spans 27 evaluation dimensions, featuring 24K multiple-choice questions with precise human annotations."
            }
          }
        },
        {
          "id": "opencompass_1635",
          "name": "KnowLogic",
          "version": "1.0.0",
          "description": "KnowLogic is a knowledge-driven synthetic benchmark designed to evaluate the reasoning abilities of large language models (LLMs). It includes 5400 bilingual (Chinese and English) questions across various domains, covering different aspects of commonsense knowledge and logical reasoning.",
          "url": "opencompass/opencompass_1635.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1635",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1635",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/pokerwf/KnowLogic",
            "paperLink": "https://arxiv.org/abs/2503.06218",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "55",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-13 15:31:21",
            "supportOnlineEval": false,
            "updateDate": "2025-03-13 15:31:21",
            "createDate": "2025-03-13 14:36:03",
            "desc": {
              "cn": "KnowLogic æ˜¯ä¸€ä¸ªä»¥çŸ¥è¯†é©±åŠ¨çš„åˆæˆåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ˆLLMsï¼‰ã€‚å®ƒåŒ…å«æ¶µç›–å„ä¸ªé¢†åŸŸã€æ¶µç›–å¸¸è¯†çŸ¥è¯†å’Œé€»è¾‘æ¨ç†ä¸åŒæ–¹é¢çš„ 5400 ä¸ªä¸­è‹±åŒè¯­é—®é¢˜ã€‚",
              "en": "KnowLogic is a knowledge-driven synthetic benchmark designed to evaluate the reasoning abilities of large language models (LLMs). It includes 5400 bilingual (Chinese and English) questions across various domains, covering different aspects of commonsense knowledge and logical reasoning."
            }
          }
        },
        {
          "id": "opencompass_1636",
          "name": "UrbanVideo-Bench",
          "version": "1.0.0",
          "description": "The benchmark is designed to evaluate whether video-large language models (Video-LLMs) can naturally process continuous first-person visual observations like humans, enabling recall, perception, reasoning, and navigation.",
          "url": "opencompass/opencompass_1636.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1636",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1636",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/EmbodiedCity/UrbanVideo-Bench.code",
            "paperLink": "https://arxiv.org/abs/2503.06157",
            "officialWebsiteLink": "https://embodiedcity.github.io/UrbanVideo-Bench/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "55",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-13 15:31:18",
            "supportOnlineEval": false,
            "updateDate": "2025-03-13 15:31:18",
            "createDate": "2025-03-13 14:54:33",
            "desc": {
              "cn": "UrbanVideo-Benchæ—¨åœ¨è¯„ä¼°è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰æ˜¯å¦èƒ½å¤Ÿåƒäººç±»ä¸€æ ·è‡ªç„¶åœ°å¤„ç†è¿ç»­çš„ç¬¬ä¸€äººç§°è§†è§‰è§‚å¯Ÿï¼Œå®ç°å›å¿†ã€æ„ŸçŸ¥ã€æ¨ç†å’Œå¯¼èˆªã€‚",
              "en": "The benchmark is designed to evaluate whether video-large language models (Video-LLMs) can naturally process continuous first-person visual observations like humans, enabling recall, perception, reasoning, and navigation."
            }
          }
        },
        {
          "id": "opencompass_1645",
          "name": "MastermindEval",
          "version": "1.0.0",
          "description": "Evaluating Reasoning Capabilities of LLMs Using the Mastermind Board Game.",
          "url": "opencompass/opencompass_1645.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1645",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1645",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/flairNLP/mastermind",
            "paperLink": "https://arxiv.org/abs/2503.05891",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "54",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-18 16:17:33",
            "supportOnlineEval": false,
            "updateDate": "2025-03-18 16:17:33",
            "createDate": "2025-03-18 16:08:48",
            "desc": {
              "cn": "MastermindEvalä½¿ç”¨çŒœè°œæ¸¸æˆæ£‹ç›˜è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚\n",
              "en": "Evaluating Reasoning Capabilities of LLMs Using the Mastermind Board Game."
            }
          }
        },
        {
          "id": "opencompass_1585",
          "name": "MMAD",
          "version": "1.0.0",
          "description": "MMAD is the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. \nResearchers defined seven key subtasks of MLLMs in industrial inspection and designed a novel pipeline to generate the MMAD dataset with 39,672 questions for 8,366 industrial images. ",
          "url": "opencompass/opencompass_1585.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1585",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1585",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "VQA",
                "en": "VQA"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/jam-cc/MMAD",
            "paperLink": "https://arxiv.org/abs/2410.09453",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "53",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:47:27",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:47:27",
            "createDate": "2025-03-04 13:48:12",
            "desc": {
              "cn": "MMADæ˜¯ç¬¬ä¸€ä¸ªå·¥ä¸šå¼‚å¸¸æ£€æµ‹é¢†åŸŸçš„å…¨è°± MLLMs åŸºå‡†ï¼Œç ”ç©¶äººå‘˜å®šä¹‰äº†å·¥ä¸šæ£€æµ‹ä¸­ MLLMs çš„ä¸ƒä¸ªå…³é”®å­ä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„æµç¨‹æ¥ç”ŸæˆåŒ…å« 39,672 ä¸ªé—®é¢˜ä»¥åŠ 8,366 ä¸ªå·¥ä¸šå›¾åƒçš„ MMAD æ•°æ®é›†ã€‚",
              "en": "MMAD is the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. \nResearchers defined seven key subtasks of MLLMs in industrial inspection and designed a novel pipeline to generate the MMAD dataset with 39,672 questions for 8,366 industrial images. "
            }
          }
        },
        {
          "id": "opencompass_1717",
          "name": "ViLBench",
          "version": "1.0.0",
          "description": "ViLBench is a benchmark designed to evaluate vision-language models. It features 600 examples from 5 datasets, selected based on the criterion that process reward models offer greater improvements over output reward models in guiding generations.",
          "url": "opencompass/opencompass_1717.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1717",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1717",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "vision-language",
                "en": "vision-language"
              },
              {
                "cn": "math",
                "en": "math"
              },
              {
                "cn": "reasoning",
                "en": "reasoning"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2503.20271",
            "officialWebsiteLink": "https://ucsc-vlaa.github.io/ViLBench/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "62408676",
              "name": null,
              "avatar": null,
              "nickname": "ImKe"
            },
            "lookNum": "53",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-08 10:14:31",
            "supportOnlineEval": false,
            "updateDate": "2025-04-08 10:14:31",
            "createDate": "2025-04-08 02:53:29",
            "desc": {
              "cn": "ViLBench æ˜¯ä¸€é¡¹æ—¨åœ¨è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ•°æ®é›†ï¼Œå…¶å¼ºè°ƒå¯¹æ¨¡å‹è¿›è¡Œç»†ç²’åº¦çš„é€æ­¥æ¨ç†èƒ½åŠ›æµ‹è¯•ã€‚è¯¥åŸºå‡†å…±åŒ…å«600ä¸ªç»è¿‡ä¸¥æ ¼ç­›é€‰çš„æ ·æœ¬ï¼Œæ¥æºäºäº”ä¸ªä¸åŒçš„è§†è§‰-è¯­è¨€æ•°æ®é›†ï¼Œç­›é€‰æ ‡å‡†æ˜¯åœ¨æ¨¡å‹ç­”æ¡ˆé€‰æ‹©è¿‡ç¨‹ä¸­ï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆprocess-reward modelï¼‰ç›¸è¾ƒäºè¾“å‡ºå¥–åŠ±æ¨¡å‹ï¼ˆoutput-reward modelï¼‰å…·æœ‰æ›´æ˜¾è‘—çš„æ€§èƒ½æå‡æ•ˆæœã€‚",
              "en": "ViLBench is a benchmark designed to evaluate vision-language models. It features 600 examples from 5 datasets, selected based on the criterion that process reward models offer greater improvements over output reward models in guiding generations."
            }
          }
        },
        {
          "id": "opencompass_1510",
          "name": "LongVideoBench",
          "version": "1.0.0",
          "description": "LongVideoBench tests LMMs' understanding of long videos. It's a question-answering benchmark with video-language interleaved inputs up to an hour long and comprises 3,763 web-collected videos with subtitles across diverse themes,",
          "url": "opencompass/opencompass_1510.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1510",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1510",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/longvideobench/LongVideoBench",
            "paperLink": "https://arxiv.org/abs/2407.15754",
            "officialWebsiteLink": "https://longvideobench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "53",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:51:19",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:51:19",
            "createDate": "2025-02-17 15:50:45",
            "desc": {
              "cn": "LongVideoBenchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„é•¿è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œæ˜¯åŸºäºäº¤é”™é•¿è§†é¢‘è¯­æ–™æ„å»ºçš„é—®ç­”é›†ï¼ŒåŒ…å«3763ä¸ªä¸åŒä¸»é¢˜çš„å¸¦å­—å¹•çš„è§†é¢‘ã€‚",
              "en": "LongVideoBench tests LMMs' understanding of long videos. It's a question-answering benchmark with video-language interleaved inputs up to an hour long and comprises 3,763 web-collected videos with subtitles across diverse themes,"
            }
          }
        },
        {
          "id": "opencompass_1513",
          "name": "CG-Bench",
          "version": "1.0.0",
          "description": "CG-Bench is meant for evaluating MLLMs' long video understanding, including 12,129 QA pairs from 1219 videos in 3 major question types: perception, reasoning, and hallucination.",
          "url": "opencompass/opencompass_1513.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1513",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1513",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/CG-Bench/CG-Bench",
            "paperLink": "https://arxiv.org/abs/2412.12075v1",
            "officialWebsiteLink": "https://cg-bench.github.io/leaderboard/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "53",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:51:42",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:51:42",
            "createDate": "2025-02-17 16:29:23",
            "desc": {
              "cn": "CG-Benchç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„é•¿è§†é¢‘ç†è§£èƒ½åŠ›ï¼ŒåŸºäº1219ä¸ªè§†é¢‘è®¾è®¡äº†12129ä¸ªæ¶µç›–æ„ŸçŸ¥ã€æ¨ç†å’Œå¹»è§‰ä¸‰ç§é—®é¢˜ç±»å‹çš„QAå¯¹ã€‚",
              "en": "CG-Bench is meant for evaluating MLLMs' long video understanding, including 12,129 QA pairs from 1219 videos in 3 major question types: perception, reasoning, and hallucination."
            }
          }
        },
        {
          "id": "opencompass_1571",
          "name": "BRIGHT",
          "version": "1.0.0",
          "description": "BRIGHT is the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. ",
          "url": "opencompass/opencompass_1571.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1571",
          "sample_count": 1000,
          "traits": [
            "Strong Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1571",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¼ºæ¨ç†",
                "en": "Strong Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/xlang-ai/BRIGHT",
            "paperLink": "https://arxiv.org/abs/2407.12883",
            "officialWebsiteLink": "https://brightbenchmark.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "52",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:50:14",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:50:14",
            "createDate": "2025-02-27 18:29:17",
            "desc": {
              "cn": "BRIGHT æ˜¯ç¬¬ä¸€ä¸ªéœ€è¦å¤§é‡æ¨ç†æ¥æ£€ç´¢ç›¸å…³æ–‡æ¡£çš„æ–‡æœ¬æ£€ç´¢åŸºå‡†ã€‚",
              "en": "BRIGHT is the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. "
            }
          }
        },
        {
          "id": "opencompass_1658",
          "name": "MiLiC-Eval",
          "version": "1.0.0",
          "description": "MiLiC-Eval is an NLP evaluation suite for Minority Languages in China, covering Tibetan (bo), Uyghur (ug), Kazakh (kk, in the Kazakh Arabic script), and Mongolian (mn, in the traditional Mongolian script).",
          "url": "opencompass/opencompass_1658.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1658",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1658",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/luciusssss/MiLiC-Eval",
            "paperLink": "https://arxiv.org/abs/2503.01150",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "52",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-20 14:26:55",
            "supportOnlineEval": false,
            "updateDate": "2025-03-20 14:26:55",
            "createDate": "2025-03-20 14:21:29",
            "desc": {
              "cn": "MiLiC-Eval æ˜¯é’ˆå¯¹ä¸­å›½å°‘æ•°æ°‘æ—è¯­è¨€çš„ NLP è¯„ä¼°å¥—ä»¶ï¼Œæ¶µç›–è—è¯­ï¼ˆboï¼‰ã€ç»´å¾å°”è¯­ï¼ˆugï¼‰ã€å“ˆè¨å…‹è¯­ï¼ˆkkï¼Œä½¿ç”¨å“ˆè¨å…‹é˜¿æ‹‰ä¼¯æ–‡è„šæœ¬ï¼‰å’Œè’™å¤è¯­ï¼ˆmnï¼Œä½¿ç”¨ä¼ ç»Ÿè’™å¤æ–‡è„šæœ¬ï¼‰ã€‚",
              "en": "MiLiC-Eval is an NLP evaluation suite for Minority Languages in China, covering Tibetan (bo), Uyghur (ug), Kazakh (kk, in the Kazakh Arabic script), and Mongolian (mn, in the traditional Mongolian script)."
            }
          }
        },
        {
          "id": "opencompass_1671",
          "name": "Forensics-bench",
          "version": "1.0.0",
          "description": "A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models",
          "url": "opencompass/opencompass_1671.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1671",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Safety"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1671",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "å®‰å…¨",
                "en": "Safety"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Forgery Detection",
                "en": "Forgery Detection"
              },
              {
                "cn": "Large Vision Language Models",
                "en": "Large Vision Language Models"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Forensics-Bench/Forensics-Bench",
            "paperLink": "https://arxiv.org/pdf/2503.15024",
            "officialWebsiteLink": "https://forensics-bench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "5001578",
              "name": null,
              "avatar": null,
              "nickname": "åŠ²åŠ²"
            },
            "lookNum": "52",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-22 12:30:46",
            "supportOnlineEval": false,
            "updateDate": "2025-04-22 12:30:46",
            "createDate": "2025-03-24 22:19:51",
            "desc": {
              "cn": "A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models",
              "en": "A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models"
            }
          }
        },
        {
          "id": "opencompass_1633",
          "name": "ProJudge",
          "version": "1.0.0",
          "description": "ProJudge is a comprehensive, multi-modal, multi-discipline, and multi-difficulty benchmark specifically designed for evaluating abilities of MLLM-based process judges.It comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and",
          "url": "opencompass/opencompass_1633.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1633",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1633",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/jiaxin-ai/ProJudge",
            "paperLink": "https://arxiv.org/abs/2503.06553",
            "officialWebsiteLink": "https://projudge.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "50",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-13 15:31:28",
            "supportOnlineEval": false,
            "updateDate": "2025-03-13 15:31:28",
            "createDate": "2025-03-13 14:14:35",
            "desc": {
              "cn": "ProJudge æ˜¯ä¸€ä¸ªé’ˆå¯¹åŸºäº MLLM çš„è¿‡ç¨‹è£åˆ¤èƒ½åŠ›çš„å…¨é¢ã€å¤šæ¨¡æ€ã€å¤šå­¦ç§‘å’Œå¤šéš¾åº¦çš„åŸºå‡†ã€‚å®ƒåŒ…å« 2,400 ä¸ªæµ‹è¯•æ¡ˆä¾‹å’Œ 50,118 ä¸ªæ­¥éª¤çº§æ ‡ç­¾ï¼Œæ¶µç›–å››ä¸ªç§‘å­¦å­¦ç§‘ï¼Œéš¾åº¦çº§åˆ«å’Œå†…å®¹å¤šæ ·åŒ–ã€‚",
              "en": "ProJudge is a comprehensive, multi-modal, multi-discipline, and multi-difficulty benchmark specifically designed for evaluating abilities of MLLM-based process judges.It comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and"
            }
          }
        },
        {
          "id": "opencompass_1634",
          "name": "VisualSimpleQA",
          "version": "1.0.0",
          "description": "VisualSimpleQA is a multimodal fact-seeking benchmark with two key features. ",
          "url": "opencompass/opencompass_1634.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1634",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1634",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2503.06492",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "50",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-13 15:31:25",
            "supportOnlineEval": false,
            "updateDate": "2025-03-13 15:31:25",
            "createDate": "2025-03-13 14:29:04",
            "desc": {
              "cn": "VisualSimpleQA æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€äº‹å®å¯»æ±‚åŸºå‡†ï¼Œå…·æœ‰ä¸¤ä¸ªå…³é”®ç‰¹æ€§ã€‚é¦–å…ˆï¼Œå®ƒä½¿è§†è§‰å’Œè¯­è¨€æ¨¡æ€ä¸­ LVLMs çš„è¯„ä¼°æ›´åŠ ç®€åŒ–å’Œè§£è€¦ã€‚å…¶æ¬¡ï¼Œå®ƒçº³å…¥äº†æ˜ç¡®çš„éš¾åº¦æ ‡å‡†ï¼Œä»¥æŒ‡å¯¼äººå·¥æ ‡æ³¨å¹¶ä¿ƒè¿›æå–å…·æœ‰æŒ‘æˆ˜æ€§çš„å­é›†ï¼Œå³ VisualSimpleQA-hardã€‚",
              "en": "VisualSimpleQA is a multimodal fact-seeking benchmark with two key features. "
            }
          }
        },
        {
          "id": "opencompass_1712",
          "name": "ToolHop",
          "version": "1.0.0",
          "description": "ToolHop is a dataset specifically designed for rigorous evaluation of multi-hop tool use, which ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach.",
          "url": "opencompass/opencompass_1712.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1712",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1712",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              },
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://huggingface.co/datasets/bytedance-research/ToolHop",
            "paperLink": "https://arxiv.org/abs/2501.02506",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "62407087",
              "name": null,
              "avatar": null,
              "nickname": "å°¼æ‘©"
            },
            "lookNum": "50",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-07 15:15:34",
            "supportOnlineEval": false,
            "updateDate": "2025-04-07 15:15:34",
            "createDate": "2025-04-05 23:37:16",
            "desc": {
              "cn": "ToolHopæ˜¯ä¸€ä¸ªé€šè¿‡æŸ¥è¯¢é©±åŠ¨æ„å»ºçš„æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè¯„æµ‹å¤§æ¨¡å‹çš„å¤šè·³å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œå…·å¤‡å¤šæ ·åŒ–çš„æŸ¥è¯¢ã€æœ‰æ„ä¹‰çš„ç›¸äº’ä¾èµ–å…³ç³»ã€æœ¬åœ°å¯æ‰§è¡Œçš„å·¥å…·ã€è¯¦ç»†çš„åé¦ˆå’Œå¯éªŒè¯çš„ç­”æ¡ˆäº”å¤§ç‰¹å¾ã€‚",
              "en": "ToolHop is a dataset specifically designed for rigorous evaluation of multi-hop tool use, which ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach."
            }
          }
        },
        {
          "id": "opencompass_1604",
          "name": "EgoNormia",
          "version": "1.0.0",
          "description": "EgoNormia is a challenging QA benchmark that tests VLMs' ability to reason over norms in context. The datset consists of 1,853 physically grounded egocentric interaction clips from Ego4D and corresponding five-way multiple-choice questions tasks for each.",
          "url": "opencompass/opencompass_1604.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1604",
          "sample_count": 1000,
          "traits": [
            "Examination",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1604",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Open-Social-World/EgoNormia",
            "paperLink": "https://arxiv.org/abs/2502.20490",
            "officialWebsiteLink": "https://opensocial.world/leaderboard",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "49",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:45:43",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:45:43",
            "createDate": "2025-03-06 13:51:13",
            "desc": {
              "cn": "EgoNormia æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®ç­”åŸºå‡†ï¼Œç”¨äºæµ‹è¯• VLMs åœ¨ä¸Šä¸‹æ–‡ä¸­æ¨ç†è§„èŒƒçš„èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª Ego4D çš„ 1,853 ä¸ªç‰©ç†åŸºç¡€åŒ–çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„äº¤äº’å‰ªè¾‘ï¼Œä»¥åŠæ¯ä¸ªå‰ªè¾‘å¯¹åº”çš„äº”é€‰ä¸€å¤šé¡¹é€‰æ‹©é¢˜ä»»åŠ¡ã€‚",
              "en": "EgoNormia is a challenging QA benchmark that tests VLMs' ability to reason over norms in context. The datset consists of 1,853 physically grounded egocentric interaction clips from Ego4D and corresponding five-way multiple-choice questions tasks for each."
            }
          }
        },
        {
          "id": "opencompass_1395",
          "name": "SEED-Bench-2-Plus",
          "version": "1.0.0",
          "description": "SEED-Bench-2-Plus, a benchmark specifically designed for evaluating text-rich visual comprehension of MLLMs. The benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs.",
          "url": "opencompass/opencompass_1395.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1395",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1395",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "text-rich image",
                "en": "text-rich image"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
            "paperLink": "https://arxiv.org/pdf/2404.16790",
            "officialWebsiteLink": "https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2-plus",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "40068663",
              "name": null,
              "avatar": null,
              "nickname": "è±ªà¼™à¾‡"
            },
            "lookNum": "49",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-01-15 18:33:41",
            "supportOnlineEval": false,
            "updateDate": "2025-01-15 18:33:41",
            "createDate": "2025-01-15 17:20:44",
            "desc": {
              "cn": "SEED-Bench-2-Plus, a benchmark specifically designed for evaluating text-rich visual comprehension of MLLMs. The benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs.",
              "en": "SEED-Bench-2-Plus, a benchmark specifically designed for evaluating text-rich visual comprehension of MLLMs. The benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs."
            }
          }
        },
        {
          "id": "opencompass_1502",
          "name": "MTVQA",
          "version": "1.0.0",
          "description": "MTVQA is designed to evaluate LMMs' multilingual text understanding, featuring high-quality human expert annotations across 9 diverse languages. ",
          "url": "opencompass/opencompass_1502.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1502",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1502",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "VQA",
                "en": "VQA"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/bytedance/MTVQA",
            "paperLink": "https://arxiv.org/abs/2405.11985",
            "officialWebsiteLink": "https://bytedance.github.io/MTVQA/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "49",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:50:52",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:50:52",
            "createDate": "2025-02-14 20:41:37",
            "desc": {
              "cn": "MTVQAç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹ç†è§£å¤šè¯­è¨€æ–‡æœ¬çš„èƒ½åŠ›ï¼ŒåŒ…å«æ¥è‡ª9ç§è¯­è¨€çš„ç”±äººç±»ä¸“å®¶æ³¨é‡Šçš„é«˜è´¨é‡æ•°æ®ã€‚",
              "en": "MTVQA is designed to evaluate LMMs' multilingual text understanding, featuring high-quality human expert annotations across 9 diverse languages. "
            }
          }
        },
        {
          "id": "opencompass_1533",
          "name": "NutritionQA",
          "version": "1.0.0",
          "description": "CoSyn-400K dataset contains 9 categories of synthetc text-rich images with 2.7M instruction-tuning data",
          "url": "opencompass/opencompass_1533.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1533",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1533",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Multimodal",
                "en": "Multimodal"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/allenai/pixmo-docs",
            "paperLink": "https://arxiv.org/abs/2502.14846",
            "officialWebsiteLink": "https://yueyang1996.github.io/cosyn/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "49",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 17:00:55",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 17:00:55",
            "createDate": "2025-02-24 13:51:34",
            "desc": {
              "cn": "é€šè¿‡ä»£ç å¼•å¯¼çš„åˆæˆå¤šæ¨¡æ€æ•°æ®ç”Ÿæˆæ‰©å±•æ–‡æœ¬ä¸°å¯Œå›¾åƒç†è§£ï¼ŒCoSyn-400K æ•°æ®é›†åŒ…å« 9 ç±»åˆæˆæ–‡æœ¬ä¸°å¯Œå›¾åƒï¼Œä»¥åŠ 270 ä¸‡æ¡æŒ‡ä»¤å¾®è°ƒæ•°æ®ã€‚",
              "en": "CoSyn-400K dataset contains 9 categories of synthetc text-rich images with 2.7M instruction-tuning data"
            }
          }
        },
        {
          "id": "opencompass_1555",
          "name": "WildBench",
          "version": "1.0.0",
          "description": "Weintroduce WildBench, an automated evaluation framework designed to bench-mark large language models (LLMs) using challenging, real-world user queries, which consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs.",
          "url": "opencompass/opencompass_1555.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1555",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1555",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/allenai/WildBench",
            "paperLink": "https://arxiv.org/abs/2406.04770",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "48",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:49:39",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:49:39",
            "createDate": "2025-02-26 17:05:44",
            "desc": {
              "cn": "WildBenchæ¨å‡ºè‡ªåŠ¨è¯„ä¼°æ¡†æ¶å’Œæ•°æ®é›†ï¼ŒåŸºäºçœŸå®ç”¨æˆ·éš¾é¢˜è¯„æµ‹å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…å«ä»é€¾ç™¾ä¸‡äººæœºå¯¹è¯æ—¥å¿—ä¸­ç²¾é€‰çš„1,024ä¸ªä»»åŠ¡æ ·æœ¬ã€‚",
              "en": "Weintroduce WildBench, an automated evaluation framework designed to bench-mark large language models (LLMs) using challenging, real-world user queries, which consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs."
            }
          }
        },
        {
          "id": "opencompass_1632",
          "name": "ProBench",
          "version": "1.0.0",
          "description": "ProBench is a benchmark that contains open-ended multimodal queries that require intensive expert-level knowledge to solve. ",
          "url": "opencompass/opencompass_1632.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1632",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1632",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Yan98/ProBench_eval",
            "paperLink": "https://arxiv.org/abs/2503.06885",
            "officialWebsiteLink": "https://yan98.github.io/ProBench/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "48",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-13 15:31:13",
            "supportOnlineEval": false,
            "updateDate": "2025-03-13 15:31:13",
            "createDate": "2025-03-13 14:00:05",
            "desc": {
              "cn": "ProBenchæ˜¯ä¸€ä¸ªåŒ…å«éœ€è¦å¤§é‡ä¸“å®¶çº§çŸ¥è¯†æ¥è§£å†³çš„å¼€æ”¾å¼å¤šæ¨¡æ€æŸ¥è¯¢çš„åŸºå‡†ã€‚ProBench åŒ…å« 10 ä¸ªä»»åŠ¡é¢†åŸŸå’Œ 56 ä¸ªå­é¢†åŸŸï¼Œæ”¯æŒ 17 ç§è¯­è¨€ï¼Œå¹¶æ”¯æŒæœ€å¤š 13 è½®å¯¹è¯ã€‚",
              "en": "ProBench is a benchmark that contains open-ended multimodal queries that require intensive expert-level knowledge to solve. "
            }
          }
        },
        {
          "id": "opencompass_1512",
          "name": "MLVU",
          "version": "1.0.0",
          "description": "MLVU test MLLMs' understanding of multi-task long videos, encompassing diverse tasks based on various long videos. ",
          "url": "opencompass/opencompass_1512.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1512",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1512",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/JUNJIE99/MLVU",
            "paperLink": "https://arxiv.org/abs/2406.04264",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "47",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:51:32",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:51:32",
            "createDate": "2025-02-17 16:07:19",
            "desc": {
              "cn": "MLVUç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„é•¿è§†é¢‘ç†è§£èƒ½åŠ›ï¼ŒåŒ…å«é¢å‘å„ç§ç±»å‹é•¿è§†é¢‘çš„å¤šæ ·åŒ–çš„è¯„ä¼°ä»»åŠ¡ã€‚",
              "en": "MLVU test MLLMs' understanding of multi-task long videos, encompassing diverse tasks based on various long videos. "
            }
          }
        },
        {
          "id": "opencompass_1640",
          "name": "EMMA",
          "version": "1.0.0",
          "description": "EMMA is composed of 2,788 problems, of which 1,796 are newly constructed, across four domains. Within each subject, we further provide fine-grained labels for each question based on the specific skills it measures.",
          "url": "opencompass/opencompass_1640.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1640",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1640",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/hychaochao/EMMA",
            "paperLink": "https://www.arxiv.org/abs/2501.05444",
            "officialWebsiteLink": "https://emma-benchmark.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "46",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-17 10:46:29",
            "supportOnlineEval": false,
            "updateDate": "2025-03-17 10:46:29",
            "createDate": "2025-03-17 10:44:03",
            "desc": {
              "cn": "EMMA ç”± 2,788 ä¸ªé—®é¢˜ç»„æˆï¼Œå…¶ä¸­ 1,796 ä¸ªæ˜¯æ–°æ„å»ºçš„ï¼Œæ¶µç›–å››ä¸ªé¢†åŸŸã€‚åœ¨æ¯ä¸ªä¸»é¢˜ä¸­ï¼Œæˆ‘ä»¬æ ¹æ®æ‰€æµ‹é‡çš„å…·ä½“æŠ€èƒ½ä¸ºæ¯ä¸ªé—®é¢˜æä¾›ç»†ç²’åº¦æ ‡ç­¾ã€‚",
              "en": "EMMA is composed of 2,788 problems, of which 1,796 are newly constructed, across four domains. Within each subject, we further provide fine-grained labels for each question based on the specific skills it measures."
            }
          }
        },
        {
          "id": "opencompass_1621",
          "name": "ProcessBench",
          "version": "1.0.0",
          "description": "ProcessBench can measure the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. ",
          "url": "opencompass/opencompass_1621.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1621",
          "sample_count": 1000,
          "traits": [
            "Examination",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1621",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/QwenLM/ProcessBench",
            "paperLink": "https://arxiv.org/abs/2412.06559",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "45",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-12 11:08:00",
            "supportOnlineEval": false,
            "updateDate": "2025-03-12 11:08:00",
            "createDate": "2025-03-11 15:28:49",
            "desc": {
              "cn": "ProcessBenchï¼Œç”¨äºè¡¡é‡è¯†åˆ«æ•°å­¦æ¨ç†ä¸­é”™è¯¯æ­¥éª¤çš„èƒ½åŠ›ã€‚å®ƒåŒ…å« 3,400 ä¸ªæµ‹è¯•æ¡ˆä¾‹ï¼Œä¸»è¦å…³æ³¨ç«èµ›å’Œå¥¥æ—åŒ¹å…‹çº§åˆ«çš„æ•°å­¦é—®é¢˜ã€‚",
              "en": "ProcessBench can measure the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. "
            }
          }
        },
        {
          "id": "opencompass_1656",
          "name": "RFUAV",
          "version": "1.0.0",
          "description": "RFUAV offers a comprehensive benchmark dataset for Radio-Frequency (RF)-based drone detection and identification.",
          "url": "opencompass/opencompass_1656.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1656",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1656",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/kitoweeknd/RFUAV/",
            "paperLink": "https://arxiv.org/pdf/2503.09033",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "45",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-20 14:27:02",
            "supportOnlineEval": false,
            "updateDate": "2025-03-20 14:27:02",
            "createDate": "2025-03-20 14:05:07",
            "desc": {
              "cn": "RFUAV æä¾›äº†ä¸€ä¸ªåŸºäºå°„é¢‘ï¼ˆRFï¼‰çš„æ— äººæœºæ£€æµ‹å’Œè¯†åˆ«çš„å…¨é¢åŸºå‡†æ•°æ®é›†ã€‚",
              "en": "RFUAV offers a comprehensive benchmark dataset for Radio-Frequency (RF)-based drone detection and identification."
            }
          }
        },
        {
          "id": "opencompass_1552",
          "name": "CEB",
          "version": "1.0.0",
          "description": "CEB evaluates LLM bias compositionally, featuring 11k samples characterized across bias types, social groups, and tasks.",
          "url": "opencompass/opencompass_1552.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1552",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1552",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/SongW-SW/CEB",
            "paperLink": "https://arxiv.org/abs/2407.02408",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "44",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-06 16:49:21",
            "supportOnlineEval": false,
            "updateDate": "2025-03-06 16:49:21",
            "createDate": "2025-02-26 14:20:34",
            "desc": {
              "cn": "CEBæ˜¯ä¸€ä¸ªç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹åå·®çš„ç»„æˆè¯„ä¼°åŸºå‡†ï¼Œå¼•å…¥äº†åŒ…å« 11,004 ä¸ªæ ·æœ¬çš„ç»„æˆè¯„ä¼°åŸºå‡†ï¼Œä»åå·®ç±»å‹ã€ç¤¾ä¼šç¾¤ä½“å’Œä»»åŠ¡ä¸‰ä¸ªç»´åº¦æè¿°æ¯ä¸ªæ•°æ®é›†ã€‚",
              "en": "CEB evaluates LLM bias compositionally, featuring 11k samples characterized across bias types, social groups, and tasks."
            }
          }
        },
        {
          "id": "opencompass_1670",
          "name": "IndicMMLU-Pro",
          "version": "1.0.0",
          "description": "IndicMMLU-Pro provides a standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models.",
          "url": "opencompass/opencompass_1670.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1670",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1670",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2501.15747",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "43",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-24 18:48:16",
            "supportOnlineEval": false,
            "updateDate": "2025-03-24 18:48:16",
            "createDate": "2025-03-24 18:46:36",
            "desc": {
              "cn": "IndicMMLU-Pro æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ¨åŠ¨å°åº¦è¯­ç³»è¯­è¨€ AI çš„ç ”ç©¶è¾¹ç•Œï¼Œä¿ƒè¿›æ›´å‡†ç¡®ã€é«˜æ•ˆå’Œå…·æœ‰æ–‡åŒ–æ•æ„Ÿæ€§çš„æ¨¡å‹çš„å‘å±•ã€‚",
              "en": "IndicMMLU-Pro provides a standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models."
            }
          }
        },
        {
          "id": "opencompass_1679",
          "name": "ContextualJudgeBench",
          "version": "1.0.0",
          "description": "ContextualJudgeBench is a pairwise benchmark with 2,000 samples for evaluating LLM-as-judge models in two contextual settings: Contextual QA and summarization. We propose a pairwise evaluation hierarchy and generate splits for our proposed hierarchy.",
          "url": "opencompass/opencompass_1679.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1679",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1679",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/SalesforceAIResearch/ContextualJudgeBench",
            "paperLink": "https://arxiv.org/abs/2503.15620",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "43",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-26 15:56:25",
            "supportOnlineEval": false,
            "updateDate": "2025-03-26 15:56:25",
            "createDate": "2025-03-26 14:40:00",
            "desc": {
              "cn": "ContextualJudgeBench æ˜¯ä¸€ä¸ªåŒ…å« 2,000 ä¸ªæ ·æœ¬çš„æˆå¯¹åŸºå‡†ï¼Œç”¨äºè¯„ä¼°åœ¨ä¸¤ä¸ªä¸Šä¸‹æ–‡ç¯å¢ƒä¸‹çš„LLM-as-judge æ¨¡å‹ï¼šä¸Šä¸‹æ–‡é—®ç­”å’Œæ‘˜è¦ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªæˆå¯¹è¯„ä¼°å±‚æ¬¡ç»“æ„ï¼Œå¹¶ä¸ºæˆ‘ä»¬çš„å±‚æ¬¡ç»“æ„ç”Ÿæˆåˆ†å‰²ã€‚",
              "en": "ContextualJudgeBench is a pairwise benchmark with 2,000 samples for evaluating LLM-as-judge models in two contextual settings: Contextual QA and summarization. We propose a pairwise evaluation hierarchy and generate splits for our proposed hierarchy."
            }
          }
        },
        {
          "id": "opencompass_1624",
          "name": "CodeElo",
          "version": "1.0.0",
          "description": "Researchers introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. ",
          "url": "opencompass/opencompass_1624.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1624",
          "sample_count": 1000,
          "traits": [
            "Code"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1624",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ä»£ç ",
                "en": "Code"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/QwenLM/CodeElo",
            "paperLink": "https://arxiv.org/abs/2501.01257",
            "officialWebsiteLink": "https://codeelo-bench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "41",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-12 11:07:44",
            "supportOnlineEval": false,
            "updateDate": "2025-03-12 11:07:44",
            "createDate": "2025-03-11 15:53:47",
            "desc": {
              "cn": "CodeEloï¼Œè¿™æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–çš„ç«èµ›çº§ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼Œæœ‰æ•ˆè§£å†³äº†æ‰€æœ‰è¿™äº›æŒ‘æˆ˜ã€‚CodeElo åŸºå‡†æµ‹è¯•ä¸»è¦åŸºäºå®˜æ–¹ CodeForces å¹³å°ï¼Œå¹¶å°½å¯èƒ½ä¸è¯¥å¹³å°ä¿æŒä¸€è‡´ã€‚",
              "en": "Researchers introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. "
            }
          }
        },
        {
          "id": "opencompass_1730",
          "name": "PaperBench",
          "version": "1.0.0",
          "description": "PaperBench is a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research.",
          "url": "opencompass/opencompass_1730.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1730",
          "sample_count": 1000,
          "traits": [
            "Agent",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1730",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              },
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/openai/preparedness",
            "paperLink": "https://arxiv.org/abs/2504.01848",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "41",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-11 17:12:27",
            "supportOnlineEval": false,
            "updateDate": "2025-04-11 17:12:27",
            "createDate": "2025-04-11 14:10:05",
            "desc": {
              "cn": "PaperBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°AIä»£ç†å¤åˆ¶æœ€æ–°AIç ”ç©¶èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚",
              "en": "PaperBench is a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research."
            }
          }
        },
        {
          "id": "opencompass_1622",
          "name": "CORAL",
          "version": "1.0.0",
          "description": "Researchers present a large-scale conversational RAG benchmark named CORAL and propose a unified framework for standardizing and evaluating various conversational RAG baselines.",
          "url": "opencompass/opencompass_1622.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1622",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1622",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "RAG",
                "en": "RAG"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Ariya12138/CORAL",
            "paperLink": "https://arxiv.org/abs/2410.23090",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "36",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-12 11:07:55",
            "supportOnlineEval": false,
            "updateDate": "2025-03-12 11:07:55",
            "createDate": "2025-03-11 15:39:20",
            "desc": {
              "cn": "CORAL æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¯¹è¯ RAG åŸºå‡†ï¼ŒåŒ…å«ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºæ ‡å‡†åŒ–å’Œè¯„ä¼°å„ç§å¯¹è¯ RAG åŸºçº¿ã€‚",
              "en": "Researchers present a large-scale conversational RAG benchmark named CORAL and propose a unified framework for standardizing and evaluating various conversational RAG baselines."
            }
          }
        },
        {
          "id": "opencompass_1623",
          "name": "MJ-Bench",
          "version": "1.0.0",
          "description": "MJ-Bench incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. ",
          "url": "opencompass/opencompass_1623.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1623",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1623",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/MJ-Bench/MJ-Bench",
            "paperLink": "https://arxiv.org/abs/2407.04842",
            "officialWebsiteLink": "https://mj-bench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "35",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-12 11:07:50",
            "supportOnlineEval": false,
            "updateDate": "2025-03-12 11:07:50",
            "createDate": "2025-03-11 15:46:38",
            "desc": {
              "cn": "MJ-Benchï¼Œå®ƒåŒ…å«äº†ä¸€ä¸ªç»¼åˆçš„åå¥½æ•°æ®é›†ï¼Œç”¨äºä»å››ä¸ªå…³é”®è§’åº¦è¯„ä¼°å¤šæ¨¡æ€è¯„å§”åœ¨ä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹æä¾›åé¦ˆæ–¹é¢çš„èƒ½åŠ›ï¼šå¯¹é½ã€å®‰å…¨æ€§ã€å›¾åƒè´¨é‡å’Œåè§ã€‚",
              "en": "MJ-Bench incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. "
            }
          }
        },
        {
          "id": "opencompass_1668",
          "name": "TimeTravel",
          "version": "1.0.0",
          "description": "TimeTravel Taxonomy maps artifacts from 10 civilizations, 266 cultures, and 10k+ verified samples for AI-driven historical analysis.",
          "url": "opencompass/opencompass_1668.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1668",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1668",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/mbzuai-oryx/TimeTravel",
            "paperLink": "https://arxiv.org/abs/2502.14865",
            "officialWebsiteLink": "https://mbzuai-oryx.github.io/TimeTravel/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "35",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-24 18:48:21",
            "supportOnlineEval": false,
            "updateDate": "2025-03-24 18:48:21",
            "createDate": "2025-03-24 17:05:23",
            "desc": {
              "cn": "æ—¶é—´æ—…è¡Œåˆ†ç±»å°†æ¥è‡ª 10 ä¸ªæ–‡æ˜ã€266 ä¸ªæ–‡åŒ–ä»¥åŠ 10k+ä¸ªéªŒè¯æ ·æœ¬çš„æ–‡ç‰©æ˜ å°„ï¼Œç”¨äº AI é©±åŠ¨çš„å†å²åˆ†æã€‚",
              "en": "TimeTravel Taxonomy maps artifacts from 10 civilizations, 266 cultures, and 10k+ verified samples for AI-driven historical analysis."
            }
          }
        },
        {
          "id": "opencompass_1681",
          "name": "PRMBench_Preview",
          "version": "1.0.0",
          "description": "PRMBench is a benchmark dataset for evaluating process-level reward models (PRMs). It consists of 6,216 data instances, each containing a question, a solution process, and a modified process with errors.",
          "url": "opencompass/opencompass_1681.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1681",
          "sample_count": 1000,
          "traits": [
            "Examination"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1681",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å­¦ç§‘",
                "en": "Examination"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/ssmisya/PRMBench",
            "paperLink": "https://arxiv.org/abs/2501.03124",
            "officialWebsiteLink": "https://prmbench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "35",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-26 15:56:18",
            "supportOnlineEval": false,
            "updateDate": "2025-03-26 15:56:18",
            "createDate": "2025-03-26 15:07:01",
            "desc": {
              "cn": "PRMBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„åŸºå‡†æ•°æ®é›†ã€‚å®ƒåŒ…å« 6,216 ä¸ªæ•°æ®å®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹åŒ…å«ä¸€ä¸ªé—®é¢˜ã€ä¸€ä¸ªè§£å†³æ–¹æ¡ˆè¿‡ç¨‹ä»¥åŠä¸€ä¸ªåŒ…å«é”™è¯¯çš„ä¿®æ”¹è¿‡ç¨‹ã€‚",
              "en": "PRMBench is a benchmark dataset for evaluating process-level reward models (PRMs). It consists of 6,216 data instances, each containing a question, a solution process, and a modified process with errors."
            }
          }
        },
        {
          "id": "opencompass_1751",
          "name": "MultiLoKo",
          "version": "1.0.0",
          "description": "MultiLoKo is a multilingual knowledge benchmark, covering 30 languages plus English.",
          "url": "opencompass/opencompass_1751.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1751",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1751",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/facebookresearch/multiloko/",
            "paperLink": "https://arxiv.org/abs/2504.10356",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "35",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-21 12:34:33",
            "supportOnlineEval": false,
            "updateDate": "2025-04-21 12:34:33",
            "createDate": "2025-04-21 11:55:37",
            "desc": {
              "cn": "MultiLoKoæ˜¯ä¸€ä¸ªå¤šè¯­è¨€çŸ¥è¯†åŸºå‡†ï¼Œæ¶µç›–30ç§è¯­è¨€åŠè‹±è¯­ã€‚",
              "en": "MultiLoKo is a multilingual knowledge benchmark, covering 30 languages plus English."
            }
          }
        },
        {
          "id": "opencompass_1678",
          "name": "RSMMVP",
          "version": "1.0.0",
          "description": "This dataset follows a similar procedure to the original MMVP benchmark on natural images but directed towards the remote sensing domain. Challenging visual patterns are identified based on CLIP blind pairs, accompanied with the correpsonding questions, options and ground-truth answer.",
          "url": "opencompass/opencompass_1678.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1678",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1678",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2503.15816",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "33",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-26 15:56:27",
            "supportOnlineEval": false,
            "updateDate": "2025-03-26 15:56:27",
            "createDate": "2025-03-26 14:35:19",
            "desc": {
              "cn": "RSMMVPéµå¾ªä¸åŸå§‹ MMVP åŸºå‡†åœ¨è‡ªç„¶å›¾åƒä¸Šçš„ç±»ä¼¼æµç¨‹ï¼Œä½†é’ˆå¯¹é¥æ„Ÿé¢†åŸŸã€‚æ ¹æ® CLIP ç›²å¯¹è¯†åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æ¨¡å¼ï¼Œå¹¶é™„å¸¦ç›¸åº”çš„é—®é¢˜ã€é€‰é¡¹å’ŒçœŸå®ç­”æ¡ˆã€‚",
              "en": "This dataset follows a similar procedure to the original MMVP benchmark on natural images but directed towards the remote sensing domain. Challenging visual patterns are identified based on CLIP blind pairs, accompanied with the correpsonding questions, options and ground-truth answer."
            }
          }
        },
        {
          "id": "opencompass_1744",
          "name": "StyleRec",
          "version": "1.0.0",
          "description": "A Benchmark Dataset for Prompt Recovery in Writing Style Transformation.",
          "url": "opencompass/opencompass_1744.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1744",
          "sample_count": 1000,
          "traits": [
            "Language",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1744",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              },
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "NLP",
                "en": "NLP"
              },
              {
                "cn": "Writing Style Transformation",
                "en": "Writing Style Transformation"
              },
              {
                "cn": "Prompt Recovery",
                "en": "Prompt Recovery"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/promptrecovery501/StyleRec",
            "paperLink": "https://arxiv.org/abs/2504.04373",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "18903058",
              "name": "qianertongre",
              "avatar": null,
              "nickname": "qianertongre"
            },
            "lookNum": "33",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-16 11:32:28",
            "supportOnlineEval": false,
            "updateDate": "2025-04-16 11:32:28",
            "createDate": "2025-04-15 13:45:08",
            "desc": {
              "cn": "åŸºäºå†™ä½œé£æ ¼è½¬æ¢çš„æç¤ºè¯æ¢å¤çš„è¯„æµ‹é›†",
              "en": "A Benchmark Dataset for Prompt Recovery in Writing Style Transformation."
            }
          }
        },
        {
          "id": "opencompass_1667",
          "name": "MicroVQA",
          "version": "1.0.0",
          "description": "MicroVQA is a benchmark that evaluates LLM reasoning on multiple-choice questions about microscopy images, created by expert biologists.",
          "url": "opencompass/opencompass_1667.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1667",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1667",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/jmhb0/microvqa",
            "paperLink": "https://arxiv.org/abs/2503.13399",
            "officialWebsiteLink": "https://jmhb0.github.io/microvqa/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "32",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-24 18:48:38",
            "supportOnlineEval": false,
            "updateDate": "2025-03-24 18:48:38",
            "createDate": "2025-03-24 16:37:21",
            "desc": {
              "cn": "MicroVQAï¼Œä¸€ä¸ªè¯„ä¼°å…³äºæ˜¾å¾®é•œå›¾åƒçš„å¤šé€‰é¢˜æ¨ç†åŸºå‡†ï¼Œç”±ä¸“å®¶ç”Ÿç‰©å­¦å®¶åˆ›å»ºï¼Œæ—¨åœ¨åæ˜ ç”Ÿç‰©ç ”ç©¶ä¸­èƒ½å¤Ÿæœ‰æ„ä¹‰åœ°ååŠ©çš„ä»»åŠ¡ï¼Œæ¯ä¸ªé—®é¢˜éƒ½éœ€è¦å¤šæ¨¡æ€æ¨ç†ã€‚",
              "en": "MicroVQA is a benchmark that evaluates LLM reasoning on multiple-choice questions about microscopy images, created by expert biologists."
            }
          }
        },
        {
          "id": "opencompass_1626",
          "name": "CRAG",
          "version": "1.0.0",
          "description": "The Comprehensive RAG Benchmark (CRAG) is a rich and comprehensive factual question answering benchmark designed to advance research in RAG. Besides question-answer pairs, CRAG provides mock APIs to simulate web and knowledge graph search. ",
          "url": "opencompass/opencompass_1626.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1626",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1626",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "RAG",
                "en": "RAG"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/facebookresearch/CRAG",
            "paperLink": "https://arxiv.org/abs/2406.04744",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "31",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-12 11:07:30",
            "supportOnlineEval": false,
            "updateDate": "2025-03-12 11:07:30",
            "createDate": "2025-03-11 16:16:34",
            "desc": {
              "cn": "CRAGæ˜¯ä¸€ä¸ªä¸°å¯Œä¸”å…¨é¢çš„åŸºäºäº‹å®çš„é—®é¢˜å›ç­”åŸºå‡†ï¼Œæ—¨åœ¨æ¨è¿› RAG ç ”ç©¶ã€‚é™¤äº†é—®ç­”å¯¹ä¹‹å¤–ï¼ŒCRAG è¿˜æä¾›äº†æ¨¡æ‹Ÿç½‘é¡µå’ŒçŸ¥è¯†å›¾è°±æœç´¢çš„æ¨¡æ‹Ÿ APIã€‚",
              "en": "The Comprehensive RAG Benchmark (CRAG) is a rich and comprehensive factual question answering benchmark designed to advance research in RAG. Besides question-answer pairs, CRAG provides mock APIs to simulate web and knowledge graph search. "
            }
          }
        },
        {
          "id": "opencompass_1738",
          "name": "RUListening",
          "version": "1.0.0",
          "description": "RUListening: Robust Understanding through Listening, an automated QA generation framework for evaluating multimodal perception.",
          "url": "opencompass/opencompass_1738.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1738",
          "sample_count": 1000,
          "traits": [
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1738",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Multimodal",
                "en": "Multimodal"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2504.00369",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "31",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-11 17:15:12",
            "supportOnlineEval": false,
            "updateDate": "2025-04-11 17:15:12",
            "createDate": "2025-04-11 16:07:18",
            "desc": {
              "cn": "RUListeningï¼šé€šè¿‡è†å¬çš„ç¨³å¥ç†è§£ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ„ŸçŸ¥çš„è‡ªåŠ¨é—®ç­”ç”Ÿæˆæ¡†æ¶ã€‚",
              "en": "RUListening: Robust Understanding through Listening, an automated QA generation framework for evaluating multimodal perception."
            }
          }
        },
        {
          "id": "opencompass_1675",
          "name": "PokerBench",
          "version": "1.0.0",
          "description": "PokerBench contains natural language game scenarios and optimal decisions computed by solvers in No Limit Texas Holdâ€™em. It is divided into pre-flop and post-flop datasets, each with training and test splits. ",
          "url": "opencompass/opencompass_1675.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1675",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1675",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/pokerllm/pokerbench",
            "paperLink": "https://arxiv.org/abs/2501.08328",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "30",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-25 10:42:49",
            "supportOnlineEval": false,
            "updateDate": "2025-03-25 10:42:49",
            "createDate": "2025-03-25 10:41:14",
            "desc": {
              "cn": "PokerBenchåŒ…å«è‡ªç„¶è¯­è¨€æ¸¸æˆåœºæ™¯å’Œç”±æ±‚è§£å™¨åœ¨æ— é™åˆ¶å¾·å·æ‰‘å…‹ä¸­è®¡ç®—å‡ºçš„æœ€ä¼˜å†³ç­–ã€‚å®ƒåˆ†ä¸ºå‰æ³¨å’Œåæ³¨æ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†éƒ½åŒ…å«è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚",
              "en": "PokerBench contains natural language game scenarios and optimal decisions computed by solvers in No Limit Texas Holdâ€™em. It is divided into pre-flop and post-flop datasets, each with training and test splits. "
            }
          }
        },
        {
          "id": "opencompass_1735",
          "name": "WorldScore",
          "version": "1.0.0",
          "description": "WorldScore benchmark is the first unified benchmark for world generation.",
          "url": "opencompass/opencompass_1735.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1735",
          "sample_count": 1000,
          "traits": [
            "Creation"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1735",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "åˆ›ä½œ",
                "en": "Creation"
              },
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "è§†é¢‘ç”Ÿæˆ",
                "en": "è§†é¢‘ç”Ÿæˆ"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/haoyi-duan/WorldScore",
            "paperLink": "https://arxiv.org/abs/2504.00983",
            "officialWebsiteLink": "https://haoyi-duan.github.io/WorldScore/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "30",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-11 17:14:14",
            "supportOnlineEval": false,
            "updateDate": "2025-04-11 17:14:14",
            "createDate": "2025-04-11 15:23:05",
            "desc": {
              "cn": "WorldScoreåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºä¸–ç•Œç”Ÿæˆçš„ç»Ÿä¸€åŸºå‡†æµ‹è¯•ã€‚",
              "en": "WorldScore benchmark is the first unified benchmark for world generation."
            }
          }
        },
        {
          "id": "opencompass_1778",
          "name": "AgentRewardBench",
          "version": "1.0.0",
          "description": "AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. ",
          "url": "opencompass/opencompass_1778.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1778",
          "sample_count": 1000,
          "traits": [
            "Agent",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1778",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              },
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://agent-reward-bench.github.io/",
            "paperLink": "https://arxiv.org/abs/2504.08942",
            "officialWebsiteLink": "https://agent-reward-bench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "30",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-25 17:00:44",
            "supportOnlineEval": false,
            "updateDate": "2025-04-25 17:00:44",
            "createDate": "2025-04-25 16:26:45",
            "desc": {
              "cn": "AgentRewardBench æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„åˆ¤è€…è¯„ä¼°ç½‘ç»œä»£ç†æœ‰æ•ˆæ€§çš„åŸºå‡†æµ‹è¯•ã€‚AgentRewardBench åŒ…å«æ¥è‡ª 5 ä¸ªåŸºå‡†æµ‹è¯•å’Œ 4 ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„ 1302 æ¡è½¨è¿¹ã€‚",
              "en": "AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. "
            }
          }
        },
        {
          "id": "opencompass_1704",
          "name": "Mono2Stereo",
          "version": "1.0.0",
          "description": "For evaluating stereo image conversion, it provides test data for five different scenarios: animation, indoor, outdoor, complex, and simple, with a total of approximately 2,500 test sample pairs. It also offers evaluation metrics for assessing the stereo effect.",
          "url": "opencompass/opencompass_1704.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1704",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1704",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Stereo Conversion",
                "en": "Stereo Conversion"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/song2yu/Mono2Stereo",
            "paperLink": "https://arxiv.org/abs/2503.22262",
            "officialWebsiteLink": "https://mono2stereo-bench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "52300297",
              "name": null,
              "avatar": null,
              "nickname": "è¿·è—"
            },
            "lookNum": "28",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-07 10:30:36",
            "supportOnlineEval": false,
            "updateDate": "2025-04-07 10:30:36",
            "createDate": "2025-04-02 17:37:32",
            "desc": {
              "cn": "ç”¨äºæµ‹è¯„ç«‹ä½“å½±åƒè½¬æ¢ï¼Œæä¾›äº†åŠ¨ç”»ï¼Œå®¤å†…ï¼Œå®¤å¤–ï¼Œå¤æ‚ï¼Œç®€å•å…±äº”ç§åœºæ™¯çš„æµ‹è¯•æ•°æ®ï¼Œæ€»å…±çº¦2500å¯¹æµ‹è¯•æ ·æœ¬ã€‚å¹¶æä¾›äº†ç”¨äºæµ‹è¯„ç«‹ä½“æ•ˆæœçš„è¯„ä»·æŒ‡æ ‡-ç«‹ä½“äº¤å¹¶æ¯”ã€‚",
              "en": "For evaluating stereo image conversion, it provides test data for five different scenarios: animation, indoor, outdoor, complex, and simple, with a total of approximately 2,500 test sample pairs. It also offers evaluation metrics for assessing the stereo effect."
            }
          }
        },
        {
          "id": "opencompass_1740",
          "name": "GPT-ImgEval",
          "version": "1.0.0",
          "description": "GPT-ImgEval, quantitatively and qualitatively diagnoses GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis.",
          "url": "opencompass/opencompass_1740.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1740",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1740",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/PicoTrex/GPT-ImgEval",
            "paperLink": "https://arxiv.org/abs/2504.02782",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "28",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-11 17:16:12",
            "supportOnlineEval": false,
            "updateDate": "2025-04-11 17:16:12",
            "createDate": "2025-04-11 16:58:50",
            "desc": {
              "cn": "GPT-ImgEvalï¼Œä»ä¸‰ä¸ªå…³é”®ç»´åº¦å¯¹GPT-4oçš„æ€§èƒ½è¿›è¡Œå®šé‡å’Œå®šæ€§è¯Šæ–­ï¼šï¼ˆ1ï¼‰ç”Ÿæˆè´¨é‡ï¼Œï¼ˆ2ï¼‰ç¼–è¾‘èƒ½åŠ›ï¼Œä»¥åŠï¼ˆ3ï¼‰åŸºäºä¸–ç•ŒçŸ¥è¯†çš„è¯­ä¹‰åˆæˆèƒ½åŠ›ã€‚",
              "en": "GPT-ImgEval, quantitatively and qualitatively diagnoses GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis."
            }
          }
        },
        {
          "id": "opencompass_1733",
          "name": "FEABench",
          "version": "1.0.0",
          "description": "FEABench is a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). ",
          "url": "opencompass/opencompass_1733.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1733",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1733",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "FEAåŸºå‡†æµ‹è¯•",
                "en": "FEAåŸºå‡†æµ‹è¯•"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/google/feabench/tree/main",
            "paperLink": "https://arxiv.org/abs/2504.06260",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "27",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-11 17:13:05",
            "supportOnlineEval": false,
            "updateDate": "2025-04-11 17:13:05",
            "createDate": "2025-04-11 14:42:38",
            "desc": {
              "cn": "FEABenchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å’ŒLLMä»£ç†ä½¿ç”¨æœ‰é™å…ƒåˆ†æï¼ˆFEAï¼‰æ¨¡æ‹Ÿå’Œè§£å†³ç‰©ç†ã€æ•°å­¦åŠå·¥ç¨‹é—®é¢˜èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚",
              "en": "FEABench is a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). "
            }
          }
        },
        {
          "id": "opencompass_1737",
          "name": "FortisAVQA",
          "version": "1.0.0",
          "description": " FortisAVQA is the first dataset designed to assess the robustness of AVQA models. ",
          "url": "opencompass/opencompass_1737.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1737",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1737",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "AVQA",
                "en": "AVQA"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/reml-group/fortisavqa",
            "paperLink": "https://arxiv.org/abs/2504.00487",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "27",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-11 17:14:21",
            "supportOnlineEval": false,
            "updateDate": "2025-04-11 17:14:21",
            "createDate": "2025-04-11 15:55:15",
            "desc": {
              "cn": "FortisAVQAï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°AVQAæ¨¡å‹é²æ£’æ€§çš„æ•°æ®é›†ã€‚",
              "en": " FortisAVQA is the first dataset designed to assess the robustness of AVQA models. "
            }
          }
        },
        {
          "id": "opencompass_1739",
          "name": "CrossWordBench",
          "version": "1.0.0",
          "description": "CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles.",
          "url": "opencompass/opencompass_1739.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1739",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1739",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Multimodal",
                "en": "Multimodal"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/SeanLeng1/CrossWordBench",
            "paperLink": "https://arxiv.org/abs/2504.00043",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "27",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-11 17:15:53",
            "supportOnlineEval": false,
            "updateDate": "2025-04-11 17:15:53",
            "createDate": "2025-04-11 16:14:37",
            "desc": {
              "cn": "CrossWordBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡å¡«å­—æ¸¸æˆçš„æ–¹å¼æ¥è¯„ä¼°LLMså’ŒLVLMsçš„æ¨ç†èƒ½åŠ›ã€‚",
              "en": "CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles."
            }
          }
        },
        {
          "id": "opencompass_1682",
          "name": "MotionBench",
          "version": "1.0.0",
          "description": "MotionBench, is a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models.",
          "url": "opencompass/opencompass_1682.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1682",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1682",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/THUDM/MotionBench",
            "paperLink": "https://arxiv.org/abs/2501.02955",
            "officialWebsiteLink": "https://motion-bench.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "26",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-03-26 15:56:14",
            "supportOnlineEval": false,
            "updateDate": "2025-03-26 15:56:14",
            "createDate": "2025-03-26 15:35:43",
            "desc": {
              "cn": "MotionBenchæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è§†é¢‘ç†è§£æ¨¡å‹çš„ç»†ç²’åº¦è¿åŠ¨ç†è§£èƒ½åŠ›ã€‚",
              "en": "MotionBench, is a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models."
            }
          }
        },
        {
          "id": "opencompass_1781",
          "name": "MIEB",
          "version": "1.0.0",
          "description": "Massive Image Embedding Benchmark (MIEB) is to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. ",
          "url": "opencompass/opencompass_1781.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1781",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1781",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Multimodal",
                "en": "Multimodal"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/embeddings-benchmark/mteb",
            "paperLink": "https://arxiv.org/abs/2504.10471",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "26",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-25 17:00:52",
            "supportOnlineEval": false,
            "updateDate": "2025-04-25 17:00:52",
            "createDate": "2025-04-25 16:52:48",
            "desc": {
              "cn": "MIEBç”¨äºè¯„ä¼°å›¾åƒå’Œå›¾åƒæ–‡æœ¬åµŒå…¥æ¨¡å‹åœ¨è¿„ä»Šä¸ºæ­¢æœ€å¹¿æ³›çš„èŒƒå›´å†…çš„æ€§èƒ½ã€‚",
              "en": "Massive Image Embedding Benchmark (MIEB) is to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. "
            }
          }
        },
        {
          "id": "opencompass_1706",
          "name": "KOFFVQA",
          "version": "1.0.0",
          "description": "KOFFVQA is a carefully crafted free-form visual question answering(VQA) benchmark in the Korean language consisting of 275 questions across 10 different tasks. ",
          "url": "opencompass/opencompass_1706.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1706",
          "sample_count": 1000,
          "traits": [
            "Multimodal",
            "Understanding"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1706",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ç†è§£",
                "en": "Understanding"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "VQA",
                "en": "VQA"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/maum-ai/KOFFVQA",
            "paperLink": "https://arxiv.org/abs/2503.23730",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "25",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-03 19:47:01",
            "supportOnlineEval": false,
            "updateDate": "2025-04-03 19:47:01",
            "createDate": "2025-04-03 14:31:30",
            "desc": {
              "cn": "KOFFVQAæ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„éŸ©è¯­è‡ªç”±å½¢å¼è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«10ä¸ªä¸åŒä»»åŠ¡ä¸­çš„275ä¸ªé—®é¢˜ã€‚",
              "en": "KOFFVQA is a carefully crafted free-form visual question answering(VQA) benchmark in the Korean language consisting of 275 questions across 10 different tasks. "
            }
          }
        },
        {
          "id": "opencompass_1707",
          "name": "RXRX3-CORE",
          "version": "1.0.0",
          "description": "RxRx3-core dataset is a challenge dataset in phenomics optimized for the research\ncommunity. ",
          "url": "opencompass/opencompass_1707.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1707",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1707",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Xavi3398/fer_benchmark",
            "paperLink": "https://arxiv.org/abs/2503.20428",
            "officialWebsiteLink": "https://www.rxrx.ai/rxrx3-core",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "25",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-03 19:47:14",
            "supportOnlineEval": false,
            "updateDate": "2025-04-03 19:47:14",
            "createDate": "2025-04-03 15:13:31",
            "desc": {
              "cn": "RxRx3-coreæ•°æ®é›†æ˜¯Recursionä¸ºç ”ç©¶ç¤¾åŒºä¼˜åŒ–çš„è¡¨å‹ç»„å­¦æŒ‘æˆ˜æ•°æ®é›†ã€‚",
              "en": "RxRx3-core dataset is a challenge dataset in phenomics optimized for the research\ncommunity. "
            }
          }
        },
        {
          "id": "opencompass_1732",
          "name": "SCAM",
          "version": "1.0.0",
          "description": "SCAM is the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words.",
          "url": "opencompass/opencompass_1732.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1732",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1732",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Multimodal",
                "en": "Multimodal"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Bliss-e-V/SCAM",
            "paperLink": "https://arxiv.org/abs/2504.04893",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "25",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-11 17:12:53",
            "supportOnlineEval": false,
            "updateDate": "2025-04-11 17:12:53",
            "createDate": "2025-04-11 14:29:41",
            "desc": {
              "cn": "SCAMï¼Œæ˜¯è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§ã€å¤šæ ·æ€§æœ€ä¸°å¯Œçš„çœŸå®ä¸–ç•Œæ’ç‰ˆæ”»å‡»å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«æ•°ç™¾ä¸ªå¯¹è±¡ç±»åˆ«å’Œæ”»å‡»è¯æ±‡çš„1,162å¼ å›¾åƒã€‚",
              "en": "SCAM is the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words."
            }
          }
        },
        {
          "id": "opencompass_1755",
          "name": "REAL",
          "version": "1.0.0",
          "description": "Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites",
          "url": "opencompass/opencompass_1755.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1755",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1755",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/agi-inc/agisdk",
            "paperLink": "https://arxiv.org/abs/2504.11543",
            "officialWebsiteLink": "https://www.realevals.xyz/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "25",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-21 12:34:45",
            "supportOnlineEval": false,
            "updateDate": "2025-04-21 12:34:45",
            "createDate": "2025-04-21 12:31:05",
            "desc": {
              "cn": "åœ¨çœŸå®ç½‘ç«™çš„ç¡®å®šæ€§æ¨¡æ‹Ÿä¸Šå¯¹è‡ªä¸»ä»£ç†è¿›è¡ŒåŸºå‡†æµ‹è¯•",
              "en": "Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites"
            }
          }
        },
        {
          "id": "opencompass_1742",
          "name": "U-NIAH",
          "version": "1.0.0",
          "description": "U-NIAH is a framework unifying RAG and LLM for needle-in-a-haystack tasks, based on the fictional Starlight Academy dataset. It eliminates interference from pre-trained knowledge and supports diverse, complex scenarios (e.g., multi-needle, long-needle, \"needle-in-needle\").",
          "url": "opencompass/opencompass_1742.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1742",
          "sample_count": 1000,
          "traits": [
            "Long-Context"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1742",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "é•¿æ–‡æœ¬",
                "en": "Long-Context"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "RAG",
                "en": "RAG"
              },
              {
                "cn": "Long-Context",
                "en": "Long-Context"
              },
              {
                "cn": "Needle In A Haystack",
                "en": "Needle In A Haystack"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Tongji-KGLLM/U-NIAH",
            "paperLink": "https://arxiv.org/abs/2503.00353",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "85508164",
              "name": "yunfan",
              "avatar": null,
              "nickname": "yunfan"
            },
            "lookNum": "24",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-16 11:32:40",
            "supportOnlineEval": false,
            "updateDate": "2025-04-16 11:32:40",
            "createDate": "2025-04-13 10:56:15",
            "desc": {
              "cn": "U-NIAHæ˜¯å°† RAG å’Œ LLM ç»Ÿä¸€æ˜ å°„åœ¨å¤§æµ·æé’ˆä»»åŠ¡ä¸­çš„æ¡†æ¶ã€‚æ‰€æœ‰ä»»åŠ¡åŸºäºä¸€ä¸ªè™šæ„èƒŒæ™¯ä¸‹çš„æ•°æ®é›†Starlight Academyï¼Œæ¶µç›–äº†é­”æ³•ç³»ç»Ÿã€å­¦æœ¯è¯¾ç¨‹ã€æ ¡å›­ç”Ÿæ´»ã€ç­‰å¤šä¸ªæ–¹é¢ï¼Œæ—¨åœ¨æ¶ˆé™¤é¢„è®­ç»ƒçŸ¥è¯†çš„å¹²æ‰°ï¼Œä»è€Œèƒ½å¤Ÿç‹¬ç«‹äº LLMs çš„å…ˆéªŒçŸ¥è¯†ã€‚æ¡†æ¶åŒ…å«å¤šç§è¯„ä¼°åœºæ™¯ï¼Œæ”¯æŒå¤šé’ˆï¼ˆ3ã€7ã€15ä¸ªé’ˆï¼‰å’Œé•¿é’ˆï¼ˆ400-500 tokenï¼‰é…ç½®ï¼Œè¿˜å¼•å…¥äº†â€œé’ˆä¸­é’ˆâ€ç»“æ„ï¼Œè¿›ä¸€æ­¥å¢åŠ äº†å¤æ‚æ€§ã€‚è¯¥æ•°æ®é›†é€šè¿‡å¤šæ ·åŒ–çš„åœºæ™¯å’Œåˆæˆç”Ÿæˆçš„å†…å®¹ï¼Œèƒ½å¤Ÿä»å¤šä¸ªç»´åº¦åˆ†ææ¨¡å‹åœ¨é•¿æ–‡æœ¬åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚åŒæ—¶é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼ŒU-NIAHå¯æŒç»­æ³¨å…¥æ–°çš„æŒ‘æˆ˜åœºæ™¯ã€‚",
              "en": "U-NIAH is a framework unifying RAG and LLM for needle-in-a-haystack tasks, based on the fictional Starlight Academy dataset. It eliminates interference from pre-trained knowledge and supports diverse, complex scenarios (e.g., multi-needle, long-needle, \"needle-in-needle\")."
            }
          }
        },
        {
          "id": "opencompass_1753",
          "name": "AgMMU",
          "version": "1.0.0",
          "description": "A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark",
          "url": "opencompass/opencompass_1753.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1753",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1753",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Multimodal",
                "en": "Multimodal"
              },
              {
                "cn": "å†œä¸š",
                "en": "å†œä¸š"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/AgMMU/AgMMU",
            "paperLink": "https://arxiv.org/abs/2504.10568",
            "officialWebsiteLink": "https://agmmu.github.io/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "22",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-21 12:34:40",
            "supportOnlineEval": false,
            "updateDate": "2025-04-21 12:34:40",
            "createDate": "2025-04-21 12:12:24",
            "desc": {
              "cn": "å†œä¸šç»¼åˆå¤šæ¨¡æ€ç†è§£å’Œæ¨ç†åŸºå‡†ã€‚",
              "en": "A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark"
            }
          }
        },
        {
          "id": "opencompass_1777",
          "name": "ColorBench",
          "version": "1.0.0",
          "description": "ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. ",
          "url": "opencompass/opencompass_1777.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1777",
          "sample_count": 1000,
          "traits": [
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1777",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Multimodal",
                "en": "Multimodal"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/tianyi-lab/ColorBench",
            "paperLink": "https://arxiv.org/abs/2504.10514",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "20",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-25 17:00:41",
            "supportOnlineEval": false,
            "updateDate": "2025-04-25 17:00:41",
            "createDate": "2025-04-25 16:14:54",
            "desc": {
              "cn": "ColorBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°ä¸”ç²¾å¿ƒè®¾è®¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°VLMsåœ¨é¢œè‰²ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬é¢œè‰²æ„ŸçŸ¥ã€æ¨ç†å’Œé²æ£’æ€§ã€‚",
              "en": "ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. "
            }
          }
        },
        {
          "id": "opencompass_1780",
          "name": "C-FAITH",
          "version": "1.0.0",
          "description": "C-FAITH, a Chinese QA hallucination benchmark created from 1,399 knowledge documents obtained from web scraping, totaling 60,702 entries.",
          "url": "opencompass/opencompass_1780.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1780",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1780",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/pkulcwmzx/C-FAITH",
            "paperLink": "https://arxiv.org/abs/2504.10167",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "20",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-25 17:00:49",
            "supportOnlineEval": false,
            "updateDate": "2025-04-25 17:00:49",
            "createDate": "2025-04-25 16:42:33",
            "desc": {
              "cn": " C-FAITHï¼Œè¿™æ˜¯ä¸€ä¸ªä¸­å›½ QA å¹»è§‰åŸºå‡†ï¼Œç”±ä»ç½‘ç»œæŠ“å–ä¸­è·å¾—çš„ 1,399 ä»½çŸ¥è¯†æ–‡æ¡£åˆ›å»ºï¼Œæ€»å…± 60,702 ä¸ªæ¡ç›®ã€‚",
              "en": "C-FAITH, a Chinese QA hallucination benchmark created from 1,399 knowledge documents obtained from web scraping, totaling 60,702 entries."
            }
          }
        },
        {
          "id": "opencompass_1731",
          "name": "DOVE",
          "version": "1.0.0",
          "description": "DOVE (Dataset Of Variation Evaluation) is a large-scale dataset containing prompt perturbations of various evaluation benchmarks.",
          "url": "opencompass/opencompass_1731.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1731",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1731",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "LLMæ•æ„Ÿæ€§è¯„ä¼°",
                "en": "LLMæ•æ„Ÿæ€§è¯„ä¼°"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/SLAB-NLP/DOVE",
            "paperLink": "https://arxiv.org/abs/2503.01622",
            "officialWebsiteLink": "https://slab-nlp.github.io/DOVE/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "19",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-11 17:12:41",
            "supportOnlineEval": false,
            "updateDate": "2025-04-11 17:12:41",
            "createDate": "2025-04-11 14:22:36",
            "desc": {
              "cn": "DOVEï¼ˆå˜å¼‚è¯„ä¼°æ•°æ®é›†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«äº†å„ç§è¯„ä¼°åŸºå‡†çš„æç¤ºæ‰°åŠ¨ã€‚",
              "en": "DOVE (Dataset Of Variation Evaluation) is a large-scale dataset containing prompt perturbations of various evaluation benchmarks."
            }
          }
        },
        {
          "id": "opencompass_1752",
          "name": "LLM-SRBench",
          "version": "1.0.0",
          "description": "LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization.",
          "url": "opencompass/opencompass_1752.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1752",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1752",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Strong Reasoning",
                "en": "Strong Reasoning"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/deep-symbolic-mathematics/llm-srbench",
            "paperLink": "https://arxiv.org/abs/2504.10415",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "19",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-21 12:34:37",
            "supportOnlineEval": false,
            "updateDate": "2025-04-21 12:34:37",
            "createDate": "2025-04-21 12:02:24",
            "desc": {
              "cn": "LLM-SRBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«239ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å››ä¸ªç§‘å­¦é¢†åŸŸï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°åŸºäºLLMsçš„ç§‘å­¦æ–¹ç¨‹å¼å‘ç°æ–¹æ³•ï¼ŒåŒæ—¶é˜²æ­¢ç®€å•è®°å¿†ã€‚",
              "en": "LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization."
            }
          }
        },
        {
          "id": "opencompass_1734",
          "name": "Thai_local_benchmark",
          "version": "1.0.0",
          "description": "Thai local dialect benchmark covers Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai, evaluating LLMs on five NLP tasks: summarization, question answering, translation, conversation, and food-related tasks.",
          "url": "opencompass/opencompass_1734.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1734",
          "sample_count": 1000,
          "traits": [
            "Language"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1734",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "è¯­è¨€",
                "en": "Language"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/mrpeerat/Thai_local_benchmark",
            "paperLink": "https://arxiv.org/abs/2504.05898",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "18",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-11 17:13:26",
            "supportOnlineEval": false,
            "updateDate": "2025-04-11 17:13:26",
            "createDate": "2025-04-11 14:57:01",
            "desc": {
              "cn": "è¿™æ˜¯ä¸€ä¸ªæ¶µç›–æ³°å›½åŒ—éƒ¨ï¼ˆå…°çº³ï¼‰ã€ä¸œåŒ—éƒ¨ï¼ˆä¼Šæ£®ï¼‰å’Œå—éƒ¨ï¼ˆä¸¹å¸ƒç½—ï¼‰æ–¹è¨€çš„æ³°å›½åœ°æ–¹æ–¹è¨€åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äº”é¡¹è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼šæ€»ç»“ã€é—®ç­”ã€ç¿»è¯‘ã€å¯¹è¯ä»¥åŠä¸é£Ÿç‰©ç›¸å…³çš„ä»»åŠ¡ã€‚",
              "en": "Thai local dialect benchmark covers Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai, evaluating LLMs on five NLP tasks: summarization, question answering, translation, conversation, and food-related tasks."
            }
          }
        },
        {
          "id": "opencompass_1801",
          "name": "OmniGIRL",
          "version": "1.0.0",
          "description": "A multi-modal, multi-language benchmark dataset for the GitHub Issue Resolution task.",
          "url": "opencompass/opencompass_1801.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1801",
          "sample_count": 1000,
          "traits": [
            "Agent",
            "Code",
            "Multimodal"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1801",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å¤šæ¨¡æ€",
                "en": "Multimodal"
              },
              {
                "cn": "ä»£ç ",
                "en": "Code"
              },
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Github Issue Resolution",
                "en": "Github Issue Resolution"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/DeepSoftwareAnalytics/OmniGIRL",
            "paperLink": "https://arxiv.org/abs/2505.04606",
            "officialWebsiteLink": "https://deepsoftwareanalytics.github.io/omnigirl_leaderboard.html",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "26208047",
              "name": null,
              "avatar": null,
              "nickname": "gnohgnailoug"
            },
            "lookNum": "17",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-05-09 21:26:49",
            "supportOnlineEval": false,
            "updateDate": "2025-05-09 21:26:49",
            "createDate": "2025-05-09 20:52:15",
            "desc": {
              "cn": "ä¸€ä¸ªé¢å‘ GitHub Issue ResoLutionä»»åŠ¡çš„å¤šè¯­è¨€ã€å¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«ä»¥ä¸‹ç‰¹ç‚¹: 1.æ”¯æŒ Pythonã€Javaã€JSã€TS å››ç§ä¸»æµç¼–ç¨‹è¯­è¨€ï¼Œ2. è¾“å…¥ä¿¡æ¯æ¶µç›–æ–‡æœ¬ã€å›¾åƒã€ç½‘é¡µç­‰å¤šç§æ¨¡æ€ï¼Œ3. æä¾›å¯å¤ç°çš„ Docker è¯„ä¼°ç¯å¢ƒã€‚",
              "en": "A multi-modal, multi-language benchmark dataset for the GitHub Issue Resolution task."
            }
          }
        },
        {
          "id": "opencompass_1754",
          "name": "OpenTuringBench",
          "version": "1.0.0",
          "description": "OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate machine-generated text detectors on the Turing Test and Authorship Attribution problems. ",
          "url": "opencompass/opencompass_1754.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1754",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1754",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "",
            "paperLink": "https://arxiv.org/abs/2504.11369",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "16",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-21 12:34:42",
            "supportOnlineEval": false,
            "updateDate": "2025-04-21 12:34:42",
            "createDate": "2025-04-21 12:26:14",
            "desc": {
              "cn": "OpenTuringBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºOLLMsçš„æ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è®­ç»ƒå’Œè¯„ä¼°æœºå™¨ç”Ÿæˆæ–‡æœ¬æ£€æµ‹å™¨åœ¨å›¾çµæµ‹è¯•å’Œä½œè€…å½’å±é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚",
              "en": "OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate machine-generated text detectors on the Turing Test and Authorship Attribution problems. "
            }
          }
        },
        {
          "id": "opencompass_1779",
          "name": "MLRC-Bench",
          "version": "1.0.0",
          "description": "MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. ",
          "url": "opencompass/opencompass_1779.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1779",
          "sample_count": 1000,
          "traits": [
            "Agent"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1779",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ™ºèƒ½ä½“",
                "en": "Agent"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/yunx-z/MLRC-Bench",
            "paperLink": "https://arxiv.org/abs/2504.09702",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "15",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-25 17:00:47",
            "supportOnlineEval": false,
            "updateDate": "2025-04-25 17:00:47",
            "createDate": "2025-04-25 16:34:27",
            "desc": {
              "cn": " MLRC-Benchæ—¨åœ¨é‡åŒ–å¤§æ¨¡å‹ä»£ç†å¦‚ä½•æœ‰æ•ˆåœ°åº”å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æœºå™¨å­¦ä¹  ï¼ˆMLï¼‰ ç ”ç©¶ç«èµ›ã€‚",
              "en": "MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. "
            }
          }
        },
        {
          "id": "opencompass_1784",
          "name": "xVerify",
          "version": "1.0.0",
          "description": "xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions",
          "url": "opencompass/opencompass_1784.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1784",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1784",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/IAAR-Shanghai/xVerify",
            "paperLink": "https://arxiv.org/abs/2504.10481",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "12",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-28 11:36:52",
            "supportOnlineEval": false,
            "updateDate": "2025-04-28 11:36:52",
            "createDate": "2025-04-28 11:24:15",
            "desc": {
              "cn": "xVerifyï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ¨ç†æ¨¡å‹è¯„ä¼°çš„é«˜æ•ˆç­”æ¡ˆéªŒè¯å™¨ã€‚xVerify åœ¨ç­‰ä»·åˆ¤æ–­æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°ç¡®å®šæ¨ç†æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦ç­‰åŒäºå„ç§ç±»å‹å®¢è§‚é—®é¢˜çš„å‚è€ƒç­”æ¡ˆã€‚",
              "en": "xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions"
            }
          }
        },
        {
          "id": "opencompass_1786",
          "name": "HypoEval",
          "version": "1.0.0",
          "description": "HypoEval, Hypothesis-guided Evaluation framework, which first uses a small corpus of human evaluations to generate more detailed rubrics for human judgments and then incorporates a checklist-like approach to combine LLM's assigned scores on each decomposed dimension to acquire overall scores. ",
          "url": "opencompass/opencompass_1786.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1786",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1786",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/ChicagoHAI/HypoEval",
            "paperLink": "https://arxiv.org/abs/2504.07174",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "12",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-28 11:36:49",
            "supportOnlineEval": false,
            "updateDate": "2025-04-28 11:36:49",
            "createDate": "2025-04-28 11:34:44",
            "desc": {
              "cn": "HypoEvalï¼Œå³å‡è®¾æŒ‡å¯¼çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–å…ˆä½¿ç”¨ä¸€å°éƒ¨åˆ†äººå·¥è¯„ä¼°æ¥ç”Ÿæˆæ›´è¯¦ç»†çš„äººç±»åˆ¤æ–­é‡è§„ï¼Œç„¶åé‡‡ç”¨ç±»ä¼¼æ¸…å•çš„æ–¹æ³•ï¼Œå°† LLM åœ¨æ¯ä¸ªåˆ†è§£ç»´åº¦ä¸Šçš„åˆ†é…åˆ†æ•°ç»“åˆèµ·æ¥ï¼Œä»¥è·å¾—æ€»åˆ†ã€‚",
              "en": "HypoEval, Hypothesis-guided Evaluation framework, which first uses a small corpus of human evaluations to generate more detailed rubrics for human judgments and then incorporates a checklist-like approach to combine LLM's assigned scores on each decomposed dimension to acquire overall scores. "
            }
          }
        },
        {
          "id": "opencompass_1787",
          "name": "LiveLongBench",
          "version": "1.0.0",
          "description": "LiveLongBench is the first spoken long-text benchmark designed to address the challenges of long-context understanding in real-world dialogues, characterized by speech-specific features, high redundancy, and uneven information density. Existing benchmarks fail to capture these complexities, limiting",
          "url": "opencompass/opencompass_1787.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1787",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1787",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [
              {
                "cn": "Spoken text",
                "en": "Spoken text"
              },
              {
                "cn": "Long context",
                "en": "Long context"
              },
              {
                "cn": "Live streams",
                "en": "Live streams"
              }
            ],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/Yarayx/livelongbench",
            "paperLink": "https://arxiv.org/abs/2504.17366",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "10207810",
              "name": null,
              "avatar": null,
              "nickname": "Yarayx"
            },
            "lookNum": "12",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-05-06 11:02:03",
            "supportOnlineEval": false,
            "updateDate": "2025-05-06 11:02:03",
            "createDate": "2025-04-28 16:54:00",
            "desc": {
              "cn": "LiveLongBench æ˜¯é¦–ä¸ªé¢å‘å£è¯­é•¿æ–‡æœ¬ç†è§£çš„åŸºå‡†æµ‹è¯•ï¼ŒåŸºäºç›´æ’­å†…å®¹æ„å»ºï¼Œæ¶µç›–æ£€ç´¢ç±»ã€æ¨ç†ç±»åŠæ··åˆç±»ä¸‰ç§ä»»åŠ¡ç±»å‹ï¼Œé’ˆå¯¹ç°å®å¯¹è¯ä¸­å­˜åœ¨çš„è¯­éŸ³ç‰¹æ€§ã€é«˜å†—ä½™æ€§å’Œä¿¡æ¯å¯†åº¦ä¸å‡ç­‰æŒ‘æˆ˜ã€‚",
              "en": "LiveLongBench is the first spoken long-text benchmark designed to address the challenges of long-context understanding in real-world dialogues, characterized by speech-specific features, high redundancy, and uneven information density. Existing benchmarks fail to capture these complexities, limiting"
            }
          }
        },
        {
          "id": "opencompass_1783",
          "name": "NPPC",
          "version": "1.0.0",
          "description": "Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. ",
          "url": "opencompass/opencompass_1783.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1783",
          "sample_count": 1000,
          "traits": [
            "Reasoning"
          ],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1783",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "æ¨ç†",
                "en": "Reasoning"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/SMU-DIGA/nppc",
            "paperLink": "https://arxiv.org/abs/2504.11239",
            "officialWebsiteLink": "",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "11",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-28 11:36:56",
            "supportOnlineEval": false,
            "updateDate": "2025-04-28 11:36:56",
            "createDate": "2025-04-28 11:16:50",
            "desc": {
              "cn": "éç¡®å®šæ€§å¤šé¡¹å¼æ—¶é—´é—®é¢˜æŒ‘æˆ˜ ï¼ˆNPPCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ–­æ‰©å±•çš„ LLM æ¨ç†åŸºå‡†ã€‚",
              "en": "Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. "
            }
          }
        },
        {
          "id": "opencompass_1782",
          "name": "HypoBench",
          "version": "1.0.0",
          "description": "HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate. HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets. ",
          "url": "opencompass/opencompass_1782.jsonl",
          "size_mb": 1.0,
          "checksum": "sha256:placeholder_opencompass_1782",
          "sample_count": 1000,
          "traits": [],
          "metadata": {
            "format": "jsonl",
            "language": "en",
            "domain": "general",
            "difficulty": "easy",
            "requires_auth": false,
            "estimated_input_tokens": 100,
            "estimated_output_tokens": 50
          },
          "original_data": {
            "id": "1782",
            "language": [],
            "domains": [],
            "concepts": [],
            "humans_vs_llm_qualifications": [],
            "task_type": [],
            "modalities": [],
            "sample_questions_answers": {},
            "advantages_disadvantages": {},
            "emoji": "",
            "dimensions": [
              {
                "cn": "å…¶ä»–",
                "en": "Other"
              }
            ],
            "subDimensions": [],
            "tags": [],
            "topicTags": [],
            "benchCertificateLevel": 0,
            "githubLink": "https://github.com/ChicagoHAI/HypoBench",
            "paperLink": "https://arxiv.org/abs/2504.11524",
            "officialWebsiteLink": "https://chicagohai.github.io/HypoBench/",
            "leaderboardLink": false,
            "creatorInfo": {
              "uid": "4006165",
              "name": "opencompass",
              "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
              "nickname": "OpenCompass å¸å—"
            },
            "lookNum": "10",
            "top": false,
            "state": 3,
            "publicFlag": 1,
            "reviewBlockError": null,
            "reviewSuccessDate": "2025-04-28 11:36:59",
            "supportOnlineEval": false,
            "updateDate": "2025-04-28 11:36:59",
            "createDate": "2025-04-28 11:08:05",
            "desc": {
              "cn": "HypoBenchï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨ä»å¤šä¸ªæ–¹é¢è¯„ä¼° LLM å’Œå‡è®¾ç”Ÿæˆæ–¹æ³•ï¼ŒåŒ…æ‹¬å®ç”¨æ€§ã€æ³›åŒ–æ€§å’Œå‡è®¾å‘ç°ç‡ã€‚HypoBench åŒ…æ‹¬ 7 ä¸ªçœŸå®ä»»åŠ¡å’Œ 5 ä¸ªåˆæˆä»»åŠ¡ï¼Œå…·æœ‰ 194 ä¸ªä¸åŒçš„æ•°æ®é›†ã€‚",
              "en": "HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate. HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets. "
            }
          }
        }
      ]
    }
  },
  "update_policy": {
    "check_interval_hours": 24,
    "auto_update": false,
    "notify_on_update": true,
    "backup_before_update": true,
    "rollback_on_failure": true
  },
  "authentication": {
    "required_for": [
      "bud_custom"
    ],
    "method": "oauth2",
    "token_endpoint": "https://auth.bud.eco/token"
  },
  "migration": null,
  "changelog": {
    "2025.01.04": [
      "Added GSM8K dataset for mathematical reasoning",
      "Added HumanEval dataset for code generation",
      "Added MMLU dataset for broad knowledge evaluation",
      "Added Bud Multilingual QA dataset",
      "Added Bud Safety Evaluation dataset",
      "Enhanced metadata with token estimates"
    ]
  }
}