[
    {
        "id": "1694",
        "name": "MaritimeBench",
        "estimated_input_tokens": 1000,
        "estimated_output_tokens": 500,
        "language": ["English", "Chinese"],
        "domains": ["Maritime", "Logistics"],
        "concepts": ["Navigation", "Shipping Law"],
        "humans_vs_llm_qualifications": ["Expert", "Novice"],
        "task_type": ["Multiple Choice", "Knowledge Assessment"],
        "modalities": ["text"],
        "sample_questions_answers": {
            "examples": [
                {
                    "question": "What is the maximum draft allowed for a vessel entering the Suez Canal?",
                    "options": ["A) 20.1 meters", "B) 22.5 meters", "C) 24.0 meters", "D) 26.8 meters"],
                    "correct_answer": "C) 24.0 meters",
                    "explanation": "The Suez Canal Authority sets the maximum draft at 24.0 meters for vessels transiting the canal."
                },
                {
                    "question": "According to MARPOL Annex VI, what is the global sulfur limit for marine fuel oil?",
                    "options": ["A) 0.1%", "B) 0.5%", "C) 1.0%", "D) 3.5%"],
                    "correct_answer": "B) 0.5%",
                    "explanation": "Since January 1, 2020, the global sulfur limit for marine fuel oil is 0.5% m/m as per MARPOL Annex VI."
                }
            ],
            "total_questions": 1888,
            "question_format": "Multiple choice questions with 4 options each",
            "difficulty_levels": ["Beginner", "Intermediate", "Advanced", "Expert"]
        },
        "advantages_disadvantages": {
            "advantages": [
                "Comprehensive coverage of maritime industry knowledge",
                "Based on authoritative industry standards and regulations",
                "Large dataset with 1,888 high-quality questions",
                "Bilingual support (English and Chinese)",
                "Covers both theoretical knowledge and practical applications",
                "Regular updates to reflect current maritime regulations"
            ],
            "disadvantages": [
                "Limited to multiple-choice format only",
                "Requires domain expertise for proper evaluation",
                "May not cover emerging maritime technologies",
                "Cultural bias towards certain maritime regions",
                "No practical simulation or hands-on assessment"
            ]
        },
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            },
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "航运",
                "en": "航运"
            },
            {
                "cn": "知识",
                "en": "知识"
            },
            {
                "cn": "海运",
                "en": "海运"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 4,
        "githubLink": "",
        "paperLink": "",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "067099",
            "name": "wangxiangyu",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/067099-bd016c2a-6b89-4ba3-aaa1-6cc12a7ca88f.png",
            "nickname": "Hi-Dolphin"
        },
        "lookNum": "274",
        "top": true,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-22 11:41:39",
        "supportOnlineEval": false,
        "updateDate": "2025-04-22 11:41:39",
        "createDate": "2025-04-01 09:13:05",
        "desc": {
            "cn": "MaritimeBench 致力于构建一套科学、公平且严谨的航运知识评估体系。基于行业权威标准，我们持续维护并更新高质量的航运数据集——其中包含1,888道客观选择题（MCQ格式），以全面、多维度地量化模型在航运各领域的能力表现。",
            "en": "MaritimeBench builds a scientific, fair maritime knowledge assessment system. With 1,888 MCQs based on industry standards, we evaluate models' capabilities across shipping domains."
        }
    },
    {
        "id": "1558",
        "name": "MM-AlignBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/PhoenixZ810/OmniAlign-V",
        "paperLink": "https://arxiv.org/abs/2502.18411",
        "officialWebsiteLink": "https://phoenixz810.github.io/OmniAlign-V/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "825",
        "top": true,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-05 09:55:00",
        "supportOnlineEval": false,
        "updateDate": "2025-03-05 09:55:00",
        "createDate": "2025-03-05 09:54:39",
        "desc": {
            "cn": "用于评估 MLLM 与人类偏好的一致性的基准。它包含 252 个高质量、人类标注的样本，具有不同的图像类型和开放式问题。它仿照 Arena 风格的基准，使用 GPT-4o 作为评判模型，Claude-Sonnet-3 作为参考模型。",
            "en": "A benchmark for evaluating MLLMs' alignment with human preferences. It includes 252 high-quality, human-annotated samples with diverse image types and open-ended questions. Modeled after Arena-style benchmarks, it uses GPT-4o as the judge model and Claude-Sonnet-3 as the reference model."
        }
    },
    {
        "id": "1509",
        "name": "MVBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/OpenGVLab/Ask-Anything",
        "paperLink": "https://arxiv.org/abs/2311.17005",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "423",
        "top": true,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-02-19 14:28:53",
        "supportOnlineEval": false,
        "updateDate": "2025-02-19 14:28:53",
        "createDate": "2025-02-17 15:17:00",
        "desc": {
            "cn": "MVBench用于评估多模态大模型在动态视频任务中的时间理解能力，由20个单帧内容无法有效解决的挑战性的视频任务组成。",
            "en": "MVBench can test MLLMs' temporal understanding in the dynamic video tasks. It covers 20 challenging video tasks that cannot be effectively solved with a single frame."
        }
    },
    {
        "id": "1375",
        "name": "VBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "创作",
                "en": "Creation"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Vchitect/VBench",
        "paperLink": "https://arxiv.org/abs/2311.17982",
        "officialWebsiteLink": "https://vchitect.github.io/VBench-project/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "440",
        "top": true,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:23:36",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:23:36",
        "createDate": "2025-01-10 16:36:24",
        "desc": {
            "cn": "VBench用于评估多模态大模型的视频生成质量，包含16个视频生成维度及1个人类偏好注释数据集。",
            "en": "VBench is a comprehensive benchmark evaluates video generation quality. It comprises 16 dimensions in video generation, and also provides a dataset of human preference annotations. "
        }
    },
    {
        "id": "1397",
        "name": "LiveMathBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            },
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 3,
        "githubLink": "https://github.com/open-compass/GPassK",
        "paperLink": "https://arxiv.org/abs/2412.13147",
        "officialWebsiteLink": "https://open-compass.github.io/GPassK/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "903",
        "top": true,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-16 20:30:57",
        "supportOnlineEval": false,
        "updateDate": "2025-01-16 20:30:57",
        "createDate": "2025-01-15 18:21:51",
        "desc": {
            "cn": "LiveMathBench用于评估大语言模型在复杂推理方面的表现，由极具挑战性的现代数学问题组成。",
            "en": "LiveMathBench can capture LLM capabilities in complex reasoning tasks, including challenging latest question sets from various mathematical competitions. "
        }
    },
    {
        "id": "1370",
        "name": "MathVision",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            },
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/mathllm/MATH-V",
        "paperLink": "https://arxiv.org/abs/2402.14804",
        "officialWebsiteLink": "https://mathllm.github.io/mathvision/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "564",
        "top": true,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-10 18:23:49",
        "supportOnlineEval": false,
        "updateDate": "2025-01-10 18:23:49",
        "createDate": "2025-01-10 18:19:40",
        "desc": {
            "cn": "MathVision用于评估多模态大模型的数学推理能力，由涵盖16个数学领域、跨越5个难度级别的3040个高质量数学问题组成。",
            "en": "MathVision measures multimodal mathematical reasoning capabilities through a meticulously curated collection of 3,040 high-quality mathematical problems spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty."
        }
    },
    {
        "id": "1371",
        "name": "MathVerse",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            },
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/ZrrSkywalker/MathVerse",
        "paperLink": "https://arxiv.org/abs/2403.14624",
        "officialWebsiteLink": "https://mathverse-cuhk.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "287",
        "top": true,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-10 18:00:01",
        "supportOnlineEval": false,
        "updateDate": "2025-01-10 18:00:01",
        "createDate": "2025-01-10 14:29:47",
        "desc": {
            "cn": "MathVerse用于评估多模态大模型的视觉数学问题解决能力，包含2612个高质量、多主题的数学问题。",
            "en": "MathVerse is intended for evaluating MLLMs' visual math problem-solving, containing 2,612 high-quality, multi-subject math problems with diagrams. "
        }
    },
    {
        "id": "1374",
        "name": "DynaMath",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            },
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/DynaMath/DynaMath",
        "paperLink": "https://arxiv.org/abs/2411.00836",
        "officialWebsiteLink": "https://dynamath.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "218",
        "top": true,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-10 18:25:13",
        "supportOnlineEval": false,
        "updateDate": "2025-01-10 18:25:13",
        "createDate": "2025-01-10 18:24:27",
        "desc": {
            "cn": " DynaMath用于评估多模态大模型的数学能力，包括501个高质量、多主题的种子问题，每个问题都以Python程序表示，能够自动生成大量具体的多样化问题。",
            "en": "DynaMath is a dynamic visual math benchmark designed for in-depth assessment of VLMs. It includes 501 high-quality, multi-topic seed questions, each represented as a Python program enabling the automatic generation of a much larger set of concrete questions."
        }
    },
    {
        "id": "1178",
        "name": "MathVista",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            },
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/lupantech/MathVista",
        "paperLink": "https://arxiv.org/pdf/2310.02255",
        "officialWebsiteLink": "https://mathvista.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "347",
        "top": true,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-10 18:23:44",
        "supportOnlineEval": false,
        "updateDate": "2025-01-10 18:23:44",
        "createDate": "2025-01-10 18:23:16",
        "desc": {
            "cn": "MathVista用于评估多模态大模型的数学能力，结合了丰富的数学和视觉任务挑战。它由6141个示例组成，来自28个涉及数学的现有多模态数据集和3个新创建的数据集。",
            "en": "MathVista is a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets."
        }
    },
    {
        "id": "1253",
        "name": "BigCodeBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            },
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/bigcode-project/bigcodebench/",
        "paperLink": "https://arxiv.org/abs/2406.15877",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "725",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-30 16:30:39",
        "supportOnlineEval": false,
        "updateDate": "2024-12-30 16:30:39",
        "createDate": "2024-12-30 16:24:34",
        "desc": {
            "cn": "BigCodeBench用于评估LLM的代码生成能力，包含1140个可以调用139个库和7个域的多个函数来完成的细粒度任务。",
            "en": "BigCodeBench is a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. "
        }
    },
    {
        "id": "539",
        "name": "BBH",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [
            {
                "cn": "reasoning",
                "en": "reasoning"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/suzgunmirac/BIG-Bench-Hard",
        "paperLink": "https://arxiv.org/pdf/2210.09261.pdf",
        "officialWebsiteLink": "https://github.com/suzgunmirac/BIG-Bench-Hard",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "17087",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:31:41",
        "supportOnlineEval": true,
        "updateDate": "2024-09-12 19:31:41",
        "createDate": "2024-09-12 19:26:57",
        "desc": {
            "cn": "BIG Bench-Hard（BBH）是BIG Bench的一个子集，它是一个用于语言模型的多样化评估套件。BBH专注于BIG Bench的23项具有挑战性的任务，这些任务被发现超出了当前语言模型的能力。",
            "en": "BIG-Bench Hard (BBH) is a subset of the BIG-Bench, a diverse evaluation suite for language models. BBH focuses on a suite of 23 challenging tasks from BIG-Bench that were found to be beyond the capabilities of current language models."
        }
    },
    {
        "id": "534",
        "name": "MATH",
        "emoji": "",
        "dimensions": [
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [
            {
                "cn": "math",
                "en": "math"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/hendrycks/math",
        "paperLink": "https://arxiv.org/pdf/2103.03874.pdf",
        "officialWebsiteLink": "https://huggingface.co/datasets/hendrycks/competition_math",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "12831",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:31:29",
        "supportOnlineEval": true,
        "updateDate": "2024-09-12 19:31:29",
        "createDate": "2024-09-12 19:28:19",
        "desc": {
            "cn": "MATH 是一个包含 12,500 个具有挑战性的竞赛数学问题的新数据集。 MATH 中的每个问题都有完整的分步解决方案。",
            "en": "MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution."
        }
    },
    {
        "id": "542",
        "name": "LongBench",
        "emoji": "🪑",
        "dimensions": [
            {
                "cn": "长文本",
                "en": "Long-Context"
            }
        ],
        "subDimensions": [
            {
                "cn": "long-context",
                "en": "long-context"
            }
        ],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/THUDM/LongBench",
        "paperLink": "https://arxiv.org/pdf/2308.14508.pdf",
        "officialWebsiteLink": "https://huggingface.co/datasets/THUDM/LongBench",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "8734",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:32:08",
        "supportOnlineEval": false,
        "updateDate": "2024-09-12 19:32:08",
        "createDate": "2024-09-12 19:26:26",
        "desc": {
            "cn": "LongBench 是一个多任务、中英双语、针对大语言模型长文本理解能力的评测基准。",
            "en": "LongBench is a benchmark for bilingual, multitask, and comprehensive assessment of long context understanding capabilities of large language models."
        }
    },
    {
        "id": "535",
        "name": "GSM8K",
        "emoji": "",
        "dimensions": [
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [
            {
                "cn": "math",
                "en": "math"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/openai/grade-school-math",
        "paperLink": "https://arxiv.org/pdf/2110.14168.pdf",
        "officialWebsiteLink": "https://huggingface.co/datasets/gsm8k",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "8452",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:31:32",
        "supportOnlineEval": true,
        "updateDate": "2024-09-12 19:31:32",
        "createDate": "2024-09-12 19:27:59",
        "desc": {
            "cn": "GSM8K 是一个包含 8,500 个高质量、语言多样化的小学数学单词问题的数据集，由人类问题编写者创建。该数据集分为 7,500 个训练问题和 1,000 个测试问题。这些问题的解题步骤在 2 到 8 步之间，解题过程主要涉及使用基本算术运算（+ - × ÷）进行一连串的基本计算，从而得出最终答案。一个聪明的初中生应该能够解决每一个问题。它可用于多步数学推理。",
            "en": "GSM8K is a dataset of 8,500 high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7,500 training problems and 1,000 test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ − × ÷) to reach the final answer."
        }
    },
    {
        "id": "512",
        "name": "TriviaQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/mandarjoshi90/triviaqa",
        "paperLink": "https://arxiv.org/abs/1705.03551",
        "officialWebsiteLink": "http://nlp.cs.washington.edu/triviaqa/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "5894",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": true,
        "updateDate": "2024-08-02 09:45:49",
        "createDate": "2024-01-11 14:10:43",
        "desc": {
            "cn": "TriviaqQA是一个阅读理解数据集，包含超过65万个问题-答案-证据三元组。其包括95K个问答对，由冷知识爱好者和独立收集的事实性文档撰写，平均每个问题6个，为回答问题提供高质量的远程监督。\n",
            "en": "TriviaqQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaqQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions."
        }
    },
    {
        "id": "895",
        "name": "Fin-Eva",
        "emoji": "📈",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [
            {
                "cn": "knowledge;Finance",
                "en": "knowledge;Finance"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 4,
        "githubLink": "https://github.com/alipay/financial_evaluation_dataset",
        "paperLink": "",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "50074087",
            "name": "Ant_Group",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50074087-492c3cdd-1529-4e49-84ad-8cc2f8c86893.png",
            "nickname": "蚂蚁集团"
        },
        "lookNum": "5368",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:40:45",
        "supportOnlineEval": false,
        "updateDate": "2024-09-12 19:40:45",
        "createDate": "2024-09-12 19:36:26",
        "desc": {
            "cn": "蚂蚁集团、上海财经大学联合推出金融评测集Fin-Eva Version 1.0，覆盖财富管理、保险、投资研究等多个金融场景以及金融专业主题学科，总评测题数目达到13,000+。",
            "en": "Ant Group and Shanghai University of Finance and Economics jointly launched the financial benchmark，Fin-Eva Version 1.0, covering multiple financial scenarios and subjects such as wealth management, insurance, investment research. The number of this benchmark's questions reaches 13,000+."
        }
    },
    {
        "id": "540",
        "name": "T-Eval",
        "emoji": "🗽",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 3,
        "githubLink": "https://github.com/open-compass/T-Eval",
        "paperLink": "https://arxiv.org/abs/2312.14033",
        "officialWebsiteLink": "https://open-compass.github.io/T-Eval/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "5298",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-01-25 15:15:04",
        "createDate": "2024-01-25 15:15:04",
        "desc": {
            "cn": "T-Eval 评估了 LLM 的工具使用能力，并将其分解为指令遵循、规划、推理、检索、理解和审查等子能力",
            "en": "T-Eval evaluates the tool utilization capabilities of LLMs and decomposing them into instruction following, planning, reasoning, retrieval, understanding, and review."
        }
    },
    {
        "id": "541",
        "name": "L-Eval",
        "emoji": "🦾",
        "dimensions": [
            {
                "cn": "长文本",
                "en": "Long-Context"
            }
        ],
        "subDimensions": [
            {
                "cn": "long-context",
                "en": "long-context"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/OpenLMLab/LEval",
        "paperLink": "https://arxiv.org/pdf/2307.11088",
        "officialWebsiteLink": "https://huggingface.co/datasets/L4NLP/LEval",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "4908",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:32:04",
        "supportOnlineEval": false,
        "updateDate": "2024-09-12 19:32:04",
        "createDate": "2024-09-12 19:26:40",
        "desc": {
            "cn": "L-Eval 是一个全面的长上下文语言模型（LCLMs）评估套件，包括 20 个子任务、508 个长文档和超过 2,000 个人工标记的查询-响应对。它涵盖了多种问答风格、领域和输入长度（3,000 至 200,000 个 token）。",
            "en": "L-Eval is a comprehensive Long Context Language Models (LCLMs) evaluation suite with 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3k～200k tokens)."
        }
    },
    {
        "id": "496",
        "name": "C-Eval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/SJTU-LIT/ceval",
        "paperLink": "https://arxiv.org/abs/2305.08322",
        "officialWebsiteLink": "https://cevalbenchmark.com/index.html",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "4636",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": true,
        "updateDate": "2024-01-11 14:09:22",
        "createDate": "2024-01-11 14:09:22",
        "desc": {
            "cn": "C-Eval 是一个全面的中文基础模型评估套件。它包含了13948个多项选择题，涵盖了52个不同的学科和四个难度级别。",
            "en": "C-Eval is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels."
        }
    },
    {
        "id": "498",
        "name": "MMLU",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/hendrycks/test",
        "paperLink": "https://arxiv.org/abs/2009.03300",
        "officialWebsiteLink": "https://github.com/hendrycks/test",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "4522",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": true,
        "updateDate": "2024-08-02 09:45:42",
        "createDate": "2024-01-11 14:09:29",
        "desc": {
            "cn": "MMLU (Massive Multitask Language Understanding) 是一个新的基准测试，旨在通过在零次学习和少次学习的环境中评估模型来测量预训练期间获得的知识。这使得基准测试更具挑战性，且更接近我们评估人类的方式。该基准测试涵盖了STEM、人文学科、社会科学等57个主题。其难度范围从小学级别到专业级别，旨在测试世界知识和解决问题的能力。测试主题范围从传统领域，如数学和历史，到更专业的领域，如法律和伦理学。题目的精细度和广度使该基准测试成为识别模型盲点的理想选择。",
            "en": "MMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics. The granularity and breadth of the subjects makes the benchmark ideal for identifying a model’s blind spots."
        }
    },
    {
        "id": "948",
        "name": "SecBench",
        "emoji": "🔍",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [
            {
                "cn": "Safety",
                "en": "Safety"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 4,
        "githubLink": "",
        "paperLink": "",
        "officialWebsiteLink": "https://secbench.org/dataset",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "50163763",
            "name": "Tencent",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50163763-2065e33a-96ba-4182-9708-b1967a29e6a4.png",
            "nickname": "Tencent"
        },
        "lookNum": "4400",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:40:48",
        "supportOnlineEval": false,
        "updateDate": "2024-09-12 19:40:48",
        "createDate": "2024-09-12 19:35:12",
        "desc": {
            "cn": "腾讯朱雀实验室和腾讯安全科恩实验室联合腾讯混元大模型团队、清华大学江勇教授/夏树涛教授团队、香港理工大学罗夏朴教授研究团队以及上海人工智能实验室OpenCompass团队，通过建设安全大模型评测基准SecBench，为安全大模型研发提供公平、公正、客观、全面的评测能力，推动安全大模型建设。",
            "en": "Tencent Zhuque Lab and Tencent Security Keen Lab, together with Tencent Huyuan Team, Professor Jiang Yong/Professor Xia Shutao's team from Tsinghua University, Professor Luo Xiapu's research team from Hong Kong Polytechnic University, and OpenCompass team from Shanghai Artificial Intelligence Laboratory, have jointly built a safety benchmark, SecBench. We provide fair, impartial, objective, and comprehensive evaluation capabilities and promote the construction of large model in security dimension."
        }
    },
    {
        "id": "537",
        "name": "HumanEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [
            {
                "cn": "code",
                "en": "code"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/openai/human-eval",
        "paperLink": "https://arxiv.org/pdf/2107.03374.pdf",
        "officialWebsiteLink": "https://huggingface.co/datasets/openai_humaneval",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "3586",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:31:35",
        "supportOnlineEval": true,
        "updateDate": "2024-09-12 19:31:35",
        "createDate": "2024-09-12 19:27:36",
        "desc": {
            "cn": "这是 \"Evaluating Large Language Models Trained on Code\" 论文中描述的 HumanEval 问题解决数据集的评估工具包。它用于测量从文档脚本合成程序的功能正确性。它由 164 个原始编程问题组成，评估语言理解能力、算法和简单数学，其中一些问题与简单的软件面试题类似。",
            "en": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases."
        }
    },
    {
        "id": "945",
        "name": "Flames",
        "emoji": "💡",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [
            {
                "cn": "Safety",
                "en": "Safety"
            }
        ],
        "tags": [],
        "topicTags": [
            {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
            }
        ],
        "benchCertificateLevel": 4,
        "githubLink": "https://github.com/AIFlames/Flames",
        "paperLink": "https://arxiv.org/abs/2311.06899",
        "officialWebsiteLink": "https://flames.opencompass.org.cn/leaderboard",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "2982",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-16 11:02:53",
        "supportOnlineEval": false,
        "updateDate": "2024-10-16 11:02:53",
        "createDate": "2024-10-15 15:34:25",
        "desc": {
            "cn": "Flames 是上海人工智能实验室和复旦大学 NLP团队开发的 LLM 价值对齐方向的中文高度对抗性基准。Flames 精心设计了一个由 2,251 个高度对抗性、人工创建的提示词成的评测集，每个提示词都经过精心设计，以探究特定的价值维度（即公平、安全、道德、合法、数据保护）。目前，Flames 发布了 1,000 个提示词供公众使用（Flames_1k_Chinese）。",
            "en": "Flames is a highly adversarial benchmark in Chinese for LLM's value alignment evaluation developed by Shanghai AI Lab and Fudan NLP Group. Flames meticulously designs a dataset of 2,251 highly adversarial, manually crafted prompts, each tailored to probe a specific value dimension (i.e., Fairness, Safety, Morality, Legality, Data protection). Currently,  Flames releases 1,000 prompts for public use (Flames_1k_Chinese)."
        }
    },
    {
        "id": "499",
        "name": "CMMLU",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/haonan-li/CMMLU/tree/1b8ce5813be6a011b1042f2d8fc7f0806e2ac0bc",
        "paperLink": "https://arxiv.org/abs/2306.09212",
        "officialWebsiteLink": "https://github.com/haonan-li/CMMLU/tree/1b8ce5813be6a011b1042f2d8fc7f0806e2ac0bc",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "2540",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": true,
        "updateDate": "2024-08-02 09:46:14",
        "createDate": "2024-01-11 14:09:33",
        "desc": {
            "cn": "CMMLU是一个综合性的中文评估基准，专门用于评估语言模型在中文语境下的知识和推理能力。CMMLU涵盖了从基础学科到高级专业水平的67个主题。它包括：需要计算和推理的自然科学，需要知识的人文科学和社会科学,以及需要生活常识的中国驾驶规则等。此外，CMMLU中的许多任务具有中国特定的答案，可能在其他地区或语言中并不普遍适用。因此是一个完全中国化的中文测试基准。",
            "en": "CMMLU is a comprehensive Chinese evaluation benchmark specifically designed to assess the knowledge and reasoning abilities of language models in the context of the Chinese language. CMMLU covers 67 topics ranging from basic subjects to advanced professional levels. It includes tasks that require calculations and reasoning in natural sciences, as well as tasks involving knowledge from humanities, social sciences, and practical aspects like Chinese driving rules. Moreover, many tasks within CMMLU have answers specific to China, which might not be universally applicable in other regions or languages. As a result, CMMLU serves as a fully localized Chinese evaluation benchmark."
        }
    },
    {
        "id": "1003",
        "name": "S-Eval",
        "emoji": "⚖️",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "LLM",
                "en": "LLM"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 4,
        "githubLink": "https://github.com/IS2Lab/S-Eval",
        "paperLink": "https://arxiv.org/abs/2405.14191",
        "officialWebsiteLink": "https://s-eval.github.io",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50151947",
            "name": "IS2Lab",
            "avatar": null,
            "nickname": "IS2Lab"
        },
        "lookNum": "2126",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-05-06 11:02:57",
        "supportOnlineEval": false,
        "updateDate": "2025-05-06 11:02:57",
        "createDate": "2025-04-22 16:28:30",
        "desc": {
            "cn": "S-Eval 是一个针对 LLM 的全新全面、多维、开放式安全评估基准，包含 102 个风险子类别的 220,000 个评估提示（仍在积极扩展中）和 10 个高级越狱攻击。",
            "en": "S-Eval is a new comprehensive, multi-dimensional and open-ended safety evaluation benchmark for LLMs consisting of 220,000 evaluation prompts (still in active expansion) across 102 risk subcategories and 10 advanced jailbreak attacks."
        }
    },
    {
        "id": "531",
        "name": "HellaSwag",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [
            {
                "cn": "reasoning",
                "en": "reasoning"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/rowanz/hellaswag",
        "paperLink": "https://arxiv.org/abs/1905.07830",
        "officialWebsiteLink": "https://allenai.org/data/hellaswag",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "2020",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:31:26",
        "supportOnlineEval": true,
        "updateDate": "2024-09-12 19:31:26",
        "createDate": "2024-09-12 19:28:41",
        "desc": {
            "cn": "HellaSwag 是一个用于评估常识性自然语言推理的数据集，HellaSwag的问题对于最先进的模型来说是特别困难的，尽管它的问题对于人类来说非常轻松就能回答的（> 95% 的准确率）。它由7万多道多项选择题组成，每道题都有一个场景和四种可能的答案，需要选择最合理的答案。这些问题来自两个领域：activitynet和wikihow，分别涉及视频和文本场景。这些问题的正确答案是下一个事件的真实句子，而错误答案是通过对抗技术生成的并经过人类验证，这些答案可以欺骗机器但不能欺骗人类。",
            "en": "HellaSwag is a challenge dataset for evaluating commonsense natural language inference, which is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy). It consists of 70k multiple choice questions, each with a scenario and four possible endings, which requires to select the most reasonable ending. These questions come from two domains:activitynet and wikihow, involving video and text scenarios respectively. The correct answers of these questions are the real sentences for the next event, while the incorrect answers are adversarially generated and human verified, so as to fool machines but not humans."
        }
    },
    {
        "id": "692",
        "name": "ChemBench",
        "emoji": "🧪",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [
            {
                "cn": "Name_Conversion",
                "en": "Name_Conversion"
            },
            {
                "cn": "Property_Prediction",
                "en": "Property_Prediction"
            },
            {
                "cn": "Mol2caption",
                "en": "Mol2caption"
            },
            {
                "cn": "Caption2mol",
                "en": "Caption2mol"
            },
            {
                "cn": "Product_Prediction",
                "en": "Product_Prediction"
            },
            {
                "cn": "Retrosynthesis",
                "en": "Retrosynthesis"
            },
            {
                "cn": "Yield_Prediction",
                "en": "Yield_Prediction"
            },
            {
                "cn": "Temperature_Prediction",
                "en": "Temperature_Prediction"
            },
            {
                "cn": "Solvent_Prediction",
                "en": "Solvent_Prediction"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 4,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2402.06852",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "50029510",
            "name": "OpenScienceLab",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50029510-baa30340-1e57-432f-9b2b-6f656752c65e.png",
            "nickname": "OpenScienceLab"
        },
        "lookNum": "2003",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:51:38",
        "supportOnlineEval": false,
        "updateDate": "2024-09-12 19:51:38",
        "createDate": "2024-09-12 19:49:11",
        "desc": {
            "cn": "ChemBench是一个包含了九项化学核心任务，4100个高质量单选问答的大语言模型化学能力评测基准.",
            "en": "ChemBench is a large-scale chemistry competency evaluation benchmark for language models, which includes nine chemistry core tasks and 4100 high-quality single-choice questions and answers."
        }
    },
    {
        "id": "538",
        "name": "MBPP",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [
            {
                "cn": "code",
                "en": "code"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/google-research/google-research/tree/master/mbpp",
        "paperLink": "https://arxiv.org/pdf/2108.07732v1",
        "officialWebsiteLink": "https://huggingface.co/datasets/mbpp",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1966",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:31:38",
        "supportOnlineEval": true,
        "updateDate": "2024-09-12 19:31:38",
        "createDate": "2024-09-12 19:27:17",
        "desc": {
            "cn": "该基准测试由大约1000个入门级程序员可以解决的众包Python编程问题组成，涵盖编程基础知识、标准库功能等。每个问题都由任务描述、代码解决方案和3个自动化测试用例组成。",
            "en": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases."
        }
    },
    {
        "id": "497",
        "name": "AGIEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [
            {
                "cn": "examination",
                "en": "examination"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/ruixiangcui/AGIEval",
        "paperLink": "https://arxiv.org/pdf/2304.06364",
        "officialWebsiteLink": "https://github.com/ruixiangcui/AGIEval",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1580",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:31:13",
        "supportOnlineEval": false,
        "updateDate": "2024-09-12 19:31:13",
        "createDate": "2024-09-12 19:30:02",
        "desc": {
            "cn": "AGIEval是一个以人为中心的基准测试，专门设计用于评估基础模型在涉及人类认知和问题解决的任务中的一般能力。该基准测试源自20个官方、公开和高标准的入学和资格考试，例如普通大学入学考试（例如中国高考和美国SAT）、法学院入学考试、数学竞赛、律师资格考试以及国家公务员考试",
            "en": "AGIEval is a human-centric benchmark specifically designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving. This benchmark is derived from 20 official, public, and high-standard admission and qualification exams intended for general human test-takers, such as general college admission tests (e.g., Chinese College Entrance Exam (Gaokao) and American SAT), law school admission tests, math competitions, lawyer qualification tests, and national civil service exams."
        }
    },
    {
        "id": "505",
        "name": "CHID",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/chujiezheng/ChID-Dataset",
        "paperLink": "https://arxiv.org/abs/1906.01265",
        "officialWebsiteLink": "https://github.com/chujiezheng/ChID-Dataset",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1562",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:48:03",
        "createDate": "2024-01-11 14:10:02",
        "desc": {
            "cn": "CHID是一个中文成语阅读理解任务，要求根据上下文选择正确的成语填空，共有10个候选成语。",
            "en": " CHID is a chinese idiom reading comprehension task, which requires to select the correct idiom to fill in the blank according to the context, with 10 candidate idioms."
        }
    },
    {
        "id": "529",
        "name": "COPA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://people.ict.usc.edu/~gordon/copa.html",
        "paperLink": "https://arxiv.org/abs/1905.00537",
        "officialWebsiteLink": "https://people.ict.usc.edu/~gordon/copa.html",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1504",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:51:45",
        "createDate": "2024-01-11 14:11:52",
        "desc": {
            "cn": "COPA是一个因果推断任务，要求根据给定的前提，选择正确的因果关系。\n",
            "en": "COPA is a causal inference task, which requires to select the correct causal relation based on the given premise."
        }
    },
    {
        "id": "502",
        "name": "ARC-c",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://allenai.org/data/arc",
        "paperLink": "https://arxiv.org/pdf/1803.05457.pdf",
        "officialWebsiteLink": "https://allenai.org/data/arc",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1502",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:48:39",
        "createDate": "2024-01-11 14:09:48",
        "desc": {
            "cn": "AI2的推理挑战（ARC）数据集是一个多项选择问题回答数据集，包含了从三年级到九年级的科学考试中提取的问题。该数据集分为两个部分：简单和挑战，其中后者包含了需要推理能力的更难的问题。大多数问题有4个答案选项，仅有不到1％的问题有3个或5个答案选项。",
            "en": "The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. "
        }
    },
    {
        "id": "518",
        "name": "OpenbookQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/allenai/OpenBookQA",
        "paperLink": "https://arxiv.org/abs/1809.02789",
        "officialWebsiteLink": "https://allenai.org/data/open-book-qa",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1408",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:48:08",
        "createDate": "2024-01-11 14:11:07",
        "desc": {
            "cn": "OpenBookQA包含需要多步推理、运用常识知识、深入理解文本等能力的问题，是一种新型的问答数据集，其模式借鉴了开放式书本考试，用于评估人类对某一主题理解的程度。",
            "en": "OpenBookQA contains questions that require multi-step reasoning, application of common-sense knowledge, and in-depth comprehension of text. It is a new type of question-answering dataset, modeled after open-book exams, designed to assess human understanding of a specific topic."
        }
    },
    {
        "id": "631",
        "name": "OpenFinData",
        "emoji": "🦾",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [
            {
                "cn": "Finance",
                "en": "Finance"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 4,
        "githubLink": "https://github.com/open-compass/OpenFinData",
        "paperLink": "",
        "officialWebsiteLink": "https://openfindata.org/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "50029256",
            "name": "eastmoney",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50029256-0729a0fd-9e97-4c38-b9f9-a580ad6f6a54.png",
            "nickname": "eastmoney"
        },
        "lookNum": "1405",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:40:43",
        "supportOnlineEval": false,
        "updateDate": "2024-09-12 19:40:43",
        "createDate": "2024-09-12 19:39:10",
        "desc": {
            "cn": "OpenFinData是由东方财富与上海人工智能实验室联合发布的开源金融评测数据集。该数据集代表了最真实的产业场景需求，是目前场景最全、专业性最深的金融评测数据集。它基于东方财富实际金融业务的多样化丰富场景，旨在为金融科技领域的研究者和开发者提供一个高质量的数据资源。",
            "en": "OpenFinData is an open-source financial evaluation dataset jointly released by EastMoney and Shanghai Artificial Intelligence Laboratory. This dataset represents the most realistic industrial scenario requirements and is currently the most comprehensive and professional financial evaluation dataset."
        }
    },
    {
        "id": "511",
        "name": "CommonSenseQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/jonathanherzig/commonsenseqa",
        "paperLink": "https://arxiv.org/abs/1811.00937",
        "officialWebsiteLink": "https://www.tau-nlp.sites.tau.ac.il/commonsenseqa",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1296",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:48:32",
        "createDate": "2024-01-11 14:10:39",
        "desc": {
            "cn": "CommonsenseQA是一个选择题数据集，它需要不同类型的常识知识来预测正确答案。它包含12,102个问题，有一个正确答案和四个干扰答案。\n",
            "en": "\nCommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers. \n"
        }
    },
    {
        "id": "514",
        "name": "C3",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [
            {
                "cn": "understanding",
                "en": "understanding"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/nlpdata/c3",
        "paperLink": "https://arxiv.org/abs/1904.09679",
        "officialWebsiteLink": "https://github.com/nlpdata/c3",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1247",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:31:23",
        "supportOnlineEval": false,
        "updateDate": "2024-09-12 19:31:23",
        "createDate": "2024-09-12 19:29:14",
        "desc": {
            "cn": "一个自由形式的多项选择中文机器阅读理解数据集（C3），包含13369篇文献（对话或更正式的混合体裁文本）及其相关的19577道自由选择题，这些问题都是从汉语作为第二语言的考试中收集到的",
            "en": "A free-form multiple-Choice Chinese machine reading Comprehension dataset (C3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations"
        }
    },
    {
        "id": "513",
        "name": "NQ",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/google-research-datasets/natural-questions",
        "paperLink": "https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b8c26e4347adc3453c15d96a09e6f7f102293f71.pdf",
        "officialWebsiteLink": "https://ai.google.com/research/NaturalQuestions/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1236",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": true,
        "updateDate": "2024-08-02 09:48:17",
        "createDate": "2024-01-11 14:10:47",
        "desc": {
            "cn": "NQ 数据集来自于真实用户的问题，它要求 QA 系统阅读和理解整个维基百科文章，这些文章可能包含也可能不包含问题的答案。由真实用户问题构成，以及需要阅读整个页面才能找到答案的要求，比以往的 QA 数据集更现实和更具挑战性的任务。",
            "en": "NQ (NaturalQuestion) corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. The inclusion of real user questions, and the requirement that solutions should read an entire page to find the answer, cause NQ to be a more realistic and challenging task than prior QA datasets."
        }
    },
    {
        "id": "564",
        "name": "LV-Eval",
        "emoji": "🗽",
        "dimensions": [
            {
                "cn": "长文本",
                "en": "long-context"
            }
        ],
        "subDimensions": [
            {
                "cn": "Question Answering",
                "en": "Question Answering"
            },
            {
                "cn": "Synthetic",
                "en": "Synthetic"
            },
            {
                "cn": "Confusing Evidence",
                "en": "Confusing Evidence"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 4,
        "githubLink": "https://github.com/infinigence/LVEval",
        "paperLink": "https://arxiv.org/abs/2402.05136",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "066910",
            "name": null,
            "avatar": null,
            "nickname": "OpenXLab-KGJYN5OmL"
        },
        "lookNum": "1218",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 10:32:21",
        "createDate": "2024-02-18 11:35:12",
        "desc": {
            "cn": "LV-Eval是一个具备5个长度等级（16k、32k、64k、128k和256k）、最大文本测试长度达到256k的长文本评测基准。LV-Eval的平均文本长度达到102,380字，最小/最大文本长度为11,896/387,406字。LV-Eval主要有两类评测任务——单跳QA和多跳QA，共包含11个涵盖中英文的评测数据子集。",
            "en": "LV-Eval is a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. The average number of words is 102,380, and the Min/Max number of words is 11,896/387,406. It features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets."
        }
    },
    {
        "id": "510",
        "name": "BoolQ",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [
            {
                "cn": "knowledge",
                "en": "knowledge"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/google-research-datasets/boolean-questions",
        "paperLink": "https://arxiv.org/abs/1905.10044",
        "officialWebsiteLink": "https://github.com/google-research-datasets/boolean-questions",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1156",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:31:18",
        "supportOnlineEval": false,
        "updateDate": "2024-09-12 19:31:18",
        "createDate": "2024-09-12 19:29:42",
        "desc": {
            "cn": "BoolQ是一个包含15942个示例的是/否问题的问答数据集。这些问题是自然生成的——即在无prompt和无约束的环境中产生的。每个例子都是一个三元组(问题、段落、答案)，页面标题是可选的附加上下文。",
            "en": "BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring ---they are generated in unprompted and unconstrained settings. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context."
        }
    },
    {
        "id": "1206",
        "name": "MMBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 3,
        "githubLink": "https://github.com/open-compass/MMBench",
        "paperLink": "https://arxiv.org/abs/2307.06281",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1126",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-30 16:30:49",
        "supportOnlineEval": false,
        "updateDate": "2024-12-30 16:30:49",
        "createDate": "2024-12-30 16:16:18",
        "desc": {
            "cn": "MMBench是OpenCompass 研究团队自建的视觉语言模型评测数据集，可实现从感知到认知能力逐级细分评估。此评测基准包含3000 道单项选择题 ，覆盖 20个细粒度评估维度。",
            "en": "MMBench is a collection of benchmarks to evaluate the multi-modal understanding capability of large vision language models (LVLMs). This benchmark contains 3,000 multiple-choice questions covering 20 fine-grained assessment dimensions."
        }
    },
    {
        "id": "500",
        "name": "GAOKAO-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/OpenLMLab/GAOKAO-Bench",
        "paperLink": "https://arxiv.org/abs/2305.12474",
        "officialWebsiteLink": "https://github.com/OpenLMLab/GAOKAO-Bench",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1100",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": true,
        "updateDate": "2024-08-02 09:49:06",
        "createDate": "2024-01-11 14:09:41",
        "desc": {
            "cn": "GAOKAO-bench是一个以中国高考题目为数据集，旨在提供和人类对齐的，直观，高效地测评大模型语言理解能力、逻辑推理能力的测评框架",
            "en": "GAOKAO-bench is an evaluation framework that utilizes Chinese high school entrance examination (GAOKAO) questions as a dataset to evaluate the language understanding and logical reasoning abilities of large language models."
        }
    },
    {
        "id": "543",
        "name": "HumanEval-X",
        "emoji": "🗽",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [
            {
                "cn": "code",
                "en": "code"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/THUDM/CodeGeeX",
        "paperLink": "https://arxiv.org/abs/2303.17568",
        "officialWebsiteLink": "https://huggingface.co/datasets/THUDM/humaneval-x",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1021",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-12 19:32:11",
        "supportOnlineEval": false,
        "updateDate": "2024-09-12 19:32:11",
        "createDate": "2024-09-12 19:25:43",
        "desc": {
            "cn": "HumanEval-X 是一个用于评估代码生成模型的多语言能力的基准测试。它包含了820个高质量的人工制作的数据样本（每个都有测试案例），包括Python、C++、Java、JavaScript和Go语言，可用于各种任务，如代码生成和翻译。",
            "en": "HumanEval-X is a benchmark for evaluating the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks, such as code generation and translation."
        }
    },
    {
        "id": "504",
        "name": "WiC",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://pilehvar.github.io/wic/",
        "paperLink": "https://arxiv.org/abs/1808.09121",
        "officialWebsiteLink": "https://pilehvar.github.io/wic/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "1009",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:48:51",
        "createDate": "2024-01-11 14:09:58",
        "desc": {
            "cn": "Word-in-Context是一个词义消歧任务，被视为句子对的二元分类。给定两个文本片段和一个在两个句子中都出现的多义词，任务是确定该词在两个句子中是否具有相同的含义。",
            "en": "WiC is a benchmark for the evaluation of context-sensitive word embeddings. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. In fact, the dataset can also be viewed as an application of Word Sense Disambiguation in practise."
        }
    },
    {
        "id": "509",
        "name": "Flores",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/facebookresearch/flores/",
        "paperLink": "https://arxiv.org/abs/2106.03193",
        "officialWebsiteLink": "https://github.com/facebookresearch/flores/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "889",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:50:59",
        "createDate": "2024-01-11 14:10:32",
        "desc": {
            "cn": "Flores是一个用于评估低资源语言机器翻译的基准数据集，它包含了从维基百科翻译的句子，涉及英语和四种低资源语言，分别是尼泊尔语、僧伽罗语、高棉语和普什图语。Flores有两个版本，我们这里使用的是第一个版本Flores-101，它包含有除英语外的101种语言。",
            "en": "Flores is a benchmark dataset for machine translation between English and low-resource languages, which consists of sentences translated from Wikipedia, involving English and four low-resource languages, namely Nepali, Sinhala, Khmer and Pashto. Flores has two versions, we use the Flores-101 version here, which is the second version including 101 languages besides english. "
        }
    },
    {
        "id": "524",
        "name": "CMNLI",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/CLUEbenchmark/CLUE",
        "paperLink": "https://arxiv.org/abs/2004.05986",
        "officialWebsiteLink": "https://www.cluebenchmarks.com/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "883",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:49:37",
        "createDate": "2024-01-11 14:11:34",
        "desc": {
            "cn": "CMNLI是一个中文自然语言推理任务，要求根据两个句子判断它们之间的逻辑关系，有三种关系：蕴含、矛盾和中立。",
            "en": "CMNLI  is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral."
        }
    },
    {
        "id": "520",
        "name": "LCSTS",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "http://icrc.hitsz.edu.cn/Article/show/139.html",
        "paperLink": "https://arxiv.org/abs/1506.05865",
        "officialWebsiteLink": "http://icrc.hitsz.edu.cn/Article/show/139.html",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "882",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:49:18",
        "createDate": "2024-01-11 14:11:16",
        "desc": {
            "cn": "LCSTS是一个大规模的中文短文本摘要数据集，从中国微博网站新浪微博中构建而成，并已开源。该数据集包含超过 200 万条真实的中文短文本，每个文本都提供了一个简短的摘要。",
            "en": "LCSTS is a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text."
        }
    },
    {
        "id": "1052",
        "name": "CaLM",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/OpenCausaLab/CaLM",
        "paperLink": "https://arxiv.org/abs/2405.00622",
        "officialWebsiteLink": "https://opencausalab.github.io/CaLM/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50204289",
            "name": "OpenCausaLab",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50204289-b2e0a1f4-5d24-4237-b62a-ed8731d8834d.png",
            "nickname": "OpenCausaLab"
        },
        "lookNum": "825",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-23 16:42:32",
        "supportOnlineEval": false,
        "updateDate": "2024-09-23 16:42:32",
        "createDate": "2024-09-13 13:55:09",
        "desc": {
            "cn": "CaLM是上海人工智能实验室联合同济大学、上海交通大学、北京大学及商汤科技发布首个大模型因果推理开放评测体系。首次从因果推理角度提出评估框架，为AI研究者打造可靠评测工具，从而为推进大模型认知能力向人类水平看齐提供指标参考。",
            "en": "CaLM is the first comprehensive benchmark for evaluating the causal reasoning capabilities of language models. The CaLM framework establishes a foundational taxonomy consisting of four modules: causal target, adaptation, metric, and error.\n"
        }
    },
    {
        "id": "532",
        "name": "PIQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/francois-rozet/piqa",
        "paperLink": "https://arxiv.org/abs/1911.11641",
        "officialWebsiteLink": "https://yonatanbisk.com/piqa/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "811",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:49:25",
        "createDate": "2024-01-11 14:12:04",
        "desc": {
            "cn": "PIQA是一个物理交互问答任务，要求根据给定的场景和两个可能的解决方案，选择最合理的方案。这个任务是为了测试模型在物理常识方面的知识。这个数据集包含了16000个训练样本，800个开发样本和2000个测试样本，所有的文本都是英文文本。",
            "en": "PIQA is a physical interaction question answering task, which requires to select the most reasonable solution based on the given scenario and two possible solutions. This task is designed to test the model's knowledge in physical commonsense. This dataset consists of 16k training samples, 800 development samples and 2k test samples, all on English text."
        }
    },
    {
        "id": "533",
        "name": "SIQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "",
        "paperLink": "https://arxiv.org/pdf/1904.09728.pdf",
        "officialWebsiteLink": "https://leaderboard.allenai.org/socialiqa/submissions/public",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "811",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:49:13",
        "createDate": "2024-01-11 14:12:08",
        "desc": {
            "cn": "SIQA 是一个社会交互问答任务，要求根据给定的场景和三个可能的后续行为，选择最合理的行为。这个任务是为了测试模型在社会常识方面的知识。这个数据集包含了 38963 个训练样本，1951 个开发样本和 1960 个测试样本，所有的文本都是英文文本。",
            "en": "SIQA is a social interaction question answering task, which requires to select the most reasonable behavior based on the given scenario and three possible subsequent behaviors. This task is designed to test the model's knowledge in social commonsense. This dataset consists of 38,963 training samples, 1,951 development samples and 1,960 test samples, all on English text."
        }
    },
    {
        "id": "519",
        "name": "CSL",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/ydli-ai/CSL",
        "paperLink": "https://arxiv.org/abs/2209.05034",
        "officialWebsiteLink": "https://github.com/ydli-ai/CSL",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "804",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:50:33",
        "createDate": "2024-01-11 14:11:11",
        "desc": {
            "cn": "CSL是一个大规模的中文科技文献数据集，包含 39.6 万篇论文的标题、摘要、关键词和学术领域信息。",
            "en": "CSL is a large-scale Chinese Scientific Literature dataset, which contains the titles, abstracts, keywords and academic fields of 396k papers."
        }
    },
    {
        "id": "536",
        "name": "DROP",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/allenai/allennlp-reading-comprehension/blob/master/allennlp_rc/eval/drop_eval.py",
        "paperLink": "https://arxiv.org/abs/1903.00161",
        "officialWebsiteLink": "https://allennlp.org/drop",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "787",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:50:10",
        "createDate": "2024-01-11 14:12:19",
        "desc": {
            "cn": "DROP 是一个测试段落综合理解能力的 QA 数据集。在这个众包、对抗性创建的 96K 问题解答基准中，系统必须解析问题中的多个引用，将它们映射到段落中，并对它们执行离散操作（如加法、计数或排序）。",
            "en": "DROP is a QA dataset which tests comprehensive understanding of paragraphs. In this crowdsourced, adversarially-created, 96k question-answering benchmark, a system must resolve multiple references in a question, map them onto a paragraph, and perform discrete operations over them (such as addition, counting, or sorting)."
        }
    },
    {
        "id": "503",
        "name": "ARC-e",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://allenai.org/data/arc",
        "paperLink": "https://arxiv.org/pdf/1803.05457.pdf",
        "officialWebsiteLink": "https://allenai.org/data/arc",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "785",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:49:30",
        "createDate": "2024-01-11 14:09:54",
        "desc": {
            "cn": "AI2的推理挑战（ARC）数据集是一个多项选择问题回答数据集，包含了从三年级到九年级的科学考试中提取的问题。该数据集分为两个部分：简单和挑战，其中后者包含了需要推理能力的更难的问题。大多数问题有4个答案选项，仅有不到1％的问题有3个或5个答案选项。",
            "en": "The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. "
        }
    },
    {
        "id": "924",
        "name": "CS-Bench",
        "emoji": "😜",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            },
            {
                "cn": "代码",
                "en": "Code"
            },
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/csbench/csbench",
        "paperLink": "https://arxiv.org/pdf/2406.08587",
        "officialWebsiteLink": "https://csbench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "50103486",
            "name": null,
            "avatar": null,
            "nickname": "LeonDiao0427"
        },
        "lookNum": "784",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-07-11 09:52:59",
        "createDate": "2024-07-11 09:48:45",
        "desc": {
            "cn": "CS-Bench 第一个专门用于评估LLMs在计算机科学中表现的双语（中英文）基准。CS-Bench包括约5,000个精心策划的测试样本，涵盖了计算机科学4个关键领域中的26个子领域，并包括各种任务形式和知识推理的划分。",
            "en": "CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning."
        }
    },
    {
        "id": "1073",
        "name": "SafetyBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/thu-coai/SafetyBench",
        "paperLink": "https://arxiv.org/pdf/2309.07045",
        "officialWebsiteLink": "https://llmbench.ai/safety",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "5018933",
            "name": "thu-coai",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/5018933-17d059c5-d271-410a-82a9-ef8966927c24.png",
            "nickname": "thu-coai"
        },
        "lookNum": "735",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-27 17:50:17",
        "supportOnlineEval": false,
        "updateDate": "2024-09-27 17:50:17",
        "createDate": "2024-09-27 12:52:37",
        "desc": {
            "cn": "SafetyBench 是一个全面的基准，用于评估大型语言模型（LLMs）的安全性，包含 11,435 道多样化的选择题，涵盖 7 个不同的安全关注类别。SafetyBench 还包含中文和英文的数据，方便双语评估。",
            "en": " SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data."
        }
    },
    {
        "id": "521",
        "name": "XSum",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/EdinburghNLP/XSum",
        "paperLink": "https://arxiv.org/abs/1808.08745",
        "officialWebsiteLink": "https://github.com/EdinburghNLP/XSum",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "726",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:50:47",
        "createDate": "2024-01-11 14:11:21",
        "desc": {
            "cn": "XSum是一个单文档摘要任务，不支持抽取式策略，需要采用抽象建模方法。其思想是创建一个简短的一句话新闻摘要，回答“这篇文章是关于什么的？”的问题。该数据集通过从英国广播公司（BBC）收集在线文章，得到了大量的现实数据。",
            "en": "XSum is a single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question “What is the article about?”. The dataset collects real-world, large scale data by harvesting online articles from the British Broadcasting Corporation (BBC)."
        }
    },
    {
        "id": "1097",
        "name": "HelloBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "长文本",
                "en": "Long-Context"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 3,
        "githubLink": "https://github.com/Quehry/HelloBench",
        "paperLink": "https://arxiv.org/pdf/2409.16191",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "711",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-30 15:32:45",
        "supportOnlineEval": false,
        "updateDate": "2024-09-30 15:32:45",
        "createDate": "2024-09-30 15:06:26",
        "desc": {
            "cn": "HelloBench为长文本生成基准，这是一个全面的、开放式的基准，用于评估LLM在生成长文本方面的性能。基于Bloom的分类法，HelloBench将长文本生成任务分为五个子任务：开放式QA、摘要、聊天、文本完成和启发式文本生成。",
            "en": "HelloBench is a hierarchical long text generation benchmark to evaluate LLMs' performance in generating long text.  Based on Bloom's Taxonomy, HelloBench categorizes long text generation tasks into five subtasks: open-ended QA, summarization, chat, text completion, and heuristic text generation."
        }
    },
    {
        "id": "506",
        "name": "AFQMC",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/IDEA-CCNL/Fengshenbang-LM/",
        "paperLink": "https://arxiv.org/abs/2209.02970",
        "officialWebsiteLink": "https://tianchi.aliyun.com/dataset/106411",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "708",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:50:04",
        "createDate": "2024-01-11 14:10:07",
        "desc": {
            "cn": "AFQMC一个蚂蚁金服中文语义相似度任务，要求判断两个句子是否具有相同的语义。",
            "en": " AFQMC is an Ant Financial chinese semantic similarity task, which requires to judge whether two sentences have the same meaning or not."
        }
    },
    {
        "id": "516",
        "name": "RACE(High)",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/qizhex/RACE_AR_baselines",
        "paperLink": "https://arxiv.org/abs/1704.04683",
        "officialWebsiteLink": "https://www.cs.cmu.edu/~glai1/data/race/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "694",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:49:43",
        "createDate": "2024-01-11 14:10:58",
        "desc": {
            "cn": "RACE 是一个大规模的阅读理解数据集，包含超过 28,000 个段落和近 100,000 个问题。该数据集是从中国的英语考试中收集而来，这些考试是为中学和高中学生设计的。\n",
            "en": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. "
        }
    },
    {
        "id": "507",
        "name": "WSC",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html",
        "paperLink": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.729.9814&rep=rep1&type=pdf",
        "officialWebsiteLink": "https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "684",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:51:25",
        "createDate": "2024-01-11 14:10:14",
        "desc": {
            "cn": "WSC是一个代词消歧任务，要求根据上下文判断代词指代的是哪个名词。",
            "en": "WSC is a pronoun disambiguation task, which requires to determine which noun the pronoun refers to according to the context."
        }
    },
    {
        "id": "544",
        "name": "DS-1000",
        "emoji": "🗽",
        "dimensions": [
            {
                "cn": "代码",
                "en": "code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/xlang-ai/DS-1000",
        "paperLink": "https://arxiv.org/pdf/2211.11501.pdf",
        "officialWebsiteLink": "https://ds1000-code-gen.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "673",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-01-26 15:43:03",
        "createDate": "2024-01-26 15:43:03",
        "desc": {
            "cn": "DS-1000 是一个代码生成基准测试，包含一千个数据科学问题，涵盖七个Python库，其特点是（1）反映多样化、现实且实用的用例，（2）具有可靠的度量标准，（3）通过扰乱问题来防止记忆化。",
            "en": "DS-1000 is a code generation benchmark with a thousand data science questions spanning seven Python libraries that (1) reflects diverse, realistic, and practical use cases, (2) has a reliable metric, (3) defends against memorization by perturbing questions."
        }
    },
    {
        "id": "525",
        "name": "OCNLI",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/cluebenchmark/OCNLI",
        "paperLink": "https://arxiv.org/abs/2010.05444",
        "officialWebsiteLink": "https://github.com/cluebenchmark/OCNLI",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "671",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:50:54",
        "createDate": "2024-01-11 14:11:37",
        "desc": {
            "cn": "OCNLI是一个中文自然语言推理任务，要求根据两个句子判断它们之间的逻辑关系，有三种关系：蕴含、矛盾和中立。",
            "en": "OCNLI is a Chinese natural language inference task, which requires to determine the logical relation between two sentences, with three relations: entailment, contradiction and neutral."
        }
    },
    {
        "id": "517",
        "name": "RACE(Middle)",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/qizhex/RACE_AR_baselines",
        "paperLink": "https://arxiv.org/abs/1704.04683",
        "officialWebsiteLink": "https://www.cs.cmu.edu/~glai1/data/race/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "663",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:52:17",
        "createDate": "2024-01-11 14:11:03",
        "desc": {
            "cn": "RACE 是一个大规模的阅读理解数据集，包含超过 28,000 个段落和近 100,000 个问题。该数据集是从中国的英语考试中收集而来，这些考试是为中学和高中学生设计的。\n",
            "en": "RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. "
        }
    },
    {
        "id": "508",
        "name": "TyDiQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/google-research-datasets/tydiqa",
        "paperLink": "https://arxiv.org/abs/2003.05002",
        "officialWebsiteLink": "https://ai.google.com/research/tydiqa",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "647",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-01-11 14:10:23",
        "createDate": "2024-01-11 14:10:23",
        "desc": {
            "cn": "TyDi QA 是一个涵盖 11 种不同语言的问题回答数据集，包含 20.4 万个问题-答案对。TyDi QA 的语言种类多样，涵盖了语言学特征的各种类型。",
            "en": "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language expresses."
        }
    },
    {
        "id": "523",
        "name": "LAMBADA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://zenodo.org/record/2630551",
        "paperLink": "https://arxiv.org/abs/1606.06031",
        "officialWebsiteLink": "https://zenodo.org/record/2630551",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "636",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:51:18",
        "createDate": "2024-01-11 14:11:30",
        "desc": {
            "cn": "LAMBADA 通过一个单词预测任务来评估计算模型对文本理解的能力。LAMBADA 是有如下特点的一组叙述性文章：如果面对整篇文章，人们可以猜测它们的最后一个单词，但如果他们只看到目标单词前面的最后一句话，就无法猜测。为了在 LAMBADA 上由好的效果，模型不能仅仅依赖于局部上下文，而必须能够跟踪更广泛的话语信息。\nLAMBADA 数据集是从 BookCorpus 中提取的，包括 10,022 段落，分为 4,869 个开发段落和 5,153 个测试段落，共计 2.03 亿个单词。",
            "en": "The LAMBADA evaluates the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse.\nThe LAMBADA dataset is extracted from BookCorpus and consists of 10'022 passages, divided into 4'869 development and 5'153 test passages, comprising 203 million words."
        }
    },
    {
        "id": "557",
        "name": "OCRBench",
        "emoji": "👹",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [
            {
                "cn": "vision-language",
                "en": "vision-language"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Yuliang-Liu/MultimodalOCR",
        "paperLink": "https://arxiv.org/abs/2305.07895",
        "officialWebsiteLink": "https://huggingface.co/spaces/echo840/ocrbench-leaderboard",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "40041912",
            "name": "echo840",
            "avatar": null,
            "nickname": "echo840"
        },
        "lookNum": "636",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-01-30 10:43:27",
        "createDate": "2024-01-30 10:43:27",
        "desc": {
            "cn": "OCRBench对 GPT4V 和 Gemini 等大型多模态模型在各种文本相关的视觉任务中的表现进行了全面的评估，包括文本识别、场景文本为中心的视觉问答 (VQA)、面向文档的 VQA、关键信息提取 (KIE) 和手写数学表达式识别 (HMER)。",
            "en": "OCRBench provides a comprehensive evaluation of Large Multimodal Models, such as GPT4V and Gemini, in various text-related visual tasks including Text Recognition, Scene Text-Centric Visual Question Answering (VQA), Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER). "
        }
    },
    {
        "id": "1172",
        "name": "MT-Bench-101",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/mtbench101/mt-bench-101",
        "paperLink": "https://arxiv.org/pdf/2402.14762",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "606",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-27 17:23:10",
        "supportOnlineEval": false,
        "updateDate": "2024-10-27 17:23:10",
        "createDate": "2024-10-21 14:33:55",
        "desc": {
            "cn": "MT-Bench-101 专门设计用于评估 LLMs 在多轮对话中的细粒度能力。通过对真实多轮对话数据的详细分析，构建了一个三层级的能力分类法，涵盖 1388 个多轮对话中的 4208 个轮次，涉及 13 种不同的任务。",
            "en": "MT-Bench-101 is specifically designed to evaluate the finegrained abilities of LLMs in multi-turn dialogues. "
        }
    },
    {
        "id": "1136",
        "name": "IFEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "指令跟随",
                "en": "Instruct"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/google-research/google-research/tree/master/instruction_following_eval",
        "paperLink": "https://arxiv.org/pdf/2311.07911",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "581",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-14 20:15:14",
        "supportOnlineEval": false,
        "updateDate": "2024-10-14 20:15:14",
        "createDate": "2024-10-12 16:05:28",
        "desc": {
            "cn": "IFEval 是一个简单且易于复现的评估基准。它关注一组“可验证的指令”，例如“写超过 400 个单词”和“至少提到关键词 AI 3 次”。",
            "en": "IFEval is a straightforward and easy-to reproduce evaluation benchmark. It focuses on a set of “verifiable instructions” such as “write in more than 400 words” and “mention the keyword of AI at least 3 times”."
        }
    },
    {
        "id": "530",
        "name": "ReCoRD",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://sheng-z.github.io/ReCoRD-explorer/",
        "paperLink": "https://arxiv.org/abs/1905.00537",
        "officialWebsiteLink": "https://sheng-z.github.io/ReCoRD-explorer/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "563",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 11:35:25",
        "createDate": "2024-01-11 14:11:57",
        "desc": {
            "cn": "ReCoRD是一个阅读理解任务，要求根据给定的新闻文章和问题，从文章中抽取出答案。\n",
            "en": "ReCoRD is a reading comprehension task, which requires to extract the answer from the article based on the given news article and question."
        }
    },
    {
        "id": "1085",
        "name": "InfoBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "指令跟随",
                "en": "Instruct"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/qinyiwei/InfoBench",
        "paperLink": "https://aclanthology.org/2024.findings-acl.772.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50186103",
            "name": "Tencent-AI-Lab",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186103-0a763557-5cd0-4d44-b53a-aa07caf31eb7.png",
            "nickname": "Tencent-AI-Lab"
        },
        "lookNum": "532",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-29 16:24:48",
        "supportOnlineEval": false,
        "updateDate": "2024-09-29 16:24:48",
        "createDate": "2024-09-29 13:22:54",
        "desc": {
            "cn": "InfoBench 是一个指令追随评测基准，包含 500 条多样化的指令和 2,250 个分解问题，涵盖多个约束类别。",
            "en": "InfoBench is a benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories.\n"
        }
    },
    {
        "id": "1155",
        "name": "Ada-LEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "长文本",
                "en": "Long-Context"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
            }
        ],
        "benchCertificateLevel": 3,
        "githubLink": "https://github.com/open-compass/Ada-LEval",
        "paperLink": "https://aclanthology.org/2024.naacl-long.205.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "531",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-27 17:23:13",
        "supportOnlineEval": false,
        "updateDate": "2024-10-27 17:23:13",
        "createDate": "2024-10-15 16:58:09",
        "desc": {
            "cn": "Ada-LEval 用于评估大型语言模型（LLMs）对长上下文的理解能力。Ada-LEval 包含两个具有挑战性的子集，TSort 和 BestAnswer，能够更可靠地评估 LLMs 的长上下文能力。",
            "en": "Ada-LEval is a length-adaptable benchmark for evaluating the long-context understanding\nof LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable\na more reliable evaluation of LLMs’ long context capabilities."
        }
    },
    {
        "id": "522",
        "name": "EPRSTMT",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/CLUEbenchmark/FewCLUE",
        "paperLink": "https://arxiv.org/abs/2107.07498",
        "officialWebsiteLink": "https://github.com/CLUEbenchmark/FewCLUE",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "503",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:52:03",
        "createDate": "2024-01-11 14:11:26",
        "desc": {
            "cn": "EPRSTMT，也称作电子商务产品评论情感分析数据集，是一个基于电子商务平台上的产品评论的二元情感分析数据集。每个样本都被标记为积极或消极。该数据集由北京师范大学 ICIP 实验室收集。",
            "en": "(E-commerce Product Review Dataset for Sentiment Analysis), also known as EPRSTMT, is a binary sentiment analysis dataset based on product reviews on e-commerce platform. Each sample is labelled as Positive or Negative. It collect by ICIP Lab of Beijing Normal University."
        }
    },
    {
        "id": "527",
        "name": "AX-g",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/rudinger/winogender-schemas",
        "paperLink": "https://arxiv.org/abs/1905.00537",
        "officialWebsiteLink": "https://github.com/rudinger/winogender-schemas",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "482",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 11:37:55",
        "createDate": "2024-01-11 14:11:44",
        "desc": {
            "cn": "AX-g是一个Winogender诊断任务，要求根据给定的句子和代词，判断代词指代的是哪个名词。这个任务是从Winogender数据集中选取了一部分数据，主要用来测试模型在处理性别偏见和性别歧视方面的能力。",
            "en": "AX-g is a Winogender diagnostic task, which requires to determine which noun the pronoun refers to according to the given sentence and pronoun. This task is selected from a subset of the Winogender dataset, mainly used to test the model's ability in dealing with gender bias and discrimination."
        }
    },
    {
        "id": "528",
        "name": "RTE",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://tac.nist.gov//2011/RTE/index.html",
        "paperLink": "https://arxiv.org/abs/1905.00537",
        "officialWebsiteLink": "https://tac.nist.gov//2011/RTE/index.html",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "460",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 09:52:48",
        "createDate": "2024-01-11 14:11:48",
        "desc": {
            "cn": "RTE是一个自然语言推理任务，要求根据给定的句子对，判断它们之间的逻辑关系，有三种关系：蕴含、矛盾和中立。",
            "en": "RTE is a natural language inference task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral."
        }
    },
    {
        "id": "1096",
        "name": "TruthfulQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/sylinrl/TruthfulQA",
        "paperLink": "https://arxiv.org/pdf/2109.07958",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "460",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-30 15:32:49",
        "supportOnlineEval": false,
        "updateDate": "2024-09-30 15:32:49",
        "createDate": "2024-09-29 18:03:55",
        "desc": {
            "cn": "TruthfulQA 用于测量语言模型在回答问题时的真实度。该基准包含 817 个问题，涵盖 38 个类别，包括健康、法律、金融和政治。",
            "en": "TruthfulQA is  a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics."
        }
    },
    {
        "id": "526",
        "name": "AX-b",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://gluebenchmark.com/diagnostics",
        "paperLink": "https://arxiv.org/abs/1905.00537",
        "officialWebsiteLink": "https://gluebenchmark.com/diagnostics",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "423",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-02 11:38:03",
        "createDate": "2024-01-11 14:11:41",
        "desc": {
            "cn": "AX-b是一个广覆盖诊断任务，要求根据给定的句子对，判断它们之间的逻辑关系，有三种关系：蕴含、矛盾和中立。这个任务是从GLUE的广覆盖诊断数据集中选取了一部分数据，主要用来测试模型在语法、语义、世界知识等方面的理解能力。",
            "en": "AX-b is a broad-coverage diagnostic task, which requires to determine the logical relation between the given sentence pair, with three relations: entailment, contradiction and neutral. This task is selected from a subset of the GLUE broad-coverage diagnostic dataset, mainly used to test the model's understanding ability in grammar, semantics, world knowledge and so on."
        }
    },
    {
        "id": "1074",
        "name": "NewsBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "创作",
                "en": "Creation"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/IAAR-Shanghai/NewsBench",
        "paperLink": "https://arxiv.org/pdf/2403.00862",
        "officialWebsiteLink": "https://iaar-shanghai.github.io/NewsBench/#/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50187574",
            "name": "IAAR-Shanghai",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50187574-4cf5b8da-55c3-4c75-aa9b-9c2d053b5b6d.png",
            "nickname": "IAAR-Shanghai"
        },
        "lookNum": "408",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-27 17:50:27",
        "supportOnlineEval": false,
        "updateDate": "2024-09-27 17:50:27",
        "createDate": "2024-09-27 13:08:42",
        "desc": {
            "cn": "NewsBench 是一个新颖的评估框架，旨在系统性地评估大型语言模型在中文新闻编辑能力上的表现。构建的基准数据集聚焦于写作能力的四个方面和安全遵循的六个方面，包含 1,267 个手动精心设计的测试样本，类型包括选择题和简答题，涵盖 24 个新闻领域的五项编辑任务。",
            "en": "NewsBench is a novel evaluation framework to systematically assess the capabilities of Large Language Models (LLMs) for editorial capabilities in Chinese journalism. "
        }
    },
    {
        "id": "1108",
        "name": "HotpotQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/hotpotqa/hotpot",
        "paperLink": "https://arxiv.org/pdf/1809.09600",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "402",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-10 18:36:24",
        "supportOnlineEval": false,
        "updateDate": "2025-01-10 18:36:24",
        "createDate": "2025-01-10 18:28:24",
        "desc": {
            "cn": "HotpotQA 用于评估大语言模型的推理能力，包含 113,000 个基于维基百科的问题和答案。",
            "en": "HOTPOTQA is a dataset with 113k Wikipedia-based question-answer pairs."
        }
    },
    {
        "id": "1121",
        "name": "HaluEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/RUCAIBox/HaluEval",
        "paperLink": "https://arxiv.org/pdf/2305.11747",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "395",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-13 15:31:34",
        "supportOnlineEval": false,
        "updateDate": "2025-03-13 15:31:34",
        "createDate": "2025-02-13 18:15:01",
        "desc": {
            "cn": "HaluEval用于评估大语言模型识别幻觉的能力，包含 5,000 条普通用户查询及 ChatGPT 的回答，以及来自三个任务的 30,000 个特定任务示例，即问答、基于知识的对话和文本摘要。",
            "en": "HaluEval evaluates the performance of LLMs in recognizing hallucination. It includes 5,000 general user queries with ChatGPT responses and 30,000 task-specific examples from three tasks, i.e., question answering, knowledge-grounded dialogue, and text summarization."
        }
    },
    {
        "id": "568",
        "name": "CriticBench",
        "emoji": "😂",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [
            {
                "cn": "critique",
                "en": "critique"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/open-compass/CriticBench",
        "paperLink": "https://arxiv.org/abs/2402.13764",
        "officialWebsiteLink": "https://open-compass.github.io/CriticBench",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "40025311",
            "name": "Tian-Lan",
            "avatar": null,
            "nickname": "Tian-Lan"
        },
        "lookNum": "376",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-02-23 19:41:43",
        "createDate": "2024-02-23 18:01:59",
        "desc": {
            "cn": "CriticBench是一个新颖的基准，旨在全面可靠地评估LLM的四个关键批判能力维度。CriticBench包括九项不同的任务，每项任务都评估LLM在不同质量粒度水平上对响应进行批评的能力。",
            "en": "CriticBench, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. CriticBench encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity."
        }
    },
    {
        "id": "930",
        "name": "MR-Ben-Meta-Reasoning-Benchmark",
        "emoji": "🗽",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [
            {
                "cn": "long-context",
                "en": "long-context"
            },
            {
                "cn": "understanding",
                "en": "understanding"
            },
            {
                "cn": "knowledge",
                "en": "knowledge"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/dvlab-research/Mr-Ben",
        "paperLink": "https://arxiv.org/abs/2406.13975",
        "officialWebsiteLink": "https://randolph-zeng.github.io/Mr-Ben.github.io/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50104209",
            "name": null,
            "avatar": null,
            "nickname": "OpenXLab-lFJ83HfdP"
        },
        "lookNum": "368",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-07-12 13:38:55",
        "createDate": "2024-07-12 11:20:05",
        "desc": {
            "cn": "本工作联合MIT,清华,剑桥等知名院校, 提出了一个评测大语言模型对复杂问题的推理过程的“阅卷”批改能力的评测数据集，有别于以前的以结果匹配为评测模式的数据集MR-Ben，我们的数据集基于GSM8K[1], MMLU[2], LogiQA[3], MHPP[4]等数据集经由细致的高水平人工标注构建而成，显著地增加了难度及区分度。我们细致地分析了包括claude3.5, GPT4-Turbo, Kimi, Zhipu, Yi-Large, Qwen2, DeepseekCoderv2 等国内外一线的大语言模型，发现开源的模型在复杂推理的场景下有望追上顶尖的闭源模型。该评测数据集的所有数据均已开源，并且支持一键评测。欢迎所有做大模型训练的小伙伴向我们分享你的评测结果，我们会及时更新榜单。",
            "en": "Welcome to the dataset page for the Meta-Reasoning Benchmark associated with our recent publication `Mr-Ben: A Comprehensive Meta-Reasoning Benchmark for Large Language Models`. We have provided a demo evaluate script for you to try out benchmark in mere two steps. We encourage everyone to try out our benchmark in the SOTA models and return its results to us. We would be happy to include it in the eval_results and update the evaluation tables below for you."
        }
    },
    {
        "id": "1075",
        "name": "AlignBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/THUDM/AlignBench",
        "paperLink": "https://aclanthology.org/2024.acl-long.624.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "043910",
            "name": "THUDM",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/043910-ad5eb16e-0c3f-4e1b-b1d6-e9dc8ab355a7.png",
            "nickname": "智谱.AI"
        },
        "lookNum": "355",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-11-01 13:39:39",
        "supportOnlineEval": false,
        "updateDate": "2024-11-01 13:39:39",
        "createDate": "2024-10-29 17:11:05",
        "desc": {
            "cn": "AlignBench 是一个用于评估中文大语言模型对齐性能的全面、多维度的评测基准。AlignBench 构建了人类参与的数据构建流程，来保证评测数据的动态更新。AlignBench 采用多维度、规则校准的模型评价方法（LLM-as-Judge），并且结合思维链（Chain-of-Thought）生成对模型回复的多维度分析和最终的综合评分，增强了评测的高可靠性和可解释性。",
            "en": "ALIGNBENCH is a comprehensive multidimensional benchmark for evaluating LLMs’ alignment in Chinese. We tailor a humanin-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references."
        }
    },
    {
        "id": "1089",
        "name": "MathBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 3,
        "githubLink": "https://github.com/open-compass/MathBench",
        "paperLink": "https://aclanthology.org/2024.findings-acl.411.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "355",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-30 15:32:54",
        "supportOnlineEval": false,
        "updateDate": "2024-09-30 15:32:54",
        "createDate": "2024-09-29 15:21:55",
        "desc": {
            "cn": "MathBench 严格评估大型语言模型的数学能力。MathBench 涉及广泛的数学学科，提供对理论理解和实际问题解决技能的详细评估。",
            "en": "MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large\nlanguage models. MathBench spans a wide range of mathematical disciplines, offering a\ndetailed evaluation of both theoretical understanding and practical problem-solving skills."
        }
    },
    {
        "id": "1077",
        "name": "SALAD-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/OpenSafetyLab/SALAD-BENCH",
        "paperLink": "https://aclanthology.org/2024.findings-acl.235.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50186740",
            "name": "OpenSafetyLab",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186740-5e27e10c-3894-4bfd-b0c9-2079f251882d.png",
            "nickname": "OpenSafetyLab"
        },
        "lookNum": "331",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-29 16:25:08",
        "supportOnlineEval": false,
        "updateDate": "2024-09-29 16:25:08",
        "createDate": "2024-09-27 16:14:48",
        "desc": {
            "cn": "SALAD-Bench 是一个专门用于评估大型语言模型（LLMs）、攻击和防御方法的安全基准。SALAD-Bench 的特点在于其广泛性，超越了传统基准，具有大规模、丰富的多样性、复杂的三层分类法以及多功能性。",
            "en": "SALAD-Bench, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy\nspanning three levels, and versatile functionalities."
        }
    },
    {
        "id": "1135",
        "name": "GPQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/idavidrein/gpqa",
        "paperLink": "https://arxiv.org/pdf/2311.12022",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "330",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-14 20:26:40",
        "supportOnlineEval": false,
        "updateDate": "2024-10-14 20:26:40",
        "createDate": "2024-10-12 16:00:16",
        "desc": {
            "cn": "GPQA 包含 448 道由生物学、物理学和化学领域专家撰写的多项选择题。",
            "en": "GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry."
        }
    },
    {
        "id": "1080",
        "name": "E-EVAL",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/AI-EDU-LAB/E-EVAL",
        "paperLink": "https://aclanthology.org/2024.findings-acl.462.pdf",
        "officialWebsiteLink": "https://eevalbenchmark.com/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50186339",
            "name": "AI-EDU-LAB",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186339-e3a36811-6e48-440b-a243-b7d59c6d4bd7.png",
            "nickname": "AI-EDU-LAB"
        },
        "lookNum": "314",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-29 16:25:01",
        "supportOnlineEval": false,
        "updateDate": "2024-09-29 16:25:01",
        "createDate": "2024-09-27 18:25:46",
        "desc": {
            "cn": "E-EVAL 是首个专门针对中国 K-12 教育的综合评估基准。E-EVAL 包含 4,351 道选择题，涵盖小学、初中和高中各个年级，涉及多种学科。",
            "en": "E-EVAL is the first comprehensive evaluation benchmark specifically tailored for Chinese K-12 education. E-EVAL comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels, covering a diverse array of subjects.\n"
        }
    },
    {
        "id": "1276",
        "name": "MMLU-Pro",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/TIGER-AI-Lab/MMLU-Pro",
        "paperLink": "https://arxiv.org/abs/2406.01574",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "312",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-25 14:44:23",
        "supportOnlineEval": false,
        "updateDate": "2024-12-25 14:44:23",
        "createDate": "2024-12-24 19:48:37",
        "desc": {
            "cn": "MMLU-Pro是MMLU的扩展版本，涵盖了更具挑战性、以推理为重点的问题，并将选择集从4个选项扩展到10个选项。",
            "en": "MMLU-Pro is the extension of MMLU, integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options."
        }
    },
    {
        "id": "1128",
        "name": "Gorilla",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/ShishirPatil/gorilla",
        "paperLink": "https://arxiv.org/pdf/2305.15334",
        "officialWebsiteLink": "https://gorilla.cs.berkeley.edu/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "306",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-16 11:03:02",
        "supportOnlineEval": false,
        "updateDate": "2024-10-16 11:03:02",
        "createDate": "2024-10-11 15:12:19",
        "desc": {
            "cn": "Gorilla 使大语言模型能够通过调用 API 使用工具。针对自然语言查询，Gorilla 能够生成语义和语法上正确的 API 调用。",
            "en": "Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. "
        }
    },
    {
        "id": "1141",
        "name": "CHARM",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/opendatalab/CHARM",
        "paperLink": "https://aclanthology.org/2024.acl-long.604.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "300",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-19 18:04:11",
        "supportOnlineEval": false,
        "updateDate": "2024-10-19 18:04:11",
        "createDate": "2024-10-14 14:20:06",
        "desc": {
            "cn": "CHARM 是首个全面深入评估大语言模型（LLMs）在中文中的常识推理能力的基准，涵盖了全球通用的常识和特定于中国的常识。",
            "en": "CHARM is the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense."
        }
    },
    {
        "id": "1219",
        "name": "CS-Eval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            },
            {
                "cn": "知识",
                "en": "Knowledge"
            },
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/CS-EVAL/CS-Eval",
        "paperLink": "https://arxiv.org/pdf/2411.16239",
        "officialWebsiteLink": "https://cs-eval.com",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "41601314",
            "name": null,
            "avatar": null,
            "nickname": "OpenXLab-JdyYs9Oa3"
        },
        "lookNum": "278",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-11-27 16:53:44",
        "supportOnlineEval": false,
        "updateDate": "2024-11-27 16:53:44",
        "createDate": "2024-11-26 10:42:49",
        "desc": {
            "cn": "CS-Eval 是由阿里安全、复旦大学和中国科学院大学联合建立的大模型网络安全能力评测集。数据集覆盖11个网络安全大类领域、42个子类领域，提供知识型和实战型的综合评估任务，支持用户自主评测，同时为大模型落地网络安全提供参考和启发。\n",
            "en": "CS-Eval is a large language model cybersecurity capability evaluation suite jointly established by Alibaba Security, Fudan University, and the University of Chinese Academy of Sciences. The dataset encompasses 11 major cybersecurity categories and 42 subcategories, offering comprehensive assessment "
        }
    },
    {
        "id": "1070",
        "name": "OlympiadBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/OpenBMB/OlympiadBench",
        "paperLink": "https://arxiv.org/pdf/2402.14008",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "044167",
            "name": "OpenBMB",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/044167-87da8649-04d0-47ea-90e1-a77a1d2e9585.png",
            "nickname": "OpenBMB"
        },
        "lookNum": "272",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-27 16:53:44",
        "supportOnlineEval": false,
        "updateDate": "2024-09-27 16:53:44",
        "createDate": "2024-09-26 17:51:57",
        "desc": {
            "cn": "OlympiadBench 是一个奥林匹克级别的双语多模态科学基准，包含来自奥林匹克级数学和物理竞赛的8,476道题目，包括中国高考。每道题目都配有专家级别的注释，提供逐步推理的详细说明。",
            "en": "OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring\n8,476 problems from Olympiad-level mathematics and physics competitions, including the\nChinese college entrance exam. Each problem is detailed with expert-level annotations\nfor step-by-step reasoning. "
        }
    },
    {
        "id": "1351",
        "name": "AgentHarm",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            },
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/UKGovernmentBEIS/inspect_evals/tree/main/src/inspect_evals/agentharm",
        "paperLink": "https://arxiv.org/abs/2410.09024",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "262",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:23:28",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:23:28",
        "createDate": "2025-01-09 16:22:26",
        "desc": {
            "cn": "AgentHarm用于评估LLM智能体对越狱攻击的鲁棒性，包括110套恶意智能体任务（其中有440个强化任务），涵盖欺诈、网络犯罪和骚扰等11个危害类别。",
            "en": "AgentHarm tests the robustness of LLMs to jailbreak attacks. It includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment."
        }
    },
    {
        "id": "1538",
        "name": "SuperGPQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/SuperGPQA/SuperGPQA/",
        "paperLink": "https://arxiv.org/abs/2502.14739",
        "officialWebsiteLink": "https://supergpqa.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "255",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-02-26 18:52:28",
        "supportOnlineEval": false,
        "updateDate": "2025-02-26 18:52:28",
        "createDate": "2025-02-26 09:51:42",
        "desc": {
            "cn": "SuperGPQA，这是一个旨在评估大型语言模型在 285 个研究生学科领域的知识和推理能力的全面基准。SuperGPQA 每个学科至少包含 50 个问题，涵盖广泛的硕士研究生学科主题。",
            "en": "SuperGPQA, a comprehensive benchmark designed to evaluate the knowledge and reasoning abilities of Large Language Models (LLMs) across 285 graduate-level disciplines. SuperGPQA features at least 50 questions per discipline, covering a broad spectrum of graduate-level topics."
        }
    },
    {
        "id": "1091",
        "name": "RoleLLM",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/InteractiveNLP-Team/RoleLLM-public",
        "paperLink": "https://aclanthology.org/2024.findings-acl.878.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "50187516",
            "name": "InteractiveNLP-Team",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50187516-87c7afab-16ed-48c4-8147-1298485c7231.png",
            "nickname": "InteractiveNLP-Team"
        },
        "lookNum": "255",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-30 15:54:49",
        "supportOnlineEval": false,
        "updateDate": "2024-09-30 15:54:49",
        "createDate": "2024-09-29 16:37:55",
        "desc": {
            "cn": "RoleLLM 是一个角色扮演的数据构建和评估框架，同时提供闭源和开源模型的解决方案（RoleGPT、RoleLLaMA、RoleGLM）。",
            "en": "RoleLLM is a role-playing framework of data construction and evaluation (RoleBench), as well as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMA, RoleGLM). We also propose Context-Instruct for long-text knowledge extraction and role-specific knowledge injection."
        }
    },
    {
        "id": "1137",
        "name": "CMB",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/FreedomIntelligence/CMB",
        "paperLink": "https://arxiv.org/pdf/2308.08833",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "251",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-16 11:02:56",
        "supportOnlineEval": false,
        "updateDate": "2024-10-16 11:02:56",
        "createDate": "2024-10-15 16:31:07",
        "desc": {
            "cn": "CMB 是一个综合医学基准，专为中文而设计，并完全依赖于本土的中文语言和文化框架中。",
            "en": "CMB is Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework."
        }
    },
    {
        "id": "1082",
        "name": "StudentEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Wellesley-EASEL-lab/StudentEval",
        "paperLink": "https://aclanthology.org/2024.findings-acl.501.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50186257",
            "name": "Wellesley-EASEL-lab",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186257-118d14f5-a2d6-40c7-b3c5-712031ca526e.png",
            "nickname": "Wellesley-EASEL-lab"
        },
        "lookNum": "249",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-29 16:24:55",
        "supportOnlineEval": false,
        "updateDate": "2024-09-29 16:24:55",
        "createDate": "2024-09-29 11:16:00",
        "desc": {
            "cn": "StudentEval 包含 1,749 个由 80 名仅完成一门入门 Python 课程的学生撰写的提示。StudentEval 中包含许多非专家提示，描述相同的问题，使得探索提示成功的关键因素成为可能。",
            "en": "STUDENTEVAL contains 1,749 prompts written by 80 students who have only completed one introductory Python course. STUDENTEVAL contains numerous non-expert prompts describing the same problem, enabling exploration of key factors in prompt success. "
        }
    },
    {
        "id": "1244",
        "name": "Omni-MATH",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            },
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/KbsdJames/Omni-MATH",
        "paperLink": "https://arxiv.org/abs/2410.07985",
        "officialWebsiteLink": "https://omni-math.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "248",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-30 16:29:59",
        "supportOnlineEval": false,
        "updateDate": "2024-12-30 16:29:59",
        "createDate": "2024-12-30 16:20:58",
        "desc": {
            "cn": "Omni-MATH用于评估LLM在奥林匹克水平上的数学推理能力，包括4428道竞赛级问题，并带有严格的人工注释。这些问题被精心分类为超过33个子领域，涵盖10多个不同的难度级别。",
            "en": "Omni-MATH focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels"
        }
    },
    {
        "id": "1081",
        "name": "NaturalCodeBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/THUDM/NaturalCodeBench",
        "paperLink": "https://aclanthology.org/2024.findings-acl.471.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "043910",
            "name": "THUDM",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/043910-ad5eb16e-0c3f-4e1b-b1d6-e9dc8ab355a7.png",
            "nickname": "智谱.AI"
        },
        "lookNum": "244",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-29 16:24:58",
        "supportOnlineEval": false,
        "updateDate": "2024-09-29 16:24:58",
        "createDate": "2024-09-29 10:05:45",
        "desc": {
            "cn": "NaturalCodeBench 是一个具有挑战性的代码基准，旨在反映真实编码任务中的复杂性和多样性。NaturalCodeBench 包含 402 个高质量的 Python 和 Java 问题，这些问题是从在线编码服务的自然用户查询中精心挑选的，涵盖了 6 个不同的领域。",
            "en": "NaturalCodeBench is challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks. NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains."
        }
    },
    {
        "id": "1083",
        "name": "GAOKAO-MM",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/OpenMOSS/GAOKAO-MM",
        "paperLink": "https://aclanthology.org/2024.findings-acl.521.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50175431",
            "name": "OpenMOSS",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50175431-71e98faf-7e15-46ce-8c84-56eb373cf63a.png",
            "nickname": "OpenMOSS"
        },
        "lookNum": "240",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-29 16:24:53",
        "supportOnlineEval": false,
        "updateDate": "2024-09-29 16:24:53",
        "createDate": "2024-09-29 11:48:20",
        "desc": {
            "cn": "GAOKAO-MM 是一个基于中国高考的多模态基准，包含 8 个科目和 12 种图像类型，例如图表、函数图、地图和照片。GAOKAO-MM 源自本土中文语境，并对模型的能力设置了人类水平的要求，包括感知、理解、知识和推理。",
            "en": "GAOKAO-MM is a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. "
        }
    },
    {
        "id": "1115",
        "name": "MathQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://math-qa.github.io/math-QA/",
        "paperLink": "https://arxiv.org/pdf/1905.13319v1",
        "officialWebsiteLink": "https://math-qa.github.io/math-QA/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "239",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-11 19:59:27",
        "supportOnlineEval": false,
        "updateDate": "2024-10-11 19:59:27",
        "createDate": "2024-10-10 14:14:12",
        "desc": {
            "cn": "MathQA 是一个大规模、多样化的数据集，包含 37,000 道英语多项选择数学文字问题，涵盖多个数学领域类别。",
            "en": "MathQA is a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs\ncorresponding to word problems in the AQuA dataset."
        }
    },
    {
        "id": "1079",
        "name": "CFLUE",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/aliyun/cflue",
        "paperLink": "https://aclanthology.org/2024.findings-acl.337.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50186956",
            "name": "Alibaba_Cloud",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186956-08003a88-f404-4b02-a282-c0c377547acd.png",
            "nickname": "Alibaba_Cloud"
        },
        "lookNum": "237",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-29 16:25:03",
        "supportOnlineEval": false,
        "updateDate": "2024-09-29 16:25:03",
        "createDate": "2024-09-27 17:54:49",
        "desc": {
            "cn": "CFLUE 是中国金融语言理解评估基准，旨在评估大型语言模型（LLMs）在各个维度上的能力。具体而言，CFLUE 提供了针对知识评估和应用评估量身定制的数据集。在知识评估方面，它包含超过 38,000 道选择题及相关的解决方案解释。",
            "en": "CFLUE is the Chinese Financial Language Understanding Evaluation benchmark, designed to assess the capability of LLMs across various dimensions. "
        }
    },
    {
        "id": "1093",
        "name": "APPS",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/hendrycks/apps",
        "paperLink": "https://arxiv.org/pdf/2105.09938",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "237",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-30 15:32:52",
        "supportOnlineEval": false,
        "updateDate": "2024-09-30 15:32:52",
        "createDate": "2024-09-29 17:01:36",
        "desc": {
            "cn": "APPS 是一个代码生成评测基准，该评测基准测量模型根据任意自然语言规范生成令人满意的 Python 代码的能力。",
            "en": "APPS is a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code."
        }
    },
    {
        "id": "1134",
        "name": "TheoremQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/TIGER-AI-Lab/TheoremQA",
        "paperLink": "https://arxiv.org/pdf/2305.12524",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "237",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-14 20:15:11",
        "supportOnlineEval": false,
        "updateDate": "2024-10-14 20:15:11",
        "createDate": "2024-10-12 13:48:08",
        "desc": {
            "cn": "TheoremQA 是第一个基于定理的问题回答数据集，旨在评估 AI 模型应用定理解决复杂科学问题的能力。该数据集由领域专家精心策划，包含 800 个高质量问题，涵盖来自数学、物理、电气与计算机科学以及金融的 350 个定理。",
            "en": "TheoremQA is the first theorem-driven question-answering dataset designed to evaluate AI models’ capabilities to apply theorems to solve challenging science problems. It is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance."
        }
    },
    {
        "id": "1207",
        "name": "MMBench-Video",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 3,
        "githubLink": "https://github.com/open-compass/VLMEvalKit",
        "paperLink": "https://arxiv.org/abs/2406.14515",
        "officialWebsiteLink": "https://mmbench-video.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "237",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 17:04:03",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 17:04:03",
        "createDate": "2025-01-21 14:14:14",
        "desc": {
            "cn": "MMBench-Video是全面视频理解评测基准，覆盖长视频、多镜头，评估MLLMs时序理解能力。包含16类共600+视频以及人工标注问答对。",
            "en": "MMBench-Video is a comprehensive video understanding evaluation benchmark that covers long videos, multiple shots, and evaluates the temporal understanding ability of MLLMs. Contains over 600 videos, 16 categories, and manually annotated Q&A pairs."
        }
    },
    {
        "id": "1109",
        "name": "WinoGrande",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/allenai/winogrande",
        "paperLink": "https://arxiv.org/pdf/1907.10641",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "236",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-09 20:03:23",
        "supportOnlineEval": true,
        "updateDate": "2024-10-09 20:03:23",
        "createDate": "2024-10-09 18:41:18",
        "desc": {
            "cn": "WINOGRANDE 包含 44,000 个问题，受到 WSC 设计的启发，但进行了调整，以提高数据集的规模和难度。",
            "en": "WINOGRANDE is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset."
        }
    },
    {
        "id": "1124",
        "name": "RealToxicityPrompts",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/allenai/real-toxicity-prompts",
        "paperLink": "https://aclanthology.org/2020.findings-emnlp.301.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "236",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-11 20:24:44",
        "supportOnlineEval": false,
        "updateDate": "2024-10-11 20:24:44",
        "createDate": "2024-10-11 13:38:08",
        "desc": {
            "cn": "RealToxicityPrompts 是一个包含 100,000 个自然出现的、句子级提示的数据集，这些提示来自于大量的英语网络文本，并配有来自广泛使用的毒性分类器的毒性评分。",
            "en": "RealToxicityPrompts is a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely used toxicity classiﬁer. "
        }
    },
    {
        "id": "1287",
        "name": "MedBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Medical",
                "en": "Medical"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/open-compass/opencompass/tree/main/opencompass/datasets/medbench/",
        "paperLink": "https://www.sciopen.com/article/10.26599/BDMA.2024.9020044",
        "officialWebsiteLink": "https://medbench.opencompass.org.cn/home",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "235",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-27 14:45:32",
        "supportOnlineEval": false,
        "updateDate": "2024-12-27 14:45:32",
        "createDate": "2024-12-27 14:01:19",
        "desc": {
            "cn": "MedBench致力于打造一个科学、公平且严谨的中文医疗大模型评测体系及开放平台。我们基于医学权威标准，不断更新维护高质量的医学数据集，全方位多维度量化模型在各个医学维度的能力。",
            "en": "MedBench is committed to building a scientific, fair and rigorous Chinese medical model evaluation system and open platform. Based on authoritative standards, we constantly update and maintain high-quality datasets, and comprehensively quantify capabilities of models in various medical dimensions."
        }
    },
    {
        "id": "1245",
        "name": "KOR-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/KOR-Bench/KOR-Bench",
        "paperLink": "https://arxiv.org/abs/2410.06526",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "234",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 11:12:43",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 11:12:43",
        "createDate": "2025-01-10 19:42:40",
        "desc": {
            "cn": "KOR-Bench用于评估大语言模型的推理能力，包括五个任务类别：操作、逻辑、密码、拼图和反事实。",
            "en": "Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench) encompasses five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual. KOR-Bench emphasizes the effectiveness of models in applying new rule descriptions to solve novel rule-driven questions. "
        }
    },
    {
        "id": "1088",
        "name": "UHGEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/IAAR-Shanghai/UHGEval",
        "paperLink": "https://arxiv.org/pdf/2311.15296",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "230",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-10 10:34:42",
        "supportOnlineEval": false,
        "updateDate": "2024-10-10 10:34:42",
        "createDate": "2024-10-09 20:11:08",
        "desc": {
            "cn": "UHGEval 基准，包含由限制条件最小的大语言模型（LLMs）生成的幻觉。",
            "en": "UHGEval(Unconstrained Hallucination Generation Evaluation) benchmark contains hallucinations generated by LLMs with minimal restrictions."
        }
    },
    {
        "id": "1328",
        "name": "GTA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/open-compass/GTA",
        "paperLink": "https://arxiv.org/abs/2407.08713",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "223",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-06 16:32:41",
        "supportOnlineEval": false,
        "updateDate": "2025-01-06 16:32:41",
        "createDate": "2025-01-03 11:35:50",
        "desc": {
            "cn": "GTA用于评估LLM调用工具解决实际问题的能力，由229个真实任务和可执行工具链组成。",
            "en": "GTA is meant for LLM's tool-use evaluations under real-world scenarios, including 229 real-world tasks and executable tool chains."
        }
    },
    {
        "id": "1078",
        "name": "DebugBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/thunlp/DebugBench",
        "paperLink": "https://aclanthology.org/2024.findings-acl.247.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50188779",
            "name": "THUNLP",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50188779-1d9e7f11-f6dd-4c37-a464-f75de27aa50a.png",
            "nickname": "THUNLP"
        },
        "lookNum": "222",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-29 16:25:05",
        "supportOnlineEval": false,
        "updateDate": "2024-09-29 16:25:05",
        "createDate": "2024-09-27 17:44:25",
        "desc": {
            "cn": "DebugBench 是一个包含 4,253 个实例的 LLM 调试基准。它涵盖了 C++、Java 和 Python 中的四个主要错误类别和 18 种次要类型。",
            "en": "DebugBench is an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. "
        }
    },
    {
        "id": "1133",
        "name": "Spider",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/taoyds/spider/tree/master/evaluation_examples",
        "paperLink": "https://arxiv.org/pdf/1809.08887v5",
        "officialWebsiteLink": "https://yale-lily.github.io/spider",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "221",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-16 11:03:12",
        "supportOnlineEval": false,
        "updateDate": "2024-10-16 11:03:12",
        "createDate": "2024-10-12 13:37:46",
        "desc": {
            "cn": "Spider 是一个大规模、复杂且跨领域的语义解析和文本到 SQL 数据集。Spider 挑战的目标是开发自然语言接口以访问跨领域数据库。该数据集包含 10,181 个问题和 5,693 个独特的复杂 SQL 查询，涵盖 200 个包含多个表的数据库，涉及 138 个不同的领域。",
            "en": "Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. "
        }
    },
    {
        "id": "1100",
        "name": "ScienceQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://scienceqa.github.io/",
        "paperLink": "https://lupantech.github.io/papers/neurips22_scienceqa.pdf",
        "officialWebsiteLink": "https://scienceqa.github.io/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "219",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-09 20:03:37",
        "supportOnlineEval": false,
        "updateDate": "2024-10-09 20:03:37",
        "createDate": "2024-10-09 14:29:43",
        "desc": {
            "cn": "SCIENCEQA 包含约 21,000 道多模态选择题，涵盖多种科学主题，并附有相应的讲座和解释的答案注释。",
            "en": "SCIENCEQA is a new benchmark that consists of ∼21k multimodal multiple choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations."
        }
    },
    {
        "id": "1086",
        "name": "Belebele",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/facebookresearch/belebele",
        "paperLink": "https://arxiv.org/pdf/2308.16884",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "211",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-30 15:33:02",
        "supportOnlineEval": false,
        "updateDate": "2024-09-30 15:33:02",
        "createDate": "2024-09-29 13:42:49",
        "desc": {
            "cn": "BELEBELE 是一个多项选择机器阅读理解（MRC）数据集，涵盖 122 种语言变体。该数据集显著扩展了自然语言理解（NLU）基准的语言覆盖范围，使得可以在高、中、低资源语言中评估文本模型。",
            "en": "BELEBELE, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in\nhigh-, medium-, and low-resource languages."
        }
    },
    {
        "id": "1123",
        "name": "Crows-Pairs",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/nyu-mll/crows-pairs",
        "paperLink": "https://arxiv.org/pdf/2010.00133v1",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "210",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-11 20:24:47",
        "supportOnlineEval": false,
        "updateDate": "2024-10-11 20:24:47",
        "createDate": "2024-10-11 13:33:41",
        "desc": {
            "cn": "CrowS-Pairs 包含 1508 个示例，涵盖与九种偏见类型相关的刻板印象，例如种族、宗教和年龄。在 CrowS-Pairs 中，模型会接收到两句话：一句是更具刻板印象的，另一句则是较少刻板印象的。",
            "en": "CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping."
        }
    },
    {
        "id": "1087",
        "name": "Reveal",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://reveal-dataset.github.io/",
        "paperLink": "https://arxiv.org/pdf/2402.00559",
        "officialWebsiteLink": "https://reveal-dataset.github.io/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "203",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-30 15:32:59",
        "supportOnlineEval": false,
        "updateDate": "2024-09-30 15:32:59",
        "createDate": "2024-09-29 14:26:40",
        "desc": {
            "cn": "Reveal 是一个用于基准测试开放域问答环境中复杂链式推理自动验证器的新数据集。Reveal 包含关于语言模型答案中每个推理步骤的相关性、证据段落的归因和逻辑正确性的全面标签，涵盖多种数据集和最先进的语言模型。",
            "en": "REVEAL(ReasoningVerification Evaluation) is a new dataset to benchmark automatic verifiers of complex Chain-ofThought reasoning in open-domain question answering settings. "
        }
    },
    {
        "id": "1069",
        "name": "AIR-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/OFA-Sys/AIR-Bench",
        "paperLink": "https://arxiv.org/pdf/2402.07729",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50186884",
            "name": "OFA-Sys",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186884-c56d3538-0c00-4d32-86ed-c28d0817e429.jfif",
            "nickname": "OFA-Sys"
        },
        "lookNum": "199",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-27 16:53:47",
        "supportOnlineEval": false,
        "updateDate": "2024-09-27 16:53:47",
        "createDate": "2024-09-26 16:07:49",
        "desc": {
            "cn": "AIR-Bench 是第一个旨在评估 LALMs 理解各种音频信号（包括人类语言、自然声音和音乐）能力的基准，并进一步评估其以文本形式与人类互动的能力。AIR-Bench 包括两个维度：基础基准和聊天基准。前者由19个任务组成，包含约19,000个单选题，旨在检查LALMs的基本单任务能力。后者包含2,000个开放式问答数据实例，直接评估模型对复杂音频的理解及其遵循指令的能力。",
            "en": "AIR-Bench is the first benchmark designed to evaluate the\nability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format."
        }
    },
    {
        "id": "1148",
        "name": "AbsPyramid",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/HKUST-KnowComp/AbsPyramid",
        "paperLink": "https://aclanthology.org/2024.findings-naacl.252.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50109576",
            "name": "HKUST-KnowComp",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50109576-bd7795e6-da4b-40f0-a4e6-979ddb3230c2.png",
            "nickname": "OpenXLab-dJloo3kUv"
        },
        "lookNum": "199",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-27 17:22:48",
        "supportOnlineEval": false,
        "updateDate": "2024-10-27 17:22:48",
        "createDate": "2024-10-15 14:04:22",
        "desc": {
            "cn": "ABSPYRAMID 是包含 221,000 条文本描述的抽象知识,收集了多种事件的三个组成部分的抽象知识，以全面评估语言模型在开放域中的抽象能力。",
            "en": "ABSPYRAMID is a unified entailment graph of 221K textual descriptions of abstraction knowledge. ABSPYRAMID collects abstract knowledge for three components\nof diverse events to comprehensively evaluate the abstraction ability of language\nmodels in the open domain."
        }
    },
    {
        "id": "1164",
        "name": "TaskBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/microsoft/JARVIS/tree/main/taskbench",
        "paperLink": "https://arxiv.org/pdf/2311.18760",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "197",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-06 16:32:37",
        "supportOnlineEval": false,
        "updateDate": "2025-01-06 16:32:37",
        "createDate": "2024-10-18 15:47:56",
        "desc": {
            "cn": "TaskBench旨在评估LLM在任务自动化方面的能力，包含面向任务分解、工具调用和参数预测三个关键阶段的28271个样本。",
            "en": "TaskBench aims to evaluate the capability of LLMs in task automation, containing 28,271 samples spanning 3 critical stages: task decomposition, tool invocation, and parameter prediction."
        }
    },
    {
        "id": "1266",
        "name": "BABILong",
        "emoji": "",
        "dimensions": [
            {
                "cn": "长文本",
                "en": "Long-Context"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/booydar/babilong",
        "paperLink": "https://arxiv.org/abs/2406.10149",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "193",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-25 14:14:54",
        "supportOnlineEval": false,
        "updateDate": "2024-12-25 14:14:54",
        "createDate": "2024-12-25 14:14:32",
        "desc": {
            "cn": "BABILong旨在测试语言模型对分布在极长文档中的事实进行推理的能力，涵盖事实链接、简单归纳、推导、计数和处理列表/集合等20种各类推理任务。",
            "en": "BABILong is designed to test language models' ability to reason across facts distributed in extremely long documents. It contains a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. "
        }
    },
    {
        "id": "1248",
        "name": "MMMU",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Multimodal",
                "en": "Multimodal"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/MMMU-Benchmark/MMMU",
        "paperLink": "https://arxiv.org/abs/2311.16502",
        "officialWebsiteLink": "https://mmmu-benchmark.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "189",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-22 18:09:32",
        "supportOnlineEval": false,
        "updateDate": "2024-12-22 18:09:32",
        "createDate": "2024-12-20 19:45:07",
        "desc": {
            "cn": "MMMU用于评估多模态大模型在复杂多学科任务中的表现，包括从大学考试和教科书中精心收集的11.5K多模态问题，涵盖六个核心学科：艺术与设计、商业、科学、健康与医学、人文与社会科学以及技术与工程。",
            "en": "MMMU is a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks."
        }
    },
    {
        "id": "1129",
        "name": "WikiSQL",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/salesforce/WikiSQL",
        "paperLink": "https://arxiv.org/pdf/1709.00103v7",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "187",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-16 11:03:06",
        "supportOnlineEval": false,
        "updateDate": "2024-10-16 11:03:06",
        "createDate": "2024-10-11 15:27:39",
        "desc": {
            "cn": "WikiSQL 是一个包含 80,654 个手动标注示例的问题和 SQL 查询的数据集，分布在来自维基百科的 24,241 个表格中。",
            "en": "WikiSQL is a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia that is an order of magnitude larger than comparable datasets."
        }
    },
    {
        "id": "1076",
        "name": "PCA-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/pkunlp-icler/PCA-EVAL",
        "paperLink": "https://aclanthology.org/2024.findings-acl.64.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50188713",
            "name": "PKUNLP-ICLER",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50188713-85c05c22-aa65-44f2-b06f-aa859ed819eb.png",
            "nickname": "PKUNLP-ICLER"
        },
        "lookNum": "186",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-27 17:50:33",
        "supportOnlineEval": false,
        "updateDate": "2024-09-27 17:50:33",
        "createDate": "2024-09-27 15:11:18",
        "desc": {
            "cn": "PCA-Bench 是一个多模态决策基准，用于评估多模态大型语言模型（MLLMs）的综合能力。与之前专注于简单任务和单个模型能力的基准不同，PCA-Bench 引入了三个复杂场景：自动驾驶、家庭机器人和开放世界游戏。",
            "en": "PCA-Bench is a multimodal decisionmaking benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous\nbenchmarks focusing on simplistic tasks and individual model capability."
        }
    },
    {
        "id": "1599",
        "name": "GAIA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2311.12983",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "185",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 11:17:15",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 11:17:15",
        "createDate": "2025-03-06 11:15:47",
        "desc": {
            "cn": "GAIA 是一个旨在评估下一代LLMs（由于增加了工具、高效的提示、访问搜索等功能而具有增强能力的LLMs）的基准，由超过 450 个非平凡问题组成，这些问题有明确的答案，需要不同层次的工具和自主性来解决。",
            "en": "GAIA is a benchmark which aims at evaluating next-generation LLMs (LLMs with augmented capabilities due to added tooling, efficient prompting, access to search, etc), mading of more than 450 non-trivial question with an unambiguous answer, requiring different levels of tooling and autonomy to solve."
        }
    },
    {
        "id": "1542",
        "name": "StructFlowBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "指令跟随",
                "en": "Instruct"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/MLGroupJLU/StructFlowBench",
        "paperLink": "https://arxiv.org/abs/2502.14494",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "182",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-02-25 19:09:02",
        "supportOnlineEval": false,
        "updateDate": "2025-02-25 19:09:02",
        "createDate": "2025-02-25 12:46:25",
        "desc": {
            "cn": "StructFlowBench，这是一个包含155条数据的结构化标注多轮基准，它利用结构驱动生成范式来增强复杂对话场景的模拟。",
            "en": "StructFlowBench is a structurally annotated multi-turn benchmark that leverages a structure-driven generation paradigm to enhance the simulation of complex dialogue scenarios."
        }
    },
    {
        "id": "1359",
        "name": "SEED-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
        "paperLink": "https://arxiv.org/abs/2307.16125",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "178",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:23:55",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:23:55",
        "createDate": "2025-01-09 21:34:54",
        "desc": {
            "cn": "SEED-Bench用于评估多模态大模型的理解能力，包括对图像和视频的理解，由跨越12个评估维度的19K道多项选择题组成。",
            "en": "SEED-Bench aims at the evaluation of generative comprehension in MLLMs, consisting of 19K multiple choice questions, which spans 12 evaluation dimensions including the comprehension of both the image and video modality. "
        }
    },
    {
        "id": "1132",
        "name": "TabFact",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/wenhuchen/Table-Fact-Checking",
        "paperLink": "https://arxiv.org/pdf/1909.02164v5",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "175",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-16 11:03:09",
        "supportOnlineEval": false,
        "updateDate": "2024-10-16 11:03:09",
        "createDate": "2024-10-12 11:35:24",
        "desc": {
            "cn": "TabFac 包含 117,854 条手动标注的语句，涉及 16,573 个维基百科表格，是第一个评估结构化数据上语言推理的数据集，涉及在符号和语言两个方面的混合推理能力。",
            "en": "TabFac consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED. "
        }
    },
    {
        "id": "1175",
        "name": "MMStar",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/MMStar-Benchmark/MMStar",
        "paperLink": "https://arxiv.org/pdf/2403.20330",
        "officialWebsiteLink": "https://mmstar-benchmark.github.io/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "173",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 11:27:36",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 11:27:36",
        "createDate": "2024-10-21 16:21:15",
        "desc": {
            "cn": "MMStar 是一个多模态基准，包含 1,500 个经过人工精心挑选的样本。MMStar 评估 6 项核心能力和 18 个详细维度，旨在通过精心平衡和净化的样本，评估 LVLM 的多模态能力。",
            "en": "MMStar is an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs’ multi-modal capacities with carefully balanced and purified samples."
        }
    },
    {
        "id": "1084",
        "name": "StableToolBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/THUNLP-MT/StableToolBench",
        "paperLink": "https://aclanthology.org/2024.findings-acl.664.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "50186303",
            "name": "THUNLP-MT",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50186303-a2701a3a-208f-4cc9-88b7-e09515c70865.png",
            "nickname": "THUNLP-MT"
        },
        "lookNum": "172",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-29 16:24:50",
        "supportOnlineEval": false,
        "updateDate": "2024-09-29 16:24:50",
        "createDate": "2024-09-29 13:09:10",
        "desc": {
            "cn": "StableToolBench 是一个从 ToolBench 发展而来的基准，提出了一个虚拟 API 服务器和稳定的评估系统。虚拟 API 服务器包含一个缓存系统和 API 模拟器，这些组件相辅相成，以缓解 API 状态变化带来的影响。",
            "en": "StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status."
        }
    },
    {
        "id": "1152",
        "name": "SportQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/haotianxia/SportQA",
        "paperLink": "https://aclanthology.org/2024.naacl-long.283.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "172",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-27 17:23:16",
        "supportOnlineEval": false,
        "updateDate": "2024-10-27 17:23:16",
        "createDate": "2024-10-15 15:51:00",
        "desc": {
            "cn": "SportQA 专门用于评估大型语言模型（LLMs）在体育理解方面的能力。SportQA 包含超过 70,000 道多项选择题，分为三个不同的难度级别，针对从基本历史事实到复杂情境推理任务的各种体育知识。",
            "en": "SportQA aims to evaluate LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenariobased reasoning tasks."
        }
    },
    {
        "id": "1098",
        "name": "GrailQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/dki-lab/GrailQA",
        "paperLink": "https://arxiv.org/pdf/2011.07743",
        "officialWebsiteLink": "https://dki-lab.github.io/GrailQA/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "171",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-09 20:03:43",
        "supportOnlineEval": false,
        "updateDate": "2024-10-09 20:03:43",
        "createDate": "2024-10-09 11:14:58",
        "desc": {
            "cn": "GrailQA 是一个大规模高质量数据集，用于知识库问答，包含 64,331 个问题，并附有答案和不同语法的相应逻辑形式。",
            "en": "GrailQA is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). "
        }
    },
    {
        "id": "1125",
        "name": "Mind2Web",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/OSU-NLP-Group/Mind2Web",
        "paperLink": "https://arxiv.org/pdf/2306.06070",
        "officialWebsiteLink": "https://osu-nlp-group.github.io/Mind2Web/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "170",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-11 20:24:41",
        "supportOnlineEval": false,
        "updateDate": "2024-10-11 20:24:41",
        "createDate": "2024-10-11 13:51:20",
        "desc": {
            "cn": "MIND2WEB 是首个用于开发和评估通用网页代理的数据集，能够根据语言指令在任何网站上完成复杂任务。",
            "en": "MIND2WEB is the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website."
        }
    },
    {
        "id": "1072",
        "name": "xCodeEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/ntunlp/xCodeEval",
        "paperLink": "https://arxiv.org/pdf/2303.03004",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "50187454",
            "name": "NTU-NLP",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50187454-16b303e1-3675-4b47-b9ec-83f20eef9b46.png",
            "nickname": "NTU-NLP"
        },
        "lookNum": "167",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-09-27 16:53:40",
        "supportOnlineEval": false,
        "updateDate": "2024-09-27 16:53:40",
        "createDate": "2024-09-27 12:40:47",
        "desc": {
            "cn": "xCodeEval 是迄今为止最大的可执行多语言多任务基准，包含 2500 万个文档级编码示例（165 亿个标记），来自约 7500 个独特问题，涵盖多达 11 种编程语言。它包括 7 个任务，涉及代码理解、生成、翻译和检索。",
            "en": "xCodeEval is the largest executable multilingual multitask benchmark to date consisting of 25M document-level coding examples (16.5B tokens) from about 7.5K unique problems covering up to 11 programming languages with execution-level parallelism."
        }
    },
    {
        "id": "1116",
        "name": "AQUA-RAT",
        "emoji": "",
        "dimensions": [
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/google-deepmind/AQuA",
        "paperLink": "https://arxiv.org/pdf/1705.04146",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "160",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-11 19:59:29",
        "supportOnlineEval": false,
        "updateDate": "2024-10-11 19:59:29",
        "createDate": "2024-10-10 14:20:23",
        "desc": {
            "cn": "AQUA-RAT 包含代数文字问题。该数据集由约 100,000 道带有自然语言推理的代数文字问题组成。",
            "en": "AQUA-RAT contains the algebraic word problems. The dataset consists of about 100,000 algebraic word problems with natural language rationales."
        }
    },
    {
        "id": "1017",
        "name": "Yue_Benchmark",
        "emoji": "🐲",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [
            {
                "cn": "factual generation",
                "en": "factual generation"
            },
            {
                "cn": "complex reasoning",
                "en": "complex reasoning"
            },
            {
                "cn": "general knowledge",
                "en": "general knowledge"
            },
            {
                "cn": "mathematical logic",
                "en": "mathematical logic"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/jiangjyjy/Yue-Benchmark",
        "paperLink": "https://arxiv.org/abs/2408.16756",
        "officialWebsiteLink": "https://huggingface.co/datasets/BillBao/Yue-Benchmark",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "50157836",
            "name": null,
            "avatar": null,
            "nickname": "enemy"
        },
        "lookNum": "157",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-30 11:30:05",
        "supportOnlineEval": false,
        "updateDate": "2024-12-30 11:30:05",
        "createDate": "2024-12-28 23:41:20",
        "desc": {
            "cn": "Yue_Benchmark用于评估粤语大型语言模型（LLMs）。该评测集包含：Yue TruthtyQA、Yue-GSM8K、Yue-ARC-C、Yue MMLU和Yue TRANS，侧重于粤语语言理解和生成的不同方面，为评估LLM粤语能力提供了一种全面的方法。",
            "en": "The benchmarks introduced for evaluating large language models (LLMs) on Cantonese include Yue-TruthfulQA, Yue-GSM8K, Yue-ARC-C, Yue-MMLU, and Yue-TRANS. Each of these benchmarks focuses on different aspects of language understanding and generation in Cantonese, offering a comprehensive means of assessing the capabilities of LLMs in handling this language."
        }
    },
    {
        "id": "1142",
        "name": "MIRAGE",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "ACL 2024",
                "en": "ACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://teddy-xionggz.github.io/benchmark-medical-rag/",
        "paperLink": "https://aclanthology.org/2024.findings-acl.372.pdf",
        "officialWebsiteLink": "https://teddy-xionggz.github.io/benchmark-medical-rag/",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "153",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-19 18:04:07",
        "supportOnlineEval": false,
        "updateDate": "2024-10-19 18:04:07",
        "createDate": "2024-10-14 18:05:57",
        "desc": {
            "cn": "MIRAGE 是首个此类基准，包含来自五个医学问答数据集的 7,663 个问题。",
            "en": "MIRAGE is a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. "
        }
    },
    {
        "id": "1251",
        "name": "PlanBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/karthikv792/LLMs-Planning",
        "paperLink": "https://arxiv.org/abs/2206.10498",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "147",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-30 16:29:45",
        "supportOnlineEval": false,
        "updateDate": "2024-12-30 16:29:45",
        "createDate": "2024-12-30 16:23:47",
        "desc": {
            "cn": "PlanBench用于评估LLM的规划能力，基于自动化规划社区（尤其是在国际规划竞赛）中涉及的各种领域来测试大模型在规划或推理行动和变更方面的能力。",
            "en": "PlanBench is an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. "
        }
    },
    {
        "id": "1413",
        "name": "LiveCodeBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/LiveCodeBench/LiveCodeBench",
        "paperLink": "https://arxiv.org/abs/2403.07974",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "146",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:47:04",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:47:04",
        "createDate": "2025-01-17 20:40:50",
        "desc": {
            "cn": "LiveCodeBench用于评估大语言模型的代码能力，包含来自LeetCode、AtCoder和CodeForces的动态更新的问题，并在代码生成能力的基础上将更广泛的相关能力纳入考量。",
            "en": "LiveCodeBench evaluates LLMs' coding abilities. It continuously collects new problems over time from contests across LeetCode, AtCoder, and CodeForces. Notably, it also focuses on a broader range of code related capabilities besides code generation."
        }
    },
    {
        "id": "1335",
        "name": "LTMbenchmark",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/GoodAI/goodai-ltm-benchmark",
        "paperLink": "https://arxiv.org/abs/2409.20222",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "144",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 14:05:39",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 14:05:39",
        "createDate": "2025-01-03 16:10:04",
        "desc": {
            "cn": "LTMbenchmark通过动态对话任务评估智能体的长期记忆、持续学习和信息集成能力。",
            "en": "LTMbenchmark assess the long-term memory, continual learning, and information integration capabilities of the agents via dynamic conversational tasks. "
        }
    },
    {
        "id": "1252",
        "name": "RE-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            },
            {
                "cn": "代码",
                "en": "Code"
            },
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/METR/ai-rd-tasks/tree/main",
        "paperLink": "https://arxiv.org/abs/2411.15114",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "143",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-30 16:30:41",
        "supportOnlineEval": false,
        "updateDate": "2024-12-30 16:30:41",
        "createDate": "2024-12-30 16:24:14",
        "desc": {
            "cn": "RE-Bench用于评估AI智能体研发的自动化能力，它由61位人类专家71次在7个具有挑战性的开放式ML研究工程环境中的8小时尝试的数据组成。",
            "en": "RE-Bench (Research Engineering Benchmark, v1) consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts."
        }
    },
    {
        "id": "1576",
        "name": "PhysReason",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2502.12054",
        "officialWebsiteLink": "https://dxzxy12138.github.io/PhysReason/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "139",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-04 16:59:45",
        "supportOnlineEval": false,
        "updateDate": "2025-03-04 16:59:45",
        "createDate": "2025-03-03 16:02:34",
        "desc": {
            "cn": "PhysReason 是一个包含 1,200 个物理问题的综合物理推理基准，涵盖多个领域，重点关注基于知识（25%）和推理（75%）的问题。",
            "en": "PhysReason is a comprehensive physics-based reasoning benchmark consisting of 1,200 physics problems spanning multiple domains, with a focus on both knowledge-based (25%) and reasoning-based (75%) questions."
        }
    },
    {
        "id": "1641",
        "name": "MedAgents-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            },
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Medical",
                "en": "Medical"
            },
            {
                "cn": "Agent",
                "en": "Agent"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/gersteinlab/medagents-benchmark",
        "paperLink": "https://arxiv.org/abs/2503.07459",
        "officialWebsiteLink": "https://github.com/gersteinlab/medagents-benchmark",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "26204315",
            "name": null,
            "avatar": null,
            "nickname": "super-dainiu"
        },
        "lookNum": "136",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-17 13:59:33",
        "supportOnlineEval": false,
        "updateDate": "2025-03-17 13:59:33",
        "createDate": "2025-03-17 12:44:07",
        "desc": {
            "cn": "MedAgentsBench是一个专注于复杂医学推理的基准测试，从七个医学数据集中精选了862个挑战性问题。这些数据集包括MedQA、PubMedQA、MedMCQA、MedBullets、MedExQA、MedXpertQA和MMLU/MMLU-Pro，涵盖了从医学执照考试到研究文献的多种医学问题。\n该基准选择少于50%模型能正确回答的问题，确保医学知识领域全面覆盖，并优先选择需要多步临床推理的问题。这解决了现有评估中简单问题普遍存在、评估协议不一致，以及缺乏性能-成本-时间分析的局限。",
            "en": "MedAgentsBench features 862 challenging medical questions from seven datasets, focusing on cases where models struggle. It emphasizes multi-step clinical reasoning and addresses limitations in existing benchmarks by eliminating simple questions and standardizing evaluation protocols."
        }
    },
    {
        "id": "1322",
        "name": "InfiBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/infi-coder/infibench-evaluation-harness/",
        "paperLink": "https://arxiv.org/abs/2404.07940",
        "officialWebsiteLink": "https://infi-coder.github.io/infibench/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "135",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 14:05:42",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 14:05:42",
        "createDate": "2024-12-31 15:59:24",
        "desc": {
            "cn": "InfiBench用于评测LLM回答代码相关问题的能力，包括涵盖15种编程语言的234个精心挑选的高质量Stack Overflow问题。",
            "en": "InfiBench is a large-scale freeform question-answering (QA) benchmark for code to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages."
        }
    },
    {
        "id": "1562",
        "name": "BenchMAX",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/CONE-MT/BenchMAX",
        "paperLink": "https://arxiv.org/abs/2502.07346",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "134",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-03 10:56:52",
        "supportOnlineEval": false,
        "updateDate": "2025-03-03 10:56:52",
        "createDate": "2025-02-27 11:05:13",
        "desc": {
            "cn": "BenchMAX 是一个全面、高质量的多向并行多语言基准，包含 10 个任务，旨在评估 17 种不同语言的关键能力。",
            "en": "BenchMAX is a comprehensive, high-quality, and multiway parallel multilingual benchmark comprising 10 tasks designed to assess crucial capabilities across 17 diverse language."
        }
    },
    {
        "id": "1283",
        "name": "P-MMEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2411.09116",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "43209637",
            "name": "Qwen",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/43209637-2eaaac15-94a9-4382-9047-16ae007f33f6.png",
            "nickname": "Qwen"
        },
        "lookNum": "132",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-25 19:59:39",
        "supportOnlineEval": false,
        "updateDate": "2024-12-25 19:59:39",
        "createDate": "2024-12-25 19:57:03",
        "desc": {
            "cn": "P-MMEval是一个全面的多语言多任务基准，涵盖了高效的基础和专项能力数据集。",
            "en": "P-MMEval is a comprehensive multilingual multitask benchmark, covering effective fundamental and capability-specialized datasets."
        }
    },
    {
        "id": "1334",
        "name": "MMDU",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Liuziyu77/MMDU",
        "paperLink": "https://arxiv.org/abs/2406.11833",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "132",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 16:19:55",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 16:19:55",
        "createDate": "2025-01-03 14:51:38",
        "desc": {
            "cn": "MMDU用于评估大型视觉语言模型的多图像多轮对话能力，包含110个高质量的多图像多轮对话，由1600多个附有详细的长篇答案的问题组成。",
            "en": "MMDU is intended for evaluating the multi-image multi-turn dialogue capabilities. It comprises 110 high-quality multi-image multi-turn dialogues with more than 1600 questions, each accompanied by detailed long-form answers. "
        }
    },
    {
        "id": "1117",
        "name": "NaturalProofs",
        "emoji": "",
        "dimensions": [
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/wellecks/naturalproofs#naturalproofs-dataset",
        "paperLink": "https://arxiv.org/pdf/2104.01112v2",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "132",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-11 19:59:32",
        "supportOnlineEval": false,
        "updateDate": "2024-10-11 19:59:32",
        "createDate": "2024-10-10 14:39:51",
        "desc": {
            "cn": "NATURALPROOFS 是一个多领域的数学语句及其证明的语料库，采用自然数学语言编写。整合了广泛覆盖、深入覆盖和低资源数学来源，便于评估分布内和零样本泛化的能力。",
            "en": "NATURALPROOFS is a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language. NATURALPROOFS unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization."
        }
    },
    {
        "id": "1238",
        "name": "LINGOLY",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/am-bean/lingOly",
        "paperLink": "https://arxiv.org/abs/2406.06196",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "131",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-25 11:46:22",
        "supportOnlineEval": false,
        "updateDate": "2024-12-25 11:46:22",
        "createDate": "2024-12-20 14:03:59",
        "desc": {
            "cn": "LingOly由低资源和已灭绝语言的奥林匹克级别语言推理谜题组成，用于评估大语言模型的高级推理能力。其中涵盖了90多种语言，共有1133个涉及6种格式和5个人工难度级别的问题。",
            "en": "Aiming at evaluating LLM's advanced reasoning abilities, LingOly is composed of  olympiad-level linguistic reasoning puzzles in low-resource and extinct languages. It covers more than 90 mostly low-resource languages, and contains 1,133 problems across 6 formats and 5 levels of human difficulty."
        }
    },
    {
        "id": "1246",
        "name": "LiveBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/livebench/livebench",
        "paperLink": "https://arxiv.org/abs/2406.19314",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "128",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-22 18:09:35",
        "supportOnlineEval": false,
        "updateDate": "2024-12-22 18:09:35",
        "createDate": "2024-12-20 19:35:51",
        "desc": {
            "cn": "LiveBench是一个LLM基准测试，涵盖数学、编码、推理、语言、指令遵循和数据分析，包含基于最近发布的数学竞赛、arXiv 论文、新闻文章和数据集的问题，及经典基准测试（如 Big-Bench Hard、AMPS 和 IFEval）的更难、无污染的任务。",
            "en": "LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. "
        }
    },
    {
        "id": "1107",
        "name": "StrategyQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/eladsegal/strategyqa",
        "paperLink": "https://arxiv.org/pdf/2101.02235",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "125",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-09 20:03:28",
        "supportOnlineEval": false,
        "updateDate": "2024-10-09 20:03:28",
        "createDate": "2024-10-09 17:54:13",
        "desc": {
            "cn": "STRATEGYQA 是一个问答基准，其中所需的推理步骤在问题中是隐含的，可以通过策略进行推断。",
            "en": "STRATEGYQA is a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy"
        }
    },
    {
        "id": "1239",
        "name": "CVQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2406.05967",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "124",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-30 16:30:03",
        "supportOnlineEval": false,
        "updateDate": "2024-12-30 16:30:03",
        "createDate": "2024-12-30 16:18:03",
        "desc": {
            "cn": "CVQA是一种新的文化多元化多语言视觉问答基准，旨在涵盖丰富的语言和文化，包括来自四大洲30个国家/地区的文化向图像和10000个问题。",
            "en": "CVQA is a new culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures. It includes culturally-driven images and 10k questions from across 30 countries on 4 continents."
        }
    },
    {
        "id": "1146",
        "name": "BUST",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/IDSIA-NLP/BUST",
        "paperLink": "https://aclanthology.org/2024.naacl-long.444.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "50126080",
            "name": "IDSIA-NLP",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50126080-9bda56f6-aaad-43e3-86d5-1278eb268d20.png",
            "nickname": "OpenXLab-rR7CGPrkS"
        },
        "lookNum": "123",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-27 17:23:25",
        "supportOnlineEval": false,
        "updateDate": "2024-10-27 17:23:25",
        "createDate": "2024-10-15 13:46:56",
        "desc": {
            "cn": "BUST 是一个综合基准，旨在评估合成文本检测器，BUST 使用多种指标来评估检测器，包括语言特征、可读性和作者态度。",
            "en": "BUST is a comprehensive benchmark for evaluating synthetic text detectors, focusing on their effectiveness against outputs from various Large Language Models (LLMs). "
        }
    },
    {
        "id": "1355",
        "name": "HallusionBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/tianyi-lab/HallusionBench",
        "paperLink": "https://arxiv.org/abs/2310.14566",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "122",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 11:27:39",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 11:27:39",
        "createDate": "2025-01-09 20:08:24",
        "desc": {
            "cn": "HallusionBench是一个专为评估图像上下文推理而设计的综合基准测试，包括346张图像和1129个问题。",
            "en": "HallusionBench is a comprehensive benchmark designed for the evaluation of image-context reasoning, comprising 346 images paired with 1129 questions."
        }
    },
    {
        "id": "1113",
        "name": "SVAMP",
        "emoji": "",
        "dimensions": [
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 1,
        "githubLink": "https://github.com/arkilpatel/SVAMP",
        "paperLink": "https://arxiv.org/pdf/2103.07191v2",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "121",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-11 19:59:21",
        "supportOnlineEval": false,
        "updateDate": "2024-10-11 19:59:21",
        "createDate": "2024-10-10 13:41:55",
        "desc": {
            "cn": "SVAMP 是一个包含算术文字问题的数据集，最高适用于四年级的学生，是通过对现有数据集中的文字问题应用简单变体而生成。",
            "en": "SVAMP includes one-unknown arithmetic word problems with grade level up to 4 by applying simple variations over word problems in an existing dataset. SVAMP further highlights the brittle nature of existing models when trained on these benchmark datasets."
        }
    },
    {
        "id": "1572",
        "name": "LOKI",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/opendatalab/LOKI",
        "paperLink": "https://arxiv.org/abs/2410.09732",
        "officialWebsiteLink": "https://opendatalab.github.io/LOKI/#fingding",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "118",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-18 15:43:49",
        "supportOnlineEval": false,
        "updateDate": "2025-03-18 15:43:49",
        "createDate": "2025-03-18 15:43:15",
        "desc": {
            "cn": "LOKI是一个多模态合成数据检测基准，专门设计用于全面评估 LMMs 在检测合成数据方面的能力。",
            "en": "LOKI, a multimodal synthetic data detection benchmark, designed specifically to comprehensively assess the capabilities of LMMs in detecting synthetic data."
        }
    },
    {
        "id": "1541",
        "name": "VLM2-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/vlm2-bench/VLM2-Bench",
        "paperLink": "https://arxiv.org/abs/2502.12084",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "117",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-02-26 18:53:47",
        "supportOnlineEval": false,
        "updateDate": "2025-02-26 18:53:47",
        "createDate": "2025-02-25 20:23:33",
        "desc": {
            "cn": "VLM²-Bench 是第一个全面评估视觉语言模型（VLMs）在多图像序列和视频中视觉链接匹配线索能力的基准。该基准包括 9 个子任务，超过 3000 个测试案例，旨在评估人类日常使用的根本视觉链接能力。",
            "en": "VLM²-Bench is the first comprehensive benchmark that evaluates vision-language models' (VLMs) ability to visually link matching cues across multi-image sequences and videos. The benchmark consists of 9 subtasks with over 3,000 test cases."
        }
    },
    {
        "id": "1349",
        "name": "CyberSecEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks",
        "paperLink": "https://arxiv.org/abs/2312.04724",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "116",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:23:31",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:23:31",
        "createDate": "2025-01-09 12:17:41",
        "desc": {
            "cn": "CyberSecEval旨在评估LLM的安全性，聚焦于大模型生成不安全代码的倾向以及当被要求协助网络攻击时的合规性水平。",
            "en": "CyberSecEval is a comprehensive benchmark developed to help bolster the cybersecurity of LLMs. It provides a thorough evaluation in two crucial security domains: the propensity to generate insecure code and the level of compliance when asked to assist in cyberattacks."
        }
    },
    {
        "id": "1643",
        "name": "Creation-MMBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "长文本",
                "en": "Long-Context"
            },
            {
                "cn": "创作",
                "en": "Creation"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 3,
        "githubLink": "https://github.com/open-compass/Creation-MMBench",
        "paperLink": "",
        "officialWebsiteLink": "https://open-compass.github.io/Creation-MMBench/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "40047277",
            "name": "fangxy-09",
            "avatar": "https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKFASuxOhldeP1BsgbQewWr71yNBNAehdVp5Qqrxq0D6hQ0libbrYZs9n5GVtoicy4uOvtNrmebicSKQ/132",
            "nickname": "FangXinyu-0913"
        },
        "lookNum": "112",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-05-06 11:00:38",
        "supportOnlineEval": false,
        "updateDate": "2025-05-06 11:00:38",
        "createDate": "2025-04-30 12:29:15",
        "desc": {
            "cn": "专为评估 多模态大模型 的创作能力而设计的多模态基准。采用两个不同指标对模型的基础感知能力和深层次视觉创作能力进行评估，采用GPT-4o作为评判模型进行评估。",
            "en": "A multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs. It features three main aspects: 1. Comprehensive Creation Benchmark for MLLM and LLM. 2. Robust Evaluation Methodology. 3. Attractive Experiment Insight. "
        }
    },
    {
        "id": "1543",
        "name": "KITAB-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/mbzuai-oryx/KITAB-Bench",
        "paperLink": "https://arxiv.org/abs/2502.14949",
        "officialWebsiteLink": "https://mbzuai-oryx.github.io/KITAB-Bench/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "110",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-05 13:36:30",
        "supportOnlineEval": false,
        "updateDate": "2025-03-05 13:36:30",
        "createDate": "2025-03-05 13:36:03",
        "desc": {
            "cn": "KITAB-Bench是一个全面多领域阿拉伯文 OCR 和文档理解基准，包含 36 个子领域，超过 8,809 个样本，经过精心挑选，以严格评估阿拉伯 OCR 和文档分析所需的基本技能。",
            "en": "KITAB-Bench is a Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding,  and spans 36 sub-domains with over 8,809 samples, carefully curated to rigorously evaluate essential skills required for Arabic OCR and document analysis."
        }
    },
    {
        "id": "1103",
        "name": "QASC",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/allenai/qasc",
        "paperLink": "https://arxiv.org/pdf/1910.11473",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "110",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-09 20:03:32",
        "supportOnlineEval": false,
        "updateDate": "2024-10-09 20:03:32",
        "createDate": "2024-10-09 16:45:32",
        "desc": {
            "cn": "QASC 是一个专注于句子组合的问答数据集。它包含 9,980 道小学科学的多项选择题（8,134 道用于训练，926 道用于开发，920 道用于测试），并配有一个包含 1,700 万个句子的语料库。",
            "en": "QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences."
        }
    },
    {
        "id": "1242",
        "name": "AgentBoard",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/hkust-nlp/AgentBoard/tree/main",
        "paperLink": "https://arxiv.org/abs/2401.13178",
        "officialWebsiteLink": "https://hkust-nlp.github.io/agentboard/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "109",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-25 11:46:29",
        "supportOnlineEval": false,
        "updateDate": "2024-12-25 11:46:29",
        "createDate": "2024-12-20 17:19:44",
        "desc": {
            "cn": "AgentBoard专用于LLM Agent的分析评估，它提供了一个精细指标用于捕获增量进步，以及一个全面的评估工具包，能基于交互式可视化评估进行多方面分析。",
            "en": "AgentBoard is tailored to analytical evaluation of LLM agents. It offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization."
        }
    },
    {
        "id": "1357",
        "name": "MME",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation",
        "paperLink": "https://arxiv.org/abs/2306.13394",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "108",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 16:03:56",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 16:03:56",
        "createDate": "2025-01-09 20:56:27",
        "desc": {
            "cn": "MME是一个全面的多模态大模型评估基准，涵盖14 个考察感知和认知能力的子任务。",
            "en": "MME is a comprehensive MLLM evaluation benchmark. It measures both perception and cognition abilities on a total of 14 subtasks."
        }
    },
    {
        "id": "1000",
        "name": "Q-Bench",
        "emoji": "🐲",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [
            {
                "cn": "yes-or-no",
                "en": "yes-or-no"
            },
            {
                "cn": "what",
                "en": "what"
            },
            {
                "cn": "how",
                "en": "how"
            },
            {
                "cn": "distortion",
                "en": "distortion"
            },
            {
                "cn": "others",
                "en": "others"
            },
            {
                "cn": "in-context distortion",
                "en": "in-context distortion"
            },
            {
                "cn": "in-context others",
                "en": "in-context others"
            },
            {
                "cn": "overall",
                "en": "overall"
            }
        ],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Q-Future/Q-Bench",
        "paperLink": "https://arxiv.org/abs/2309.14181",
        "officialWebsiteLink": "https://q-future.github.io/Q-Bench",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "50151145",
            "name": null,
            "avatar": null,
            "nickname": "Orange"
        },
        "lookNum": "108",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": null,
        "supportOnlineEval": false,
        "updateDate": "2024-08-16 20:27:46",
        "createDate": "2024-08-13 09:33:41",
        "desc": {
            "cn": "Q-Bench/Q-Bench+是一个面向多模态大模型底层视觉理解的数据集。此数据集从底层视觉的感知、描述、评价能力出发来对多模态大模型进行完整的测试，测试的对象既包括单张图像也包括图像对。",
            "en": "Q-Bench/Q-Bench+ is a benchmark for general-purpose foundation models on low-level vision."
        }
    },
    {
        "id": "1564",
        "name": "EmbodiedBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/EmbodiedBench/EmbodiedBench",
        "paperLink": "https://arxiv.org/abs/2502.09560",
        "officialWebsiteLink": "https://embodiedbench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "106",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-03 11:00:23",
        "supportOnlineEval": false,
        "updateDate": "2025-03-03 11:00:23",
        "createDate": "2025-02-27 11:25:40",
        "desc": {
            "cn": "EmbodiedBench，这是一个旨在评估多模态大型语言模型作为具身智能体的全面基准。",
            "en": "EmbodiedBench is a comprehensive benchmark designed to evaluate Multi-modal Large Language Models (MLLMs) as embodied agents."
        }
    },
    {
        "id": "1617",
        "name": "IFIR",
        "emoji": "",
        "dimensions": [
            {
                "cn": "指令跟随",
                "en": "Instruct"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/SighingSnow/IFIR",
        "paperLink": "https://arxiv.org/abs/2503.04644",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "106",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-10 17:11:56",
        "supportOnlineEval": false,
        "updateDate": "2025-03-10 17:11:56",
        "createDate": "2025-03-10 11:01:15",
        "desc": {
            "cn": " IFIR，这是第一个旨在评估专家领域指令跟随信息检索（IR）的综合基准。IFIR 包含 2,426 个高质量示例，涵盖四个专业领域（金融、法律、医疗保健和科学文献）的八个子集。",
            "en": "IFIR is the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature."
        }
    },
    {
        "id": "1367",
        "name": "A-OKVQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            },
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "VQA",
                "en": "VQA"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/allenai/aokvqa",
        "paperLink": "https://arxiv.org/abs/2206.01718",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "106",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:23:39",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:23:39",
        "createDate": "2025-01-10 12:28:01",
        "desc": {
            "cn": "A-OKVQA用于评估多模态大模型的常识及推理能力，由25K个不同的问题组成，需要对图像中描述的场景进行某种形式的常识性推理来回答。",
            "en": "A-OKVQA assesses commonsense reasoning abilities. It is a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer."
        }
    },
    {
        "id": "1532",
        "name": "ZeroBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/jonathan-roberts1/zerobench",
        "paperLink": "https://arxiv.org/abs/2502.09696",
        "officialWebsiteLink": "https://zerobench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "106",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-02-25 19:54:07",
        "supportOnlineEval": false,
        "updateDate": "2025-02-25 19:54:07",
        "createDate": "2025-02-24 13:40:00",
        "desc": {
            "cn": "ZeroBench 是针对多模态模型（LMMs）的具有挑战性的视觉推理基准。它由一组主要的 100 个高质量人工问题组成，涵盖多个领域、推理类型和图像类型。ZeroBench 中的问题经过设计和校准，已经超出了当前前沿模型的能力范围。",
            "en": "ZeroBench is a challenging visual reasoning benchmark for LMMs. It consists of a main set of 100 high-quality, manually curated questions covering numerous domains, reasoning types and image type. Questions have been designed and calibrated to be beyond the capabilities of current frontier models.\n"
        }
    },
    {
        "id": "1546",
        "name": "CodeCriticBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/multimodal-art-projection/CodeCriticBench",
        "paperLink": "https://arxiv.org/abs/2502.16614",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "105",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-02-26 18:41:24",
        "supportOnlineEval": false,
        "updateDate": "2025-02-26 18:41:24",
        "createDate": "2025-02-26 11:00:09",
        "desc": {
            "cn": "CodeCriticBench ，旨在系统地评估LLMs在代码生成和代码问答任务中的批评能力。其涵盖 10 个不同的标准，数据集根据难度分为三个等级，共包含4.3k个样本，确保了难度级别的平衡分布。",
            "en": "CodeCriticBench assesses LLMs' critiquing ability in code generation and QA tasks. Covering 10 criteria, it features a 4.3k-samples dataset with three difficulty levels and balanced distribution."
        }
    },
    {
        "id": "1333",
        "name": "RedCode",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            },
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/AI-secure/RedCode",
        "paperLink": "https://arxiv.org/abs/2411.07781",
        "officialWebsiteLink": "https://redcode-agent.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "104",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 11:12:36",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 11:12:36",
        "createDate": "2025-01-10 19:41:07",
        "desc": {
            "cn": "RedCode旨在为LLM代码智能体的安全性提供全面实用的评估，包括来自8个领域25种关键漏洞的4050个风险测试用例，以及160个生成有害代码的提示。",
            "en": "RedCode provides comprehensive and practical evaluations on the safety of code agents, including 4,050 risky test cases covering 25 types of critical vulnerabilities spanning 8 domains and 160 prompts aiming to generate harmful code or software."
        }
    },
    {
        "id": "1250",
        "name": "Collie",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/princeton-nlp/Collie",
        "paperLink": "https://arxiv.org/abs/2307.08689",
        "officialWebsiteLink": "https://collie-benchmark.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "103",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-30 16:29:49",
        "supportOnlineEval": false,
        "updateDate": "2024-12-30 16:29:49",
        "createDate": "2024-12-30 16:23:20",
        "desc": {
            "cn": "COLLIE用于评估大模型在约束性文本生成任务中的表现，可指定具有不同生成级别（单词、句子、段落、段落）和建模挑战（例如，语言理解、逻辑推理、计数、语义规划）的丰富组合。",
            "en": "COLLIE is a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning)."
        }
    },
    {
        "id": "1149",
        "name": "InstruSum",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/yale-nlp/InstruSum",
        "paperLink": "https://aclanthology.org/2024.findings-naacl.280.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "50107925",
            "name": "yale-nlp",
            "avatar": null,
            "nickname": "OpenXLab-kJ37KHAvs"
        },
        "lookNum": "101",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-27 17:22:45",
        "supportOnlineEval": false,
        "updateDate": "2024-10-27 17:22:45",
        "createDate": "2024-10-15 14:35:05",
        "desc": {
            "cn": "InstruSum 是评测指令可控的文本摘要任务，模型输入包括源文章和对所需摘要特征的自然语言要求。",
            "en": "InstruSum evaluates the task of instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics.\n"
        }
    },
    {
        "id": "1332",
        "name": "UniBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/facebookresearch/unibench",
        "paperLink": "https://arxiv.org/abs/2408.04810",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "100",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-08 11:56:31",
        "supportOnlineEval": false,
        "updateDate": "2025-01-08 11:56:31",
        "createDate": "2025-01-03 14:22:05",
        "desc": {
            "cn": "UniBench旨在评估VLM的推理能力，包括50个基准测试，涵盖对象识别、空间感知、计数等任务。",
            "en": "UniBench is meant for evaluating VLMs' reasoning abilities. It is a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. "
        }
    },
    {
        "id": "1241",
        "name": "MedCalc-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/ncbi-nlp/MedCalc-Bench",
        "paperLink": "https://arxiv.org/abs/2406.12036",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "99",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-25 11:46:27",
        "supportOnlineEval": false,
        "updateDate": "2024-12-25 11:46:27",
        "createDate": "2024-12-20 17:03:18",
        "desc": {
            "cn": "MedCalc-Bench专注于评估LLM的医学计算能力，包含来自55个不同医学计算任务的1000个经过人工审查的实例。",
            "en": " MedCalc-Bench focuses on evaluating the medical calculation capability of LLMs. It contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks."
        }
    },
    {
        "id": "1748",
        "name": "VisualPuzzles",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            },
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/neulab/VisualPuzzles",
        "paperLink": "https://arxiv.org/abs/2504.10342",
        "officialWebsiteLink": "https://neulab.github.io/VisualPuzzles/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "53009543",
            "name": null,
            "avatar": null,
            "nickname": "OpenXLab-oErCdNaBY"
        },
        "lookNum": "98",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-18 17:11:40",
        "supportOnlineEval": false,
        "updateDate": "2025-04-18 17:11:40",
        "createDate": "2025-04-17 21:40:00",
        "desc": {
            "cn": "LLM 能考公务员吗？我们做了个测试…\n\n近年来，大模型（LLM）的能力突飞猛进，似乎“越来越聪明”了。但有一个关键问题仍然摆在眼前：\n🤔 它们真的会“推理”吗？\n\n🚀 我们设计了一个名为 VisualPuzzles 🧩 的全新数据集，专门用来回答这个问题：\n脱离专业知识的支持，大模型能靠逻辑本身解题吗？\n我们从多个来源精心挑选或改编了 1168 道图文逻辑题，其中一个重要来源便是中国国家公务员考试行测中的逻辑推理题（没错，真·考公难度）🎯",
            "en": "VisualPuzzles is a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of 1168 diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. "
        }
    },
    {
        "id": "1275",
        "name": "MMLongBench-Doc",
        "emoji": "",
        "dimensions": [
            {
                "cn": "长文本",
                "en": "Long-Context"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/mayubo2333/MMLongBench-Doc",
        "paperLink": "https://arxiv.org/abs/2407.01523",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "97",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 14:14:42",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 14:14:42",
        "createDate": "2025-01-07 14:12:34",
        "desc": {
            "cn": "MMLONGBENCH-DOC是一个长上下文的多模态基准，由1062个专家注释的问题组成。",
            "en": "MMLongBench-Doc is a long-context multi-modal benchmark comprising 1,062 expert-annotated questions."
        }
    },
    {
        "id": "1548",
        "name": "MedHallu",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Medical",
                "en": "Medical"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/MedHallu/MedHalu",
        "paperLink": "https://arxiv.org/abs/2502.14302",
        "officialWebsiteLink": "https://medhallu.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "96",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-02-26 19:00:05",
        "supportOnlineEval": false,
        "updateDate": "2025-02-26 19:00:05",
        "createDate": "2025-02-26 13:33:02",
        "desc": {
            "cn": "MedHallu 旨在评估大型语言模型在医学问题解答任务中检测幻觉的能力。",
            "en": "MedHallu is a comprehensive benchmark dataset designed to evaluate the ability of large language models to detect hallucinations in medical question-answering tasks. "
        }
    },
    {
        "id": "1330",
        "name": "SG-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/MurrayTom/SG-Bench",
        "paperLink": "https://arxiv.org/abs/2410.21965",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "96",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-08 11:56:40",
        "supportOnlineEval": false,
        "updateDate": "2025-01-08 11:56:40",
        "createDate": "2025-01-03 14:07:00",
        "desc": {
            "cn": "SG-Bench用于评估LLM在不同任务和提示下的安全性，整合了生成性和判别性评估任务，并包含扩展数据以度量提示工程和越狱对安全性的影响。",
            "en": "SG-Bench assess LLM safety across various tasks and prompt types. It integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak. "
        }
    },
    {
        "id": "1268",
        "name": "CTIBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/xashru/cti-bench",
        "paperLink": "https://arxiv.org/abs/2406.07599",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "96",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-25 14:15:02",
        "supportOnlineEval": false,
        "updateDate": "2024-12-25 14:15:02",
        "createDate": "2024-12-24 15:01:43",
        "desc": {
            "cn": "CTIBench旨在评估LLM在CTI（网络安全情报）场景下的能力，包含多个数据集，专注于评测LLM在网络威胁环境中获得的知识。",
            "en": "CTIBench is a benchmark designed to assess LLMs' performance in CTI (Cyber threat intelligence) applications. It includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. "
        }
    },
    {
        "id": "1323",
        "name": "SciFIBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/jonathan-roberts1/SciFIBench",
        "paperLink": "https://arxiv.org/abs/2405.08807",
        "officialWebsiteLink": "https://scifibench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "95",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 11:12:46",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 11:12:46",
        "createDate": "2025-01-13 11:57:33",
        "desc": {
            "cn": " SciFIBench用于评估多模态大模型的科学图表解释能力，由2000个问题组成，涵盖8个类别的2种任务。",
            "en": "SciFIBench is a scientific figure interpretation benchmark for LMMs, consisting of 2000 questions split between two tasks across 8 categories. "
        }
    },
    {
        "id": "1102",
        "name": "MS_MARCO",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/microsoft/MSMARCO-Question-Answering",
        "paperLink": "https://arxiv.org/pdf/1611.09268",
        "officialWebsiteLink": "https://microsoft.github.io/msmarco/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "95",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-09 20:03:35",
        "supportOnlineEval": false,
        "updateDate": "2024-10-09 20:03:35",
        "createDate": "2024-10-09 16:38:58",
        "desc": {
            "cn": "MS MARCO 数据集包含 1,010,916 个来自 Bing 的搜索查询日志的匿名问题，每个问题都有一个人工生成的答案和 182,669 个完全由人重写的生成答案。此外，该数据集还包含从 3,563,535 个由 Bing 检索的网页文档中提取的 8,841,823 个段落，这些段落提供了策划自然语言答案所需的信息。",
            "en": "MS MARCO comprises of 1,010,916 anonymized questions—sampled from Bing’s search query logs—each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages."
        }
    },
    {
        "id": "1523",
        "name": "MMIE",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Multimodal",
                "en": "Multimodal"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Lillianwei-h/MMIE",
        "paperLink": "https://arxiv.org/abs/2410.10139",
        "officialWebsiteLink": "https://mmie-bench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "95",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:58:15",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:58:15",
        "createDate": "2025-02-21 11:23:50",
        "desc": {
            "cn": " MMIE，这是一个大规模知识密集型基准，用于评估大型视觉-语言模型（LVLMs）中的交错多模态理解和生成。",
            "en": " MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs)."
        }
    },
    {
        "id": "1273",
        "name": "MolPuzzle",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/KehanGuo2/MolPuzzle",
        "paperLink": "https://openreview.net/pdf?id=t1mAXb4Cop",
        "officialWebsiteLink": "https://kehanguo2.github.io/Molpuzzle.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "95",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 14:14:38",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 14:14:38",
        "createDate": "2025-01-07 14:07:05",
        "desc": {
            "cn": "MolPuzzle用于考察MLM的推理能力，包含234个结构解析实例以及超过18000个QA样本。",
            "en": "MolPuzzle is a benchmark for evaluating MLM's reasoning ability, comprising 234 instances of structure elucidation, which feature over 18,000 QA samples."
        }
    },
    {
        "id": "1324",
        "name": "FLUB",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/THUKElab/FLUB",
        "paperLink": "https://arxiv.org/abs/2402.11100",
        "officialWebsiteLink": "https://thukelab.github.io/FLUB/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "94",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 14:05:50",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 14:05:50",
        "createDate": "2025-01-02 19:42:41",
        "desc": {
            "cn": "FLUB用于评估LLM的推理和理解能力，其中包含3个难度递进的任务，由从真实互联网环境中收集的狡猾、幽默和误导性的文本构成。",
            "en": "FLUB evaluates the reasoning and understanding abilities of LLMs. It includes three tasks with increasing difficulty, consisting of the tricky, humorous, and misleading texts collected from the real internet environment."
        }
    },
    {
        "id": "1336",
        "name": "CompBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/RaptorMai/CompBench",
        "paperLink": "https://arxiv.org/abs/2407.16837",
        "officialWebsiteLink": "https://compbench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "94",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 14:05:36",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 14:05:36",
        "createDate": "2025-01-03 16:21:25",
        "desc": {
            "cn": "CompBench旨在评估多模态大模型的比较推理能力，包含约4万个图像对及8个维度的视觉配对问题。",
            "en": "CompBench is designed to evaluate the comparative reasoning capability of multimodal large language models, including a collection of around 40K image pairs and visually oriented questions covering 8 dimensions"
        }
    },
    {
        "id": "1337",
        "name": "JailTrickBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/usail-hkust/JailTrickBench",
        "paperLink": "https://arxiv.org/abs/2406.09324",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "94",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 14:05:47",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 14:05:47",
        "createDate": "2025-01-03 16:33:01",
        "desc": {
            "cn": "JailTrickBench用于评估LLM应对各种越狱攻击的能力，涵盖从目标级和攻击级2个角度实施越狱攻击的8个关键因素。",
            "en": "JailTrickBench can evaluate the impact of various attack settings on LLM performance, including 8 key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives."
        }
    },
    {
        "id": "1331",
        "name": "MedSafetyBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/AI4LIFE-GROUP/med-safety-bench",
        "paperLink": "https://arxiv.org/abs/2403.03744",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "93",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-06 16:32:34",
        "supportOnlineEval": false,
        "updateDate": "2025-01-06 16:32:34",
        "createDate": "2025-01-03 14:14:58",
        "desc": {
            "cn": "MedSafetyBench用于评估LLM在医疗安全上的表现，包含1800个由有害请求和安全响应组成的医疗安全场景。",
            "en": "MedSafetyBench is designed to measure the medical safety of LLMs. It includes 1,800 medical safety demonstrations, where each safety demonstration consists of a harmful medical request and a corresponding safe response. "
        }
    },
    {
        "id": "1269",
        "name": "ConvBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/shirlyliu64/ConvBench",
        "paperLink": "https://arxiv.org/abs/2403.20194",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "93",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 14:14:55",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 14:14:55",
        "createDate": "2025-01-07 14:14:09",
        "desc": {
            "cn": "ConvBench是专用于大型视觉语言模型 （LVLM）的新型多轮对话评估基准，由基于215 个反映实际需求的任务的577个多轮次对话组成。 ",
            "en": "ConvBench is a novel multi-turn conversation evaluation benchmark tailored for Large Vision-Language Models (LVLMs). It comprises 577 meticulously curated multi-turn conversations encompassing 215 tasks reflective of real-world demands."
        }
    },
    {
        "id": "1451",
        "name": "MME-RealWorld",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/yfzhang114/MME-RealWorld",
        "paperLink": "https://arxiv.org/abs/2408.13257",
        "officialWebsiteLink": "https://mme-realworld.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "91",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-05 15:26:47",
        "supportOnlineEval": false,
        "updateDate": "2025-03-05 15:26:47",
        "createDate": "2025-01-27 12:05:30",
        "desc": {
            "cn": "MME-RealWorld用于评估多模态大模型对真实场景的理解能力，包含13366个平均2000*1500像素的高分辨率图像。",
            "en": "MME-RealWorld evaluates MLLMs' real-world recognition, featuring 13,366 high-resolution images averaging 2,000 × 1,500 pixels. "
        }
    },
    {
        "id": "1565",
        "name": "MME-CoT",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/CaraJ7/MME-CoT",
        "paperLink": "https://arxiv.org/abs/2502.09621",
        "officialWebsiteLink": "https://mmecot.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "90",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-03 11:01:40",
        "supportOnlineEval": false,
        "updateDate": "2025-03-03 11:01:40",
        "createDate": "2025-02-27 11:38:00",
        "desc": {
            "cn": "MME-CoT，一个专门用于评估 LMMs CoT 推理性能的基准，涵盖六个领域：数学、科学、OCR、逻辑、时空和一般场景。",
            "en": "MME-CoT is a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes."
        }
    },
    {
        "id": "1270",
        "name": "Spider2-V",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/xlang-ai/Spider2-V",
        "paperLink": "https://arxiv.org/abs/2407.10956",
        "officialWebsiteLink": "https://spider2-v.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "90",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 14:14:51",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 14:14:51",
        "createDate": "2025-01-07 14:07:32",
        "desc": {
            "cn": "Spider2-V是第一个专注于专业数据科学和工程工作流程的多模态代理基准测试，整合了20 个企业级专业应用程序，包含来自真实计算机环境的494 个真实任务。",
            "en": "Spider2-V is the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications."
        }
    },
    {
        "id": "1582",
        "name": "CodeMMLU",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/FSoft-AI4Code/CodeMMLU",
        "paperLink": "https://arxiv.org/abs/2406.15877",
        "officialWebsiteLink": "https://fsoft-ai4code.github.io/codemmlu/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "89",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:46:54",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:46:54",
        "createDate": "2025-03-04 11:00:59",
        "desc": {
            "cn": "CodeMMLU 是一个旨在评估大型语言模型（LLMs）在编码和软件知识方面能力的全面基准。它基于多项选择题回答（MCQA）的结构，涵盖了广泛的编程任务和领域，包括代码生成、缺陷检测、软件工程原则等。",
            "en": "CodeMMLU is a comprehensive benchmark designed to evaluate the capabilities of large language models (LLMs) in coding and software knowledge. It builds upon the structure of multiple-choice question answering (MCQA) to cover a wide range of programming tasks and domains."
        }
    },
    {
        "id": "1566",
        "name": "MM-IQ",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/AceCHQ/MMIQ",
        "paperLink": "https://arxiv.org/abs/2502.00698",
        "officialWebsiteLink": "https://acechq.github.io/MMIQ-benchmark/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "88",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-03 11:03:22",
        "supportOnlineEval": false,
        "updateDate": "2025-03-03 11:03:22",
        "createDate": "2025-02-27 13:43:06",
        "desc": {
            "cn": "MM-IQ，这是一个包含 2,710 个精心挑选的测试项目的综合评估框架，涵盖了 8 种不同的推理范式。",
            "en": "MM-IQ is a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms."
        }
    },
    {
        "id": "1701",
        "name": "WritingBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "长文本",
                "en": "Long-Context"
            },
            {
                "cn": "创作",
                "en": "Creation"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/X-PLUG/WritingBench",
        "paperLink": "https://arxiv.org/pdf/2503.05244",
        "officialWebsiteLink": "https://modelscope.cn/studios/iic/DeepWriting",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "52302232",
            "name": null,
            "avatar": null,
            "nickname": "OpenXLab-qSFSXYARf"
        },
        "lookNum": "88",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-03 19:45:43",
        "supportOnlineEval": false,
        "updateDate": "2025-04-03 19:45:43",
        "createDate": "2025-04-02 15:22:21",
        "desc": {
            "cn": "WritingBench: A Comprehensive Benchmark for Generative Writing",
            "en": "WritingBench: A Comprehensive Benchmark for Generative Writing"
        }
    },
    {
        "id": "1272",
        "name": "GSM1k",
        "emoji": "",
        "dimensions": [
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/scaleapi/gsm1k_eval",
        "paperLink": "https://arxiv.org/abs/2405.00332",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "88",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-25 14:36:23",
        "supportOnlineEval": false,
        "updateDate": "2024-12-25 14:36:23",
        "createDate": "2024-12-24 19:16:04",
        "desc": {
            "cn": "GSM1k可用于评估LLM的数学推理能力；它与GSM8k保持了风格和复杂性的一致，同时考虑了数据泄露和过拟合的问题。",
            "en": "GSM1k is meant for evaluating LLM's math reasoning ability. It mirrors the style and complexity of the established GSM8k benchmark while consider the problem of data-leaking and overfitting"
        }
    },
    {
        "id": "1114",
        "name": "ASDiv",
        "emoji": "",
        "dimensions": [
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/chaochun/nlu-asdiv-dataset/tree/master",
        "paperLink": "https://aclanthology.org/2020.acl-main.92.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "87",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-11 19:59:24",
        "supportOnlineEval": false,
        "updateDate": "2024-10-11 19:59:24",
        "createDate": "2024-10-10 14:06:57",
        "desc": {
            "cn": "ASDiv 是一个新的数学文字问题（MWP）语料库，包含多样的词汇模式，覆盖广泛的问题类型。每个问题提供对应的方程和答案。它进一步标注了相应的问题类型和年级水平，可用于测试系统的能力，并指明问题的难度等级。",
            "en": "ASDiv is a new MWP corpus that contains diverse lexicon patterns with wide problem type coverage. Each problem provides consistent equations and answers. It is further annotated with the corresponding problem type and grade level."
        }
    },
    {
        "id": "1361",
        "name": "RealworldQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "86",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 17:25:35",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 17:25:35",
        "createDate": "2025-01-09 22:10:13",
        "desc": {
            "cn": "RealWorldQA用于评估多模态模型在现实世界中的空间理解能力，包含765张图像，每张图像都配有一个问题和易于验证的答案。",
            "en": "RealWorldQA is a benchmark designed for real-world understanding, including 765 images, each accompanied by a question and a verifiable answer."
        }
    },
    {
        "id": "1365",
        "name": "BLINK",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/zeyofu/BLINK_Benchmark",
        "paperLink": "https://arxiv.org/abs/2404.12390",
        "officialWebsiteLink": "https://zeyofu.github.io/blink/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "86",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:23:42",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:23:42",
        "createDate": "2025-01-10 12:06:09",
        "desc": {
            "cn": "BLINK用于评估多模态大模型的视觉感知能力，包含来自14个经典计算机视觉任务的3807道多项选择题。",
            "en": "BLINK focuses on MLLMs' core visual perception abilities. It contains 3,807 multiple-choice questions spanning 14 classic computer vision tasks. "
        }
    },
    {
        "id": "1648",
        "name": "DME",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/yangyue5114/DME",
        "paperLink": "https://arxiv.org/abs/2410.08695",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "86",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-18 17:39:57",
        "supportOnlineEval": false,
        "updateDate": "2025-03-18 17:39:57",
        "createDate": "2025-03-18 17:37:18",
        "desc": {
            "cn": "VLB 为 LVLMs 提供了一种稳健且全面的评估，降低了数据污染并具有灵活的复杂性。基于 LlavaBench 和 MMvet，我们精心制作了两个更具挑战性的数据集版本：LlavaBench_hard 和 MMvet_hard。这些是我们动态策略中最具挑战性的多模态组合（V1+L4）。",
            "en": "VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. Based on LlavaBench and MMvet, we have curated two more challenging versions of the datasets: LlavaBench_hard and MMvet_hard. These are the hardest multimodal combinations."
        }
    },
    {
        "id": "1319",
        "name": "ActionAtlas",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/mrsalehi/action-atlas",
        "paperLink": "https://arxiv.org/abs/2410.05774",
        "officialWebsiteLink": "https://mrsalehi.github.io/action-atlas/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "85",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 15:07:55",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 15:07:55",
        "createDate": "2024-12-31 15:19:23",
        "desc": {
            "cn": "ActionAtlas是一个多项选择视频问答基准测试，包括934个视频，展示了56项运动中的580个独特动作，选项共包含1896个动作。",
            "en": "ActionAtlas is a multiple-choice video question answering benchmark, including 934 videos showcasing 580 unique actions across 56 sports, with a total of 1896 actions within choices."
        }
    },
    {
        "id": "1705",
        "name": "OlymMATH",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            },
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/RUCAIBox/OlymMATH",
        "paperLink": "https://arxiv.org/abs/2503.21380",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "52300476",
            "name": "CoderBak",
            "avatar": null,
            "nickname": "CoderBak"
        },
        "lookNum": "85",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-03 19:46:03",
        "supportOnlineEval": false,
        "updateDate": "2025-04-03 19:46:03",
        "createDate": "2025-04-02 23:55:49",
        "desc": {
            "cn": "一个包含 200 道奥林匹克数学题的基准测试，涵盖代数、几何、数论和组合。我们提供英文和中文版本，并提供两个难度等级：EASY（AIME水平）用于测试标准推理能力，以及 HARD 用于挑战高级模型。",
            "en": "A benchmark of 200 Olympiad math problems across algebra, geometry, number theory, and combinatorics. Available in English and Chinese, it features two difficulty levels: EASY (AIME-level) to test standard reasoning, and HARD to challenge advanced models."
        }
    },
    {
        "id": "1326",
        "name": "MedJourney",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Medical-AI-Learning/MedJourney",
        "paperLink": "https://openreview.net/pdf?id=XXaIoJyYs7",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "84",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-06 16:32:46",
        "supportOnlineEval": false,
        "updateDate": "2025-01-06 16:32:46",
        "createDate": "2025-01-02 20:04:00",
        "desc": {
            "cn": "MedJourney用于评估 LLM 在真实临床环境中的有效性，其中包含多个任务，涵盖来自患者就诊典型流程的4个阶段的12个数据集。",
            "en": "MedJourney offers a comprehensive assessment of LLMs' effectiveness in real-world clinical settings. It includes multiple tasks from 4 stages of a typical patient's hospital visit journey and comprises 12 datasets."
        }
    },
    {
        "id": "1099",
        "name": "MKQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/apple/ml-mkqa",
        "paperLink": "https://arxiv.org/pdf/2007.15207",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "84",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-09 20:03:40",
        "supportOnlineEval": false,
        "updateDate": "2024-10-09 20:03:40",
        "createDate": "2024-10-09 14:12:42",
        "desc": {
            "cn": "MKQA 是一个开放域问答评估集，包含 10,000 对问题和答案，涵盖 26 种类型多样的语言（总计 260,000 对问题和答案）。",
            "en": "MKQA is an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). "
        }
    },
    {
        "id": "1325",
        "name": "LLM-Uncertainty-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/smartyfh/LLM-Uncertainty-Bench",
        "paperLink": "https://arxiv.org/abs/2401.12794",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "83",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-06 14:21:33",
        "supportOnlineEval": false,
        "updateDate": "2025-01-06 14:21:33",
        "createDate": "2025-01-02 19:52:18",
        "desc": {
            "cn": "LLM-Uncertainty-Bench将不确定性纳入LLM评估，包含5个具有代表性的自然语言处理任务，每个任务都有包含10000个实例的数据集支撑。",
            "en": "LLM-Uncertainty-Bench is a new benchmarking approach for LLMs that integrates uncertainty quantification. It spans 5 representative natural language processing tasks, each has a dataset with 10,000 instances."
        }
    },
    {
        "id": "1606",
        "name": "MCiteBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/caiyuhu/MCiteBench",
        "paperLink": "https://arxiv.org/abs/2503.02589",
        "officialWebsiteLink": "https://caiyuhu.github.io/MCiteBench/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "83",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-20 14:11:57",
        "supportOnlineEval": false,
        "updateDate": "2025-03-20 14:11:57",
        "createDate": "2025-03-20 14:11:46",
        "desc": {
            "cn": "MCiteBench 是一个用于评估多模态大模型中多模态引用文本生成的基准，由 1,749 篇学术论文中的 3,000 个样本组成，包括 2,000 个解释任务和 1,000 个定位任务，在文本、图表、表格和混合模态中平衡证据。",
            "en": "MCiteBench is a benchmark to evaluate multimodal citation text generation in MLLMs. It consists of 3,000 samples from 1,749 academic papers, featuring 2,000 Explanation tasks and 1,000 Locating tasks, with balanced evidence across text, figures, tables, and mixed modalities."
        }
    },
    {
        "id": "1120",
        "name": "ProofNet",
        "emoji": "",
        "dimensions": [
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/zhangir-azerbayev/ProofNet",
        "paperLink": "https://arxiv.org/pdf/2302.12433",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "81",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-10 20:21:42",
        "supportOnlineEval": false,
        "updateDate": "2024-10-10 20:21:42",
        "createDate": "2024-10-10 14:54:13",
        "desc": {
            "cn": "ProofNet 是一个用于本科数学的自动形式化和形式证明的基准。包含 371 个示例，每个示例包括一个 Lean 3 中的形式定理陈述、一个自然语言定理陈述和一个自然语言证明。这些问题主要来自流行的本科纯数学教材，涵盖实分析、复分析、线性代数、抽象代数和拓扑等主题。",
            "en": "ProofNet is a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. "
        }
    },
    {
        "id": "1376",
        "name": "GenAI-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/TIGER-AI-Lab/GenAI-Bench",
        "paperLink": "https://arxiv.org/abs/2406.13743",
        "officialWebsiteLink": "https://linzhiqiu.github.io/papers/genai_bench/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "81",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:23:33",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:23:33",
        "createDate": "2025-01-10 17:02:53",
        "desc": {
            "cn": "GenAI-Bench用于衡量MLLM判断AI生成内容质量的能力，包含超过40000个人工评分用以评估大模型与人类偏好的一致性。",
            "en": "GenAI-Bench is a benchmark designed to benchmark MLLMs’s ability in judging the quality of AI generative contents, containing over 40,000 human ratings to evaluate the performance of MLLMs on aligning with human preferences."
        }
    },
    {
        "id": "1579",
        "name": "PosterSum",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/saxenarohit/postersum",
        "paperLink": "https://arxiv.org/abs/2502.17540",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "79",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:46:01",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:46:01",
        "createDate": "2025-03-05 18:56:15",
        "desc": {
            "cn": "PosterSum 数据集是一个多模态基准数据集，旨在将科学海报总结成研究论文摘要。该数据集包含从2022-2024年的主要机器学习会议（包括 ICLR、ICML 和 NeurIPS）收集的 16,305 篇研究海报。",
            "en": "The PosterSum dataset is a multimodal benchmark designed for the summarization of scientific posters into research paper abstracts. The dataset consists of 16,305 research posters collected from major machine learning conferences, including ICLR, ICML, and NeurIPS, spanning the years 2022-2024. "
        }
    },
    {
        "id": "1607",
        "name": "ToolRet",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/mangopy/tool-retrieval-benchmark",
        "paperLink": "https://arxiv.org/abs/2503.01763",
        "officialWebsiteLink": "https://mangopy.github.io/tool-retrieval-benchmark/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "78",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:43:53",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:43:53",
        "createDate": "2025-03-06 14:40:00",
        "desc": {
            "cn": "ToolRet是一个包含 7.6k 个不同检索任务的异构工具检索基准，以及从现有数据集中收集的 43k 个工具语料库。",
            "en": "ToolRet is a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets."
        }
    },
    {
        "id": "1453",
        "name": "MMIU",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/OpenGVLab/MMIU",
        "paperLink": "https://arxiv.org/abs/2408.02718",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "78",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-02-26 18:55:33",
        "supportOnlineEval": false,
        "updateDate": "2025-02-26 18:55:33",
        "createDate": "2025-01-27 14:55:19",
        "desc": {
            "cn": "MMIU用于评估多模态大模型的多图理解能力，包含7种类型的多图像关系、52个任务、77K图像和11K精心策划的多选题。",
            "en": "MMIU test MLLMs' multi-image understanding capabilities, encompassesing 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions."
        }
    },
    {
        "id": "1554",
        "name": "JL1-CD",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/circleLZY/MTKD-CD",
        "paperLink": "https://arxiv.org/abs/2502.13407",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "77",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:46:12",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:46:12",
        "createDate": "2025-03-05 18:54:32",
        "desc": {
            "cn": "JL1-CD 是一个大规模、亚米级、全包含的开源遥感影像变化检测（CD）数据集。它包含 5000 对 512×512 像素的卫星影像，分辨率为 0.5 至 0.75 米，覆盖中国多个地区的各种地表变化。",
            "en": "JL1-CD is a large-scale, sub-meter, all-inclusive open-source dataset for remote sensing image change detection (CD). It contains 5,000 pairs of 512×512 pixel satellite images with a resolution of 0.5 to 0.75 meters, covering various types of surface changes in multiple regions of China. "
        }
    },
    {
        "id": "1145",
        "name": "FREB-TQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/boschresearch/FREB-TQA",
        "paperLink": "https://aclanthology.org/2024.naacl-long.137.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "50109778",
            "name": "Bosch_Research",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/50109778-dd360e6a-5366-4ab9-8d82-260b4e376600.png",
            "nickname": "OpenXLab-XxvqMQwcb"
        },
        "lookNum": "77",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-27 17:23:19",
        "supportOnlineEval": false,
        "updateDate": "2024-10-27 17:23:19",
        "createDate": "2024-10-15 13:24:34",
        "desc": {
            "cn": "FREB-TQA 是一个细粒度的稳健性评估基准，专注于表格问答（TQA）。",
            "en": "FREB-TQA is a Fine-grained Robustness Evaluation Benchmark for Table Question Answering. "
        }
    },
    {
        "id": "1243",
        "name": "EmbodiedAgentInterface",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Embodied Decision Making",
                "en": "Embodied Decision Making"
            }
        ],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/embodied-agent-interface/embodied-agent-interface",
        "paperLink": "https://arxiv.org/abs/2410.07166",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "77",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-25 11:46:31",
        "supportOnlineEval": false,
        "updateDate": "2024-12-25 11:46:31",
        "createDate": "2024-12-20 17:40:13",
        "desc": {
            "cn": "Embodied Agent Interface支持各种类型的任务和基于LLM模块的输入输出规范的形式化，对LLM在不同子任务中的性能进行了全面评估，指出了基于LLM的具身AI系统的优势和劣势。",
            "en": "Embodied Agent Interface supports the formalization of various types of tasks and input-output specifications of LLM-based modules, offering a comprehensive assessment of LLMs' performance for different subtasks and pinpointing the strengths and weaknesses in LLM-powered embodied AI systems."
        }
    },
    {
        "id": "1605",
        "name": "Deepfake-Eval-2024",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/nuriachandra/Deepfake-Eval-2024",
        "paperLink": "https://arxiv.org/abs/2503.02857",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "76",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:45:17",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:45:17",
        "createDate": "2025-03-06 14:04:04",
        "desc": {
            "cn": "Deepfake-Eval-2024 是一个真实场景的深度伪造数据集。该数据集包含44小时的视频、56.5小时的音频和1,975张图像，涵盖当代篡改技术、多样化的媒体内容、来自88个不同网站来源的素材以及52种不同语言。",
            "en": "Deepfake-Eval-2024 is an in-the-wild deepfake dataset. Deepfake-Eval-2024 contains 44 hours of videos, 56.5 hours of audio, and 1,975 images, encompassing contemporary manipulation technologies, diverse media content, 88 different website sources, and 52 different languages. "
        }
    },
    {
        "id": "1618",
        "name": "FedMABench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/wwh0411/FedMABench",
        "paperLink": "https://arxiv.org/abs/2503.05143",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "75",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-10 17:12:07",
        "supportOnlineEval": false,
        "updateDate": "2025-03-10 17:12:07",
        "createDate": "2025-03-10 17:06:15",
        "desc": {
            "cn": "FedMABench 是一个开源的联邦训练和评估移动代理的基准，特别为异构场景设计。",
            "en": "FedMABench is an open-source benchmark for federated training and evaluation of mobile agents, specifically designed for heterogeneous scenarios."
        }
    },
    {
        "id": "1736",
        "name": "MMTB",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/yupeijei1997/MMTB",
        "paperLink": "https://arxiv.org/abs/2504.02623",
        "officialWebsiteLink": "https://harrywgcn.github.io/mmtb-leaderboard/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "90306731",
            "name": null,
            "avatar": null,
            "nickname": "yupeijei1997"
        },
        "lookNum": "75",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-11 17:08:39",
        "supportOnlineEval": false,
        "updateDate": "2025-04-11 17:08:39",
        "createDate": "2025-04-11 15:30:25",
        "desc": {
            "cn": "大型语言模型（LLM）凭借其先进的理解能力和规划能力，在作为工具调用的智能体方面展现出了巨大的潜力。用户越来越依赖基于大型语言模型的智能体，通过迭代交互来解决复杂任务。 然而，现有的基准测试主要是在单任务场景中评估智能体，无法体现现实世界的复杂性。为了填补这一空白，我们提出了 Multi-Mission Tool Bench 测试。在这个基准测试中，每个测试用例都包含多个相互关联的任务。这种设计要求智能体能够动态适应不断变化的需求。此外，所提出的基准测试探索了在固定任务数量下所有可能的任务切换模式。具体而言，我们提出了一个多智能体数据生成框架来构建这个基准测试。我们还提出了一种利用动态决策树来",
            "en": "Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly acce"
        }
    },
    {
        "id": "1581",
        "name": "HoloBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            },
            {
                "cn": "长文本",
                "en": "Long-Context"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/megagonlabs/holobench",
        "paperLink": "https://arxiv.org/abs/2410.11996",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "74",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:46:42",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:46:42",
        "createDate": "2025-03-04 10:53:51",
        "desc": {
            "cn": "HoloBench 是一个用于评估长上下文语言模型（LCLMs）在扩展文本上下文中进行整体推理能力的基准测试。",
            "en": "HoloBench is a benchmark designed to evaluate the ability of long-context language models (LCLMs) to perform holistic reasoning over extended text contexts. "
        }
    },
    {
        "id": "1364",
        "name": "MMT-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/OpenGVLab/MMT-Bench",
        "paperLink": "https://arxiv.org/abs/2404.16006",
        "officialWebsiteLink": "https://mmt-bench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "74",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:23:44",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:23:44",
        "createDate": "2025-01-10 11:56:21",
        "desc": {
            "cn": "MMT-Bench考察多模态大模型的视觉识别、定位、推理和规划能力，包括31325个多选视觉问题，涵盖了32个核心元任务和162个多模态理解子任务。",
            "en": "MMT-Bench is designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. It comprises 31,325 multi-choice visual questions, covering 32 core meta-tasks and 162 subtasks in multimodal understanding."
        }
    },
    {
        "id": "1279",
        "name": "WhodunitBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/jun0wanan/WhodunitBench-Murder_Mystery_Games",
        "paperLink": "https://openreview.net/pdf?id=qmvtDIfbmS",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "74",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-25 18:10:25",
        "supportOnlineEval": false,
        "updateDate": "2024-12-25 18:10:25",
        "createDate": "2024-12-24 20:24:49",
        "desc": {
            "cn": "WhodunitBench用于评估大型多模式代理在复杂任务场景下的动态评估。",
            "en": "WhodunitBench is used to evaluate large multimodal agent under complex tasks and dynamic scenarios."
        }
    },
    {
        "id": "1329",
        "name": "OlympicArena",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/GAIR-NLP/OlympicArena",
        "paperLink": "https://arxiv.org/abs/2406.12753",
        "officialWebsiteLink": "https://gair-nlp.github.io/OlympicArena/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "73",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-06 16:32:39",
        "supportOnlineEval": false,
        "updateDate": "2025-01-06 16:32:39",
        "createDate": "2025-01-03 11:50:50",
        "desc": {
            "cn": "OlympicArena用于评估大模型的认知推理能力，包含来自7个领域、62项国际奥林匹克比赛的11163个双语问题。",
            "en": "OlympicArena evaluates cognitive reasoning abilities. It includes 11,163 bilingual problems spanning seven fields and 62 international Olympic competitions. "
        }
    },
    {
        "id": "1772",
        "name": "S1-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            },
            {
                "cn": "指令跟随",
                "en": "Instruct"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "系统1",
                "en": "系统1"
            },
            {
                "cn": "快思考",
                "en": "快思考"
            },
            {
                "cn": "LRM",
                "en": "LRM"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/WYRipple/S1_Bench",
        "paperLink": "https://arxiv.org/abs/2504.10368",
        "officialWebsiteLink": "",
        "leaderboardLink": true,
        "creatorInfo": {
            "uid": "86305501",
            "name": "WYRipple",
            "avatar": "https://thirdwx.qlogo.cn/mmopen/vi_32/skbupBhJ87rXicbT3SlhqHwQCLK486tTT6yJribPhbj5sMzbyU6F17SBErBljjNXtRqGcCd8ZBt0MzIPsEFz0hdWNg7JRf5EL1X5FDkvqC3xU/132",
            "nickname": "WYRipple"
        },
        "lookNum": "72",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-25 17:00:35",
        "supportOnlineEval": false,
        "updateDate": "2025-04-25 17:00:35",
        "createDate": "2025-04-24 17:48:23",
        "desc": {
            "cn": "S1-Bench是一个新颖的基准，旨在评估大模型在简单任务中的表现，这些任务更倾向于直观的系统1思维，而非深思熟虑的系统2推理。尽管大模型在复杂推理任务中通过明确的思维链取得了显著突破，但它们对深度分析思维的依赖可能限制了其系统1思维能力。此外，目前缺乏评估大模型在需要此类能力的任务中表现的基准。为了填补这一空白，S1-Bench提供了一组简单、多样且自然清晰的问题，涵盖多个领域和语言，专门设计用于评估大模型在此类任务中的表现。",
            "en": "S1-Bench is a novel benchmark designed to evaluate the performance of LRMs on simple tasks that are more aligned with intuitive System 1 thinking, rather than deliberate System 2 reasoning. S1-Bench offers a set of simple, diverse, and naturally clear questions across multiple domains and languages."
        }
    },
    {
        "id": "1609",
        "name": "MASK",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/centerforaisafety/mask",
        "paperLink": "https://arxiv.org/abs/2503.03750",
        "officialWebsiteLink": "https://www.mask-benchmark.ai/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "71",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:43:36",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:43:36",
        "createDate": "2025-03-06 16:39:16",
        "desc": {
            "cn": "MASK 评估提供了一个严格的基准，用于评估大型语言模型中的诚实度，通过测量模型在受到诱使说谎的激励时是否保持真实性。公共集包含 1,028 个高质量的人标注示例，涵盖六个不同的原型。",
            "en": "The MASK evaluation provides a rigorous benchmark for evaluating honesty in large language models by measuring whether models remain truthful when incentivized to lie. The public set contains 1,028 high-quality human-labeled examples across six distinct archetypes."
        }
    },
    {
        "id": "1278",
        "name": "ChronoMagic-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "创作",
                "en": "Creation"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/PKU-YuanGroup/ChronoMagic-Bench",
        "paperLink": "https://arxiv.org/abs/2406.18522",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "71",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 14:14:44",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 14:14:44",
        "createDate": "2025-01-07 14:13:34",
        "desc": {
            "cn": "ChronoMagic-Bench用来评估 T2V （文本到视频 ）模型在延时视频生成中的时间和变形能力，引入了1649个提示和真实世界的视频作为参考。",
            "en": "ChronoMagic-Bench can evaluate the temporal and metamorphic capabilities of the T2V (text-to-video) models in time-lapse video generation, introducing 1,649 prompts and real-world videos as references."
        }
    },
    {
        "id": "1280",
        "name": "AMBROSIA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/saparina/ambrosia",
        "paperLink": "https://arxiv.org/abs/2406.19073",
        "officialWebsiteLink": "https://ambrosia-benchmark.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "70",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-25 14:31:36",
        "supportOnlineEval": false,
        "updateDate": "2024-12-25 14:31:36",
        "createDate": "2024-12-24 20:31:47",
        "desc": {
            "cn": "AMBROSIA是识别和解释text-to-SQL中歧义请求的新基准，其中包含三种不同类型的歧义（范围歧义、附件歧义和模糊性）问题、它们的解释和相应的SQL查询。",
            "en": "AMBROSIA is a new benchmark for recognizing and interpreting ambiguous requests in text-to-SQL. It contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries."
        }
    },
    {
        "id": "1537",
        "name": "MVL-SIB",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            },
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2502.12852",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "70",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-03 11:48:04",
        "supportOnlineEval": false,
        "updateDate": "2025-03-03 11:48:04",
        "createDate": "2025-02-24 18:19:14",
        "desc": {
            "cn": "MVL-SIB 是一个多语言数据集，提供了涵盖 205 种语言和 7 个主题类别的图像-句子对（ entertainment ， geography ， health ， politics ， science ， sports ， travel ）。它通过扩展 SIB-200 基准构建而成。",
            "en": "MVL-SIB is a multilingual dataset that provides image-sentence pairs spanning 205 languages and 7 topical categories (entertainment, geography, health, politics, science, sports, travel). It was constructed by extending the SIB-200 benchmark."
        }
    },
    {
        "id": "1524",
        "name": "RM-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "VQA",
                "en": "VQA"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/THU-KEG/RM-Bench",
        "paperLink": "https://arxiv.org/abs/2410.16184",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "70",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 17:00:31",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 17:00:31",
        "createDate": "2025-02-21 11:34:57",
        "desc": {
            "cn": "RM-Bench，一个用于评估语言模型奖励模型的基准数据集",
            "en": "RM-Bench, a benchmark dataset for evaluating reward models of language modeling."
        }
    },
    {
        "id": "1358",
        "name": "Video-MME",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/BradyFU/Video-MME",
        "paperLink": "https://github.com/BradyFU/Video-MME",
        "officialWebsiteLink": "https://video-mme.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "69",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:23:57",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:23:57",
        "createDate": "2025-01-09 21:09:31",
        "desc": {
            "cn": "Video-MME用于评估多模态大模型的视频分析能力，包含900个不同长度的视频，来自6个主要视觉领域和30个子领域，总时长达254小时。",
            "en": "Video-MME is an evaluation benchmark of multi-modal LLMs in video analysis, including 900 videos in various duration with a total of 254 hours which spans 6 primary visual domains with 30 subfields."
        }
    },
    {
        "id": "1500",
        "name": "CRPE",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/OpenGVLab/all-seeing",
        "paperLink": "https://arxiv.org/abs/2402.19474",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "69",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:46:25",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:46:25",
        "createDate": "2025-02-13 19:32:36",
        "desc": {
            "cn": "CRPE用于定量评估多模态大模型的对象识别和关系理解能力，分为四部分，以单选题形式呈现。",
            "en": "CRPE is a benchmark designed to quantitatively evaluate the object recognition and relation comprehension ability of models. It consists of four splits, and the evaluation is formulated as single-choice questions. "
        }
    },
    {
        "id": "1518",
        "name": "miniCTX",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            },
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Strong Reasoning",
                "en": "Strong Reasoning"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/cmu-l3/minictx-eval",
        "paperLink": "https://www.arxiv.org/pdf/2408.03350",
        "officialWebsiteLink": "https://cmu-l3.github.io/minictx/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "69",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:53:10",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:53:10",
        "createDate": "2025-02-20 15:38:03",
        "desc": {
            "cn": "一个用于评估现实场景中神经定理证明的丰富上下文基准。通过提供前提、完整上下文、多源基准和时序分割，突出其评估模型处理随时间演变的上下文能力。",
            "en": "A context-rich benchmark for evaluating neural theorem proving in realistic scenarios, providing premises, full context, multi-source benchmark, and temporal splits, enabling evaluation of a model's ability to work with context that evolves over time."
        }
    },
    {
        "id": "1318",
        "name": "WikiContradict",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2406.13805",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "68",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-06 14:21:41",
        "supportOnlineEval": false,
        "updateDate": "2025-01-06 14:21:41",
        "createDate": "2024-12-31 14:05:23",
        "desc": {
            "cn": "WikiContradict旨在评估LLM遇到包含真实世界知识冲突的段落检索增强时的性能，由253个高质量的人工注释实例组成。",
            "en": "WikiContradict is a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts."
        }
    },
    {
        "id": "1580",
        "name": "A-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Q-Future/A-Bench",
        "paperLink": "https://arxiv.org/abs/2406.03070",
        "officialWebsiteLink": "https://a-bench-sjtu.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "68",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:46:33",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:46:33",
        "createDate": "2025-03-04 10:43:10",
        "desc": {
            "cn": "A-Bench是一个旨在诊断 LMMs 是否擅长评估 AIGIs 的基准，从 16 个文本到图像模型中采样了 2,864 个 AIGIs，每个都与由人类专家标注的问题-答案配对。",
            "en": " A-Bench is a benchmark designed to diagnose whether LMMs are masters at evaluating AIGIs. 2,864 AIGIs from 16 text-to-image models are sampled, each paired with question-answers annotated by human experts, and tested across 18 leading LMMs."
        }
    },
    {
        "id": "1147",
        "name": "M3T",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NAACL 2024",
                "en": "NAACL 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/amazon-science/m3t-multi-modal-translation-bench",
        "paperLink": "https://aclanthology.org/2024.naacl-short.41.pdf",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "5010775",
            "name": "amazon-science",
            "avatar": null,
            "nickname": "OpenXLab-hzYw4sVeC"
        },
        "lookNum": "68",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-10-27 17:23:28",
        "supportOnlineEval": false,
        "updateDate": "2024-10-27 17:23:28",
        "createDate": "2024-10-15 13:57:09",
        "desc": {
            "cn": "M3T 是旨在评估神经机器翻译（NMT）系统在翻译半结构化文档的综合任务上的表现。",
            "en": "M3T is a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents. This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications."
        }
    },
    {
        "id": "1534",
        "name": "CHASE-Code",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            },
            {
                "cn": "数学",
                "en": "Math"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/McGill-NLP/CHASE",
        "paperLink": "https://arxiv.org/pdf/2502.14678",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "68",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 17:02:59",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 17:02:59",
        "createDate": "2025-02-24 14:12:19",
        "desc": {
            "cn": "CHASE是一个无需人工参与的统一框架，用于合成生成具有挑战性的问题",
            "en": "CHASE is a unified framework to synthetically generate challenging problems using LLMs without human involvement"
        }
    },
    {
        "id": "1574",
        "name": "Text2World",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Aaron617/text2world",
        "paperLink": "https://arxiv.org/abs/2502.13092",
        "officialWebsiteLink": "https://text-to-world.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "67",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-04 16:58:03",
        "supportOnlineEval": false,
        "updateDate": "2025-03-04 16:58:03",
        "createDate": "2025-03-03 14:38:08",
        "desc": {
            "cn": "Text2World 基于规划领域定义语言 (PDDL)，拥有数百个不同的领域，并采用多标准、基于执行的衡量标准来进行更稳健的评估。",
            "en": "Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. "
        }
    },
    {
        "id": "1327",
        "name": "EHRNoteQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "知识",
                "en": "Knowledge"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/ji-youn-kim/EHRNoteQA",
        "paperLink": "https://arxiv.org/abs/2402.16040",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "67",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-06 16:32:43",
        "supportOnlineEval": false,
        "updateDate": "2025-01-06 16:32:43",
        "createDate": "2025-01-02 20:15:09",
        "desc": {
            "cn": "EHRNoteQA用于评估LLM基于电子健康记录辅助临床决策的能力，由962个问答对组成，每个问答对都与不同患者的出院总结相关联。",
            "en": "EHRNoteQA evaluates LLM's ability to assist clinical decision-making based on electronic health records, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. "
        }
    },
    {
        "id": "1608",
        "name": "SwiLTra-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2503.01372",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "67",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:43:44",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:43:44",
        "createDate": "2025-03-06 15:15:16",
        "desc": {
            "cn": "SwiLTra-Bench是一个包含超过 18 万对对齐的瑞士法律翻译语料库的全面多语言基准，包括所有瑞士语言以及英语的法律、摘要和新闻稿，旨在评估基于LLM的翻译系统。",
            "en": "SwiLTra-Bench is a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems."
        }
    },
    {
        "id": "1321",
        "name": "ShoppingMMLU",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/KL4805/ShoppingMMLU",
        "paperLink": "https://arxiv.org/abs/2410.20745",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "65",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-06 14:21:36",
        "supportOnlineEval": false,
        "updateDate": "2025-01-06 14:21:36",
        "createDate": "2024-12-31 15:44:55",
        "desc": {
            "cn": "Shopping MMLU是一个基于真实亚马逊数据的多样化多任务在线购物基准测试，由57项任务组成，涵盖概念理解、知识推理、用户行为对齐和多语言4大购物场景技能。",
            "en": "Shopping MMLU is a diverse multi-task online shopping benchmark derived from real-world Amazon data. It consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality."
        }
    },
    {
        "id": "1360",
        "name": "LLaVA-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/haotian-liu/LLaVA/blob/main/docs/LLaVA_Bench.md",
        "paperLink": "",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "65",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:23:49",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:23:49",
        "createDate": "2025-01-09 21:47:19",
        "desc": {
            "cn": "LLaVA-Bench用于评估多模态大模型应对复杂任务的能力，内含24 张图像及60个问题，包括室内和室外场景、模因、绘画、素描等。",
            "en": "LLaVA-Bench evaluates the model's ability in more challenging tasks, including a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc."
        }
    },
    {
        "id": "1539",
        "name": "MM-RLHF",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "VQA",
                "en": "VQA"
            },
            {
                "cn": "Multimodal",
                "en": "Multimodal"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Kwai-YuanQi/MM-RLHF",
        "paperLink": "https://arxiv.org/abs/2406.08487",
        "officialWebsiteLink": "https://mm-rlhf.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "64",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:50:29",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:50:29",
        "createDate": "2025-02-24 19:28:08",
        "desc": {
            "cn": "MM-RLHF，这是一个将多模态大型语言模型（MLLMs）与人类偏好对齐的全面项目，使开源多语言机器学习模型在 10 个维度和 27 个基准测试中实现持续的性能提升。",
            "en": " MM-RLHF, a comprehensive project for aligning Multimodal Large Language Models (MLLMs) with human preferences. The dataset and algorithms enable consistent performance improvements across 10 dimensions and 27 benchmarks for open-source MLLMs."
        }
    },
    {
        "id": "1356",
        "name": "MM-Vet",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/yuweihao/MM-Vet",
        "paperLink": "https://arxiv.org/abs/2308.02490",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "64",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:24:00",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:24:00",
        "createDate": "2025-01-09 20:41:34",
        "desc": {
            "cn": "MM-Vet用于评估复杂多模态任务能力，涵盖了6个核心视觉语言功能的16种功能组合。",
            "en": "MM-Vet evaluates the capabilities to deal with complicated multimodal tasks, defining 6 core VL capabilities and examining the 16 integrations of interest derived from the capability combination."
        }
    },
    {
        "id": "1557",
        "name": "AIRBench-2024",
        "emoji": "",
        "dimensions": [
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/stanford-crfm/air-bench-2024",
        "paperLink": "https://arxiv.org/abs/2407.17436",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "63",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:50:05",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:50:05",
        "createDate": "2025-02-26 17:30:51",
        "desc": {
            "cn": "AIR-Bench 2024是首个与新兴政府法规和企业政策相一致的 AI 安全基准， 将 8 项政府法规和 16 项企业政策分解为四级安全分类，涵盖了这些类别的 5,694 个多样化的提示。",
            "en": "AIR-Bench 2024, the first policy-aligned AI safety benchmark, structures 8 government regulations and 16 corporate policies into four security tiers, with 5,694 diverse prompts spanning these categories."
        }
    },
    {
        "id": "1587",
        "name": "MR-GSM8K",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/dvlab-research/MR-GSM8K",
        "paperLink": "https://arxiv.org/abs/2312.17080",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "63",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:47:12",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:47:12",
        "createDate": "2025-03-04 14:13:29",
        "desc": {
            "cn": "MR-GSM8K 是一个旨在评估最先进大型语言模型（LLMs）元推理能力的挑战性基准。它超越了传统的评估指标，专注于推理过程而非仅仅关注最终答案，从而对模型的认知能力进行更细致的评估。",
            "en": "MR-GSM8K is a challenging benchmark designed to evaluate the meta-reasoning capabilities of state-of-the-art Large Language Models (LLMs). It goes beyond traditional evaluation metrics by focusing on the reasoning process rather than just the final answer."
        }
    },
    {
        "id": "1267",
        "name": "SpreadsheetBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/RUCKBReasoning/SpreadsheetBench",
        "paperLink": "https://arxiv.org/abs/2406.14991",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "63",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-25 14:11:06",
        "supportOnlineEval": false,
        "updateDate": "2024-12-25 14:11:06",
        "createDate": "2024-12-24 14:50:13",
        "desc": {
            "cn": "SpreadsheetBench是一个具有挑战性的电子表格操作基准测试，包含912个来自在线Excel论坛的真实问题",
            "en": "SpreadsheetBench is a challenging spreadsheet manipulation benchmark built from 912 real questions gathered from online Excel forums."
        }
    },
    {
        "id": "1274",
        "name": "IMDL-BenCo",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/scu-zjz/IMDLBenCo",
        "paperLink": "https://arxiv.org/abs/2406.10580",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "63",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2024-12-30 16:30:17",
        "supportOnlineEval": false,
        "updateDate": "2024-12-30 16:30:17",
        "createDate": "2024-12-30 16:26:51",
        "desc": {
            "cn": "IMDL-BenCo提供了全面的IMDL基准测试和模块化代码库。",
            "en": "IMDL-BenCo offers a comprehensive IMDL benchmark and modular codebase."
        }
    },
    {
        "id": "1362",
        "name": "POPE",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/AoiDragon/POPE",
        "paperLink": "https://arxiv.org/abs/2305.10355",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "61",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:23:47",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:23:47",
        "createDate": "2025-01-10 10:53:21",
        "desc": {
            "cn": "POPE用于评估视觉语言模型的物体幻觉，基于轮询的查询方法设计，提供了一种更稳定、更灵活的评估方案。",
            "en": "POPE is an improved evaluation method for LVLMs' object hallucination by proposing a polling-based query method. It offers a more stable and flexible solution."
        }
    },
    {
        "id": "1680",
        "name": "BigOBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/facebookresearch/bigobench",
        "paperLink": "https://arxiv.org/abs/2503.15242",
        "officialWebsiteLink": "https://facebookresearch.github.io/BigOBench/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "61",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-26 15:56:21",
        "supportOnlineEval": false,
        "updateDate": "2025-03-26 15:56:21",
        "createDate": "2025-03-26 14:44:38",
        "desc": {
            "cn": "BigO(Bench)是一个包含约 300 个需要用 Python 解决的代码问题的基准测试，以及 3,105 个编码问题和 1,190,250 个解决方案用于训练，以评估LLMs能否找到代码解决方案的时间-空间复杂度，或者生成符合时间-空间复杂度要求的代码解决方案。",
            "en": "BigO(Bench)是一个包含约 300 个需要用 Python 解决的代码问题的基准测试，以及 3,105 个编码问题和 1,190,250 个解决方案用于训练，以评估LLMs能否找到代码解决方案的时间-空间复杂度，或者生成符合时间-空间复杂度要求的代码解决方案。"
        }
    },
    {
        "id": "1320",
        "name": "IaC-Eval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/autoiac-project/iac-eval",
        "paperLink": "https://openreview.net/pdf?id=7TCK0aBL1C",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "60",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-06 14:21:38",
        "supportOnlineEval": false,
        "updateDate": "2025-01-06 14:21:38",
        "createDate": "2024-12-31 15:35:20",
        "desc": {
            "cn": "IaC-Eval用于定量评估LLM在云IaC代码生成中的功能，其中包含458个从易到难的问题，涵盖了各种云服务。",
            "en": "IaC-Eval is meant for quantitatively evaluating the capabilities of LLMs in cloud IaC code generation, containing 458 questions ranging from simple to difficult across various cloud services."
        }
    },
    {
        "id": "1396",
        "name": "AV-Odyssey-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "audio-visual",
                "en": "audio-visual"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/AV-Odyssey/AV-Odyssey",
        "paperLink": "https://arxiv.org/pdf/2412.02611",
        "officialWebsiteLink": "https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "40068663",
            "name": null,
            "avatar": null,
            "nickname": "豪༙྇"
        },
        "lookNum": "60",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-15 18:34:14",
        "supportOnlineEval": false,
        "updateDate": "2025-01-15 18:34:14",
        "createDate": "2025-01-15 17:26:48",
        "desc": {
            "cn": "AV-Odyssey Bench. This benchmark encompasses 26 different tasks and 4,555 carefully crafted problems, each incorporating text, visual, and audio components. All data are newly collected and annotated by humans, not from any existing audio-visual dataset.",
            "en": "AV-Odyssey Bench. This benchmark encompasses 26 different tasks and 4,555 carefully crafted problems, each incorporating text, visual, and audio components. All data are newly collected and annotated by humans, not from any existing audio-visual dataset."
        }
    },
    {
        "id": "1547",
        "name": "MMIR",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/eric-ai-lab/MMIR",
        "paperLink": "https://arxiv.org/abs/2502.16033",
        "officialWebsiteLink": "https://jackie-2000.github.io/mmir.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "59",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:48:36",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:48:36",
        "createDate": "2025-02-26 11:11:06",
        "desc": {
            "cn": "用于评估多模态模型（MLLM）检测和推理布局丰富的多模态内容中的不一致性的基准。MMIR 包含 534 个具有挑战性的样本，涉及五个推理能力较强的不一致类别。",
            "en": "A benchmark for evaluating Multimodal Large Language Models (MLLMs) on detecting and reasoning about inconsistencies in layout-rich multimodal content. MMIR features 534 challenging samples across five reasoning-heavy inconsistency categories."
        }
    },
    {
        "id": "1317",
        "name": "RepLiQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/ServiceNow/repliqa",
        "paperLink": "https://arxiv.org/abs/2406.11811",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "59",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-06 14:21:43",
        "supportOnlineEval": false,
        "updateDate": "2025-01-06 14:21:43",
        "createDate": "2024-12-31 13:56:35",
        "desc": {
            "cn": "RepLiQA适用于问答和主题检索任务，集合了是5个测试集；只有当模型可以在提供的文档中找到相关内容时，才能生成准确的答案。",
            "en": "RepLiQA is suited for question-answering and topic retrieval tasks, including collection of five splits of test sets. Accurate answers can only be generated if a model can find relevant content within the provided document."
        }
    },
    {
        "id": "1654",
        "name": "V-STaR",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/V-STaR-Bench/V-STaR",
        "paperLink": "https://arxiv.org/abs/2503.11495",
        "officialWebsiteLink": "https://v-star-bench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "59",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-20 14:27:09",
        "supportOnlineEval": false,
        "updateDate": "2025-03-20 14:27:09",
        "createDate": "2025-03-20 13:44:36",
        "desc": {
            "cn": "V-STaR 是一个针对 Video-LLMs的空间时间推理基准，评估 Video-LLM在“何时”、“何地”和“何物”的上下文中明确回答问题的空间时间推理能力。",
            "en": "V-STaR is a spatio-temporal reasoning benchmark for Video-LLMs, evaluating Video-LLM’s spatio-temporal reasoning ability in answering questions explicitly in the context of “when”, “where”, and “what”."
        }
    },
    {
        "id": "1584",
        "name": "MMSearch",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/CaraJ7/MMSearch",
        "paperLink": "https://arxiv.org/abs/2409.12959",
        "officialWebsiteLink": "https://mmsearch.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "58",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:47:36",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:47:36",
        "createDate": "2025-03-04 11:25:58",
        "desc": {
            "cn": "MMSearch 是一个多模态搜索基准，旨在评估大型语言模型（LMMs）作为多模态 AI 搜索引擎的潜力。该基准包含了一个精心收集的包含 300 个查询的数据集，涵盖 14 个子领域。",
            "en": "Logo MMSearch is a multimodal search benchmark crafted to evaluate the potential of LMMs to function as a multimodal AI search engine. This benchmark encompasses a meticulously collected dataset of 300 queries spanning 14 subfields. "
        }
    },
    {
        "id": "1625",
        "name": "ubuntu_osworld",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/xlang-ai/OSWorld",
        "paperLink": "https://arxiv.org/abs/2404.07972",
        "officialWebsiteLink": "https://os-world.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "58",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-12 11:07:38",
        "supportOnlineEval": false,
        "updateDate": "2025-03-12 11:07:38",
        "createDate": "2025-03-11 16:09:49",
        "desc": {
            "cn": "OSWorld 是一个首创的、可扩展的、真实计算机环境，用于多模态智能体，支持操作系统跨平台的任务设置、基于执行的评估和交互式学习。",
            "en": "OSWorld is a first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across operating systems. "
        }
    },
    {
        "id": "1277",
        "name": "VideoGUI",
        "emoji": "",
        "dimensions": [
            {
                "cn": "创作",
                "en": "Creation"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [
            {
                "cn": "NeurIPS 2024",
                "en": "NeurIPS 2024"
            }
        ],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/showlab/videogui",
        "paperLink": "https://arxiv.org/abs/2406.10227",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "58",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-07 14:14:48",
        "supportOnlineEval": false,
        "updateDate": "2025-01-07 14:14:48",
        "createDate": "2025-01-07 14:13:51",
        "desc": {
            "cn": "VideoGUI旨在评估以视觉为中心的GUI任务上的GUI助手，来自高质量的网络教学视频，侧重于涉及专业和新颖软件和复杂活动（例如视频编辑）的任务。",
            "en": "VideoGUI is designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, it focuses on tasks involving professional and novel software and complex activities (e.g., video editing). "
        }
    },
    {
        "id": "1553",
        "name": "OmniAlign-V",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/PhoenixZ810/OmniAlign-V",
        "paperLink": "https://arxiv.org/abs/2502.18411",
        "officialWebsiteLink": "https://phoenixz810.github.io/OmniAlign-V",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "57",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:49:29",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:49:29",
        "createDate": "2025-02-26 14:41:09",
        "desc": {
            "cn": "OmniAlign-V 数据集主要关注提高多模态大型语言模型（MLLMs）与人类偏好的对齐。它包含 205k 个高质量的图像-问答对，包含开放式、创意性问题以及长篇、知识丰富、内容全面的答案。",
            "en": "OmniAlign-V datasets mainly focus on improving the alignment of Multi-modal Large Language Models(MLLMs) with human preference. It contains 205k high-quality Image-Quetion-Answer pairs with open-ended, creative quetions and long, knowledge-rich, comprehensive answers."
        }
    },
    {
        "id": "1586",
        "name": "MRAG-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "VQA",
                "en": "VQA"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/mragbench/MRAG-Bench",
        "paperLink": "https://arxiv.org/abs/2410.08182",
        "officialWebsiteLink": "https://mragbench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "57",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:47:20",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:47:20",
        "createDate": "2025-03-04 13:56:04",
        "desc": {
            "cn": "MRAG-Bench 包含 16,130 张图片和 1,353 个跨越 9 个不同场景的人标注多选题，为大型视觉语言模型（LVLM）的视觉中心多模态检索增强生成（RAG）能力提供了稳健和系统的评估。",
            "en": "MRAG-Bench consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct scenarios, providing a robust and systematic evaluation of Large Vision Language Model (LVLM)’s vision-centric multimodal retrieval-augmented generation (RAG) abilities."
        }
    },
    {
        "id": "1578",
        "name": "MMKE-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/MMKE-Bench-ICLR/MMKE-Bench",
        "paperLink": "https://arxiv.org/abs/2502.19870",
        "officialWebsiteLink": "https://mmke-bench-iclr.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "55",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-04 17:02:34",
        "supportOnlineEval": false,
        "updateDate": "2025-03-04 17:02:34",
        "createDate": "2025-03-03 16:27:20",
        "desc": {
            "cn": "MMKE-Bench是一个旨在评估 LMM 在现实场景中编辑视觉知识能力的基准，包括 33 个广泛类别中的 2,940 条知识和 8,363 张图像，以及自动生成并由人工验证的评估问题。",
            "en": " MMKE-Bench is a benchmark designed to evaluate the ability of LMMs to edit visual knowledge in real-world scenarios. It includes 2,940 pieces of knowledge and 8,363 images across 33 broad categories, with automatically generated, human-verified evaluation questions."
        }
    },
    {
        "id": "1583",
        "name": "JudgeBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/ScalerLab/JudgeBench",
        "paperLink": "https://arxiv.org/abs/2410.12784",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "55",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:48:25",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:48:25",
        "createDate": "2025-03-04 11:06:42",
        "desc": {
            "cn": "JudgeBench 是一个旨在评估基于LLM的裁判在具有挑战性的响应对上的客观正确性的基准。",
            "en": "JudgeBench is a benchmark aimed at evaluating LLM-based judges for objective correctness on challenging response pairs."
        }
    },
    {
        "id": "1363",
        "name": "SEED-Bench-2",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
        "paperLink": "https://arxiv.org/abs/2311.17092",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "55",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-14 15:23:52",
        "supportOnlineEval": false,
        "updateDate": "2025-01-14 15:23:52",
        "createDate": "2025-01-10 11:31:59",
        "desc": {
            "cn": "SEED-Bench-2用于评估多模态大模型的文本和图像生成能力，包括跨越27个维度的24K道多选题及准确的人工注释。",
            "en": "SEED-Bench-2 assesses both text and image generation of MLLMs. It spans 27 evaluation dimensions, featuring 24K multiple-choice questions with precise human annotations."
        }
    },
    {
        "id": "1635",
        "name": "KnowLogic",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/pokerwf/KnowLogic",
        "paperLink": "https://arxiv.org/abs/2503.06218",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "55",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-13 15:31:21",
        "supportOnlineEval": false,
        "updateDate": "2025-03-13 15:31:21",
        "createDate": "2025-03-13 14:36:03",
        "desc": {
            "cn": "KnowLogic 是一个以知识驱动的合成基准，旨在评估大型语言模型的推理能力（LLMs）。它包含涵盖各个领域、涵盖常识知识和逻辑推理不同方面的 5400 个中英双语问题。",
            "en": "KnowLogic is a knowledge-driven synthetic benchmark designed to evaluate the reasoning abilities of large language models (LLMs). It includes 5400 bilingual (Chinese and English) questions across various domains, covering different aspects of commonsense knowledge and logical reasoning."
        }
    },
    {
        "id": "1636",
        "name": "UrbanVideo-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/EmbodiedCity/UrbanVideo-Bench.code",
        "paperLink": "https://arxiv.org/abs/2503.06157",
        "officialWebsiteLink": "https://embodiedcity.github.io/UrbanVideo-Bench/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "55",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-13 15:31:18",
        "supportOnlineEval": false,
        "updateDate": "2025-03-13 15:31:18",
        "createDate": "2025-03-13 14:54:33",
        "desc": {
            "cn": "UrbanVideo-Bench旨在评估视频大型语言模型（Video-LLMs）是否能够像人类一样自然地处理连续的第一人称视觉观察，实现回忆、感知、推理和导航。",
            "en": "The benchmark is designed to evaluate whether video-large language models (Video-LLMs) can naturally process continuous first-person visual observations like humans, enabling recall, perception, reasoning, and navigation."
        }
    },
    {
        "id": "1645",
        "name": "MastermindEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/flairNLP/mastermind",
        "paperLink": "https://arxiv.org/abs/2503.05891",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "54",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-18 16:17:33",
        "supportOnlineEval": false,
        "updateDate": "2025-03-18 16:17:33",
        "createDate": "2025-03-18 16:08:48",
        "desc": {
            "cn": "MastermindEval使用猜谜游戏棋盘评估大型语言模型的推理能力。\n",
            "en": "Evaluating Reasoning Capabilities of LLMs Using the Mastermind Board Game."
        }
    },
    {
        "id": "1585",
        "name": "MMAD",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "VQA",
                "en": "VQA"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/jam-cc/MMAD",
        "paperLink": "https://arxiv.org/abs/2410.09453",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "53",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:47:27",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:47:27",
        "createDate": "2025-03-04 13:48:12",
        "desc": {
            "cn": "MMAD是第一个工业异常检测领域的全谱 MLLMs 基准，研究人员定义了工业检测中 MLLMs 的七个关键子任务，并设计了一个新颖的流程来生成包含 39,672 个问题以及 8,366 个工业图像的 MMAD 数据集。",
            "en": "MMAD is the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. \nResearchers defined seven key subtasks of MLLMs in industrial inspection and designed a novel pipeline to generate the MMAD dataset with 39,672 questions for 8,366 industrial images. "
        }
    },
    {
        "id": "1717",
        "name": "ViLBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "vision-language",
                "en": "vision-language"
            },
            {
                "cn": "math",
                "en": "math"
            },
            {
                "cn": "reasoning",
                "en": "reasoning"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2503.20271",
        "officialWebsiteLink": "https://ucsc-vlaa.github.io/ViLBench/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "62408676",
            "name": null,
            "avatar": null,
            "nickname": "ImKe"
        },
        "lookNum": "53",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-08 10:14:31",
        "supportOnlineEval": false,
        "updateDate": "2025-04-08 10:14:31",
        "createDate": "2025-04-08 02:53:29",
        "desc": {
            "cn": "ViLBench 是一项旨在评估视觉-语言模型的数据集，其强调对模型进行细粒度的逐步推理能力测试。该基准共包含600个经过严格筛选的样本，来源于五个不同的视觉-语言数据集，筛选标准是在模型答案选择过程中，过程奖励模型（process-reward model）相较于输出奖励模型（output-reward model）具有更显著的性能提升效果。",
            "en": "ViLBench is a benchmark designed to evaluate vision-language models. It features 600 examples from 5 datasets, selected based on the criterion that process reward models offer greater improvements over output reward models in guiding generations."
        }
    },
    {
        "id": "1510",
        "name": "LongVideoBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/longvideobench/LongVideoBench",
        "paperLink": "https://arxiv.org/abs/2407.15754",
        "officialWebsiteLink": "https://longvideobench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "53",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:51:19",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:51:19",
        "createDate": "2025-02-17 15:50:45",
        "desc": {
            "cn": "LongVideoBench用于评估多模态大模型的长视频理解能力，是基于交错长视频语料构建的问答集，包含3763个不同主题的带字幕的视频。",
            "en": "LongVideoBench tests LMMs' understanding of long videos. It's a question-answering benchmark with video-language interleaved inputs up to an hour long and comprises 3,763 web-collected videos with subtitles across diverse themes,"
        }
    },
    {
        "id": "1513",
        "name": "CG-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/CG-Bench/CG-Bench",
        "paperLink": "https://arxiv.org/abs/2412.12075v1",
        "officialWebsiteLink": "https://cg-bench.github.io/leaderboard/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "53",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:51:42",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:51:42",
        "createDate": "2025-02-17 16:29:23",
        "desc": {
            "cn": "CG-Bench用于评估多模态大模型的长视频理解能力，基于1219个视频设计了12129个涵盖感知、推理和幻觉三种问题类型的QA对。",
            "en": "CG-Bench is meant for evaluating MLLMs' long video understanding, including 12,129 QA pairs from 1219 videos in 3 major question types: perception, reasoning, and hallucination."
        }
    },
    {
        "id": "1571",
        "name": "BRIGHT",
        "emoji": "",
        "dimensions": [
            {
                "cn": "强推理",
                "en": "Strong Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/xlang-ai/BRIGHT",
        "paperLink": "https://arxiv.org/abs/2407.12883",
        "officialWebsiteLink": "https://brightbenchmark.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "52",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:50:14",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:50:14",
        "createDate": "2025-02-27 18:29:17",
        "desc": {
            "cn": "BRIGHT 是第一个需要大量推理来检索相关文档的文本检索基准。",
            "en": "BRIGHT is the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. "
        }
    },
    {
        "id": "1658",
        "name": "MiLiC-Eval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/luciusssss/MiLiC-Eval",
        "paperLink": "https://arxiv.org/abs/2503.01150",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "52",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-20 14:26:55",
        "supportOnlineEval": false,
        "updateDate": "2025-03-20 14:26:55",
        "createDate": "2025-03-20 14:21:29",
        "desc": {
            "cn": "MiLiC-Eval 是针对中国少数民族语言的 NLP 评估套件，涵盖藏语（bo）、维吾尔语（ug）、哈萨克语（kk，使用哈萨克阿拉伯文脚本）和蒙古语（mn，使用传统蒙古文脚本）。",
            "en": "MiLiC-Eval is an NLP evaluation suite for Minority Languages in China, covering Tibetan (bo), Uyghur (ug), Kazakh (kk, in the Kazakh Arabic script), and Mongolian (mn, in the traditional Mongolian script)."
        }
    },
    {
        "id": "1671",
        "name": "Forensics-bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "安全",
                "en": "Safety"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Forgery Detection",
                "en": "Forgery Detection"
            },
            {
                "cn": "Large Vision Language Models",
                "en": "Large Vision Language Models"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Forensics-Bench/Forensics-Bench",
        "paperLink": "https://arxiv.org/pdf/2503.15024",
        "officialWebsiteLink": "https://forensics-bench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "5001578",
            "name": null,
            "avatar": null,
            "nickname": "劲劲"
        },
        "lookNum": "52",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-22 12:30:46",
        "supportOnlineEval": false,
        "updateDate": "2025-04-22 12:30:46",
        "createDate": "2025-03-24 22:19:51",
        "desc": {
            "cn": "A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models",
            "en": "A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models"
        }
    },
    {
        "id": "1633",
        "name": "ProJudge",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/jiaxin-ai/ProJudge",
        "paperLink": "https://arxiv.org/abs/2503.06553",
        "officialWebsiteLink": "https://projudge.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "50",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-13 15:31:28",
        "supportOnlineEval": false,
        "updateDate": "2025-03-13 15:31:28",
        "createDate": "2025-03-13 14:14:35",
        "desc": {
            "cn": "ProJudge 是一个针对基于 MLLM 的过程裁判能力的全面、多模态、多学科和多难度的基准。它包含 2,400 个测试案例和 50,118 个步骤级标签，涵盖四个科学学科，难度级别和内容多样化。",
            "en": "ProJudge is a comprehensive, multi-modal, multi-discipline, and multi-difficulty benchmark specifically designed for evaluating abilities of MLLM-based process judges.It comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and"
        }
    },
    {
        "id": "1634",
        "name": "VisualSimpleQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2503.06492",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "50",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-13 15:31:25",
        "supportOnlineEval": false,
        "updateDate": "2025-03-13 15:31:25",
        "createDate": "2025-03-13 14:29:04",
        "desc": {
            "cn": "VisualSimpleQA 是一个多模态事实寻求基准，具有两个关键特性。首先，它使视觉和语言模态中 LVLMs 的评估更加简化和解耦。其次，它纳入了明确的难度标准，以指导人工标注并促进提取具有挑战性的子集，即 VisualSimpleQA-hard。",
            "en": "VisualSimpleQA is a multimodal fact-seeking benchmark with two key features. "
        }
    },
    {
        "id": "1712",
        "name": "ToolHop",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            },
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://huggingface.co/datasets/bytedance-research/ToolHop",
        "paperLink": "https://arxiv.org/abs/2501.02506",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "62407087",
            "name": null,
            "avatar": null,
            "nickname": "尼摩"
        },
        "lookNum": "50",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-07 15:15:34",
        "supportOnlineEval": false,
        "updateDate": "2025-04-07 15:15:34",
        "createDate": "2025-04-05 23:37:16",
        "desc": {
            "cn": "ToolHop是一个通过查询驱动构建的数据集，专门用于评测大模型的多跳工具使用能力，具备多样化的查询、有意义的相互依赖关系、本地可执行的工具、详细的反馈和可验证的答案五大特征。",
            "en": "ToolHop is a dataset specifically designed for rigorous evaluation of multi-hop tool use, which ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach."
        }
    },
    {
        "id": "1604",
        "name": "EgoNormia",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Open-Social-World/EgoNormia",
        "paperLink": "https://arxiv.org/abs/2502.20490",
        "officialWebsiteLink": "https://opensocial.world/leaderboard",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "49",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:45:43",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:45:43",
        "createDate": "2025-03-06 13:51:13",
        "desc": {
            "cn": "EgoNormia 是一个具有挑战性的问答基准，用于测试 VLMs 在上下文中推理规范的能力。该数据集包含来自 Ego4D 的 1,853 个物理基础化的以自我为中心的交互剪辑，以及每个剪辑对应的五选一多项选择题任务。",
            "en": "EgoNormia is a challenging QA benchmark that tests VLMs' ability to reason over norms in context. The datset consists of 1,853 physically grounded egocentric interaction clips from Ego4D and corresponding five-way multiple-choice questions tasks for each."
        }
    },
    {
        "id": "1395",
        "name": "SEED-Bench-2-Plus",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "text-rich image",
                "en": "text-rich image"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/AILab-CVC/SEED-Bench",
        "paperLink": "https://arxiv.org/pdf/2404.16790",
        "officialWebsiteLink": "https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2-plus",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "40068663",
            "name": null,
            "avatar": null,
            "nickname": "豪༙྇"
        },
        "lookNum": "49",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-01-15 18:33:41",
        "supportOnlineEval": false,
        "updateDate": "2025-01-15 18:33:41",
        "createDate": "2025-01-15 17:20:44",
        "desc": {
            "cn": "SEED-Bench-2-Plus, a benchmark specifically designed for evaluating text-rich visual comprehension of MLLMs. The benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs.",
            "en": "SEED-Bench-2-Plus, a benchmark specifically designed for evaluating text-rich visual comprehension of MLLMs. The benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs."
        }
    },
    {
        "id": "1502",
        "name": "MTVQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "VQA",
                "en": "VQA"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/bytedance/MTVQA",
        "paperLink": "https://arxiv.org/abs/2405.11985",
        "officialWebsiteLink": "https://bytedance.github.io/MTVQA/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "49",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:50:52",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:50:52",
        "createDate": "2025-02-14 20:41:37",
        "desc": {
            "cn": "MTVQA用于评估多模态大模型理解多语言文本的能力，包含来自9种语言的由人类专家注释的高质量数据。",
            "en": "MTVQA is designed to evaluate LMMs' multilingual text understanding, featuring high-quality human expert annotations across 9 diverse languages. "
        }
    },
    {
        "id": "1533",
        "name": "NutritionQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Multimodal",
                "en": "Multimodal"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/allenai/pixmo-docs",
        "paperLink": "https://arxiv.org/abs/2502.14846",
        "officialWebsiteLink": "https://yueyang1996.github.io/cosyn/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "49",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 17:00:55",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 17:00:55",
        "createDate": "2025-02-24 13:51:34",
        "desc": {
            "cn": "通过代码引导的合成多模态数据生成扩展文本丰富图像理解，CoSyn-400K 数据集包含 9 类合成文本丰富图像，以及 270 万条指令微调数据。",
            "en": "CoSyn-400K dataset contains 9 categories of synthetc text-rich images with 2.7M instruction-tuning data"
        }
    },
    {
        "id": "1555",
        "name": "WildBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/allenai/WildBench",
        "paperLink": "https://arxiv.org/abs/2406.04770",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "48",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:49:39",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:49:39",
        "createDate": "2025-02-26 17:05:44",
        "desc": {
            "cn": "WildBench推出自动评估框架和数据集，基于真实用户难题评测大语言模型，包含从逾百万人机对话日志中精选的1,024个任务样本。",
            "en": "Weintroduce WildBench, an automated evaluation framework designed to bench-mark large language models (LLMs) using challenging, real-world user queries, which consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs."
        }
    },
    {
        "id": "1632",
        "name": "ProBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Yan98/ProBench_eval",
        "paperLink": "https://arxiv.org/abs/2503.06885",
        "officialWebsiteLink": "https://yan98.github.io/ProBench/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "48",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-13 15:31:13",
        "supportOnlineEval": false,
        "updateDate": "2025-03-13 15:31:13",
        "createDate": "2025-03-13 14:00:05",
        "desc": {
            "cn": "ProBench是一个包含需要大量专家级知识来解决的开放式多模态查询的基准。ProBench 包含 10 个任务领域和 56 个子领域，支持 17 种语言，并支持最多 13 轮对话。",
            "en": "ProBench is a benchmark that contains open-ended multimodal queries that require intensive expert-level knowledge to solve. "
        }
    },
    {
        "id": "1512",
        "name": "MLVU",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/JUNJIE99/MLVU",
        "paperLink": "https://arxiv.org/abs/2406.04264",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "47",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:51:32",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:51:32",
        "createDate": "2025-02-17 16:07:19",
        "desc": {
            "cn": "MLVU用于评估多模态大模型的长视频理解能力，包含面向各种类型长视频的多样化的评估任务。",
            "en": "MLVU test MLLMs' understanding of multi-task long videos, encompassing diverse tasks based on various long videos. "
        }
    },
    {
        "id": "1640",
        "name": "EMMA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/hychaochao/EMMA",
        "paperLink": "https://www.arxiv.org/abs/2501.05444",
        "officialWebsiteLink": "https://emma-benchmark.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "46",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-17 10:46:29",
        "supportOnlineEval": false,
        "updateDate": "2025-03-17 10:46:29",
        "createDate": "2025-03-17 10:44:03",
        "desc": {
            "cn": "EMMA 由 2,788 个问题组成，其中 1,796 个是新构建的，涵盖四个领域。在每个主题中，我们根据所测量的具体技能为每个问题提供细粒度标签。",
            "en": "EMMA is composed of 2,788 problems, of which 1,796 are newly constructed, across four domains. Within each subject, we further provide fine-grained labels for each question based on the specific skills it measures."
        }
    },
    {
        "id": "1621",
        "name": "ProcessBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/QwenLM/ProcessBench",
        "paperLink": "https://arxiv.org/abs/2412.06559",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "45",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-12 11:08:00",
        "supportOnlineEval": false,
        "updateDate": "2025-03-12 11:08:00",
        "createDate": "2025-03-11 15:28:49",
        "desc": {
            "cn": "ProcessBench，用于衡量识别数学推理中错误步骤的能力。它包含 3,400 个测试案例，主要关注竞赛和奥林匹克级别的数学问题。",
            "en": "ProcessBench can measure the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. "
        }
    },
    {
        "id": "1656",
        "name": "RFUAV",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/kitoweeknd/RFUAV/",
        "paperLink": "https://arxiv.org/pdf/2503.09033",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "45",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-20 14:27:02",
        "supportOnlineEval": false,
        "updateDate": "2025-03-20 14:27:02",
        "createDate": "2025-03-20 14:05:07",
        "desc": {
            "cn": "RFUAV 提供了一个基于射频（RF）的无人机检测和识别的全面基准数据集。",
            "en": "RFUAV offers a comprehensive benchmark dataset for Radio-Frequency (RF)-based drone detection and identification."
        }
    },
    {
        "id": "1552",
        "name": "CEB",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/SongW-SW/CEB",
        "paperLink": "https://arxiv.org/abs/2407.02408",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "44",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-06 16:49:21",
        "supportOnlineEval": false,
        "updateDate": "2025-03-06 16:49:21",
        "createDate": "2025-02-26 14:20:34",
        "desc": {
            "cn": "CEB是一个用于大型语言模型偏差的组成评估基准，引入了包含 11,004 个样本的组成评估基准，从偏差类型、社会群体和任务三个维度描述每个数据集。",
            "en": "CEB evaluates LLM bias compositionally, featuring 11k samples characterized across bias types, social groups, and tasks."
        }
    },
    {
        "id": "1670",
        "name": "IndicMMLU-Pro",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2501.15747",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "43",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-24 18:48:16",
        "supportOnlineEval": false,
        "updateDate": "2025-03-24 18:48:16",
        "createDate": "2025-03-24 18:46:36",
        "desc": {
            "cn": "IndicMMLU-Pro 提供了一个标准化的评估框架，以推动印度语系语言 AI 的研究边界，促进更准确、高效和具有文化敏感性的模型的发展。",
            "en": "IndicMMLU-Pro provides a standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models."
        }
    },
    {
        "id": "1679",
        "name": "ContextualJudgeBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/SalesforceAIResearch/ContextualJudgeBench",
        "paperLink": "https://arxiv.org/abs/2503.15620",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "43",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-26 15:56:25",
        "supportOnlineEval": false,
        "updateDate": "2025-03-26 15:56:25",
        "createDate": "2025-03-26 14:40:00",
        "desc": {
            "cn": "ContextualJudgeBench 是一个包含 2,000 个样本的成对基准，用于评估在两个上下文环境下的LLM-as-judge 模型：上下文问答和摘要。我们提出一个成对评估层次结构，并为我们的层次结构生成分割。",
            "en": "ContextualJudgeBench is a pairwise benchmark with 2,000 samples for evaluating LLM-as-judge models in two contextual settings: Contextual QA and summarization. We propose a pairwise evaluation hierarchy and generate splits for our proposed hierarchy."
        }
    },
    {
        "id": "1624",
        "name": "CodeElo",
        "emoji": "",
        "dimensions": [
            {
                "cn": "代码",
                "en": "Code"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/QwenLM/CodeElo",
        "paperLink": "https://arxiv.org/abs/2501.01257",
        "officialWebsiteLink": "https://codeelo-bench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "41",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-12 11:07:44",
        "supportOnlineEval": false,
        "updateDate": "2025-03-12 11:07:44",
        "createDate": "2025-03-11 15:53:47",
        "desc": {
            "cn": "CodeElo，这是一个标准化的竞赛级代码生成基准测试，有效解决了所有这些挑战。CodeElo 基准测试主要基于官方 CodeForces 平台，并尽可能与该平台保持一致。",
            "en": "Researchers introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. "
        }
    },
    {
        "id": "1730",
        "name": "PaperBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            },
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/openai/preparedness",
        "paperLink": "https://arxiv.org/abs/2504.01848",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "41",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-11 17:12:27",
        "supportOnlineEval": false,
        "updateDate": "2025-04-11 17:12:27",
        "createDate": "2025-04-11 14:10:05",
        "desc": {
            "cn": "PaperBench，这是一个评估AI代理复制最新AI研究能力的基准测试。",
            "en": "PaperBench is a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research."
        }
    },
    {
        "id": "1622",
        "name": "CORAL",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "RAG",
                "en": "RAG"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Ariya12138/CORAL",
        "paperLink": "https://arxiv.org/abs/2410.23090",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "36",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-12 11:07:55",
        "supportOnlineEval": false,
        "updateDate": "2025-03-12 11:07:55",
        "createDate": "2025-03-11 15:39:20",
        "desc": {
            "cn": "CORAL 是一个大规模对话 RAG 基准，包含一个统一框架，用于标准化和评估各种对话 RAG 基线。",
            "en": "Researchers present a large-scale conversational RAG benchmark named CORAL and propose a unified framework for standardizing and evaluating various conversational RAG baselines."
        }
    },
    {
        "id": "1623",
        "name": "MJ-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/MJ-Bench/MJ-Bench",
        "paperLink": "https://arxiv.org/abs/2407.04842",
        "officialWebsiteLink": "https://mj-bench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "35",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-12 11:07:50",
        "supportOnlineEval": false,
        "updateDate": "2025-03-12 11:07:50",
        "createDate": "2025-03-11 15:46:38",
        "desc": {
            "cn": "MJ-Bench，它包含了一个综合的偏好数据集，用于从四个关键角度评估多模态评委在为图像生成模型提供反馈方面的能力：对齐、安全性、图像质量和偏见。",
            "en": "MJ-Bench incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. "
        }
    },
    {
        "id": "1668",
        "name": "TimeTravel",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/mbzuai-oryx/TimeTravel",
        "paperLink": "https://arxiv.org/abs/2502.14865",
        "officialWebsiteLink": "https://mbzuai-oryx.github.io/TimeTravel/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "35",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-24 18:48:21",
        "supportOnlineEval": false,
        "updateDate": "2025-03-24 18:48:21",
        "createDate": "2025-03-24 17:05:23",
        "desc": {
            "cn": "时间旅行分类将来自 10 个文明、266 个文化以及 10k+个验证样本的文物映射，用于 AI 驱动的历史分析。",
            "en": "TimeTravel Taxonomy maps artifacts from 10 civilizations, 266 cultures, and 10k+ verified samples for AI-driven historical analysis."
        }
    },
    {
        "id": "1681",
        "name": "PRMBench_Preview",
        "emoji": "",
        "dimensions": [
            {
                "cn": "学科",
                "en": "Examination"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/ssmisya/PRMBench",
        "paperLink": "https://arxiv.org/abs/2501.03124",
        "officialWebsiteLink": "https://prmbench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "35",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-26 15:56:18",
        "supportOnlineEval": false,
        "updateDate": "2025-03-26 15:56:18",
        "createDate": "2025-03-26 15:07:01",
        "desc": {
            "cn": "PRMBench 是一个用于评估过程级奖励模型（PRM）的基准数据集。它包含 6,216 个数据实例，每个实例包含一个问题、一个解决方案过程以及一个包含错误的修改过程。",
            "en": "PRMBench is a benchmark dataset for evaluating process-level reward models (PRMs). It consists of 6,216 data instances, each containing a question, a solution process, and a modified process with errors."
        }
    },
    {
        "id": "1751",
        "name": "MultiLoKo",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/facebookresearch/multiloko/",
        "paperLink": "https://arxiv.org/abs/2504.10356",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "35",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-21 12:34:33",
        "supportOnlineEval": false,
        "updateDate": "2025-04-21 12:34:33",
        "createDate": "2025-04-21 11:55:37",
        "desc": {
            "cn": "MultiLoKo是一个多语言知识基准，涵盖30种语言及英语。",
            "en": "MultiLoKo is a multilingual knowledge benchmark, covering 30 languages plus English."
        }
    },
    {
        "id": "1678",
        "name": "RSMMVP",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2503.15816",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "33",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-26 15:56:27",
        "supportOnlineEval": false,
        "updateDate": "2025-03-26 15:56:27",
        "createDate": "2025-03-26 14:35:19",
        "desc": {
            "cn": "RSMMVP遵循与原始 MMVP 基准在自然图像上的类似流程，但针对遥感领域。根据 CLIP 盲对识别具有挑战性的视觉模式，并附带相应的问题、选项和真实答案。",
            "en": "This dataset follows a similar procedure to the original MMVP benchmark on natural images but directed towards the remote sensing domain. Challenging visual patterns are identified based on CLIP blind pairs, accompanied with the correpsonding questions, options and ground-truth answer."
        }
    },
    {
        "id": "1744",
        "name": "StyleRec",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            },
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "NLP",
                "en": "NLP"
            },
            {
                "cn": "Writing Style Transformation",
                "en": "Writing Style Transformation"
            },
            {
                "cn": "Prompt Recovery",
                "en": "Prompt Recovery"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/promptrecovery501/StyleRec",
        "paperLink": "https://arxiv.org/abs/2504.04373",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "18903058",
            "name": "qianertongre",
            "avatar": null,
            "nickname": "qianertongre"
        },
        "lookNum": "33",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-16 11:32:28",
        "supportOnlineEval": false,
        "updateDate": "2025-04-16 11:32:28",
        "createDate": "2025-04-15 13:45:08",
        "desc": {
            "cn": "基于写作风格转换的提示词恢复的评测集",
            "en": "A Benchmark Dataset for Prompt Recovery in Writing Style Transformation."
        }
    },
    {
        "id": "1667",
        "name": "MicroVQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/jmhb0/microvqa",
        "paperLink": "https://arxiv.org/abs/2503.13399",
        "officialWebsiteLink": "https://jmhb0.github.io/microvqa/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "32",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-24 18:48:38",
        "supportOnlineEval": false,
        "updateDate": "2025-03-24 18:48:38",
        "createDate": "2025-03-24 16:37:21",
        "desc": {
            "cn": "MicroVQA，一个评估关于显微镜图像的多选题推理基准，由专家生物学家创建，旨在反映生物研究中能够有意义地协助的任务，每个问题都需要多模态推理。",
            "en": "MicroVQA is a benchmark that evaluates LLM reasoning on multiple-choice questions about microscopy images, created by expert biologists."
        }
    },
    {
        "id": "1626",
        "name": "CRAG",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "RAG",
                "en": "RAG"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/facebookresearch/CRAG",
        "paperLink": "https://arxiv.org/abs/2406.04744",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "31",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-12 11:07:30",
        "supportOnlineEval": false,
        "updateDate": "2025-03-12 11:07:30",
        "createDate": "2025-03-11 16:16:34",
        "desc": {
            "cn": "CRAG是一个丰富且全面的基于事实的问题回答基准，旨在推进 RAG 研究。除了问答对之外，CRAG 还提供了模拟网页和知识图谱搜索的模拟 API。",
            "en": "The Comprehensive RAG Benchmark (CRAG) is a rich and comprehensive factual question answering benchmark designed to advance research in RAG. Besides question-answer pairs, CRAG provides mock APIs to simulate web and knowledge graph search. "
        }
    },
    {
        "id": "1738",
        "name": "RUListening",
        "emoji": "",
        "dimensions": [
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Multimodal",
                "en": "Multimodal"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2504.00369",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "31",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-11 17:15:12",
        "supportOnlineEval": false,
        "updateDate": "2025-04-11 17:15:12",
        "createDate": "2025-04-11 16:07:18",
        "desc": {
            "cn": "RUListening：通过聆听的稳健理解，这是一个用于评估多模态感知的自动问答生成框架。",
            "en": "RUListening: Robust Understanding through Listening, an automated QA generation framework for evaluating multimodal perception."
        }
    },
    {
        "id": "1675",
        "name": "PokerBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/pokerllm/pokerbench",
        "paperLink": "https://arxiv.org/abs/2501.08328",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "30",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-25 10:42:49",
        "supportOnlineEval": false,
        "updateDate": "2025-03-25 10:42:49",
        "createDate": "2025-03-25 10:41:14",
        "desc": {
            "cn": "PokerBench包含自然语言游戏场景和由求解器在无限制德州扑克中计算出的最优决策。它分为前注和后注数据集，每个数据集都包含训练集和测试集。",
            "en": "PokerBench contains natural language game scenarios and optimal decisions computed by solvers in No Limit Texas Hold’em. It is divided into pre-flop and post-flop datasets, each with training and test splits. "
        }
    },
    {
        "id": "1735",
        "name": "WorldScore",
        "emoji": "",
        "dimensions": [
            {
                "cn": "创作",
                "en": "Creation"
            },
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "视频生成",
                "en": "视频生成"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/haoyi-duan/WorldScore",
        "paperLink": "https://arxiv.org/abs/2504.00983",
        "officialWebsiteLink": "https://haoyi-duan.github.io/WorldScore/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "30",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-11 17:14:14",
        "supportOnlineEval": false,
        "updateDate": "2025-04-11 17:14:14",
        "createDate": "2025-04-11 15:23:05",
        "desc": {
            "cn": "WorldScore基准测试，这是首个用于世界生成的统一基准测试。",
            "en": "WorldScore benchmark is the first unified benchmark for world generation."
        }
    },
    {
        "id": "1778",
        "name": "AgentRewardBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            },
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://agent-reward-bench.github.io/",
        "paperLink": "https://arxiv.org/abs/2504.08942",
        "officialWebsiteLink": "https://agent-reward-bench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "30",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-25 17:00:44",
        "supportOnlineEval": false,
        "updateDate": "2025-04-25 17:00:44",
        "createDate": "2025-04-25 16:26:45",
        "desc": {
            "cn": "AgentRewardBench 是首个用于评估大型语言模型（LLM）评判者评估网络代理有效性的基准测试。AgentRewardBench 包含来自 5 个基准测试和 4 个大型语言模型的 1302 条轨迹。",
            "en": "AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. "
        }
    },
    {
        "id": "1704",
        "name": "Mono2Stereo",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Stereo Conversion",
                "en": "Stereo Conversion"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/song2yu/Mono2Stereo",
        "paperLink": "https://arxiv.org/abs/2503.22262",
        "officialWebsiteLink": "https://mono2stereo-bench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "52300297",
            "name": null,
            "avatar": null,
            "nickname": "迷藏"
        },
        "lookNum": "28",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-07 10:30:36",
        "supportOnlineEval": false,
        "updateDate": "2025-04-07 10:30:36",
        "createDate": "2025-04-02 17:37:32",
        "desc": {
            "cn": "用于测评立体影像转换，提供了动画，室内，室外，复杂，简单共五种场景的测试数据，总共约2500对测试样本。并提供了用于测评立体效果的评价指标-立体交并比。",
            "en": "For evaluating stereo image conversion, it provides test data for five different scenarios: animation, indoor, outdoor, complex, and simple, with a total of approximately 2,500 test sample pairs. It also offers evaluation metrics for assessing the stereo effect."
        }
    },
    {
        "id": "1740",
        "name": "GPT-ImgEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/PicoTrex/GPT-ImgEval",
        "paperLink": "https://arxiv.org/abs/2504.02782",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "28",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-11 17:16:12",
        "supportOnlineEval": false,
        "updateDate": "2025-04-11 17:16:12",
        "createDate": "2025-04-11 16:58:50",
        "desc": {
            "cn": "GPT-ImgEval，从三个关键维度对GPT-4o的性能进行定量和定性诊断：（1）生成质量，（2）编辑能力，以及（3）基于世界知识的语义合成能力。",
            "en": "GPT-ImgEval, quantitatively and qualitatively diagnoses GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis."
        }
    },
    {
        "id": "1733",
        "name": "FEABench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "FEA基准测试",
                "en": "FEA基准测试"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/google/feabench/tree/main",
        "paperLink": "https://arxiv.org/abs/2504.06260",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "27",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-11 17:13:05",
        "supportOnlineEval": false,
        "updateDate": "2025-04-11 17:13:05",
        "createDate": "2025-04-11 14:42:38",
        "desc": {
            "cn": "FEABench，一个用于评估大型语言模型和LLM代理使用有限元分析（FEA）模拟和解决物理、数学及工程问题能力的基准测试。",
            "en": "FEABench is a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). "
        }
    },
    {
        "id": "1737",
        "name": "FortisAVQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "AVQA",
                "en": "AVQA"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/reml-group/fortisavqa",
        "paperLink": "https://arxiv.org/abs/2504.00487",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "27",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-11 17:14:21",
        "supportOnlineEval": false,
        "updateDate": "2025-04-11 17:14:21",
        "createDate": "2025-04-11 15:55:15",
        "desc": {
            "cn": "FortisAVQA，这是首个用于评估AVQA模型鲁棒性的数据集。",
            "en": " FortisAVQA is the first dataset designed to assess the robustness of AVQA models. "
        }
    },
    {
        "id": "1739",
        "name": "CrossWordBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Multimodal",
                "en": "Multimodal"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/SeanLeng1/CrossWordBench",
        "paperLink": "https://arxiv.org/abs/2504.00043",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "27",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-11 17:15:53",
        "supportOnlineEval": false,
        "updateDate": "2025-04-11 17:15:53",
        "createDate": "2025-04-11 16:14:37",
        "desc": {
            "cn": "CrossWordBench，这是一个基准测试，旨在通过填字游戏的方式来评估LLMs和LVLMs的推理能力。",
            "en": "CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles."
        }
    },
    {
        "id": "1682",
        "name": "MotionBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/THUDM/MotionBench",
        "paperLink": "https://arxiv.org/abs/2501.02955",
        "officialWebsiteLink": "https://motion-bench.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "26",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-03-26 15:56:14",
        "supportOnlineEval": false,
        "updateDate": "2025-03-26 15:56:14",
        "createDate": "2025-03-26 15:35:43",
        "desc": {
            "cn": "MotionBench是一个综合性的评估基准，旨在评估视频理解模型的细粒度运动理解能力。",
            "en": "MotionBench, is a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models."
        }
    },
    {
        "id": "1781",
        "name": "MIEB",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Multimodal",
                "en": "Multimodal"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/embeddings-benchmark/mteb",
        "paperLink": "https://arxiv.org/abs/2504.10471",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "26",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-25 17:00:52",
        "supportOnlineEval": false,
        "updateDate": "2025-04-25 17:00:52",
        "createDate": "2025-04-25 16:52:48",
        "desc": {
            "cn": "MIEB用于评估图像和图像文本嵌入模型在迄今为止最广泛的范围内的性能。",
            "en": "Massive Image Embedding Benchmark (MIEB) is to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. "
        }
    },
    {
        "id": "1706",
        "name": "KOFFVQA",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "理解",
                "en": "Understanding"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "VQA",
                "en": "VQA"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/maum-ai/KOFFVQA",
        "paperLink": "https://arxiv.org/abs/2503.23730",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "25",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-03 19:47:01",
        "supportOnlineEval": false,
        "updateDate": "2025-04-03 19:47:01",
        "createDate": "2025-04-03 14:31:30",
        "desc": {
            "cn": "KOFFVQA是一个精心设计的韩语自由形式视觉问答（VQA）基准测试，包含10个不同任务中的275个问题。",
            "en": "KOFFVQA is a carefully crafted free-form visual question answering(VQA) benchmark in the Korean language consisting of 275 questions across 10 different tasks. "
        }
    },
    {
        "id": "1707",
        "name": "RXRX3-CORE",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Xavi3398/fer_benchmark",
        "paperLink": "https://arxiv.org/abs/2503.20428",
        "officialWebsiteLink": "https://www.rxrx.ai/rxrx3-core",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "25",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-03 19:47:14",
        "supportOnlineEval": false,
        "updateDate": "2025-04-03 19:47:14",
        "createDate": "2025-04-03 15:13:31",
        "desc": {
            "cn": "RxRx3-core数据集是Recursion为研究社区优化的表型组学挑战数据集。",
            "en": "RxRx3-core dataset is a challenge dataset in phenomics optimized for the research\ncommunity. "
        }
    },
    {
        "id": "1732",
        "name": "SCAM",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Multimodal",
                "en": "Multimodal"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Bliss-e-V/SCAM",
        "paperLink": "https://arxiv.org/abs/2504.04893",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "25",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-11 17:12:53",
        "supportOnlineEval": false,
        "updateDate": "2025-04-11 17:12:53",
        "createDate": "2025-04-11 14:29:41",
        "desc": {
            "cn": "SCAM，是迄今为止规模最大、多样性最丰富的真实世界排版攻击图像数据集，包含数百个对象类别和攻击词汇的1,162张图像。",
            "en": "SCAM is the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words."
        }
    },
    {
        "id": "1755",
        "name": "REAL",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/agi-inc/agisdk",
        "paperLink": "https://arxiv.org/abs/2504.11543",
        "officialWebsiteLink": "https://www.realevals.xyz/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "25",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-21 12:34:45",
        "supportOnlineEval": false,
        "updateDate": "2025-04-21 12:34:45",
        "createDate": "2025-04-21 12:31:05",
        "desc": {
            "cn": "在真实网站的确定性模拟上对自主代理进行基准测试",
            "en": "Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites"
        }
    },
    {
        "id": "1742",
        "name": "U-NIAH",
        "emoji": "",
        "dimensions": [
            {
                "cn": "长文本",
                "en": "Long-Context"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "RAG",
                "en": "RAG"
            },
            {
                "cn": "Long-Context",
                "en": "Long-Context"
            },
            {
                "cn": "Needle In A Haystack",
                "en": "Needle In A Haystack"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Tongji-KGLLM/U-NIAH",
        "paperLink": "https://arxiv.org/abs/2503.00353",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "85508164",
            "name": "yunfan",
            "avatar": null,
            "nickname": "yunfan"
        },
        "lookNum": "24",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-16 11:32:40",
        "supportOnlineEval": false,
        "updateDate": "2025-04-16 11:32:40",
        "createDate": "2025-04-13 10:56:15",
        "desc": {
            "cn": "U-NIAH是将 RAG 和 LLM 统一映射在大海捞针任务中的框架。所有任务基于一个虚构背景下的数据集Starlight Academy，涵盖了魔法系统、学术课程、校园生活、等多个方面，旨在消除预训练知识的干扰，从而能够独立于 LLMs 的先验知识。框架包含多种评估场景，支持多针（3、7、15个针）和长针（400-500 token）配置，还引入了“针中针”结构，进一步增加了复杂性。该数据集通过多样化的场景和合成生成的内容，能够从多个维度分析模型在长文本场景下的性能。同时通过模块化设计，U-NIAH可持续注入新的挑战场景。",
            "en": "U-NIAH is a framework unifying RAG and LLM for needle-in-a-haystack tasks, based on the fictional Starlight Academy dataset. It eliminates interference from pre-trained knowledge and supports diverse, complex scenarios (e.g., multi-needle, long-needle, \"needle-in-needle\")."
        }
    },
    {
        "id": "1753",
        "name": "AgMMU",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Multimodal",
                "en": "Multimodal"
            },
            {
                "cn": "农业",
                "en": "农业"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/AgMMU/AgMMU",
        "paperLink": "https://arxiv.org/abs/2504.10568",
        "officialWebsiteLink": "https://agmmu.github.io/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "22",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-21 12:34:40",
        "supportOnlineEval": false,
        "updateDate": "2025-04-21 12:34:40",
        "createDate": "2025-04-21 12:12:24",
        "desc": {
            "cn": "农业综合多模态理解和推理基准。",
            "en": "A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark"
        }
    },
    {
        "id": "1777",
        "name": "ColorBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Multimodal",
                "en": "Multimodal"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/tianyi-lab/ColorBench",
        "paperLink": "https://arxiv.org/abs/2504.10514",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "20",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-25 17:00:41",
        "supportOnlineEval": false,
        "updateDate": "2025-04-25 17:00:41",
        "createDate": "2025-04-25 16:14:54",
        "desc": {
            "cn": "ColorBench，这是一个创新且精心设计的基准测试，旨在评估VLMs在颜色理解方面的能力，包括颜色感知、推理和鲁棒性。",
            "en": "ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. "
        }
    },
    {
        "id": "1780",
        "name": "C-FAITH",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/pkulcwmzx/C-FAITH",
        "paperLink": "https://arxiv.org/abs/2504.10167",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "20",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-25 17:00:49",
        "supportOnlineEval": false,
        "updateDate": "2025-04-25 17:00:49",
        "createDate": "2025-04-25 16:42:33",
        "desc": {
            "cn": " C-FAITH，这是一个中国 QA 幻觉基准，由从网络抓取中获得的 1,399 份知识文档创建，总共 60,702 个条目。",
            "en": "C-FAITH, a Chinese QA hallucination benchmark created from 1,399 knowledge documents obtained from web scraping, totaling 60,702 entries."
        }
    },
    {
        "id": "1731",
        "name": "DOVE",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "LLM敏感性评估",
                "en": "LLM敏感性评估"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/SLAB-NLP/DOVE",
        "paperLink": "https://arxiv.org/abs/2503.01622",
        "officialWebsiteLink": "https://slab-nlp.github.io/DOVE/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "19",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-11 17:12:41",
        "supportOnlineEval": false,
        "updateDate": "2025-04-11 17:12:41",
        "createDate": "2025-04-11 14:22:36",
        "desc": {
            "cn": "DOVE（变异评估数据集），这是一个大规模数据集，包含了各种评估基准的提示扰动。",
            "en": "DOVE (Dataset Of Variation Evaluation) is a large-scale dataset containing prompt perturbations of various evaluation benchmarks."
        }
    },
    {
        "id": "1752",
        "name": "LLM-SRBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Strong Reasoning",
                "en": "Strong Reasoning"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/deep-symbolic-mathematics/llm-srbench",
        "paperLink": "https://arxiv.org/abs/2504.10415",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "19",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-21 12:34:37",
        "supportOnlineEval": false,
        "updateDate": "2025-04-21 12:34:37",
        "createDate": "2025-04-21 12:02:24",
        "desc": {
            "cn": "LLM-SRBench，这是一个包含239个挑战性问题的综合基准测试，涵盖四个科学领域，专门设计用于评估基于LLMs的科学方程式发现方法，同时防止简单记忆。",
            "en": "LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization."
        }
    },
    {
        "id": "1734",
        "name": "Thai_local_benchmark",
        "emoji": "",
        "dimensions": [
            {
                "cn": "语言",
                "en": "Language"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/mrpeerat/Thai_local_benchmark",
        "paperLink": "https://arxiv.org/abs/2504.05898",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "18",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-11 17:13:26",
        "supportOnlineEval": false,
        "updateDate": "2025-04-11 17:13:26",
        "createDate": "2025-04-11 14:57:01",
        "desc": {
            "cn": "这是一个涵盖泰国北部（兰纳）、东北部（伊森）和南部（丹布罗）方言的泰国地方方言基准测试，评估大型语言模型在五项自然语言处理任务上的表现：总结、问答、翻译、对话以及与食物相关的任务。",
            "en": "Thai local dialect benchmark covers Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai, evaluating LLMs on five NLP tasks: summarization, question answering, translation, conversation, and food-related tasks."
        }
    },
    {
        "id": "1801",
        "name": "OmniGIRL",
        "emoji": "",
        "dimensions": [
            {
                "cn": "多模态",
                "en": "Multimodal"
            },
            {
                "cn": "代码",
                "en": "Code"
            },
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Github Issue Resolution",
                "en": "Github Issue Resolution"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/DeepSoftwareAnalytics/OmniGIRL",
        "paperLink": "https://arxiv.org/abs/2505.04606",
        "officialWebsiteLink": "https://deepsoftwareanalytics.github.io/omnigirl_leaderboard.html",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "26208047",
            "name": null,
            "avatar": null,
            "nickname": "gnohgnailoug"
        },
        "lookNum": "17",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-05-09 21:26:49",
        "supportOnlineEval": false,
        "updateDate": "2025-05-09 21:26:49",
        "createDate": "2025-05-09 20:52:15",
        "desc": {
            "cn": "一个面向 GitHub Issue ResoLution任务的多语言、多模态基准数据集，包含以下特点: 1.支持 Python、Java、JS、TS 四种主流编程语言，2. 输入信息涵盖文本、图像、网页等多种模态，3. 提供可复现的 Docker 评估环境。",
            "en": "A multi-modal, multi-language benchmark dataset for the GitHub Issue Resolution task."
        }
    },
    {
        "id": "1754",
        "name": "OpenTuringBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "",
        "paperLink": "https://arxiv.org/abs/2504.11369",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "16",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-21 12:34:42",
        "supportOnlineEval": false,
        "updateDate": "2025-04-21 12:34:42",
        "createDate": "2025-04-21 12:26:14",
        "desc": {
            "cn": "OpenTuringBench，这是一个基于OLLMs的新基准测试，旨在训练和评估机器生成文本检测器在图灵测试和作者归属问题上的性能。",
            "en": "OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate machine-generated text detectors on the Turing Test and Authorship Attribution problems. "
        }
    },
    {
        "id": "1779",
        "name": "MLRC-Bench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "智能体",
                "en": "Agent"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/yunx-z/MLRC-Bench",
        "paperLink": "https://arxiv.org/abs/2504.09702",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "15",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-25 17:00:47",
        "supportOnlineEval": false,
        "updateDate": "2025-04-25 17:00:47",
        "createDate": "2025-04-25 16:34:27",
        "desc": {
            "cn": " MLRC-Bench旨在量化大模型代理如何有效地应对具有挑战性的机器学习 （ML） 研究竞赛。",
            "en": "MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. "
        }
    },
    {
        "id": "1784",
        "name": "xVerify",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/IAAR-Shanghai/xVerify",
        "paperLink": "https://arxiv.org/abs/2504.10481",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "12",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-28 11:36:52",
        "supportOnlineEval": false,
        "updateDate": "2025-04-28 11:36:52",
        "createDate": "2025-04-28 11:24:15",
        "desc": {
            "cn": "xVerify，这是一种用于推理模型评估的高效答案验证器。xVerify 在等价判断方面表现出强大的能力，使其能够有效地确定推理模型生成的答案是否等同于各种类型客观问题的参考答案。",
            "en": "xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions"
        }
    },
    {
        "id": "1786",
        "name": "HypoEval",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/ChicagoHAI/HypoEval",
        "paperLink": "https://arxiv.org/abs/2504.07174",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "12",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-28 11:36:49",
        "supportOnlineEval": false,
        "updateDate": "2025-04-28 11:36:49",
        "createDate": "2025-04-28 11:34:44",
        "desc": {
            "cn": "HypoEval，即假设指导的评估框架，该框架首先使用一小部分人工评估来生成更详细的人类判断量规，然后采用类似清单的方法，将 LLM 在每个分解维度上的分配分数结合起来，以获得总分。",
            "en": "HypoEval, Hypothesis-guided Evaluation framework, which first uses a small corpus of human evaluations to generate more detailed rubrics for human judgments and then incorporates a checklist-like approach to combine LLM's assigned scores on each decomposed dimension to acquire overall scores. "
        }
    },
    {
        "id": "1787",
        "name": "LiveLongBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [
            {
                "cn": "Spoken text",
                "en": "Spoken text"
            },
            {
                "cn": "Long context",
                "en": "Long context"
            },
            {
                "cn": "Live streams",
                "en": "Live streams"
            }
        ],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/Yarayx/livelongbench",
        "paperLink": "https://arxiv.org/abs/2504.17366",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "10207810",
            "name": null,
            "avatar": null,
            "nickname": "Yarayx"
        },
        "lookNum": "12",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-05-06 11:02:03",
        "supportOnlineEval": false,
        "updateDate": "2025-05-06 11:02:03",
        "createDate": "2025-04-28 16:54:00",
        "desc": {
            "cn": "LiveLongBench 是首个面向口语长文本理解的基准测试，基于直播内容构建，涵盖检索类、推理类及混合类三种任务类型，针对现实对话中存在的语音特性、高冗余性和信息密度不均等挑战。",
            "en": "LiveLongBench is the first spoken long-text benchmark designed to address the challenges of long-context understanding in real-world dialogues, characterized by speech-specific features, high redundancy, and uneven information density. Existing benchmarks fail to capture these complexities, limiting"
        }
    },
    {
        "id": "1783",
        "name": "NPPC",
        "emoji": "",
        "dimensions": [
            {
                "cn": "推理",
                "en": "Reasoning"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/SMU-DIGA/nppc",
        "paperLink": "https://arxiv.org/abs/2504.11239",
        "officialWebsiteLink": "",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "11",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-28 11:36:56",
        "supportOnlineEval": false,
        "updateDate": "2025-04-28 11:36:56",
        "createDate": "2025-04-28 11:16:50",
        "desc": {
            "cn": "非确定性多项式时间问题挑战 （NPPC），这是一个不断扩展的 LLM 推理基准。",
            "en": "Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. "
        }
    },
    {
        "id": "1782",
        "name": "HypoBench",
        "emoji": "",
        "dimensions": [
            {
                "cn": "其他",
                "en": "Other"
            }
        ],
        "subDimensions": [],
        "tags": [],
        "topicTags": [],
        "benchCertificateLevel": 0,
        "githubLink": "https://github.com/ChicagoHAI/HypoBench",
        "paperLink": "https://arxiv.org/abs/2504.11524",
        "officialWebsiteLink": "https://chicagohai.github.io/HypoBench/",
        "leaderboardLink": false,
        "creatorInfo": {
            "uid": "4006165",
            "name": "opencompass",
            "avatar": "https://static.openxlab.org.cn/sso/avatar/4006165-2d37997a-97cd-49eb-83ac-66a0f6a6c6b7.png",
            "nickname": "OpenCompass 司南"
        },
        "lookNum": "10",
        "top": false,
        "state": 3,
        "publicFlag": 1,
        "reviewBlockError": null,
        "reviewSuccessDate": "2025-04-28 11:36:59",
        "supportOnlineEval": false,
        "updateDate": "2025-04-28 11:36:59",
        "createDate": "2025-04-28 11:08:05",
        "desc": {
            "cn": "HypoBench，这是一种新颖的基准，旨在从多个方面评估 LLM 和假设生成方法，包括实用性、泛化性和假设发现率。HypoBench 包括 7 个真实任务和 5 个合成任务，具有 194 个不同的数据集。",
            "en": "HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate. HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets. "
        }
    }
]